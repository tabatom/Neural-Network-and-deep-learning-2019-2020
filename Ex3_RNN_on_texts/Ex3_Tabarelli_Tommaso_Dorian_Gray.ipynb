{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional Neuroscience\n",
    "Accademic year 2019-2020\n",
    "Homework 3\n",
    "\n",
    "Author: Tommaso Tabarelli\n",
    "Period: december 2019\n",
    "\"\"\"\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import reduce\n",
    "from torch import optim, nn\n",
    "from network import Network, train_batch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "\tdef __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "\t\t# Call the parent init function (required!)\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Define recurrent layer\n",
    "\t\tself.rnn = nn.LSTM(input_size=input_size, \n",
    "\t\t\t\t\t\thidden_size=hidden_units,\n",
    "\t\t\t\t\t\tnum_layers=layers_num,\n",
    "\t\t\t\t\t\tdropout=dropout_prob,\n",
    "\t\t\t\t\t\tbatch_first=True)\n",
    "\t\t# Define output layer\n",
    "\t\tself.out = nn.Linear(hidden_units, input_size)\n",
    "\n",
    "\tdef forward(self, x, state=None):\n",
    "\t\t# LSTM\n",
    "\t\tx, rnn_state = self.rnn(x, state)\n",
    "\t\t# Linear layer\n",
    "\t\tx = self.out(x)\n",
    "\t\treturn x, rnn_state\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_batch(net, batch_onehot, loss_fn, optimizer):\n",
    "\n",
    "\t### Prepare network input and labels\n",
    "\t# Get the labels (the last letter of each sequence)\n",
    "\tlabels_onehot = batch_onehot[:, -1, :]\n",
    "\tlabels_numbers = labels_onehot.argmax(dim=1)\n",
    "\t# Remove the labels from the input tensor\n",
    "\tnet_input = batch_onehot[:, :-1, :]\n",
    "\t# batch_onehot.shape =   [50, 100, 38]\n",
    "\t# labels_onehot.shape =  [50, 38]\n",
    "\t# labels_numbers.shape = [50]\n",
    "\t# net_input.shape =      [50, 99, 38]\n",
    "\n",
    "\t### Forward pass\n",
    "\t# Eventually clear previous recorded gradients\n",
    "\toptimizer.zero_grad()\n",
    "\t# Forward pass\n",
    "\tnet_out, _ = net(net_input)\n",
    "\n",
    "\t### Update network\n",
    "\t# Evaluate loss only for last output\n",
    "\tloss = loss_fn(net_out[:, -1, :], labels_numbers)\n",
    "\t# Backward pass\n",
    "\tloss.backward()\n",
    "\t# Update\n",
    "\toptimizer.step()\n",
    "\t# Return average batch loss\n",
    "\treturn float(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--out_dir'], dest='out_dir', nargs=None, const=None, default='model', type=<class 'str'>, choices=None, help='Where to save models and params', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "##############################\n",
    "## PARAMETERS\n",
    "##############################\n",
    "parser = argparse.ArgumentParser(description='Train the Blake sonnet generator network.')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--datasetpath', type=str, default='Songs_of_innocence.txt',\n",
    "                        help='Path of the train txt file')\n",
    "parser.add_argument('--crop_len',    type=int, default=100,\n",
    "                        help='Number of input letters')\n",
    "#parser.add_argument('--alphabet_len',   type=int,   default=,                help='Number of letters in the alphabet')\n",
    "\n",
    "# Network\n",
    "parser.add_argument('--hidden_units',   type=int,   default=128,    help='Number of RNN hidden units')\n",
    "parser.add_argument('--layers_num',     type=int,   default=2,      help='Number of RNN stacked layers')\n",
    "parser.add_argument('--dropout_prob',   type=float, default=0.3,    help='Dropout probability')\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--batchsize',  type=int, default=154,  help='Training batch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=1000, help='Number of training epochs')\n",
    "\n",
    "# Save\n",
    "parser.add_argument('--out_dir', type=str, default='model', help='Where to save models and params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WildeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, crop_len, transform=None):\n",
    "        \n",
    "        ### Load data\n",
    "        text = open(filepath, 'r').read()\n",
    "\n",
    "        # Removing titles\n",
    "        text = re.split('\\n{7}', text)[1]\n",
    "        \n",
    "        # Lowering all text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Extract the chapters (divided by '\\n{5}')\n",
    "        chap_list = re.split('\\n\\n\\n\\n\\n\\n', text)\n",
    "        \n",
    "        # Remove double new lines\n",
    "        chap_list = list(map(lambda s: re.sub('\\n{2,3}', '\\n', s), chap_list))\n",
    "        # Saving only the chapters which are sufficiently long\n",
    "        chap_list = [x for x in chap_list if len(x) > crop_len + 100]\n",
    "\n",
    "\n",
    "        ### Char to number\n",
    "        alphabet = list(set(text))\t\t# \"set\" function divides the text in single characters (not ordered)\n",
    "        alphabet.sort()\t\t\t\t# sorting not ordered characters\n",
    "        print('Found letters:', alphabet)\n",
    "        # Building dictionaries\n",
    "        char_to_number = {char: number for number, char in enumerate(alphabet)}\n",
    "        number_to_char = {number: char for number, char in enumerate(alphabet)}\n",
    "\n",
    "        ### Store data\n",
    "        self.chap_list = chap_list\n",
    "        self.transform = transform\n",
    "        self.char_to_number = char_to_number\n",
    "        self.number_to_char = number_to_char\n",
    "        # In Wilde there are no \"stange\" chars to encode\n",
    "        self.alphabet = alphabet\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chap_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sonnet text\n",
    "        text = self.chap_list[idx]\n",
    "        # Encode with numbers\n",
    "        encoded = encode_text(self.char_to_number, text)\n",
    "\n",
    "        # Create sample\n",
    "        sample = {'text': text, 'encoded': encoded}\n",
    "        # Transform (if defined)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "def encode_text(char_to_number, text):\n",
    "    i = -1\n",
    "    for c in text:\n",
    "        i+=1\n",
    "        try:\n",
    "            a = char_to_number[c]\n",
    "        except:\t# If the character is not in the char_to_number dictionary, then \n",
    "            s = list(text)\n",
    "            # Encoding the not found characters (if any): \"_\" is not in the text\n",
    "            s[i]='_'\n",
    "            text=''.join(s)\n",
    "    encoded = [char_to_number[c] for c in text]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decode_text(number_to_char, encoded):\n",
    "    text = [number_to_char[c] for c in encoded]\n",
    "    # Building proper string from a list of strings (concatenating them)\n",
    "    text = reduce(lambda s1, s2: s1 + s2, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class RandomCrop():\n",
    "    \n",
    "    def __init__(self, crop_len):\n",
    "        self.crop_len = crop_len\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        text = sample['text']\n",
    "        encoded = sample['encoded']\n",
    "        # Randomly choose an index\n",
    "        tot_chars = len(text)\n",
    "        start_idx = np.random.randint(0, tot_chars - self.crop_len)\n",
    "        end_idx = start_idx + self.crop_len\n",
    "        return {**sample,\n",
    "            'text': text[start_idx: end_idx],\n",
    "            'encoded': encoded[start_idx: end_idx]}\n",
    "        \n",
    "\n",
    "def create_one_hot_matrix(encoded, alphabet_len):\n",
    "    # Create one hot matrix\n",
    "    encoded_onehot = np.zeros([len(encoded), alphabet_len])\n",
    "    tot_chars = len(encoded)\n",
    "    # Placing ONEs at the respective position of the letters: \"encoded\" indeed have numbers that encode the letters,\n",
    "    # \tand here it is used as index dimension\n",
    "    encoded_onehot[np.arange(tot_chars), encoded] = 1\n",
    "    return encoded_onehot\n",
    "\n",
    "\n",
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Load encoded text with numbers\n",
    "        encoded = np.array(sample['encoded'])\n",
    "        # Create one hot matrix\n",
    "        encoded_onehot = create_one_hot_matrix(encoded, self.alphabet_len)\n",
    "        return {**sample,\n",
    "            'encoded_onehot': encoded_onehot}\n",
    "        \n",
    "                \n",
    "class ToTensor():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        # Convert one hot encoded text to pytorch tensor\n",
    "        encoded_onehot = torch.tensor(sample['encoded_onehot']).float()\n",
    "        return {'encoded_onehot': encoded_onehot}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n",
      "Found letters: ['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', '0', '1', '2', '5', '8', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Found letters: ['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', '0', '1', '2', '5', '8', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Alphabet length: 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (rnn): LSTM(42, 1024, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (out): Linear(in_features=1024, out_features=42, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse input arguments\n",
    "training_args = json.load(open('my_training_args_Wilde.json'))\n",
    "\n",
    "#%% Check device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Selected device:', device)\n",
    "\n",
    "dataset = WildeDataset(filepath=training_args['datasetpath'], crop_len=training_args['crop_len'],\n",
    "                    transform=None)\n",
    "\n",
    "#%% Create dataset\n",
    "trans = transforms.Compose([RandomCrop(training_args['crop_len']),\n",
    "                    OneHotEncoder(len(dataset.alphabet)),\n",
    "                    ToTensor()\n",
    "                    ])\n",
    "\n",
    "dataset = WildeDataset(filepath=training_args['datasetpath'], crop_len=training_args['crop_len'],\n",
    "                    transform=trans)\n",
    "\n",
    "print(\"Alphabet length:\", len(dataset.alphabet))\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(input_size=len(dataset.alphabet), \n",
    "            hidden_units=training_args['hidden_units'], \n",
    "            layers_num=training_args['layers_num'], \n",
    "            dropout_prob=training_args['dropout_prob'])\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasetpath': 'Picture_of_Dorian_Gray.txt', 'crop_len': 100, 'hidden_units': 1024, 'layers_num': 2, 'dropout_prob': 0.2, 'batchsize': 200, 'num_epochs': 2000, 'out_dir': 'model_Wilde'}\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "## EPOCH 1/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.7330234050750732\n",
      "##################################\n",
      "## EPOCH 2/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.714447021484375\n",
      "##################################\n",
      "## EPOCH 3/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.6005587577819824\n",
      "##################################\n",
      "## EPOCH 4/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3599789142608643\n",
      "##################################\n",
      "## EPOCH 5/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.8694164752960205\n",
      "##################################\n",
      "## EPOCH 6/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.9493720531463623\n",
      "##################################\n",
      "## EPOCH 7/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0818030834198\n",
      "##################################\n",
      "## EPOCH 8/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.4316444396972656\n",
      "##################################\n",
      "## EPOCH 9/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2895255088806152\n",
      "##################################\n",
      "## EPOCH 10/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.4572041034698486\n",
      "##################################\n",
      "## EPOCH 11/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.235480785369873\n",
      "##################################\n",
      "## EPOCH 12/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.079758405685425\n",
      "##################################\n",
      "## EPOCH 13/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2370097637176514\n",
      "##################################\n",
      "## EPOCH 14/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3460700511932373\n",
      "##################################\n",
      "## EPOCH 15/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.012493133544922\n",
      "##################################\n",
      "## EPOCH 16/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.259600877761841\n",
      "##################################\n",
      "## EPOCH 17/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.99247145652771\n",
      "##################################\n",
      "## EPOCH 18/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.055576801300049\n",
      "##################################\n",
      "## EPOCH 19/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3713924884796143\n",
      "##################################\n",
      "## EPOCH 20/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.174938678741455\n",
      "##################################\n",
      "## EPOCH 21/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9120678901672363\n",
      "##################################\n",
      "## EPOCH 22/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1291582584381104\n",
      "##################################\n",
      "## EPOCH 23/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.969290018081665\n",
      "##################################\n",
      "## EPOCH 24/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1356687545776367\n",
      "##################################\n",
      "## EPOCH 25/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.840561866760254\n",
      "##################################\n",
      "## EPOCH 26/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9155728816986084\n",
      "##################################\n",
      "## EPOCH 27/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.788950204849243\n",
      "##################################\n",
      "## EPOCH 28/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.882821559906006\n",
      "##################################\n",
      "## EPOCH 29/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.02803373336792\n",
      "##################################\n",
      "## EPOCH 30/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9785094261169434\n",
      "##################################\n",
      "## EPOCH 31/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1554160118103027\n",
      "##################################\n",
      "## EPOCH 32/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.574929714202881\n",
      "##################################\n",
      "## EPOCH 33/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.73738169670105\n",
      "##################################\n",
      "## EPOCH 34/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.759490489959717\n",
      "##################################\n",
      "## EPOCH 35/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6626529693603516\n",
      "##################################\n",
      "## EPOCH 36/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4806158542633057\n",
      "##################################\n",
      "## EPOCH 37/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8943638801574707\n",
      "##################################\n",
      "## EPOCH 38/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2372803688049316\n",
      "##################################\n",
      "## EPOCH 39/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0440611839294434\n",
      "##################################\n",
      "## EPOCH 40/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7276406288146973\n",
      "##################################\n",
      "## EPOCH 41/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.88564395904541\n",
      "##################################\n",
      "## EPOCH 42/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3602256774902344\n",
      "##################################\n",
      "## EPOCH 43/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5137510299682617\n",
      "##################################\n",
      "## EPOCH 44/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.919922113418579\n",
      "##################################\n",
      "## EPOCH 45/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.84515643119812\n",
      "##################################\n",
      "## EPOCH 46/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.986922264099121\n",
      "##################################\n",
      "## EPOCH 47/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.242041826248169\n",
      "##################################\n",
      "## EPOCH 48/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.764375925064087\n",
      "##################################\n",
      "## EPOCH 49/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.673610210418701\n",
      "##################################\n",
      "## EPOCH 50/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.975374698638916\n",
      "##################################\n",
      "## EPOCH 51/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.193307399749756\n",
      "##################################\n",
      "## EPOCH 52/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.001988172531128\n",
      "##################################\n",
      "## EPOCH 53/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0289881229400635\n",
      "##################################\n",
      "## EPOCH 54/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9465928077697754\n",
      "##################################\n",
      "## EPOCH 55/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2551116943359375\n",
      "##################################\n",
      "## EPOCH 56/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9042444229125977\n",
      "##################################\n",
      "## EPOCH 57/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.019897699356079\n",
      "##################################\n",
      "## EPOCH 58/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.845033884048462\n",
      "##################################\n",
      "## EPOCH 59/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0268890857696533\n",
      "##################################\n",
      "## EPOCH 60/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.051804780960083\n",
      "##################################\n",
      "## EPOCH 61/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 3.1339457035064697\n",
      "##################################\n",
      "## EPOCH 62/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.195836305618286\n",
      "##################################\n",
      "## EPOCH 63/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2510108947753906\n",
      "##################################\n",
      "## EPOCH 64/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2353644371032715\n",
      "##################################\n",
      "## EPOCH 65/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.858429431915283\n",
      "##################################\n",
      "## EPOCH 66/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.897334337234497\n",
      "##################################\n",
      "## EPOCH 67/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1212007999420166\n",
      "##################################\n",
      "## EPOCH 68/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.671967029571533\n",
      "##################################\n",
      "## EPOCH 69/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.779456377029419\n",
      "##################################\n",
      "## EPOCH 70/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8135111331939697\n",
      "##################################\n",
      "## EPOCH 71/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9052693843841553\n",
      "##################################\n",
      "## EPOCH 72/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6712794303894043\n",
      "##################################\n",
      "## EPOCH 73/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.848710536956787\n",
      "##################################\n",
      "## EPOCH 74/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1773085594177246\n",
      "##################################\n",
      "## EPOCH 75/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.88932204246521\n",
      "##################################\n",
      "## EPOCH 76/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1109681129455566\n",
      "##################################\n",
      "## EPOCH 77/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.912611961364746\n",
      "##################################\n",
      "## EPOCH 78/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7128310203552246\n",
      "##################################\n",
      "## EPOCH 79/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2460198402404785\n",
      "##################################\n",
      "## EPOCH 80/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5392861366271973\n",
      "##################################\n",
      "## EPOCH 81/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.212493419647217\n",
      "##################################\n",
      "## EPOCH 82/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.942816972732544\n",
      "##################################\n",
      "## EPOCH 83/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.770289897918701\n",
      "##################################\n",
      "## EPOCH 84/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5805892944335938\n",
      "##################################\n",
      "## EPOCH 85/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1025032997131348\n",
      "##################################\n",
      "## EPOCH 86/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8040947914123535\n",
      "##################################\n",
      "## EPOCH 87/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.830944538116455\n",
      "##################################\n",
      "## EPOCH 88/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4173803329467773\n",
      "##################################\n",
      "## EPOCH 89/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.002107620239258\n",
      "##################################\n",
      "## EPOCH 90/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1285293102264404\n",
      "##################################\n",
      "## EPOCH 91/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7178595066070557\n",
      "##################################\n",
      "## EPOCH 92/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2021923065185547\n",
      "##################################\n",
      "## EPOCH 93/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0329556465148926\n",
      "##################################\n",
      "## EPOCH 94/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9523544311523438\n",
      "##################################\n",
      "## EPOCH 95/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0860047340393066\n",
      "##################################\n",
      "## EPOCH 96/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8685717582702637\n",
      "##################################\n",
      "## EPOCH 97/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2268261909484863\n",
      "##################################\n",
      "## EPOCH 98/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1826539039611816\n",
      "##################################\n",
      "## EPOCH 99/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6824581623077393\n",
      "##################################\n",
      "## EPOCH 100/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.931812047958374\n",
      "##################################\n",
      "## EPOCH 101/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1128287315368652\n",
      "##################################\n",
      "## EPOCH 102/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.042884349822998\n",
      "##################################\n",
      "## EPOCH 103/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.806640148162842\n",
      "##################################\n",
      "## EPOCH 104/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.88183856010437\n",
      "##################################\n",
      "## EPOCH 105/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9167027473449707\n",
      "##################################\n",
      "## EPOCH 106/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9026923179626465\n",
      "##################################\n",
      "## EPOCH 107/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.746690273284912\n",
      "##################################\n",
      "## EPOCH 108/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8707592487335205\n",
      "##################################\n",
      "## EPOCH 109/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9753525257110596\n",
      "##################################\n",
      "## EPOCH 110/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8566834926605225\n",
      "##################################\n",
      "## EPOCH 111/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1616103649139404\n",
      "##################################\n",
      "## EPOCH 112/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9046123027801514\n",
      "##################################\n",
      "## EPOCH 113/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1076247692108154\n",
      "##################################\n",
      "## EPOCH 114/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8863017559051514\n",
      "##################################\n",
      "## EPOCH 115/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9941256046295166\n",
      "##################################\n",
      "## EPOCH 116/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9076783657073975\n",
      "##################################\n",
      "## EPOCH 117/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0403366088867188\n",
      "##################################\n",
      "## EPOCH 118/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8710243701934814\n",
      "##################################\n",
      "## EPOCH 119/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0483498573303223\n",
      "##################################\n",
      "## EPOCH 120/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.721890926361084\n",
      "##################################\n",
      "## EPOCH 121/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.613550901412964\n",
      "##################################\n",
      "## EPOCH 122/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.916918992996216\n",
      "##################################\n",
      "## EPOCH 123/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1216204166412354\n",
      "##################################\n",
      "## EPOCH 124/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5934884548187256\n",
      "##################################\n",
      "## EPOCH 125/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.952993392944336\n",
      "##################################\n",
      "## EPOCH 126/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3200812339782715\n",
      "##################################\n",
      "## EPOCH 127/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.930323839187622\n",
      "##################################\n",
      "## EPOCH 128/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.828869342803955\n",
      "##################################\n",
      "## EPOCH 129/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.5936756134033203\n",
      "##################################\n",
      "## EPOCH 130/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.912019968032837\n",
      "##################################\n",
      "## EPOCH 131/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0564818382263184\n",
      "##################################\n",
      "## EPOCH 132/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7693095207214355\n",
      "##################################\n",
      "## EPOCH 133/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7697770595550537\n",
      "##################################\n",
      "## EPOCH 134/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7773072719573975\n",
      "##################################\n",
      "## EPOCH 135/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.892190456390381\n",
      "##################################\n",
      "## EPOCH 136/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.649609088897705\n",
      "##################################\n",
      "## EPOCH 137/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.770561456680298\n",
      "##################################\n",
      "## EPOCH 138/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9323723316192627\n",
      "##################################\n",
      "## EPOCH 139/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.98193621635437\n",
      "##################################\n",
      "## EPOCH 140/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8112759590148926\n",
      "##################################\n",
      "## EPOCH 141/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.140963077545166\n",
      "##################################\n",
      "## EPOCH 142/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0325818061828613\n",
      "##################################\n",
      "## EPOCH 143/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0597310066223145\n",
      "##################################\n",
      "## EPOCH 144/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.835871934890747\n",
      "##################################\n",
      "## EPOCH 145/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.852149724960327\n",
      "##################################\n",
      "## EPOCH 146/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8252387046813965\n",
      "##################################\n",
      "## EPOCH 147/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.775373697280884\n",
      "##################################\n",
      "## EPOCH 148/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2495265007019043\n",
      "##################################\n",
      "## EPOCH 149/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9028496742248535\n",
      "##################################\n",
      "## EPOCH 150/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7582333087921143\n",
      "##################################\n",
      "## EPOCH 151/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0394904613494873\n",
      "##################################\n",
      "## EPOCH 152/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.954373836517334\n",
      "##################################\n",
      "## EPOCH 153/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.210662841796875\n",
      "##################################\n",
      "## EPOCH 154/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.017864227294922\n",
      "##################################\n",
      "## EPOCH 155/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9688429832458496\n",
      "##################################\n",
      "## EPOCH 156/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0008187294006348\n",
      "##################################\n",
      "## EPOCH 157/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.767756938934326\n",
      "##################################\n",
      "## EPOCH 158/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.913358688354492\n",
      "##################################\n",
      "## EPOCH 159/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.735153913497925\n",
      "##################################\n",
      "## EPOCH 160/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.802952527999878\n",
      "##################################\n",
      "## EPOCH 161/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.764317274093628\n",
      "##################################\n",
      "## EPOCH 162/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0131547451019287\n",
      "##################################\n",
      "## EPOCH 163/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0031847953796387\n",
      "##################################\n",
      "## EPOCH 164/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.959254741668701\n",
      "##################################\n",
      "## EPOCH 165/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.02268123626709\n",
      "##################################\n",
      "## EPOCH 166/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.782377243041992\n",
      "##################################\n",
      "## EPOCH 167/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6430134773254395\n",
      "##################################\n",
      "## EPOCH 168/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.947601318359375\n",
      "##################################\n",
      "## EPOCH 169/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6455190181732178\n",
      "##################################\n",
      "## EPOCH 170/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0616767406463623\n",
      "##################################\n",
      "## EPOCH 171/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6650919914245605\n",
      "##################################\n",
      "## EPOCH 172/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0737080574035645\n",
      "##################################\n",
      "## EPOCH 173/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.64052677154541\n",
      "##################################\n",
      "## EPOCH 174/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9230899810791016\n",
      "##################################\n",
      "## EPOCH 175/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8366806507110596\n",
      "##################################\n",
      "## EPOCH 176/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0904767513275146\n",
      "##################################\n",
      "## EPOCH 177/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9221177101135254\n",
      "##################################\n",
      "## EPOCH 178/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9910666942596436\n",
      "##################################\n",
      "## EPOCH 179/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.365602970123291\n",
      "##################################\n",
      "## EPOCH 180/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9258179664611816\n",
      "##################################\n",
      "## EPOCH 181/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.5600879192352295\n",
      "##################################\n",
      "## EPOCH 182/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6846461296081543\n",
      "##################################\n",
      "## EPOCH 183/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8206372261047363\n",
      "##################################\n",
      "## EPOCH 184/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0633182525634766\n",
      "##################################\n",
      "## EPOCH 185/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.951064109802246\n",
      "##################################\n",
      "## EPOCH 186/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7775089740753174\n",
      "##################################\n",
      "## EPOCH 187/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6475424766540527\n",
      "##################################\n",
      "## EPOCH 188/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.515408754348755\n",
      "##################################\n",
      "## EPOCH 189/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6803574562072754\n",
      "##################################\n",
      "## EPOCH 190/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6736671924591064\n",
      "##################################\n",
      "## EPOCH 191/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.806748628616333\n",
      "##################################\n",
      "## EPOCH 192/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.301872730255127\n",
      "##################################\n",
      "## EPOCH 193/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.911097288131714\n",
      "##################################\n",
      "## EPOCH 194/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7147490978240967\n",
      "##################################\n",
      "## EPOCH 195/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.205961227416992\n",
      "##################################\n",
      "## EPOCH 196/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5613393783569336\n",
      "##################################\n",
      "## EPOCH 197/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7676515579223633\n",
      "##################################\n",
      "## EPOCH 198/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.822510242462158\n",
      "##################################\n",
      "## EPOCH 199/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.655799150466919\n",
      "##################################\n",
      "## EPOCH 200/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.842038869857788\n",
      "##################################\n",
      "## EPOCH 201/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7533340454101562\n",
      "##################################\n",
      "## EPOCH 202/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6921725273132324\n",
      "##################################\n",
      "## EPOCH 203/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.837080955505371\n",
      "##################################\n",
      "## EPOCH 204/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.542952299118042\n",
      "##################################\n",
      "## EPOCH 205/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6745967864990234\n",
      "##################################\n",
      "## EPOCH 206/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5429625511169434\n",
      "##################################\n",
      "## EPOCH 207/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8235957622528076\n",
      "##################################\n",
      "## EPOCH 208/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4742891788482666\n",
      "##################################\n",
      "## EPOCH 209/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7696120738983154\n",
      "##################################\n",
      "## EPOCH 210/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7258260250091553\n",
      "##################################\n",
      "## EPOCH 211/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6612210273742676\n",
      "##################################\n",
      "## EPOCH 212/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.393993854522705\n",
      "##################################\n",
      "## EPOCH 213/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4230473041534424\n",
      "##################################\n",
      "## EPOCH 214/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2979393005371094\n",
      "##################################\n",
      "## EPOCH 215/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.346940755844116\n",
      "##################################\n",
      "## EPOCH 216/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.858123302459717\n",
      "##################################\n",
      "## EPOCH 217/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.648845672607422\n",
      "##################################\n",
      "## EPOCH 218/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.511323928833008\n",
      "##################################\n",
      "## EPOCH 219/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.748810291290283\n",
      "##################################\n",
      "## EPOCH 220/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.859440326690674\n",
      "##################################\n",
      "## EPOCH 221/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7502927780151367\n",
      "##################################\n",
      "## EPOCH 222/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5828092098236084\n",
      "##################################\n",
      "## EPOCH 223/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3939242362976074\n",
      "##################################\n",
      "## EPOCH 224/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.42907452583313\n",
      "##################################\n",
      "## EPOCH 225/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8703293800354004\n",
      "##################################\n",
      "## EPOCH 226/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.580883026123047\n",
      "##################################\n",
      "## EPOCH 227/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.242269277572632\n",
      "##################################\n",
      "## EPOCH 228/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.223764181137085\n",
      "##################################\n",
      "## EPOCH 229/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.651367664337158\n",
      "##################################\n",
      "## EPOCH 230/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8393349647521973\n",
      "##################################\n",
      "## EPOCH 231/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.546154499053955\n",
      "##################################\n",
      "## EPOCH 232/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5804378986358643\n",
      "##################################\n",
      "## EPOCH 233/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3628461360931396\n",
      "##################################\n",
      "## EPOCH 234/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9595625400543213\n",
      "##################################\n",
      "## EPOCH 235/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.534585475921631\n",
      "##################################\n",
      "## EPOCH 236/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.580799102783203\n",
      "##################################\n",
      "## EPOCH 237/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7302253246307373\n",
      "##################################\n",
      "## EPOCH 238/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8566322326660156\n",
      "##################################\n",
      "## EPOCH 239/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4742486476898193\n",
      "##################################\n",
      "## EPOCH 240/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5362207889556885\n",
      "##################################\n",
      "## EPOCH 241/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.6558778285980225\n",
      "##################################\n",
      "## EPOCH 242/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.612200975418091\n",
      "##################################\n",
      "## EPOCH 243/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4139552116394043\n",
      "##################################\n",
      "## EPOCH 244/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7638700008392334\n",
      "##################################\n",
      "## EPOCH 245/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4769110679626465\n",
      "##################################\n",
      "## EPOCH 246/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.493340253829956\n",
      "##################################\n",
      "## EPOCH 247/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5659708976745605\n",
      "##################################\n",
      "## EPOCH 248/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9369893074035645\n",
      "##################################\n",
      "## EPOCH 249/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.538794755935669\n",
      "##################################\n",
      "## EPOCH 250/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3469138145446777\n",
      "##################################\n",
      "## EPOCH 251/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3891377449035645\n",
      "##################################\n",
      "## EPOCH 252/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.65847110748291\n",
      "##################################\n",
      "## EPOCH 253/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.74037504196167\n",
      "##################################\n",
      "## EPOCH 254/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1046791076660156\n",
      "##################################\n",
      "## EPOCH 255/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5949013233184814\n",
      "##################################\n",
      "## EPOCH 256/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5317530632019043\n",
      "##################################\n",
      "## EPOCH 257/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.956881523132324\n",
      "##################################\n",
      "## EPOCH 258/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.78605580329895\n",
      "##################################\n",
      "## EPOCH 259/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.544595241546631\n",
      "##################################\n",
      "## EPOCH 260/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4838783740997314\n",
      "##################################\n",
      "## EPOCH 261/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8135986328125\n",
      "##################################\n",
      "## EPOCH 262/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3613362312316895\n",
      "##################################\n",
      "## EPOCH 263/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7364325523376465\n",
      "##################################\n",
      "## EPOCH 264/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.677809238433838\n",
      "##################################\n",
      "## EPOCH 265/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6075310707092285\n",
      "##################################\n",
      "## EPOCH 266/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8042664527893066\n",
      "##################################\n",
      "## EPOCH 267/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3021206855773926\n",
      "##################################\n",
      "## EPOCH 268/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.484065294265747\n",
      "##################################\n",
      "## EPOCH 269/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6913228034973145\n",
      "##################################\n",
      "## EPOCH 270/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2461438179016113\n",
      "##################################\n",
      "## EPOCH 271/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.584897756576538\n",
      "##################################\n",
      "## EPOCH 272/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.470363140106201\n",
      "##################################\n",
      "## EPOCH 273/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5034422874450684\n",
      "##################################\n",
      "## EPOCH 274/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2869510650634766\n",
      "##################################\n",
      "## EPOCH 275/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.343113899230957\n",
      "##################################\n",
      "## EPOCH 276/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.500415086746216\n",
      "##################################\n",
      "## EPOCH 277/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.267573833465576\n",
      "##################################\n",
      "## EPOCH 278/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.109988212585449\n",
      "##################################\n",
      "## EPOCH 279/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.560814380645752\n",
      "##################################\n",
      "## EPOCH 280/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6513264179229736\n",
      "##################################\n",
      "## EPOCH 281/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5155692100524902\n",
      "##################################\n",
      "## EPOCH 282/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3706250190734863\n",
      "##################################\n",
      "## EPOCH 283/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3255012035369873\n",
      "##################################\n",
      "## EPOCH 284/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7718615531921387\n",
      "##################################\n",
      "## EPOCH 285/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4775238037109375\n",
      "##################################\n",
      "## EPOCH 286/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6178603172302246\n",
      "##################################\n",
      "## EPOCH 287/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0140786170959473\n",
      "##################################\n",
      "## EPOCH 288/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.567477226257324\n",
      "##################################\n",
      "## EPOCH 289/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.002965211868286\n",
      "##################################\n",
      "## EPOCH 290/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.37156343460083\n",
      "##################################\n",
      "## EPOCH 291/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3988819122314453\n",
      "##################################\n",
      "## EPOCH 292/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.578810691833496\n",
      "##################################\n",
      "## EPOCH 293/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2829647064208984\n",
      "##################################\n",
      "## EPOCH 294/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7722115516662598\n",
      "##################################\n",
      "## EPOCH 295/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1942763328552246\n",
      "##################################\n",
      "## EPOCH 296/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1604621410369873\n",
      "##################################\n",
      "## EPOCH 297/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7563960552215576\n",
      "##################################\n",
      "## EPOCH 298/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.459357738494873\n",
      "##################################\n",
      "## EPOCH 299/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.490492343902588\n",
      "##################################\n",
      "## EPOCH 300/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.629382371902466\n",
      "##################################\n",
      "## EPOCH 301/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.3321356773376465\n",
      "##################################\n",
      "## EPOCH 302/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3442158699035645\n",
      "##################################\n",
      "## EPOCH 303/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9688352346420288\n",
      "##################################\n",
      "## EPOCH 304/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1412291526794434\n",
      "##################################\n",
      "## EPOCH 305/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.161067247390747\n",
      "##################################\n",
      "## EPOCH 306/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6531643867492676\n",
      "##################################\n",
      "## EPOCH 307/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.351013660430908\n",
      "##################################\n",
      "## EPOCH 308/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.561422348022461\n",
      "##################################\n",
      "## EPOCH 309/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4638373851776123\n",
      "##################################\n",
      "## EPOCH 310/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.371752977371216\n",
      "##################################\n",
      "## EPOCH 311/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.526174545288086\n",
      "##################################\n",
      "## EPOCH 312/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5947585105895996\n",
      "##################################\n",
      "## EPOCH 313/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5568699836730957\n",
      "##################################\n",
      "## EPOCH 314/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0159010887145996\n",
      "##################################\n",
      "## EPOCH 315/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8327016830444336\n",
      "##################################\n",
      "## EPOCH 316/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9016813039779663\n",
      "##################################\n",
      "## EPOCH 317/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7801356315612793\n",
      "##################################\n",
      "## EPOCH 318/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.48884916305542\n",
      "##################################\n",
      "## EPOCH 319/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2057957649230957\n",
      "##################################\n",
      "## EPOCH 320/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3109612464904785\n",
      "##################################\n",
      "## EPOCH 321/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1110615730285645\n",
      "##################################\n",
      "## EPOCH 322/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7232229709625244\n",
      "##################################\n",
      "## EPOCH 323/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9232592582702637\n",
      "##################################\n",
      "## EPOCH 324/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6697754859924316\n",
      "##################################\n",
      "## EPOCH 325/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3270015716552734\n",
      "##################################\n",
      "## EPOCH 326/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4376540184020996\n",
      "##################################\n",
      "## EPOCH 327/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2624101638793945\n",
      "##################################\n",
      "## EPOCH 328/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7014105319976807\n",
      "##################################\n",
      "## EPOCH 329/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.299973964691162\n",
      "##################################\n",
      "## EPOCH 330/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.343139410018921\n",
      "##################################\n",
      "## EPOCH 331/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5143468379974365\n",
      "##################################\n",
      "## EPOCH 332/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1924185752868652\n",
      "##################################\n",
      "## EPOCH 333/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.388254165649414\n",
      "##################################\n",
      "## EPOCH 334/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.012316942214966\n",
      "##################################\n",
      "## EPOCH 335/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.537884473800659\n",
      "##################################\n",
      "## EPOCH 336/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.231309652328491\n",
      "##################################\n",
      "## EPOCH 337/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0074520111083984\n",
      "##################################\n",
      "## EPOCH 338/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.673815965652466\n",
      "##################################\n",
      "## EPOCH 339/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.499889612197876\n",
      "##################################\n",
      "## EPOCH 340/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.27909255027771\n",
      "##################################\n",
      "## EPOCH 341/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3372745513916016\n",
      "##################################\n",
      "## EPOCH 342/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1956238746643066\n",
      "##################################\n",
      "## EPOCH 343/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8658004999160767\n",
      "##################################\n",
      "## EPOCH 344/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3889458179473877\n",
      "##################################\n",
      "## EPOCH 345/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0597312450408936\n",
      "##################################\n",
      "## EPOCH 346/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.179647207260132\n",
      "##################################\n",
      "## EPOCH 347/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3934264183044434\n",
      "##################################\n",
      "## EPOCH 348/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2812652587890625\n",
      "##################################\n",
      "## EPOCH 349/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2877566814422607\n",
      "##################################\n",
      "## EPOCH 350/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7093324661254883\n",
      "##################################\n",
      "## EPOCH 351/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.099264621734619\n",
      "##################################\n",
      "## EPOCH 352/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.425557851791382\n",
      "##################################\n",
      "## EPOCH 353/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.263166904449463\n",
      "##################################\n",
      "## EPOCH 354/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4939446449279785\n",
      "##################################\n",
      "## EPOCH 355/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.085571050643921\n",
      "##################################\n",
      "## EPOCH 356/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.319934606552124\n",
      "##################################\n",
      "## EPOCH 357/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8470427989959717\n",
      "##################################\n",
      "## EPOCH 358/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.198373556137085\n",
      "##################################\n",
      "## EPOCH 359/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.180067777633667\n",
      "##################################\n",
      "## EPOCH 360/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.743882179260254\n",
      "##################################\n",
      "## EPOCH 361/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.393991470336914\n",
      "##################################\n",
      "## EPOCH 362/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2185778617858887\n",
      "##################################\n",
      "## EPOCH 363/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2773048877716064\n",
      "##################################\n",
      "## EPOCH 364/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4335227012634277\n",
      "##################################\n",
      "## EPOCH 365/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9041666984558105\n",
      "##################################\n",
      "## EPOCH 366/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3319530487060547\n",
      "##################################\n",
      "## EPOCH 367/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4607362747192383\n",
      "##################################\n",
      "## EPOCH 368/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4878668785095215\n",
      "##################################\n",
      "## EPOCH 369/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.060581922531128\n",
      "##################################\n",
      "## EPOCH 370/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3020172119140625\n",
      "##################################\n",
      "## EPOCH 371/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4947948455810547\n",
      "##################################\n",
      "## EPOCH 372/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4937744140625\n",
      "##################################\n",
      "## EPOCH 373/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.190772771835327\n",
      "##################################\n",
      "## EPOCH 374/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.255610466003418\n",
      "##################################\n",
      "## EPOCH 375/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4894587993621826\n",
      "##################################\n",
      "## EPOCH 376/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.184767246246338\n",
      "##################################\n",
      "## EPOCH 377/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0137476921081543\n",
      "##################################\n",
      "## EPOCH 378/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.400202989578247\n",
      "##################################\n",
      "## EPOCH 379/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4144020080566406\n",
      "##################################\n",
      "## EPOCH 380/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0671350955963135\n",
      "##################################\n",
      "## EPOCH 381/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0884716510772705\n",
      "##################################\n",
      "## EPOCH 382/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8309316635131836\n",
      "##################################\n",
      "## EPOCH 383/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7798341512680054\n",
      "##################################\n",
      "## EPOCH 384/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.089411735534668\n",
      "##################################\n",
      "## EPOCH 385/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.163097858428955\n",
      "##################################\n",
      "## EPOCH 386/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1015896797180176\n",
      "##################################\n",
      "## EPOCH 387/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3873807191848755\n",
      "##################################\n",
      "## EPOCH 388/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0767171382904053\n",
      "##################################\n",
      "## EPOCH 389/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.364819288253784\n",
      "##################################\n",
      "## EPOCH 390/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9298596382141113\n",
      "##################################\n",
      "## EPOCH 391/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5903486013412476\n",
      "##################################\n",
      "## EPOCH 392/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9238471984863281\n",
      "##################################\n",
      "## EPOCH 393/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.437953472137451\n",
      "##################################\n",
      "## EPOCH 394/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1622347831726074\n",
      "##################################\n",
      "## EPOCH 395/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4883837699890137\n",
      "##################################\n",
      "## EPOCH 396/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.088346004486084\n",
      "##################################\n",
      "## EPOCH 397/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8077987432479858\n",
      "##################################\n",
      "## EPOCH 398/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.114157199859619\n",
      "##################################\n",
      "## EPOCH 399/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.839664101600647\n",
      "##################################\n",
      "## EPOCH 400/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.067760705947876\n",
      "##################################\n",
      "## EPOCH 401/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.321376085281372\n",
      "##################################\n",
      "## EPOCH 402/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1006319522857666\n",
      "##################################\n",
      "## EPOCH 403/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.212623357772827\n",
      "##################################\n",
      "## EPOCH 404/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9177310466766357\n",
      "##################################\n",
      "## EPOCH 405/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6690839529037476\n",
      "##################################\n",
      "## EPOCH 406/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8939825296401978\n",
      "##################################\n",
      "## EPOCH 407/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0517048835754395\n",
      "##################################\n",
      "## EPOCH 408/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.040229320526123\n",
      "##################################\n",
      "## EPOCH 409/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8266651630401611\n",
      "##################################\n",
      "## EPOCH 410/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1768455505371094\n",
      "##################################\n",
      "## EPOCH 411/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2999484539031982\n",
      "##################################\n",
      "## EPOCH 412/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.821508765220642\n",
      "##################################\n",
      "## EPOCH 413/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.366912603378296\n",
      "##################################\n",
      "## EPOCH 414/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8249517679214478\n",
      "##################################\n",
      "## EPOCH 415/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.363391399383545\n",
      "##################################\n",
      "## EPOCH 416/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7256410121917725\n",
      "##################################\n",
      "## EPOCH 417/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0440876483917236\n",
      "##################################\n",
      "## EPOCH 418/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.030457019805908\n",
      "##################################\n",
      "## EPOCH 419/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.827794075012207\n",
      "##################################\n",
      "## EPOCH 420/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7374398708343506\n",
      "##################################\n",
      "## EPOCH 421/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7703453302383423\n",
      "##################################\n",
      "## EPOCH 422/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4509947299957275\n",
      "##################################\n",
      "## EPOCH 423/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.969513177871704\n",
      "##################################\n",
      "## EPOCH 424/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1670868396759033\n",
      "##################################\n",
      "## EPOCH 425/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1948354244232178\n",
      "##################################\n",
      "## EPOCH 426/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8852462768554688\n",
      "##################################\n",
      "## EPOCH 427/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7720019817352295\n",
      "##################################\n",
      "## EPOCH 428/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9490644931793213\n",
      "##################################\n",
      "## EPOCH 429/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0793497562408447\n",
      "##################################\n",
      "## EPOCH 430/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8142964839935303\n",
      "##################################\n",
      "## EPOCH 431/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6033036708831787\n",
      "##################################\n",
      "## EPOCH 432/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8899682760238647\n",
      "##################################\n",
      "## EPOCH 433/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4042203426361084\n",
      "##################################\n",
      "## EPOCH 434/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.051234722137451\n",
      "##################################\n",
      "## EPOCH 435/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5546530485153198\n",
      "##################################\n",
      "## EPOCH 436/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0863921642303467\n",
      "##################################\n",
      "## EPOCH 437/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.209014415740967\n",
      "##################################\n",
      "## EPOCH 438/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.480930209159851\n",
      "##################################\n",
      "## EPOCH 439/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1536993980407715\n",
      "##################################\n",
      "## EPOCH 440/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6180318593978882\n",
      "##################################\n",
      "## EPOCH 441/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1686699390411377\n",
      "##################################\n",
      "## EPOCH 442/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8089736700057983\n",
      "##################################\n",
      "## EPOCH 443/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9032083749771118\n",
      "##################################\n",
      "## EPOCH 444/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5248119831085205\n",
      "##################################\n",
      "## EPOCH 445/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8902795314788818\n",
      "##################################\n",
      "## EPOCH 446/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0868945121765137\n",
      "##################################\n",
      "## EPOCH 447/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4601309299468994\n",
      "##################################\n",
      "## EPOCH 448/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.427616834640503\n",
      "##################################\n",
      "## EPOCH 449/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6773178577423096\n",
      "##################################\n",
      "## EPOCH 450/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5594613552093506\n",
      "##################################\n",
      "## EPOCH 451/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.099142551422119\n",
      "##################################\n",
      "## EPOCH 452/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.594412326812744\n",
      "##################################\n",
      "## EPOCH 453/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.02012300491333\n",
      "##################################\n",
      "## EPOCH 454/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7736015319824219\n",
      "##################################\n",
      "## EPOCH 455/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7833302021026611\n",
      "##################################\n",
      "## EPOCH 456/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8225538730621338\n",
      "##################################\n",
      "## EPOCH 457/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1923298835754395\n",
      "##################################\n",
      "## EPOCH 458/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6778522729873657\n",
      "##################################\n",
      "## EPOCH 459/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.453979253768921\n",
      "##################################\n",
      "## EPOCH 460/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5564568042755127\n",
      "##################################\n",
      "## EPOCH 461/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.615161657333374\n",
      "##################################\n",
      "## EPOCH 462/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8276945352554321\n",
      "##################################\n",
      "## EPOCH 463/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.221731424331665\n",
      "##################################\n",
      "## EPOCH 464/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.088484764099121\n",
      "##################################\n",
      "## EPOCH 465/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4850839376449585\n",
      "##################################\n",
      "## EPOCH 466/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6928163766860962\n",
      "##################################\n",
      "## EPOCH 467/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8310070037841797\n",
      "##################################\n",
      "## EPOCH 468/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5751831531524658\n",
      "##################################\n",
      "## EPOCH 469/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4335275888442993\n",
      "##################################\n",
      "## EPOCH 470/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5368191003799438\n",
      "##################################\n",
      "## EPOCH 471/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.616802453994751\n",
      "##################################\n",
      "## EPOCH 472/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.662933349609375\n",
      "##################################\n",
      "## EPOCH 473/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7446836233139038\n",
      "##################################\n",
      "## EPOCH 474/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6790618896484375\n",
      "##################################\n",
      "## EPOCH 475/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7275718450546265\n",
      "##################################\n",
      "## EPOCH 476/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3784093856811523\n",
      "##################################\n",
      "## EPOCH 477/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4312158823013306\n",
      "##################################\n",
      "## EPOCH 478/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0834078788757324\n",
      "##################################\n",
      "## EPOCH 479/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.475929617881775\n",
      "##################################\n",
      "## EPOCH 480/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3255659341812134\n",
      "##################################\n",
      "## EPOCH 481/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5388085842132568\n",
      "##################################\n",
      "## EPOCH 482/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.590653657913208\n",
      "##################################\n",
      "## EPOCH 483/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6606075763702393\n",
      "##################################\n",
      "## EPOCH 484/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8154255151748657\n",
      "##################################\n",
      "## EPOCH 485/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.734630823135376\n",
      "##################################\n",
      "## EPOCH 486/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7688030004501343\n",
      "##################################\n",
      "## EPOCH 487/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6055490970611572\n",
      "##################################\n",
      "## EPOCH 488/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.401298999786377\n",
      "##################################\n",
      "## EPOCH 489/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0677194595336914\n",
      "##################################\n",
      "## EPOCH 490/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.539149522781372\n",
      "##################################\n",
      "## EPOCH 491/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7535171508789062\n",
      "##################################\n",
      "## EPOCH 492/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0987205505371094\n",
      "##################################\n",
      "## EPOCH 493/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2783106565475464\n",
      "##################################\n",
      "## EPOCH 494/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5346370935440063\n",
      "##################################\n",
      "## EPOCH 495/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7758560180664062\n",
      "##################################\n",
      "## EPOCH 496/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.390800952911377\n",
      "##################################\n",
      "## EPOCH 497/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.453064203262329\n",
      "##################################\n",
      "## EPOCH 498/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4698121547698975\n",
      "##################################\n",
      "## EPOCH 499/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3775454759597778\n",
      "##################################\n",
      "## EPOCH 500/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8616422414779663\n",
      "##################################\n",
      "## EPOCH 501/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3628945350646973\n",
      "##################################\n",
      "## EPOCH 502/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5195796489715576\n",
      "##################################\n",
      "## EPOCH 503/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5216931104660034\n",
      "##################################\n",
      "## EPOCH 504/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5015199184417725\n",
      "##################################\n",
      "## EPOCH 505/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8928829431533813\n",
      "##################################\n",
      "## EPOCH 506/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.129854679107666\n",
      "##################################\n",
      "## EPOCH 507/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3224170207977295\n",
      "##################################\n",
      "## EPOCH 508/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.414712905883789\n",
      "##################################\n",
      "## EPOCH 509/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8809503316879272\n",
      "##################################\n",
      "## EPOCH 510/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0380334854125977\n",
      "##################################\n",
      "## EPOCH 511/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5785443782806396\n",
      "##################################\n",
      "## EPOCH 512/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8070485591888428\n",
      "##################################\n",
      "## EPOCH 513/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3955310583114624\n",
      "##################################\n",
      "## EPOCH 514/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9581254720687866\n",
      "##################################\n",
      "## EPOCH 515/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7315292358398438\n",
      "##################################\n",
      "## EPOCH 516/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6108570098876953\n",
      "##################################\n",
      "## EPOCH 517/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5391452312469482\n",
      "##################################\n",
      "## EPOCH 518/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9169309735298157\n",
      "##################################\n",
      "## EPOCH 519/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4626880884170532\n",
      "##################################\n",
      "## EPOCH 520/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8304481506347656\n",
      "##################################\n",
      "## EPOCH 521/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3135840892791748\n",
      "##################################\n",
      "## EPOCH 522/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1878987550735474\n",
      "##################################\n",
      "## EPOCH 523/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1835078001022339\n",
      "##################################\n",
      "## EPOCH 524/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0384283065795898\n",
      "##################################\n",
      "## EPOCH 525/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8462960720062256\n",
      "##################################\n",
      "## EPOCH 526/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0421631336212158\n",
      "##################################\n",
      "## EPOCH 527/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3120744228363037\n",
      "##################################\n",
      "## EPOCH 528/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1836538314819336\n",
      "##################################\n",
      "## EPOCH 529/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.640234351158142\n",
      "##################################\n",
      "## EPOCH 530/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.201599359512329\n",
      "##################################\n",
      "## EPOCH 531/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.053605079650879\n",
      "##################################\n",
      "## EPOCH 532/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1636505126953125\n",
      "##################################\n",
      "## EPOCH 533/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.391918420791626\n",
      "##################################\n",
      "## EPOCH 534/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7795438766479492\n",
      "##################################\n",
      "## EPOCH 535/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.148298740386963\n",
      "##################################\n",
      "## EPOCH 536/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1118944883346558\n",
      "##################################\n",
      "## EPOCH 537/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2775909900665283\n",
      "##################################\n",
      "## EPOCH 538/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.377245306968689\n",
      "##################################\n",
      "## EPOCH 539/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9859569668769836\n",
      "##################################\n",
      "## EPOCH 540/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.633305311203003\n",
      "##################################\n",
      "## EPOCH 541/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.8247371912002563\n",
      "##################################\n",
      "## EPOCH 542/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.892677664756775\n",
      "##################################\n",
      "## EPOCH 543/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3733460903167725\n",
      "##################################\n",
      "## EPOCH 544/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3112621307373047\n",
      "##################################\n",
      "## EPOCH 545/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0042405128479004\n",
      "##################################\n",
      "## EPOCH 546/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3696215152740479\n",
      "##################################\n",
      "## EPOCH 547/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2740421295166016\n",
      "##################################\n",
      "## EPOCH 548/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1570453643798828\n",
      "##################################\n",
      "## EPOCH 549/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0146260261535645\n",
      "##################################\n",
      "## EPOCH 550/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.571638584136963\n",
      "##################################\n",
      "## EPOCH 551/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1550055742263794\n",
      "##################################\n",
      "## EPOCH 552/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3674803972244263\n",
      "##################################\n",
      "## EPOCH 553/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.179459571838379\n",
      "##################################\n",
      "## EPOCH 554/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3767101764678955\n",
      "##################################\n",
      "## EPOCH 555/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3834173679351807\n",
      "##################################\n",
      "## EPOCH 556/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4779857397079468\n",
      "##################################\n",
      "## EPOCH 557/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0341238975524902\n",
      "##################################\n",
      "## EPOCH 558/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7708476781845093\n",
      "##################################\n",
      "## EPOCH 559/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2064062356948853\n",
      "##################################\n",
      "## EPOCH 560/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8780724406242371\n",
      "##################################\n",
      "## EPOCH 561/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.015773057937622\n",
      "##################################\n",
      "## EPOCH 562/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9434741735458374\n",
      "##################################\n",
      "## EPOCH 563/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0675079822540283\n",
      "##################################\n",
      "## EPOCH 564/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9069738388061523\n",
      "##################################\n",
      "## EPOCH 565/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1983089447021484\n",
      "##################################\n",
      "## EPOCH 566/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1485358476638794\n",
      "##################################\n",
      "## EPOCH 567/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.367656946182251\n",
      "##################################\n",
      "## EPOCH 568/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7522767186164856\n",
      "##################################\n",
      "## EPOCH 569/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2724658250808716\n",
      "##################################\n",
      "## EPOCH 570/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8977333307266235\n",
      "##################################\n",
      "## EPOCH 571/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4210020303726196\n",
      "##################################\n",
      "## EPOCH 572/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2506630420684814\n",
      "##################################\n",
      "## EPOCH 573/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.178446650505066\n",
      "##################################\n",
      "## EPOCH 574/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4900399446487427\n",
      "##################################\n",
      "## EPOCH 575/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.010953426361084\n",
      "##################################\n",
      "## EPOCH 576/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8511133193969727\n",
      "##################################\n",
      "## EPOCH 577/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3858795166015625\n",
      "##################################\n",
      "## EPOCH 578/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4394490718841553\n",
      "##################################\n",
      "## EPOCH 579/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9612752199172974\n",
      "##################################\n",
      "## EPOCH 580/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.40614914894104\n",
      "##################################\n",
      "## EPOCH 581/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1458818912506104\n",
      "##################################\n",
      "## EPOCH 582/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3083792924880981\n",
      "##################################\n",
      "## EPOCH 583/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7346827983856201\n",
      "##################################\n",
      "## EPOCH 584/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5181963443756104\n",
      "##################################\n",
      "## EPOCH 585/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3837134838104248\n",
      "##################################\n",
      "## EPOCH 586/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8042160272598267\n",
      "##################################\n",
      "## EPOCH 587/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.17671799659729\n",
      "##################################\n",
      "## EPOCH 588/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.361243486404419\n",
      "##################################\n",
      "## EPOCH 589/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3008196353912354\n",
      "##################################\n",
      "## EPOCH 590/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9434110522270203\n",
      "##################################\n",
      "## EPOCH 591/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9438930749893188\n",
      "##################################\n",
      "## EPOCH 592/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3329050540924072\n",
      "##################################\n",
      "## EPOCH 593/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7626983523368835\n",
      "##################################\n",
      "## EPOCH 594/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5526349544525146\n",
      "##################################\n",
      "## EPOCH 595/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5506727695465088\n",
      "##################################\n",
      "## EPOCH 596/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1969492435455322\n",
      "##################################\n",
      "## EPOCH 597/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7760878205299377\n",
      "##################################\n",
      "## EPOCH 598/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1102983951568604\n",
      "##################################\n",
      "## EPOCH 599/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0119273662567139\n",
      "##################################\n",
      "## EPOCH 600/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0568310022354126\n",
      "##################################\n",
      "## EPOCH 601/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6149202585220337\n",
      "##################################\n",
      "## EPOCH 602/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0333173274993896\n",
      "##################################\n",
      "## EPOCH 603/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1591180562973022\n",
      "##################################\n",
      "## EPOCH 604/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5602004528045654\n",
      "##################################\n",
      "## EPOCH 605/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7041146755218506\n",
      "##################################\n",
      "## EPOCH 606/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8895577192306519\n",
      "##################################\n",
      "## EPOCH 607/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8395625352859497\n",
      "##################################\n",
      "## EPOCH 608/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9918841123580933\n",
      "##################################\n",
      "## EPOCH 609/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9845620393753052\n",
      "##################################\n",
      "## EPOCH 610/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4000270366668701\n",
      "##################################\n",
      "## EPOCH 611/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6640156507492065\n",
      "##################################\n",
      "## EPOCH 612/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.293609380722046\n",
      "##################################\n",
      "## EPOCH 613/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9525715112686157\n",
      "##################################\n",
      "## EPOCH 614/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.048994779586792\n",
      "##################################\n",
      "## EPOCH 615/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6104462146759033\n",
      "##################################\n",
      "## EPOCH 616/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8571200370788574\n",
      "##################################\n",
      "## EPOCH 617/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.783971905708313\n",
      "##################################\n",
      "## EPOCH 618/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6545720100402832\n",
      "##################################\n",
      "## EPOCH 619/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0275936126708984\n",
      "##################################\n",
      "## EPOCH 620/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9155737161636353\n",
      "##################################\n",
      "## EPOCH 621/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7033456563949585\n",
      "##################################\n",
      "## EPOCH 622/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1680481433868408\n",
      "##################################\n",
      "## EPOCH 623/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.41304320096969604\n",
      "##################################\n",
      "## EPOCH 624/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2979319095611572\n",
      "##################################\n",
      "## EPOCH 625/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8081633448600769\n",
      "##################################\n",
      "## EPOCH 626/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9422239065170288\n",
      "##################################\n",
      "## EPOCH 627/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.621748149394989\n",
      "##################################\n",
      "## EPOCH 628/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2872660160064697\n",
      "##################################\n",
      "## EPOCH 629/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4811551570892334\n",
      "##################################\n",
      "## EPOCH 630/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8676062822341919\n",
      "##################################\n",
      "## EPOCH 631/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.192832350730896\n",
      "##################################\n",
      "## EPOCH 632/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.37967953085899353\n",
      "##################################\n",
      "## EPOCH 633/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7312890291213989\n",
      "##################################\n",
      "## EPOCH 634/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6290405988693237\n",
      "##################################\n",
      "## EPOCH 635/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.47776347398757935\n",
      "##################################\n",
      "## EPOCH 636/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5271732807159424\n",
      "##################################\n",
      "## EPOCH 637/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0981982946395874\n",
      "##################################\n",
      "## EPOCH 638/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5954900979995728\n",
      "##################################\n",
      "## EPOCH 639/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7328882813453674\n",
      "##################################\n",
      "## EPOCH 640/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.991642951965332\n",
      "##################################\n",
      "## EPOCH 641/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7301181554794312\n",
      "##################################\n",
      "## EPOCH 642/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0146456956863403\n",
      "##################################\n",
      "## EPOCH 643/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6972338557243347\n",
      "##################################\n",
      "## EPOCH 644/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9575610160827637\n",
      "##################################\n",
      "## EPOCH 645/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0210069417953491\n",
      "##################################\n",
      "## EPOCH 646/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8705818057060242\n",
      "##################################\n",
      "## EPOCH 647/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7030870318412781\n",
      "##################################\n",
      "## EPOCH 648/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.809390664100647\n",
      "##################################\n",
      "## EPOCH 649/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1588234901428223\n",
      "##################################\n",
      "## EPOCH 650/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4735869765281677\n",
      "##################################\n",
      "## EPOCH 651/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7272342443466187\n",
      "##################################\n",
      "## EPOCH 652/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7919150590896606\n",
      "##################################\n",
      "## EPOCH 653/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6614640355110168\n",
      "##################################\n",
      "## EPOCH 654/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5322611331939697\n",
      "##################################\n",
      "## EPOCH 655/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4412573277950287\n",
      "##################################\n",
      "## EPOCH 656/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.971954345703125\n",
      "##################################\n",
      "## EPOCH 657/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3877098560333252\n",
      "##################################\n",
      "## EPOCH 658/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8039401173591614\n",
      "##################################\n",
      "## EPOCH 659/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5140491724014282\n",
      "##################################\n",
      "## EPOCH 660/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.4192579388618469\n",
      "##################################\n",
      "## EPOCH 661/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5959655046463013\n",
      "##################################\n",
      "## EPOCH 662/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4988740086555481\n",
      "##################################\n",
      "## EPOCH 663/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6311009526252747\n",
      "##################################\n",
      "## EPOCH 664/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.48495665192604065\n",
      "##################################\n",
      "## EPOCH 665/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5438826084136963\n",
      "##################################\n",
      "## EPOCH 666/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4076942503452301\n",
      "##################################\n",
      "## EPOCH 667/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8353234529495239\n",
      "##################################\n",
      "## EPOCH 668/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.45625343918800354\n",
      "##################################\n",
      "## EPOCH 669/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6263553500175476\n",
      "##################################\n",
      "## EPOCH 670/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5825140476226807\n",
      "##################################\n",
      "## EPOCH 671/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4413661062717438\n",
      "##################################\n",
      "## EPOCH 672/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7941595911979675\n",
      "##################################\n",
      "## EPOCH 673/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2223050892353058\n",
      "##################################\n",
      "## EPOCH 674/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7877503633499146\n",
      "##################################\n",
      "## EPOCH 675/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7610309720039368\n",
      "##################################\n",
      "## EPOCH 676/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2721117436885834\n",
      "##################################\n",
      "## EPOCH 677/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5296646356582642\n",
      "##################################\n",
      "## EPOCH 678/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1632577180862427\n",
      "##################################\n",
      "## EPOCH 679/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7790794968605042\n",
      "##################################\n",
      "## EPOCH 680/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.534546971321106\n",
      "##################################\n",
      "## EPOCH 681/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5521106123924255\n",
      "##################################\n",
      "## EPOCH 682/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8281728625297546\n",
      "##################################\n",
      "## EPOCH 683/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.30066654086112976\n",
      "##################################\n",
      "## EPOCH 684/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.711821436882019\n",
      "##################################\n",
      "## EPOCH 685/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5184914469718933\n",
      "##################################\n",
      "## EPOCH 686/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9748263359069824\n",
      "##################################\n",
      "## EPOCH 687/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5185608863830566\n",
      "##################################\n",
      "## EPOCH 688/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0914767980575562\n",
      "##################################\n",
      "## EPOCH 689/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22604624927043915\n",
      "##################################\n",
      "## EPOCH 690/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.771409809589386\n",
      "##################################\n",
      "## EPOCH 691/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6472169160842896\n",
      "##################################\n",
      "## EPOCH 692/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6595870852470398\n",
      "##################################\n",
      "## EPOCH 693/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4141332507133484\n",
      "##################################\n",
      "## EPOCH 694/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.942827582359314\n",
      "##################################\n",
      "## EPOCH 695/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.469384104013443\n",
      "##################################\n",
      "## EPOCH 696/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6148843169212341\n",
      "##################################\n",
      "## EPOCH 697/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6510339975357056\n",
      "##################################\n",
      "## EPOCH 698/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9752521514892578\n",
      "##################################\n",
      "## EPOCH 699/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6169558763504028\n",
      "##################################\n",
      "## EPOCH 700/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.210914969444275\n",
      "##################################\n",
      "## EPOCH 701/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8670962452888489\n",
      "##################################\n",
      "## EPOCH 702/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3605303466320038\n",
      "##################################\n",
      "## EPOCH 703/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.37418490648269653\n",
      "##################################\n",
      "## EPOCH 704/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5078204870223999\n",
      "##################################\n",
      "## EPOCH 705/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5223876237869263\n",
      "##################################\n",
      "## EPOCH 706/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4629802107810974\n",
      "##################################\n",
      "## EPOCH 707/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27585113048553467\n",
      "##################################\n",
      "## EPOCH 708/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.37068626284599304\n",
      "##################################\n",
      "## EPOCH 709/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3142852783203125\n",
      "##################################\n",
      "## EPOCH 710/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.35040855407714844\n",
      "##################################\n",
      "## EPOCH 711/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3045540153980255\n",
      "##################################\n",
      "## EPOCH 712/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3819773197174072\n",
      "##################################\n",
      "## EPOCH 713/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5876610279083252\n",
      "##################################\n",
      "## EPOCH 714/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21192386746406555\n",
      "##################################\n",
      "## EPOCH 715/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3606472313404083\n",
      "##################################\n",
      "## EPOCH 716/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6200460195541382\n",
      "##################################\n",
      "## EPOCH 717/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7593012452125549\n",
      "##################################\n",
      "## EPOCH 718/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3262811005115509\n",
      "##################################\n",
      "## EPOCH 719/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9178274869918823\n",
      "##################################\n",
      "## EPOCH 720/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8337922096252441\n",
      "##################################\n",
      "## EPOCH 721/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2601228356361389\n",
      "##################################\n",
      "## EPOCH 722/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.39925819635391235\n",
      "##################################\n",
      "## EPOCH 723/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3194713294506073\n",
      "##################################\n",
      "## EPOCH 724/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.43075627088546753\n",
      "##################################\n",
      "## EPOCH 725/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.40721410512924194\n",
      "##################################\n",
      "## EPOCH 726/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.44779330492019653\n",
      "##################################\n",
      "## EPOCH 727/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.614859938621521\n",
      "##################################\n",
      "## EPOCH 728/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15561535954475403\n",
      "##################################\n",
      "## EPOCH 729/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.30049505829811096\n",
      "##################################\n",
      "## EPOCH 730/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15234604477882385\n",
      "##################################\n",
      "## EPOCH 731/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.45892462134361267\n",
      "##################################\n",
      "## EPOCH 732/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21471914649009705\n",
      "##################################\n",
      "## EPOCH 733/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.24281246960163116\n",
      "##################################\n",
      "## EPOCH 734/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5260205268859863\n",
      "##################################\n",
      "## EPOCH 735/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.26873162388801575\n",
      "##################################\n",
      "## EPOCH 736/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20869970321655273\n",
      "##################################\n",
      "## EPOCH 737/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0108479261398315\n",
      "##################################\n",
      "## EPOCH 738/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4117599129676819\n",
      "##################################\n",
      "## EPOCH 739/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.30948740243911743\n",
      "##################################\n",
      "## EPOCH 740/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2439245730638504\n",
      "##################################\n",
      "## EPOCH 741/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.46656379103660583\n",
      "##################################\n",
      "## EPOCH 742/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23309370875358582\n",
      "##################################\n",
      "## EPOCH 743/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3383541703224182\n",
      "##################################\n",
      "## EPOCH 744/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3246518075466156\n",
      "##################################\n",
      "## EPOCH 745/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15345525741577148\n",
      "##################################\n",
      "## EPOCH 746/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11220693588256836\n",
      "##################################\n",
      "## EPOCH 747/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20766201615333557\n",
      "##################################\n",
      "## EPOCH 748/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6066213846206665\n",
      "##################################\n",
      "## EPOCH 749/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21811263263225555\n",
      "##################################\n",
      "## EPOCH 750/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2559032142162323\n",
      "##################################\n",
      "## EPOCH 751/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.28030282258987427\n",
      "##################################\n",
      "## EPOCH 752/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.30406254529953003\n",
      "##################################\n",
      "## EPOCH 753/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4555651545524597\n",
      "##################################\n",
      "## EPOCH 754/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4777890145778656\n",
      "##################################\n",
      "## EPOCH 755/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23948915302753448\n",
      "##################################\n",
      "## EPOCH 756/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.38485145568847656\n",
      "##################################\n",
      "## EPOCH 757/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3485492169857025\n",
      "##################################\n",
      "## EPOCH 758/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22091981768608093\n",
      "##################################\n",
      "## EPOCH 759/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2884462773799896\n",
      "##################################\n",
      "## EPOCH 760/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3161090016365051\n",
      "##################################\n",
      "## EPOCH 761/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3650871813297272\n",
      "##################################\n",
      "## EPOCH 762/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09375859797000885\n",
      "##################################\n",
      "## EPOCH 763/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20843505859375\n",
      "##################################\n",
      "## EPOCH 764/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2680688202381134\n",
      "##################################\n",
      "## EPOCH 765/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.30863696336746216\n",
      "##################################\n",
      "## EPOCH 766/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13796061277389526\n",
      "##################################\n",
      "## EPOCH 767/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17781846225261688\n",
      "##################################\n",
      "## EPOCH 768/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2814699709415436\n",
      "##################################\n",
      "## EPOCH 769/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4723089635372162\n",
      "##################################\n",
      "## EPOCH 770/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1607189029455185\n",
      "##################################\n",
      "## EPOCH 771/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.485828697681427\n",
      "##################################\n",
      "## EPOCH 772/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21106679737567902\n",
      "##################################\n",
      "## EPOCH 773/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5712671875953674\n",
      "##################################\n",
      "## EPOCH 774/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3258158564567566\n",
      "##################################\n",
      "## EPOCH 775/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4484402537345886\n",
      "##################################\n",
      "## EPOCH 776/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16262038052082062\n",
      "##################################\n",
      "## EPOCH 777/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21851401031017303\n",
      "##################################\n",
      "## EPOCH 778/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.29336968064308167\n",
      "##################################\n",
      "## EPOCH 779/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6845381855964661\n",
      "##################################\n",
      "## EPOCH 780/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.34460577368736267\n",
      "##################################\n",
      "## EPOCH 781/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2725420296192169\n",
      "##################################\n",
      "## EPOCH 782/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2756248414516449\n",
      "##################################\n",
      "## EPOCH 783/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.386929452419281\n",
      "##################################\n",
      "## EPOCH 784/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.259137362241745\n",
      "##################################\n",
      "## EPOCH 785/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14034713804721832\n",
      "##################################\n",
      "## EPOCH 786/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09560251981019974\n",
      "##################################\n",
      "## EPOCH 787/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5185105204582214\n",
      "##################################\n",
      "## EPOCH 788/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2002830058336258\n",
      "##################################\n",
      "## EPOCH 789/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23318791389465332\n",
      "##################################\n",
      "## EPOCH 790/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2510177493095398\n",
      "##################################\n",
      "## EPOCH 791/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1100383996963501\n",
      "##################################\n",
      "## EPOCH 792/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2885948121547699\n",
      "##################################\n",
      "## EPOCH 793/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07298644632101059\n",
      "##################################\n",
      "## EPOCH 794/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10419011116027832\n",
      "##################################\n",
      "## EPOCH 795/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15731942653656006\n",
      "##################################\n",
      "## EPOCH 796/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21611161530017853\n",
      "##################################\n",
      "## EPOCH 797/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1339520364999771\n",
      "##################################\n",
      "## EPOCH 798/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16455838084220886\n",
      "##################################\n",
      "## EPOCH 799/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.24872605502605438\n",
      "##################################\n",
      "## EPOCH 800/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09119148552417755\n",
      "##################################\n",
      "## EPOCH 801/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1979721188545227\n",
      "##################################\n",
      "## EPOCH 802/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19282737374305725\n",
      "##################################\n",
      "## EPOCH 803/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1504264771938324\n",
      "##################################\n",
      "## EPOCH 804/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23324020206928253\n",
      "##################################\n",
      "## EPOCH 805/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13105174899101257\n",
      "##################################\n",
      "## EPOCH 806/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17338591814041138\n",
      "##################################\n",
      "## EPOCH 807/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11316487938165665\n",
      "##################################\n",
      "## EPOCH 808/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1648271083831787\n",
      "##################################\n",
      "## EPOCH 809/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22072017192840576\n",
      "##################################\n",
      "## EPOCH 810/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1799117773771286\n",
      "##################################\n",
      "## EPOCH 811/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.28905898332595825\n",
      "##################################\n",
      "## EPOCH 812/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05366991087794304\n",
      "##################################\n",
      "## EPOCH 813/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7769443988800049\n",
      "##################################\n",
      "## EPOCH 814/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09660385549068451\n",
      "##################################\n",
      "## EPOCH 815/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19894126057624817\n",
      "##################################\n",
      "## EPOCH 816/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12094561010599136\n",
      "##################################\n",
      "## EPOCH 817/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20044627785682678\n",
      "##################################\n",
      "## EPOCH 818/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11800640821456909\n",
      "##################################\n",
      "## EPOCH 819/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11559461057186127\n",
      "##################################\n",
      "## EPOCH 820/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0905509889125824\n",
      "##################################\n",
      "## EPOCH 821/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05262478068470955\n",
      "##################################\n",
      "## EPOCH 822/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6349650621414185\n",
      "##################################\n",
      "## EPOCH 823/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08347786962985992\n",
      "##################################\n",
      "## EPOCH 824/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.28719186782836914\n",
      "##################################\n",
      "## EPOCH 825/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17267796397209167\n",
      "##################################\n",
      "## EPOCH 826/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10379163920879364\n",
      "##################################\n",
      "## EPOCH 827/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22443225979804993\n",
      "##################################\n",
      "## EPOCH 828/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11806714534759521\n",
      "##################################\n",
      "## EPOCH 829/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22019056975841522\n",
      "##################################\n",
      "## EPOCH 830/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1386265605688095\n",
      "##################################\n",
      "## EPOCH 831/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07863207161426544\n",
      "##################################\n",
      "## EPOCH 832/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11054066568613052\n",
      "##################################\n",
      "## EPOCH 833/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3666723370552063\n",
      "##################################\n",
      "## EPOCH 834/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10489390045404434\n",
      "##################################\n",
      "## EPOCH 835/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3224436342716217\n",
      "##################################\n",
      "## EPOCH 836/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1950652301311493\n",
      "##################################\n",
      "## EPOCH 837/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.0817193016409874\n",
      "##################################\n",
      "## EPOCH 838/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.36309072375297546\n",
      "##################################\n",
      "## EPOCH 839/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2744331359863281\n",
      "##################################\n",
      "## EPOCH 840/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06898389756679535\n",
      "##################################\n",
      "## EPOCH 841/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18534395098686218\n",
      "##################################\n",
      "## EPOCH 842/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3285239338874817\n",
      "##################################\n",
      "## EPOCH 843/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05330870673060417\n",
      "##################################\n",
      "## EPOCH 844/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12677273154258728\n",
      "##################################\n",
      "## EPOCH 845/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10209088027477264\n",
      "##################################\n",
      "## EPOCH 846/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.126796156167984\n",
      "##################################\n",
      "## EPOCH 847/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22549481689929962\n",
      "##################################\n",
      "## EPOCH 848/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1117606908082962\n",
      "##################################\n",
      "## EPOCH 849/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1382761299610138\n",
      "##################################\n",
      "## EPOCH 850/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5953851342201233\n",
      "##################################\n",
      "## EPOCH 851/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.31530100107192993\n",
      "##################################\n",
      "## EPOCH 852/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12007585912942886\n",
      "##################################\n",
      "## EPOCH 853/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07312723249197006\n",
      "##################################\n",
      "## EPOCH 854/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09910832345485687\n",
      "##################################\n",
      "## EPOCH 855/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13405664265155792\n",
      "##################################\n",
      "## EPOCH 856/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21835598349571228\n",
      "##################################\n",
      "## EPOCH 857/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07440106570720673\n",
      "##################################\n",
      "## EPOCH 858/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08921642601490021\n",
      "##################################\n",
      "## EPOCH 859/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1354445517063141\n",
      "##################################\n",
      "## EPOCH 860/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4531582295894623\n",
      "##################################\n",
      "## EPOCH 861/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.24288947880268097\n",
      "##################################\n",
      "## EPOCH 862/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.48073631525039673\n",
      "##################################\n",
      "## EPOCH 863/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20382539927959442\n",
      "##################################\n",
      "## EPOCH 864/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16492731869220734\n",
      "##################################\n",
      "## EPOCH 865/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07609385997056961\n",
      "##################################\n",
      "## EPOCH 866/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14985302090644836\n",
      "##################################\n",
      "## EPOCH 867/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14806534349918365\n",
      "##################################\n",
      "## EPOCH 868/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09830005466938019\n",
      "##################################\n",
      "## EPOCH 869/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15017229318618774\n",
      "##################################\n",
      "## EPOCH 870/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13541504740715027\n",
      "##################################\n",
      "## EPOCH 871/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10130532830953598\n",
      "##################################\n",
      "## EPOCH 872/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3932054042816162\n",
      "##################################\n",
      "## EPOCH 873/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04775163158774376\n",
      "##################################\n",
      "## EPOCH 874/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1292024850845337\n",
      "##################################\n",
      "## EPOCH 875/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19906239211559296\n",
      "##################################\n",
      "## EPOCH 876/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1082693487405777\n",
      "##################################\n",
      "## EPOCH 877/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22087225317955017\n",
      "##################################\n",
      "## EPOCH 878/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23823384940624237\n",
      "##################################\n",
      "## EPOCH 879/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09944619238376617\n",
      "##################################\n",
      "## EPOCH 880/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07291728258132935\n",
      "##################################\n",
      "## EPOCH 881/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0747390165925026\n",
      "##################################\n",
      "## EPOCH 882/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23408539593219757\n",
      "##################################\n",
      "## EPOCH 883/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5183646082878113\n",
      "##################################\n",
      "## EPOCH 884/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14065834879875183\n",
      "##################################\n",
      "## EPOCH 885/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12591971457004547\n",
      "##################################\n",
      "## EPOCH 886/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07078897953033447\n",
      "##################################\n",
      "## EPOCH 887/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.292229562997818\n",
      "##################################\n",
      "## EPOCH 888/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16983281075954437\n",
      "##################################\n",
      "## EPOCH 889/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12451343238353729\n",
      "##################################\n",
      "## EPOCH 890/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9167352914810181\n",
      "##################################\n",
      "## EPOCH 891/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21690186858177185\n",
      "##################################\n",
      "## EPOCH 892/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1662372648715973\n",
      "##################################\n",
      "## EPOCH 893/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.227569580078125\n",
      "##################################\n",
      "## EPOCH 894/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1780177652835846\n",
      "##################################\n",
      "## EPOCH 895/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2186560332775116\n",
      "##################################\n",
      "## EPOCH 896/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.13128350675106049\n",
      "##################################\n",
      "## EPOCH 897/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11393831670284271\n",
      "##################################\n",
      "## EPOCH 898/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2619298994541168\n",
      "##################################\n",
      "## EPOCH 899/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19198258221149445\n",
      "##################################\n",
      "## EPOCH 900/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12053124606609344\n",
      "##################################\n",
      "## EPOCH 901/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1643684208393097\n",
      "##################################\n",
      "## EPOCH 902/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09937892854213715\n",
      "##################################\n",
      "## EPOCH 903/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17573301494121552\n",
      "##################################\n",
      "## EPOCH 904/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1256764680147171\n",
      "##################################\n",
      "## EPOCH 905/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13832882046699524\n",
      "##################################\n",
      "## EPOCH 906/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17989584803581238\n",
      "##################################\n",
      "## EPOCH 907/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1518286168575287\n",
      "##################################\n",
      "## EPOCH 908/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.24554772675037384\n",
      "##################################\n",
      "## EPOCH 909/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08353395760059357\n",
      "##################################\n",
      "## EPOCH 910/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.082781121134758\n",
      "##################################\n",
      "## EPOCH 911/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08459018170833588\n",
      "##################################\n",
      "## EPOCH 912/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04616771638393402\n",
      "##################################\n",
      "## EPOCH 913/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10053294897079468\n",
      "##################################\n",
      "## EPOCH 914/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13910821080207825\n",
      "##################################\n",
      "## EPOCH 915/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.040256280452013016\n",
      "##################################\n",
      "## EPOCH 916/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07207191735506058\n",
      "##################################\n",
      "## EPOCH 917/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09676916897296906\n",
      "##################################\n",
      "## EPOCH 918/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.28188228607177734\n",
      "##################################\n",
      "## EPOCH 919/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13800479471683502\n",
      "##################################\n",
      "## EPOCH 920/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22888712584972382\n",
      "##################################\n",
      "## EPOCH 921/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0545925498008728\n",
      "##################################\n",
      "## EPOCH 922/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10159330070018768\n",
      "##################################\n",
      "## EPOCH 923/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09595552831888199\n",
      "##################################\n",
      "## EPOCH 924/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05962713807821274\n",
      "##################################\n",
      "## EPOCH 925/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17446771264076233\n",
      "##################################\n",
      "## EPOCH 926/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.056050874292850494\n",
      "##################################\n",
      "## EPOCH 927/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16541361808776855\n",
      "##################################\n",
      "## EPOCH 928/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04493558034300804\n",
      "##################################\n",
      "## EPOCH 929/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15056756138801575\n",
      "##################################\n",
      "## EPOCH 930/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15437570214271545\n",
      "##################################\n",
      "## EPOCH 931/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05119933933019638\n",
      "##################################\n",
      "## EPOCH 932/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08139186352491379\n",
      "##################################\n",
      "## EPOCH 933/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020975559949874878\n",
      "##################################\n",
      "## EPOCH 934/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11243639141321182\n",
      "##################################\n",
      "## EPOCH 935/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.041258711367845535\n",
      "##################################\n",
      "## EPOCH 936/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08000890910625458\n",
      "##################################\n",
      "## EPOCH 937/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08739857375621796\n",
      "##################################\n",
      "## EPOCH 938/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03789840266108513\n",
      "##################################\n",
      "## EPOCH 939/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06042799353599548\n",
      "##################################\n",
      "## EPOCH 940/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07665037363767624\n",
      "##################################\n",
      "## EPOCH 941/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1831895112991333\n",
      "##################################\n",
      "## EPOCH 942/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03915838897228241\n",
      "##################################\n",
      "## EPOCH 943/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07111333310604095\n",
      "##################################\n",
      "## EPOCH 944/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039575956761837006\n",
      "##################################\n",
      "## EPOCH 945/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05846782401204109\n",
      "##################################\n",
      "## EPOCH 946/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09005948901176453\n",
      "##################################\n",
      "## EPOCH 947/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08438614010810852\n",
      "##################################\n",
      "## EPOCH 948/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1467755287885666\n",
      "##################################\n",
      "## EPOCH 949/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05748777464032173\n",
      "##################################\n",
      "## EPOCH 950/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05496782809495926\n",
      "##################################\n",
      "## EPOCH 951/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.041890256106853485\n",
      "##################################\n",
      "## EPOCH 952/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07938717305660248\n",
      "##################################\n",
      "## EPOCH 953/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025649230927228928\n",
      "##################################\n",
      "## EPOCH 954/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10912191867828369\n",
      "##################################\n",
      "## EPOCH 955/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.0243825800716877\n",
      "##################################\n",
      "## EPOCH 956/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.029271453619003296\n",
      "##################################\n",
      "## EPOCH 957/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03289567306637764\n",
      "##################################\n",
      "## EPOCH 958/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.35994797945022583\n",
      "##################################\n",
      "## EPOCH 959/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06422159820795059\n",
      "##################################\n",
      "## EPOCH 960/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3604097068309784\n",
      "##################################\n",
      "## EPOCH 961/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07850762456655502\n",
      "##################################\n",
      "## EPOCH 962/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05078188329935074\n",
      "##################################\n",
      "## EPOCH 963/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.061624784022569656\n",
      "##################################\n",
      "## EPOCH 964/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08170278370380402\n",
      "##################################\n",
      "## EPOCH 965/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0871206670999527\n",
      "##################################\n",
      "## EPOCH 966/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04722994938492775\n",
      "##################################\n",
      "## EPOCH 967/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.035363540053367615\n",
      "##################################\n",
      "## EPOCH 968/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04535406455397606\n",
      "##################################\n",
      "## EPOCH 969/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1410253793001175\n",
      "##################################\n",
      "## EPOCH 970/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09595421701669693\n",
      "##################################\n",
      "## EPOCH 971/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05421201512217522\n",
      "##################################\n",
      "## EPOCH 972/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05680277943611145\n",
      "##################################\n",
      "## EPOCH 973/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06480121612548828\n",
      "##################################\n",
      "## EPOCH 974/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.040239062160253525\n",
      "##################################\n",
      "## EPOCH 975/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1472785770893097\n",
      "##################################\n",
      "## EPOCH 976/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07465174794197083\n",
      "##################################\n",
      "## EPOCH 977/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04805222898721695\n",
      "##################################\n",
      "## EPOCH 978/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.057719260454177856\n",
      "##################################\n",
      "## EPOCH 979/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0781838521361351\n",
      "##################################\n",
      "## EPOCH 980/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05414215475320816\n",
      "##################################\n",
      "## EPOCH 981/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05661427974700928\n",
      "##################################\n",
      "## EPOCH 982/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04507327824831009\n",
      "##################################\n",
      "## EPOCH 983/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028987165540456772\n",
      "##################################\n",
      "## EPOCH 984/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0774717852473259\n",
      "##################################\n",
      "## EPOCH 985/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04891621693968773\n",
      "##################################\n",
      "## EPOCH 986/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27643531560897827\n",
      "##################################\n",
      "## EPOCH 987/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03845769166946411\n",
      "##################################\n",
      "## EPOCH 988/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03795953840017319\n",
      "##################################\n",
      "## EPOCH 989/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0352536216378212\n",
      "##################################\n",
      "## EPOCH 990/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04778200015425682\n",
      "##################################\n",
      "## EPOCH 991/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23209676146507263\n",
      "##################################\n",
      "## EPOCH 992/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09362058341503143\n",
      "##################################\n",
      "## EPOCH 993/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08357657492160797\n",
      "##################################\n",
      "## EPOCH 994/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.058582842350006104\n",
      "##################################\n",
      "## EPOCH 995/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5158820152282715\n",
      "##################################\n",
      "## EPOCH 996/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1042165756225586\n",
      "##################################\n",
      "## EPOCH 997/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06047016382217407\n",
      "##################################\n",
      "## EPOCH 998/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05421634763479233\n",
      "##################################\n",
      "## EPOCH 999/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04434477537870407\n",
      "##################################\n",
      "## EPOCH 1000/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06423763185739517\n",
      "##################################\n",
      "## EPOCH 1001/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09723879396915436\n",
      "##################################\n",
      "## EPOCH 1002/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04299592226743698\n",
      "##################################\n",
      "## EPOCH 1003/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05202460289001465\n",
      "##################################\n",
      "## EPOCH 1004/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09210015833377838\n",
      "##################################\n",
      "## EPOCH 1005/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1752437800168991\n",
      "##################################\n",
      "## EPOCH 1006/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0657818615436554\n",
      "##################################\n",
      "## EPOCH 1007/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05443546921014786\n",
      "##################################\n",
      "## EPOCH 1008/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2884174883365631\n",
      "##################################\n",
      "## EPOCH 1009/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08768358081579208\n",
      "##################################\n",
      "## EPOCH 1010/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07980180531740189\n",
      "##################################\n",
      "## EPOCH 1011/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05881551653146744\n",
      "##################################\n",
      "## EPOCH 1012/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14138910174369812\n",
      "##################################\n",
      "## EPOCH 1013/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23718614876270294\n",
      "##################################\n",
      "## EPOCH 1014/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.08340305835008621\n",
      "##################################\n",
      "## EPOCH 1015/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09831524640321732\n",
      "##################################\n",
      "## EPOCH 1016/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09641969203948975\n",
      "##################################\n",
      "## EPOCH 1017/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.051516342908144\n",
      "##################################\n",
      "## EPOCH 1018/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03722304850816727\n",
      "##################################\n",
      "## EPOCH 1019/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05077679082751274\n",
      "##################################\n",
      "## EPOCH 1020/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15680964291095734\n",
      "##################################\n",
      "## EPOCH 1021/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1835838407278061\n",
      "##################################\n",
      "## EPOCH 1022/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1171661838889122\n",
      "##################################\n",
      "## EPOCH 1023/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02571270987391472\n",
      "##################################\n",
      "## EPOCH 1024/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05620453506708145\n",
      "##################################\n",
      "## EPOCH 1025/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12170398235321045\n",
      "##################################\n",
      "## EPOCH 1026/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05045796558260918\n",
      "##################################\n",
      "## EPOCH 1027/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05722900480031967\n",
      "##################################\n",
      "## EPOCH 1028/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04038005322217941\n",
      "##################################\n",
      "## EPOCH 1029/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03775380924344063\n",
      "##################################\n",
      "## EPOCH 1030/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08129535615444183\n",
      "##################################\n",
      "## EPOCH 1031/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07966738194227219\n",
      "##################################\n",
      "## EPOCH 1032/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04740690439939499\n",
      "##################################\n",
      "## EPOCH 1033/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04519597813487053\n",
      "##################################\n",
      "## EPOCH 1034/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.049614664167165756\n",
      "##################################\n",
      "## EPOCH 1035/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.32249146699905396\n",
      "##################################\n",
      "## EPOCH 1036/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04114240035414696\n",
      "##################################\n",
      "## EPOCH 1037/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028501618653535843\n",
      "##################################\n",
      "## EPOCH 1038/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05475820228457451\n",
      "##################################\n",
      "## EPOCH 1039/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10911989212036133\n",
      "##################################\n",
      "## EPOCH 1040/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039376210421323776\n",
      "##################################\n",
      "## EPOCH 1041/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0764698013663292\n",
      "##################################\n",
      "## EPOCH 1042/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03478496894240379\n",
      "##################################\n",
      "## EPOCH 1043/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03989461809396744\n",
      "##################################\n",
      "## EPOCH 1044/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11128777265548706\n",
      "##################################\n",
      "## EPOCH 1045/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02803610824048519\n",
      "##################################\n",
      "## EPOCH 1046/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.038924019783735275\n",
      "##################################\n",
      "## EPOCH 1047/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05088958889245987\n",
      "##################################\n",
      "## EPOCH 1048/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05594084411859512\n",
      "##################################\n",
      "## EPOCH 1049/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.049071699380874634\n",
      "##################################\n",
      "## EPOCH 1050/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07846145331859589\n",
      "##################################\n",
      "## EPOCH 1051/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0440325066447258\n",
      "##################################\n",
      "## EPOCH 1052/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.043851543217897415\n",
      "##################################\n",
      "## EPOCH 1053/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07191415876150131\n",
      "##################################\n",
      "## EPOCH 1054/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04321948438882828\n",
      "##################################\n",
      "## EPOCH 1055/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.054232269525527954\n",
      "##################################\n",
      "## EPOCH 1056/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05437592417001724\n",
      "##################################\n",
      "## EPOCH 1057/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.035342033952474594\n",
      "##################################\n",
      "## EPOCH 1058/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024743003770709038\n",
      "##################################\n",
      "## EPOCH 1059/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03325410932302475\n",
      "##################################\n",
      "## EPOCH 1060/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04899459704756737\n",
      "##################################\n",
      "## EPOCH 1061/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04715537279844284\n",
      "##################################\n",
      "## EPOCH 1062/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20885410904884338\n",
      "##################################\n",
      "## EPOCH 1063/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14182066917419434\n",
      "##################################\n",
      "## EPOCH 1064/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03968612104654312\n",
      "##################################\n",
      "## EPOCH 1065/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.032381571829319\n",
      "##################################\n",
      "## EPOCH 1066/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03652363643050194\n",
      "##################################\n",
      "## EPOCH 1067/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04188964515924454\n",
      "##################################\n",
      "## EPOCH 1068/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.607044517993927\n",
      "##################################\n",
      "## EPOCH 1069/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07167569547891617\n",
      "##################################\n",
      "## EPOCH 1070/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03513328731060028\n",
      "##################################\n",
      "## EPOCH 1071/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07744629681110382\n",
      "##################################\n",
      "## EPOCH 1072/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.057631008327007294\n",
      "##################################\n",
      "## EPOCH 1073/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.03108702227473259\n",
      "##################################\n",
      "## EPOCH 1074/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04296967759728432\n",
      "##################################\n",
      "## EPOCH 1075/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11291539669036865\n",
      "##################################\n",
      "## EPOCH 1076/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023578858003020287\n",
      "##################################\n",
      "## EPOCH 1077/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6028234362602234\n",
      "##################################\n",
      "## EPOCH 1078/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06698460876941681\n",
      "##################################\n",
      "## EPOCH 1079/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04955080896615982\n",
      "##################################\n",
      "## EPOCH 1080/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12478826195001602\n",
      "##################################\n",
      "## EPOCH 1081/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05817444995045662\n",
      "##################################\n",
      "## EPOCH 1082/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024306291714310646\n",
      "##################################\n",
      "## EPOCH 1083/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09816733002662659\n",
      "##################################\n",
      "## EPOCH 1084/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06571541726589203\n",
      "##################################\n",
      "## EPOCH 1085/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.30166566371917725\n",
      "##################################\n",
      "## EPOCH 1086/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08802860975265503\n",
      "##################################\n",
      "## EPOCH 1087/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1246853619813919\n",
      "##################################\n",
      "## EPOCH 1088/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13504382967948914\n",
      "##################################\n",
      "## EPOCH 1089/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12575459480285645\n",
      "##################################\n",
      "## EPOCH 1090/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3045026659965515\n",
      "##################################\n",
      "## EPOCH 1091/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21221187710762024\n",
      "##################################\n",
      "## EPOCH 1092/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1294008195400238\n",
      "##################################\n",
      "## EPOCH 1093/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.060742177069187164\n",
      "##################################\n",
      "## EPOCH 1094/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05499301478266716\n",
      "##################################\n",
      "## EPOCH 1095/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19746437668800354\n",
      "##################################\n",
      "## EPOCH 1096/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.205919548869133\n",
      "##################################\n",
      "## EPOCH 1097/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07355918735265732\n",
      "##################################\n",
      "## EPOCH 1098/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09006325900554657\n",
      "##################################\n",
      "## EPOCH 1099/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08713208884000778\n",
      "##################################\n",
      "## EPOCH 1100/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1124972552061081\n",
      "##################################\n",
      "## EPOCH 1101/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13065412640571594\n",
      "##################################\n",
      "## EPOCH 1102/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09915141761302948\n",
      "##################################\n",
      "## EPOCH 1103/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08638841658830643\n",
      "##################################\n",
      "## EPOCH 1104/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2811398506164551\n",
      "##################################\n",
      "## EPOCH 1105/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0597689151763916\n",
      "##################################\n",
      "## EPOCH 1106/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12585973739624023\n",
      "##################################\n",
      "## EPOCH 1107/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06374402344226837\n",
      "##################################\n",
      "## EPOCH 1108/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2169022560119629\n",
      "##################################\n",
      "## EPOCH 1109/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4097670018672943\n",
      "##################################\n",
      "## EPOCH 1110/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13659578561782837\n",
      "##################################\n",
      "## EPOCH 1111/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12486644089221954\n",
      "##################################\n",
      "## EPOCH 1112/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.057054150849580765\n",
      "##################################\n",
      "## EPOCH 1113/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2763170599937439\n",
      "##################################\n",
      "## EPOCH 1114/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27327585220336914\n",
      "##################################\n",
      "## EPOCH 1115/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3162539303302765\n",
      "##################################\n",
      "## EPOCH 1116/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.24049608409404755\n",
      "##################################\n",
      "## EPOCH 1117/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.34499913454055786\n",
      "##################################\n",
      "## EPOCH 1118/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2880392372608185\n",
      "##################################\n",
      "## EPOCH 1119/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4653599262237549\n",
      "##################################\n",
      "## EPOCH 1120/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.24487149715423584\n",
      "##################################\n",
      "## EPOCH 1121/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2390604317188263\n",
      "##################################\n",
      "## EPOCH 1122/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7718356251716614\n",
      "##################################\n",
      "## EPOCH 1123/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22602248191833496\n",
      "##################################\n",
      "## EPOCH 1124/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3517548441886902\n",
      "##################################\n",
      "## EPOCH 1125/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14403945207595825\n",
      "##################################\n",
      "## EPOCH 1126/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.130274698138237\n",
      "##################################\n",
      "## EPOCH 1127/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.313010036945343\n",
      "##################################\n",
      "## EPOCH 1128/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6444767713546753\n",
      "##################################\n",
      "## EPOCH 1129/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19195973873138428\n",
      "##################################\n",
      "## EPOCH 1130/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27521854639053345\n",
      "##################################\n",
      "## EPOCH 1131/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5412937998771667\n",
      "##################################\n",
      "## EPOCH 1132/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.13616423308849335\n",
      "##################################\n",
      "## EPOCH 1133/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5981224775314331\n",
      "##################################\n",
      "## EPOCH 1134/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4068250060081482\n",
      "##################################\n",
      "## EPOCH 1135/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2482515573501587\n",
      "##################################\n",
      "## EPOCH 1136/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.29412898421287537\n",
      "##################################\n",
      "## EPOCH 1137/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2426864206790924\n",
      "##################################\n",
      "## EPOCH 1138/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.40270060300827026\n",
      "##################################\n",
      "## EPOCH 1139/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20255224406719208\n",
      "##################################\n",
      "## EPOCH 1140/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23895354568958282\n",
      "##################################\n",
      "## EPOCH 1141/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2976705729961395\n",
      "##################################\n",
      "## EPOCH 1142/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3119783103466034\n",
      "##################################\n",
      "## EPOCH 1143/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23347607254981995\n",
      "##################################\n",
      "## EPOCH 1144/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.37718188762664795\n",
      "##################################\n",
      "## EPOCH 1145/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.33929842710494995\n",
      "##################################\n",
      "## EPOCH 1146/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3893616795539856\n",
      "##################################\n",
      "## EPOCH 1147/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3435443043708801\n",
      "##################################\n",
      "## EPOCH 1148/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2540985345840454\n",
      "##################################\n",
      "## EPOCH 1149/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.33383452892303467\n",
      "##################################\n",
      "## EPOCH 1150/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1969159096479416\n",
      "##################################\n",
      "## EPOCH 1151/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2894269526004791\n",
      "##################################\n",
      "## EPOCH 1152/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22271163761615753\n",
      "##################################\n",
      "## EPOCH 1153/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2905213236808777\n",
      "##################################\n",
      "## EPOCH 1154/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.32991403341293335\n",
      "##################################\n",
      "## EPOCH 1155/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27532845735549927\n",
      "##################################\n",
      "## EPOCH 1156/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0875234380364418\n",
      "##################################\n",
      "## EPOCH 1157/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20989671349525452\n",
      "##################################\n",
      "## EPOCH 1158/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22624389827251434\n",
      "##################################\n",
      "## EPOCH 1159/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2890511155128479\n",
      "##################################\n",
      "## EPOCH 1160/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2112622708082199\n",
      "##################################\n",
      "## EPOCH 1161/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.323161780834198\n",
      "##################################\n",
      "## EPOCH 1162/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08441396802663803\n",
      "##################################\n",
      "## EPOCH 1163/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15148617327213287\n",
      "##################################\n",
      "## EPOCH 1164/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17512395977973938\n",
      "##################################\n",
      "## EPOCH 1165/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.153294637799263\n",
      "##################################\n",
      "## EPOCH 1166/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.43592995405197144\n",
      "##################################\n",
      "## EPOCH 1167/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1568930596113205\n",
      "##################################\n",
      "## EPOCH 1168/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20013508200645447\n",
      "##################################\n",
      "## EPOCH 1169/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10775723308324814\n",
      "##################################\n",
      "## EPOCH 1170/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16215428709983826\n",
      "##################################\n",
      "## EPOCH 1171/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23541787266731262\n",
      "##################################\n",
      "## EPOCH 1172/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.29781875014305115\n",
      "##################################\n",
      "## EPOCH 1173/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08280330896377563\n",
      "##################################\n",
      "## EPOCH 1174/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2847910225391388\n",
      "##################################\n",
      "## EPOCH 1175/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14020273089408875\n",
      "##################################\n",
      "## EPOCH 1176/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07406284660100937\n",
      "##################################\n",
      "## EPOCH 1177/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2670716941356659\n",
      "##################################\n",
      "## EPOCH 1178/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.056978993117809296\n",
      "##################################\n",
      "## EPOCH 1179/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1813693344593048\n",
      "##################################\n",
      "## EPOCH 1180/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2509773075580597\n",
      "##################################\n",
      "## EPOCH 1181/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08561738580465317\n",
      "##################################\n",
      "## EPOCH 1182/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05899714305996895\n",
      "##################################\n",
      "## EPOCH 1183/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17991706728935242\n",
      "##################################\n",
      "## EPOCH 1184/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09380298852920532\n",
      "##################################\n",
      "## EPOCH 1185/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0927840918302536\n",
      "##################################\n",
      "## EPOCH 1186/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08916713297367096\n",
      "##################################\n",
      "## EPOCH 1187/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0505683496594429\n",
      "##################################\n",
      "## EPOCH 1188/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039817843586206436\n",
      "##################################\n",
      "## EPOCH 1189/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1290869563817978\n",
      "##################################\n",
      "## EPOCH 1190/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17483453452587128\n",
      "##################################\n",
      "## EPOCH 1191/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.06208556145429611\n",
      "##################################\n",
      "## EPOCH 1192/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07802566885948181\n",
      "##################################\n",
      "## EPOCH 1193/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05546460673213005\n",
      "##################################\n",
      "## EPOCH 1194/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.37142127752304077\n",
      "##################################\n",
      "## EPOCH 1195/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5766283273696899\n",
      "##################################\n",
      "## EPOCH 1196/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17269445955753326\n",
      "##################################\n",
      "## EPOCH 1197/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.032972302287817\n",
      "##################################\n",
      "## EPOCH 1198/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06513115763664246\n",
      "##################################\n",
      "## EPOCH 1199/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16893894970417023\n",
      "##################################\n",
      "## EPOCH 1200/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19328062236309052\n",
      "##################################\n",
      "## EPOCH 1201/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03657680004835129\n",
      "##################################\n",
      "## EPOCH 1202/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.060048967599868774\n",
      "##################################\n",
      "## EPOCH 1203/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12571731209754944\n",
      "##################################\n",
      "## EPOCH 1204/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2437244951725006\n",
      "##################################\n",
      "## EPOCH 1205/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11895010620355606\n",
      "##################################\n",
      "## EPOCH 1206/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7212489247322083\n",
      "##################################\n",
      "## EPOCH 1207/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14858826994895935\n",
      "##################################\n",
      "## EPOCH 1208/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14398957788944244\n",
      "##################################\n",
      "## EPOCH 1209/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03765261545777321\n",
      "##################################\n",
      "## EPOCH 1210/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0693918764591217\n",
      "##################################\n",
      "## EPOCH 1211/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.057394374161958694\n",
      "##################################\n",
      "## EPOCH 1212/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09868905693292618\n",
      "##################################\n",
      "## EPOCH 1213/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1481652557849884\n",
      "##################################\n",
      "## EPOCH 1214/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1874723732471466\n",
      "##################################\n",
      "## EPOCH 1215/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4439376890659332\n",
      "##################################\n",
      "## EPOCH 1216/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10536907613277435\n",
      "##################################\n",
      "## EPOCH 1217/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08626134693622589\n",
      "##################################\n",
      "## EPOCH 1218/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07968076318502426\n",
      "##################################\n",
      "## EPOCH 1219/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05249016731977463\n",
      "##################################\n",
      "## EPOCH 1220/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08512868732213974\n",
      "##################################\n",
      "## EPOCH 1221/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07117876410484314\n",
      "##################################\n",
      "## EPOCH 1222/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.060594718903303146\n",
      "##################################\n",
      "## EPOCH 1223/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14164969325065613\n",
      "##################################\n",
      "## EPOCH 1224/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09069745242595673\n",
      "##################################\n",
      "## EPOCH 1225/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13548782467842102\n",
      "##################################\n",
      "## EPOCH 1226/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.108005091547966\n",
      "##################################\n",
      "## EPOCH 1227/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16241145133972168\n",
      "##################################\n",
      "## EPOCH 1228/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02467271313071251\n",
      "##################################\n",
      "## EPOCH 1229/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05018044263124466\n",
      "##################################\n",
      "## EPOCH 1230/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14144733548164368\n",
      "##################################\n",
      "## EPOCH 1231/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08924807608127594\n",
      "##################################\n",
      "## EPOCH 1232/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.022490516304969788\n",
      "##################################\n",
      "## EPOCH 1233/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039739228785037994\n",
      "##################################\n",
      "## EPOCH 1234/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10368812084197998\n",
      "##################################\n",
      "## EPOCH 1235/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13097955286502838\n",
      "##################################\n",
      "## EPOCH 1236/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05968402698636055\n",
      "##################################\n",
      "## EPOCH 1237/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2518688142299652\n",
      "##################################\n",
      "## EPOCH 1238/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.050535641610622406\n",
      "##################################\n",
      "## EPOCH 1239/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.035979755222797394\n",
      "##################################\n",
      "## EPOCH 1240/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1155831590294838\n",
      "##################################\n",
      "## EPOCH 1241/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.035422421991825104\n",
      "##################################\n",
      "## EPOCH 1242/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17840804159641266\n",
      "##################################\n",
      "## EPOCH 1243/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.031865693628787994\n",
      "##################################\n",
      "## EPOCH 1244/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3261920213699341\n",
      "##################################\n",
      "## EPOCH 1245/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.026621636003255844\n",
      "##################################\n",
      "## EPOCH 1246/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04376867040991783\n",
      "##################################\n",
      "## EPOCH 1247/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02727016806602478\n",
      "##################################\n",
      "## EPOCH 1248/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024165693670511246\n",
      "##################################\n",
      "## EPOCH 1249/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06951753050088882\n",
      "##################################\n",
      "## EPOCH 1250/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.03972078114748001\n",
      "##################################\n",
      "## EPOCH 1251/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2117583304643631\n",
      "##################################\n",
      "## EPOCH 1252/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.022031938657164574\n",
      "##################################\n",
      "## EPOCH 1253/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.046680234372615814\n",
      "##################################\n",
      "## EPOCH 1254/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08316444605588913\n",
      "##################################\n",
      "## EPOCH 1255/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03939235210418701\n",
      "##################################\n",
      "## EPOCH 1256/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028411924839019775\n",
      "##################################\n",
      "## EPOCH 1257/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030615221709012985\n",
      "##################################\n",
      "## EPOCH 1258/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.43826645612716675\n",
      "##################################\n",
      "## EPOCH 1259/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.048189930617809296\n",
      "##################################\n",
      "## EPOCH 1260/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030412638559937477\n",
      "##################################\n",
      "## EPOCH 1261/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018225936219096184\n",
      "##################################\n",
      "## EPOCH 1262/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09504611790180206\n",
      "##################################\n",
      "## EPOCH 1263/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06412051618099213\n",
      "##################################\n",
      "## EPOCH 1264/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02851184643805027\n",
      "##################################\n",
      "## EPOCH 1265/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.042509764432907104\n",
      "##################################\n",
      "## EPOCH 1266/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04157145693898201\n",
      "##################################\n",
      "## EPOCH 1267/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030489837750792503\n",
      "##################################\n",
      "## EPOCH 1268/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16655458509922028\n",
      "##################################\n",
      "## EPOCH 1269/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05417357757687569\n",
      "##################################\n",
      "## EPOCH 1270/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05550050735473633\n",
      "##################################\n",
      "## EPOCH 1271/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03462769091129303\n",
      "##################################\n",
      "## EPOCH 1272/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05115966871380806\n",
      "##################################\n",
      "## EPOCH 1273/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05285715311765671\n",
      "##################################\n",
      "## EPOCH 1274/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06309561431407928\n",
      "##################################\n",
      "## EPOCH 1275/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03721695393323898\n",
      "##################################\n",
      "## EPOCH 1276/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023649340495467186\n",
      "##################################\n",
      "## EPOCH 1277/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09087803959846497\n",
      "##################################\n",
      "## EPOCH 1278/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03131391480565071\n",
      "##################################\n",
      "## EPOCH 1279/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02405577339231968\n",
      "##################################\n",
      "## EPOCH 1280/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039033692330121994\n",
      "##################################\n",
      "## EPOCH 1281/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.031222209334373474\n",
      "##################################\n",
      "## EPOCH 1282/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06522504985332489\n",
      "##################################\n",
      "## EPOCH 1283/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017676007002592087\n",
      "##################################\n",
      "## EPOCH 1284/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02860758826136589\n",
      "##################################\n",
      "## EPOCH 1285/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01750721037387848\n",
      "##################################\n",
      "## EPOCH 1286/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.019251123070716858\n",
      "##################################\n",
      "## EPOCH 1287/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02062961831688881\n",
      "##################################\n",
      "## EPOCH 1288/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05202193185687065\n",
      "##################################\n",
      "## EPOCH 1289/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02711152657866478\n",
      "##################################\n",
      "## EPOCH 1290/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03744261711835861\n",
      "##################################\n",
      "## EPOCH 1291/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.031876176595687866\n",
      "##################################\n",
      "## EPOCH 1292/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021849628537893295\n",
      "##################################\n",
      "## EPOCH 1293/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02282686159014702\n",
      "##################################\n",
      "## EPOCH 1294/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018932826817035675\n",
      "##################################\n",
      "## EPOCH 1295/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014671827666461468\n",
      "##################################\n",
      "## EPOCH 1296/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01696632243692875\n",
      "##################################\n",
      "## EPOCH 1297/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12198330461978912\n",
      "##################################\n",
      "## EPOCH 1298/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.40560436248779297\n",
      "##################################\n",
      "## EPOCH 1299/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012956234626471996\n",
      "##################################\n",
      "## EPOCH 1300/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018109995871782303\n",
      "##################################\n",
      "## EPOCH 1301/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.022744271904230118\n",
      "##################################\n",
      "## EPOCH 1302/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01940932311117649\n",
      "##################################\n",
      "## EPOCH 1303/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1561584174633026\n",
      "##################################\n",
      "## EPOCH 1304/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039479777216911316\n",
      "##################################\n",
      "## EPOCH 1305/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.029521584510803223\n",
      "##################################\n",
      "## EPOCH 1306/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.042327094823122025\n",
      "##################################\n",
      "## EPOCH 1307/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1043131947517395\n",
      "##################################\n",
      "## EPOCH 1308/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.03785286098718643\n",
      "##################################\n",
      "## EPOCH 1309/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04530138522386551\n",
      "##################################\n",
      "## EPOCH 1310/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03587905690073967\n",
      "##################################\n",
      "## EPOCH 1311/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.019820336252450943\n",
      "##################################\n",
      "## EPOCH 1312/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024200668558478355\n",
      "##################################\n",
      "## EPOCH 1313/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03707712143659592\n",
      "##################################\n",
      "## EPOCH 1314/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028740322217345238\n",
      "##################################\n",
      "## EPOCH 1315/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0221051424741745\n",
      "##################################\n",
      "## EPOCH 1316/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03161786124110222\n",
      "##################################\n",
      "## EPOCH 1317/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.029698077589273453\n",
      "##################################\n",
      "## EPOCH 1318/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027778927236795425\n",
      "##################################\n",
      "## EPOCH 1319/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15954504907131195\n",
      "##################################\n",
      "## EPOCH 1320/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15265916287899017\n",
      "##################################\n",
      "## EPOCH 1321/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011158346198499203\n",
      "##################################\n",
      "## EPOCH 1322/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02274380996823311\n",
      "##################################\n",
      "## EPOCH 1323/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04232142120599747\n",
      "##################################\n",
      "## EPOCH 1324/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01256601046770811\n",
      "##################################\n",
      "## EPOCH 1325/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015069136396050453\n",
      "##################################\n",
      "## EPOCH 1326/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010347802191972733\n",
      "##################################\n",
      "## EPOCH 1327/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.037416864186525345\n",
      "##################################\n",
      "## EPOCH 1328/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017994845286011696\n",
      "##################################\n",
      "## EPOCH 1329/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02958134189248085\n",
      "##################################\n",
      "## EPOCH 1330/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03622129186987877\n",
      "##################################\n",
      "## EPOCH 1331/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013207798823714256\n",
      "##################################\n",
      "## EPOCH 1332/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020526021718978882\n",
      "##################################\n",
      "## EPOCH 1333/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03281030058860779\n",
      "##################################\n",
      "## EPOCH 1334/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01888146996498108\n",
      "##################################\n",
      "## EPOCH 1335/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.022364264354109764\n",
      "##################################\n",
      "## EPOCH 1336/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018261099234223366\n",
      "##################################\n",
      "## EPOCH 1337/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027633577585220337\n",
      "##################################\n",
      "## EPOCH 1338/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01897772215306759\n",
      "##################################\n",
      "## EPOCH 1339/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06639351695775986\n",
      "##################################\n",
      "## EPOCH 1340/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011973544023931026\n",
      "##################################\n",
      "## EPOCH 1341/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012703773565590382\n",
      "##################################\n",
      "## EPOCH 1342/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009480365552008152\n",
      "##################################\n",
      "## EPOCH 1343/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016480447724461555\n",
      "##################################\n",
      "## EPOCH 1344/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013227157294750214\n",
      "##################################\n",
      "## EPOCH 1345/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018541017547249794\n",
      "##################################\n",
      "## EPOCH 1346/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010301376692950726\n",
      "##################################\n",
      "## EPOCH 1347/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015320956707000732\n",
      "##################################\n",
      "## EPOCH 1348/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02213202603161335\n",
      "##################################\n",
      "## EPOCH 1349/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027681317180395126\n",
      "##################################\n",
      "## EPOCH 1350/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012891188263893127\n",
      "##################################\n",
      "## EPOCH 1351/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016436096280813217\n",
      "##################################\n",
      "## EPOCH 1352/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03749685734510422\n",
      "##################################\n",
      "## EPOCH 1353/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016912564635276794\n",
      "##################################\n",
      "## EPOCH 1354/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010966633446514606\n",
      "##################################\n",
      "## EPOCH 1355/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1417149305343628\n",
      "##################################\n",
      "## EPOCH 1356/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016717035323381424\n",
      "##################################\n",
      "## EPOCH 1357/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01256805844604969\n",
      "##################################\n",
      "## EPOCH 1358/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013444741256535053\n",
      "##################################\n",
      "## EPOCH 1359/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015546256676316261\n",
      "##################################\n",
      "## EPOCH 1360/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014410734176635742\n",
      "##################################\n",
      "## EPOCH 1361/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023165950551629066\n",
      "##################################\n",
      "## EPOCH 1362/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014253455214202404\n",
      "##################################\n",
      "## EPOCH 1363/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01698465086519718\n",
      "##################################\n",
      "## EPOCH 1364/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013938439078629017\n",
      "##################################\n",
      "## EPOCH 1365/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015888184309005737\n",
      "##################################\n",
      "## EPOCH 1366/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.02042810060083866\n",
      "##################################\n",
      "## EPOCH 1367/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014335652813315392\n",
      "##################################\n",
      "## EPOCH 1368/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010662222281098366\n",
      "##################################\n",
      "## EPOCH 1369/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021285252645611763\n",
      "##################################\n",
      "## EPOCH 1370/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01726742647588253\n",
      "##################################\n",
      "## EPOCH 1371/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024659017100930214\n",
      "##################################\n",
      "## EPOCH 1372/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.029298510402441025\n",
      "##################################\n",
      "## EPOCH 1373/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018903058022260666\n",
      "##################################\n",
      "## EPOCH 1374/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008972473442554474\n",
      "##################################\n",
      "## EPOCH 1375/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012864172458648682\n",
      "##################################\n",
      "## EPOCH 1376/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11360114812850952\n",
      "##################################\n",
      "## EPOCH 1377/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012949022464454174\n",
      "##################################\n",
      "## EPOCH 1378/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021771594882011414\n",
      "##################################\n",
      "## EPOCH 1379/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013292315416038036\n",
      "##################################\n",
      "## EPOCH 1380/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.019088324159383774\n",
      "##################################\n",
      "## EPOCH 1381/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017189960926771164\n",
      "##################################\n",
      "## EPOCH 1382/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017649870365858078\n",
      "##################################\n",
      "## EPOCH 1383/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009428950026631355\n",
      "##################################\n",
      "## EPOCH 1384/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024515977129340172\n",
      "##################################\n",
      "## EPOCH 1385/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011750658974051476\n",
      "##################################\n",
      "## EPOCH 1386/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013825932517647743\n",
      "##################################\n",
      "## EPOCH 1387/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.019251707941293716\n",
      "##################################\n",
      "## EPOCH 1388/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014975463040173054\n",
      "##################################\n",
      "## EPOCH 1389/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014077544212341309\n",
      "##################################\n",
      "## EPOCH 1390/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013351662084460258\n",
      "##################################\n",
      "## EPOCH 1391/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01409977488219738\n",
      "##################################\n",
      "## EPOCH 1392/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011088048107922077\n",
      "##################################\n",
      "## EPOCH 1393/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014410671778023243\n",
      "##################################\n",
      "## EPOCH 1394/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01096239872276783\n",
      "##################################\n",
      "## EPOCH 1395/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01014095637947321\n",
      "##################################\n",
      "## EPOCH 1396/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008970458060503006\n",
      "##################################\n",
      "## EPOCH 1397/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01231235358864069\n",
      "##################################\n",
      "## EPOCH 1398/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023283284157514572\n",
      "##################################\n",
      "## EPOCH 1399/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010700191371142864\n",
      "##################################\n",
      "## EPOCH 1400/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010341798886656761\n",
      "##################################\n",
      "## EPOCH 1401/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01245432160794735\n",
      "##################################\n",
      "## EPOCH 1402/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012210697866976261\n",
      "##################################\n",
      "## EPOCH 1403/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009929792955517769\n",
      "##################################\n",
      "## EPOCH 1404/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013287806883454323\n",
      "##################################\n",
      "## EPOCH 1405/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011762147769331932\n",
      "##################################\n",
      "## EPOCH 1406/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01054998766630888\n",
      "##################################\n",
      "## EPOCH 1407/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009754987433552742\n",
      "##################################\n",
      "## EPOCH 1408/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010922759771347046\n",
      "##################################\n",
      "## EPOCH 1409/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0964033231139183\n",
      "##################################\n",
      "## EPOCH 1410/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013927599415183067\n",
      "##################################\n",
      "## EPOCH 1411/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010089912451803684\n",
      "##################################\n",
      "## EPOCH 1412/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02101268246769905\n",
      "##################################\n",
      "## EPOCH 1413/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011975128203630447\n",
      "##################################\n",
      "## EPOCH 1414/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021441351622343063\n",
      "##################################\n",
      "## EPOCH 1415/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015708668157458305\n",
      "##################################\n",
      "## EPOCH 1416/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02012844756245613\n",
      "##################################\n",
      "## EPOCH 1417/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017958780750632286\n",
      "##################################\n",
      "## EPOCH 1418/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03561955317854881\n",
      "##################################\n",
      "## EPOCH 1419/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014343680813908577\n",
      "##################################\n",
      "## EPOCH 1420/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014108574017882347\n",
      "##################################\n",
      "## EPOCH 1421/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10768790543079376\n",
      "##################################\n",
      "## EPOCH 1422/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014721713960170746\n",
      "##################################\n",
      "## EPOCH 1423/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011481603607535362\n",
      "##################################\n",
      "## EPOCH 1424/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.048238955438137054\n",
      "##################################\n",
      "## EPOCH 1425/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020484521985054016\n",
      "##################################\n",
      "## EPOCH 1426/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025828948244452477\n",
      "##################################\n",
      "## EPOCH 1427/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1143639087677002\n",
      "##################################\n",
      "## EPOCH 1428/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06301866471767426\n",
      "##################################\n",
      "## EPOCH 1429/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01252411026507616\n",
      "##################################\n",
      "## EPOCH 1430/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13167747855186462\n",
      "##################################\n",
      "## EPOCH 1431/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020826544612646103\n",
      "##################################\n",
      "## EPOCH 1432/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09345707297325134\n",
      "##################################\n",
      "## EPOCH 1433/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04597378894686699\n",
      "##################################\n",
      "## EPOCH 1434/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.022701047360897064\n",
      "##################################\n",
      "## EPOCH 1435/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028679555281996727\n",
      "##################################\n",
      "## EPOCH 1436/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06588634103536606\n",
      "##################################\n",
      "## EPOCH 1437/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024788303300738335\n",
      "##################################\n",
      "## EPOCH 1438/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05198901891708374\n",
      "##################################\n",
      "## EPOCH 1439/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02756078541278839\n",
      "##################################\n",
      "## EPOCH 1440/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027821993455290794\n",
      "##################################\n",
      "## EPOCH 1441/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028950829058885574\n",
      "##################################\n",
      "## EPOCH 1442/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.036683402955532074\n",
      "##################################\n",
      "## EPOCH 1443/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03461361676454544\n",
      "##################################\n",
      "## EPOCH 1444/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025471970438957214\n",
      "##################################\n",
      "## EPOCH 1445/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17793609201908112\n",
      "##################################\n",
      "## EPOCH 1446/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04921198636293411\n",
      "##################################\n",
      "## EPOCH 1447/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06515619158744812\n",
      "##################################\n",
      "## EPOCH 1448/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03606490418314934\n",
      "##################################\n",
      "## EPOCH 1449/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03006688319146633\n",
      "##################################\n",
      "## EPOCH 1450/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03008595108985901\n",
      "##################################\n",
      "## EPOCH 1451/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02501731552183628\n",
      "##################################\n",
      "## EPOCH 1452/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12049879133701324\n",
      "##################################\n",
      "## EPOCH 1453/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027259167283773422\n",
      "##################################\n",
      "## EPOCH 1454/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04960111901164055\n",
      "##################################\n",
      "## EPOCH 1455/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.038403190672397614\n",
      "##################################\n",
      "## EPOCH 1456/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12045319378376007\n",
      "##################################\n",
      "## EPOCH 1457/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04241625592112541\n",
      "##################################\n",
      "## EPOCH 1458/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030139271169900894\n",
      "##################################\n",
      "## EPOCH 1459/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03862150385975838\n",
      "##################################\n",
      "## EPOCH 1460/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02093259058892727\n",
      "##################################\n",
      "## EPOCH 1461/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0944051444530487\n",
      "##################################\n",
      "## EPOCH 1462/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.047848593443632126\n",
      "##################################\n",
      "## EPOCH 1463/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1344851404428482\n",
      "##################################\n",
      "## EPOCH 1464/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03612806275486946\n",
      "##################################\n",
      "## EPOCH 1465/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08500555157661438\n",
      "##################################\n",
      "## EPOCH 1466/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06911960244178772\n",
      "##################################\n",
      "## EPOCH 1467/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10413537174463272\n",
      "##################################\n",
      "## EPOCH 1468/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08824857324361801\n",
      "##################################\n",
      "## EPOCH 1469/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.042052239179611206\n",
      "##################################\n",
      "## EPOCH 1470/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04489584639668465\n",
      "##################################\n",
      "## EPOCH 1471/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08615265786647797\n",
      "##################################\n",
      "## EPOCH 1472/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08525214344263077\n",
      "##################################\n",
      "## EPOCH 1473/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014219174161553383\n",
      "##################################\n",
      "## EPOCH 1474/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07246462255716324\n",
      "##################################\n",
      "## EPOCH 1475/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02683018520474434\n",
      "##################################\n",
      "## EPOCH 1476/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05829887464642525\n",
      "##################################\n",
      "## EPOCH 1477/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05626014992594719\n",
      "##################################\n",
      "## EPOCH 1478/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06201352924108505\n",
      "##################################\n",
      "## EPOCH 1479/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1524779498577118\n",
      "##################################\n",
      "## EPOCH 1480/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3253050446510315\n",
      "##################################\n",
      "## EPOCH 1481/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.061905406415462494\n",
      "##################################\n",
      "## EPOCH 1482/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04347945377230644\n",
      "##################################\n",
      "## EPOCH 1483/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.030591245740652084\n",
      "##################################\n",
      "## EPOCH 1484/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06324408203363419\n",
      "##################################\n",
      "## EPOCH 1485/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4523347318172455\n",
      "##################################\n",
      "## EPOCH 1486/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.036155253648757935\n",
      "##################################\n",
      "## EPOCH 1487/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030083278194069862\n",
      "##################################\n",
      "## EPOCH 1488/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19848047196865082\n",
      "##################################\n",
      "## EPOCH 1489/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07117731869220734\n",
      "##################################\n",
      "## EPOCH 1490/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.050598692148923874\n",
      "##################################\n",
      "## EPOCH 1491/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12162090837955475\n",
      "##################################\n",
      "## EPOCH 1492/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13943922519683838\n",
      "##################################\n",
      "## EPOCH 1493/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07580925524234772\n",
      "##################################\n",
      "## EPOCH 1494/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1066625714302063\n",
      "##################################\n",
      "## EPOCH 1495/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06177081540226936\n",
      "##################################\n",
      "## EPOCH 1496/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09599295258522034\n",
      "##################################\n",
      "## EPOCH 1497/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06995606422424316\n",
      "##################################\n",
      "## EPOCH 1498/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06992903351783752\n",
      "##################################\n",
      "## EPOCH 1499/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0522482804954052\n",
      "##################################\n",
      "## EPOCH 1500/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05723061040043831\n",
      "##################################\n",
      "## EPOCH 1501/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.035767339169979095\n",
      "##################################\n",
      "## EPOCH 1502/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06704360246658325\n",
      "##################################\n",
      "## EPOCH 1503/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.056701160967350006\n",
      "##################################\n",
      "## EPOCH 1504/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18067866563796997\n",
      "##################################\n",
      "## EPOCH 1505/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05184892565011978\n",
      "##################################\n",
      "## EPOCH 1506/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06225140765309334\n",
      "##################################\n",
      "## EPOCH 1507/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09497735649347305\n",
      "##################################\n",
      "## EPOCH 1508/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.050784338265657425\n",
      "##################################\n",
      "## EPOCH 1509/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07503966987133026\n",
      "##################################\n",
      "## EPOCH 1510/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10933025926351547\n",
      "##################################\n",
      "## EPOCH 1511/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024745499715209007\n",
      "##################################\n",
      "## EPOCH 1512/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08791425079107285\n",
      "##################################\n",
      "## EPOCH 1513/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04990137740969658\n",
      "##################################\n",
      "## EPOCH 1514/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04549840837717056\n",
      "##################################\n",
      "## EPOCH 1515/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14577873051166534\n",
      "##################################\n",
      "## EPOCH 1516/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13162945210933685\n",
      "##################################\n",
      "## EPOCH 1517/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23788800835609436\n",
      "##################################\n",
      "## EPOCH 1518/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0975191667675972\n",
      "##################################\n",
      "## EPOCH 1519/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08626600354909897\n",
      "##################################\n",
      "## EPOCH 1520/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03040264919400215\n",
      "##################################\n",
      "## EPOCH 1521/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.022318383678793907\n",
      "##################################\n",
      "## EPOCH 1522/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03484651818871498\n",
      "##################################\n",
      "## EPOCH 1523/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14941643178462982\n",
      "##################################\n",
      "## EPOCH 1524/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27847355604171753\n",
      "##################################\n",
      "## EPOCH 1525/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18375132977962494\n",
      "##################################\n",
      "## EPOCH 1526/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06254787743091583\n",
      "##################################\n",
      "## EPOCH 1527/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10492976754903793\n",
      "##################################\n",
      "## EPOCH 1528/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09551205486059189\n",
      "##################################\n",
      "## EPOCH 1529/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.38637810945510864\n",
      "##################################\n",
      "## EPOCH 1530/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.049692340195178986\n",
      "##################################\n",
      "## EPOCH 1531/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07898957282304764\n",
      "##################################\n",
      "## EPOCH 1532/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13032343983650208\n",
      "##################################\n",
      "## EPOCH 1533/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08483843505382538\n",
      "##################################\n",
      "## EPOCH 1534/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07923681288957596\n",
      "##################################\n",
      "## EPOCH 1535/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05640098452568054\n",
      "##################################\n",
      "## EPOCH 1536/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18666405975818634\n",
      "##################################\n",
      "## EPOCH 1537/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08880840241909027\n",
      "##################################\n",
      "## EPOCH 1538/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18313848972320557\n",
      "##################################\n",
      "## EPOCH 1539/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08561620116233826\n",
      "##################################\n",
      "## EPOCH 1540/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04588942229747772\n",
      "##################################\n",
      "## EPOCH 1541/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12552255392074585\n",
      "##################################\n",
      "## EPOCH 1542/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.03808301314711571\n",
      "##################################\n",
      "## EPOCH 1543/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10636729001998901\n",
      "##################################\n",
      "## EPOCH 1544/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17459706962108612\n",
      "##################################\n",
      "## EPOCH 1545/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11275158822536469\n",
      "##################################\n",
      "## EPOCH 1546/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07289476692676544\n",
      "##################################\n",
      "## EPOCH 1547/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017433706670999527\n",
      "##################################\n",
      "## EPOCH 1548/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23954994976520538\n",
      "##################################\n",
      "## EPOCH 1549/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.052063412964344025\n",
      "##################################\n",
      "## EPOCH 1550/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.172582745552063\n",
      "##################################\n",
      "## EPOCH 1551/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05933525040745735\n",
      "##################################\n",
      "## EPOCH 1552/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05106333643198013\n",
      "##################################\n",
      "## EPOCH 1553/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08528020977973938\n",
      "##################################\n",
      "## EPOCH 1554/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1476753056049347\n",
      "##################################\n",
      "## EPOCH 1555/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06519244611263275\n",
      "##################################\n",
      "## EPOCH 1556/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21518070995807648\n",
      "##################################\n",
      "## EPOCH 1557/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.038642026484012604\n",
      "##################################\n",
      "## EPOCH 1558/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08620887249708176\n",
      "##################################\n",
      "## EPOCH 1559/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1917770951986313\n",
      "##################################\n",
      "## EPOCH 1560/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09310927242040634\n",
      "##################################\n",
      "## EPOCH 1561/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09856060892343521\n",
      "##################################\n",
      "## EPOCH 1562/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09890864044427872\n",
      "##################################\n",
      "## EPOCH 1563/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5905460715293884\n",
      "##################################\n",
      "## EPOCH 1564/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0427674800157547\n",
      "##################################\n",
      "## EPOCH 1565/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14632515609264374\n",
      "##################################\n",
      "## EPOCH 1566/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18426825106143951\n",
      "##################################\n",
      "## EPOCH 1567/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1389356255531311\n",
      "##################################\n",
      "## EPOCH 1568/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1197824627161026\n",
      "##################################\n",
      "## EPOCH 1569/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13048943877220154\n",
      "##################################\n",
      "## EPOCH 1570/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06242454797029495\n",
      "##################################\n",
      "## EPOCH 1571/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06024997681379318\n",
      "##################################\n",
      "## EPOCH 1572/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03593675047159195\n",
      "##################################\n",
      "## EPOCH 1573/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12191855907440186\n",
      "##################################\n",
      "## EPOCH 1574/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07146360725164413\n",
      "##################################\n",
      "## EPOCH 1575/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028047820553183556\n",
      "##################################\n",
      "## EPOCH 1576/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17713984847068787\n",
      "##################################\n",
      "## EPOCH 1577/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06206749007105827\n",
      "##################################\n",
      "## EPOCH 1578/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13237538933753967\n",
      "##################################\n",
      "## EPOCH 1579/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021342411637306213\n",
      "##################################\n",
      "## EPOCH 1580/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030405595898628235\n",
      "##################################\n",
      "## EPOCH 1581/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10864883661270142\n",
      "##################################\n",
      "## EPOCH 1582/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6208229064941406\n",
      "##################################\n",
      "## EPOCH 1583/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08515933901071548\n",
      "##################################\n",
      "## EPOCH 1584/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.057672131806612015\n",
      "##################################\n",
      "## EPOCH 1585/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06993769109249115\n",
      "##################################\n",
      "## EPOCH 1586/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03674330934882164\n",
      "##################################\n",
      "## EPOCH 1587/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19244208931922913\n",
      "##################################\n",
      "## EPOCH 1588/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19917897880077362\n",
      "##################################\n",
      "## EPOCH 1589/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20197197794914246\n",
      "##################################\n",
      "## EPOCH 1590/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09206143766641617\n",
      "##################################\n",
      "## EPOCH 1591/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18771390616893768\n",
      "##################################\n",
      "## EPOCH 1592/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04645143821835518\n",
      "##################################\n",
      "## EPOCH 1593/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05012951046228409\n",
      "##################################\n",
      "## EPOCH 1594/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.049973923712968826\n",
      "##################################\n",
      "## EPOCH 1595/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2532614469528198\n",
      "##################################\n",
      "## EPOCH 1596/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1426841914653778\n",
      "##################################\n",
      "## EPOCH 1597/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10461848974227905\n",
      "##################################\n",
      "## EPOCH 1598/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05598459765315056\n",
      "##################################\n",
      "## EPOCH 1599/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028216471895575523\n",
      "##################################\n",
      "## EPOCH 1600/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08662658929824829\n",
      "##################################\n",
      "## EPOCH 1601/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.05553331971168518\n",
      "##################################\n",
      "## EPOCH 1602/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05422665923833847\n",
      "##################################\n",
      "## EPOCH 1603/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0660957545042038\n",
      "##################################\n",
      "## EPOCH 1604/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03021577000617981\n",
      "##################################\n",
      "## EPOCH 1605/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04222216457128525\n",
      "##################################\n",
      "## EPOCH 1606/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.2528015077114105\n",
      "##################################\n",
      "## EPOCH 1607/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09776289761066437\n",
      "##################################\n",
      "## EPOCH 1608/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11101821810007095\n",
      "##################################\n",
      "## EPOCH 1609/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02456369623541832\n",
      "##################################\n",
      "## EPOCH 1610/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.18695595860481262\n",
      "##################################\n",
      "## EPOCH 1611/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.040763840079307556\n",
      "##################################\n",
      "## EPOCH 1612/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07687055319547653\n",
      "##################################\n",
      "## EPOCH 1613/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06260854750871658\n",
      "##################################\n",
      "## EPOCH 1614/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.055710334330797195\n",
      "##################################\n",
      "## EPOCH 1615/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05521552637219429\n",
      "##################################\n",
      "## EPOCH 1616/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.026167338714003563\n",
      "##################################\n",
      "## EPOCH 1617/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025628680363297462\n",
      "##################################\n",
      "## EPOCH 1618/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02716895565390587\n",
      "##################################\n",
      "## EPOCH 1619/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.048538174480199814\n",
      "##################################\n",
      "## EPOCH 1620/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9982444643974304\n",
      "##################################\n",
      "## EPOCH 1621/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.21038135886192322\n",
      "##################################\n",
      "## EPOCH 1622/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03206383436918259\n",
      "##################################\n",
      "## EPOCH 1623/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.038381751626729965\n",
      "##################################\n",
      "## EPOCH 1624/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11043611913919449\n",
      "##################################\n",
      "## EPOCH 1625/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16414819657802582\n",
      "##################################\n",
      "## EPOCH 1626/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.19434773921966553\n",
      "##################################\n",
      "## EPOCH 1627/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06840196996927261\n",
      "##################################\n",
      "## EPOCH 1628/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021937012672424316\n",
      "##################################\n",
      "## EPOCH 1629/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.053308289498090744\n",
      "##################################\n",
      "## EPOCH 1630/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23767757415771484\n",
      "##################################\n",
      "## EPOCH 1631/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06800057739019394\n",
      "##################################\n",
      "## EPOCH 1632/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8535795211791992\n",
      "##################################\n",
      "## EPOCH 1633/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1099202036857605\n",
      "##################################\n",
      "## EPOCH 1634/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02076895721256733\n",
      "##################################\n",
      "## EPOCH 1635/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04699981212615967\n",
      "##################################\n",
      "## EPOCH 1636/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08748458325862885\n",
      "##################################\n",
      "## EPOCH 1637/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3042784333229065\n",
      "##################################\n",
      "## EPOCH 1638/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07007011771202087\n",
      "##################################\n",
      "## EPOCH 1639/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13897298276424408\n",
      "##################################\n",
      "## EPOCH 1640/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06321793794631958\n",
      "##################################\n",
      "## EPOCH 1641/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.40312275290489197\n",
      "##################################\n",
      "## EPOCH 1642/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06823871284723282\n",
      "##################################\n",
      "## EPOCH 1643/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.31050506234169006\n",
      "##################################\n",
      "## EPOCH 1644/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5914725065231323\n",
      "##################################\n",
      "## EPOCH 1645/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03772163763642311\n",
      "##################################\n",
      "## EPOCH 1646/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22237920761108398\n",
      "##################################\n",
      "## EPOCH 1647/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07393798977136612\n",
      "##################################\n",
      "## EPOCH 1648/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07569686323404312\n",
      "##################################\n",
      "## EPOCH 1649/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10776865482330322\n",
      "##################################\n",
      "## EPOCH 1650/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.26114144921302795\n",
      "##################################\n",
      "## EPOCH 1651/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.15275797247886658\n",
      "##################################\n",
      "## EPOCH 1652/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05420023947954178\n",
      "##################################\n",
      "## EPOCH 1653/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.20096473395824432\n",
      "##################################\n",
      "## EPOCH 1654/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13063490390777588\n",
      "##################################\n",
      "## EPOCH 1655/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22357800602912903\n",
      "##################################\n",
      "## EPOCH 1656/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.060382645577192307\n",
      "##################################\n",
      "## EPOCH 1657/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.16196370124816895\n",
      "##################################\n",
      "## EPOCH 1658/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.049344826489686966\n",
      "##################################\n",
      "## EPOCH 1659/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06514587253332138\n",
      "##################################\n",
      "## EPOCH 1660/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.03558903560042381\n",
      "##################################\n",
      "## EPOCH 1661/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1466304212808609\n",
      "##################################\n",
      "## EPOCH 1662/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0685172751545906\n",
      "##################################\n",
      "## EPOCH 1663/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1952449083328247\n",
      "##################################\n",
      "## EPOCH 1664/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17337295413017273\n",
      "##################################\n",
      "## EPOCH 1665/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04357288405299187\n",
      "##################################\n",
      "## EPOCH 1666/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.22528593242168427\n",
      "##################################\n",
      "## EPOCH 1667/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030437767505645752\n",
      "##################################\n",
      "## EPOCH 1668/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.23068435490131378\n",
      "##################################\n",
      "## EPOCH 1669/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.17288440465927124\n",
      "##################################\n",
      "## EPOCH 1670/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.036182187497615814\n",
      "##################################\n",
      "## EPOCH 1671/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020410576835274696\n",
      "##################################\n",
      "## EPOCH 1672/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03875239938497543\n",
      "##################################\n",
      "## EPOCH 1673/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03704439848661423\n",
      "##################################\n",
      "## EPOCH 1674/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.39299720525741577\n",
      "##################################\n",
      "## EPOCH 1675/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025371262803673744\n",
      "##################################\n",
      "## EPOCH 1676/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04482865333557129\n",
      "##################################\n",
      "## EPOCH 1677/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.034654758870601654\n",
      "##################################\n",
      "## EPOCH 1678/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03915141895413399\n",
      "##################################\n",
      "## EPOCH 1679/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.25070756673812866\n",
      "##################################\n",
      "## EPOCH 1680/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0974600538611412\n",
      "##################################\n",
      "## EPOCH 1681/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10802344977855682\n",
      "##################################\n",
      "## EPOCH 1682/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08903764188289642\n",
      "##################################\n",
      "## EPOCH 1683/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09115857630968094\n",
      "##################################\n",
      "## EPOCH 1684/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02964370884001255\n",
      "##################################\n",
      "## EPOCH 1685/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02641366794705391\n",
      "##################################\n",
      "## EPOCH 1686/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04689280316233635\n",
      "##################################\n",
      "## EPOCH 1687/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04683895781636238\n",
      "##################################\n",
      "## EPOCH 1688/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09493701905012131\n",
      "##################################\n",
      "## EPOCH 1689/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.27996060252189636\n",
      "##################################\n",
      "## EPOCH 1690/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09102297574281693\n",
      "##################################\n",
      "## EPOCH 1691/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06531956046819687\n",
      "##################################\n",
      "## EPOCH 1692/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1030554547905922\n",
      "##################################\n",
      "## EPOCH 1693/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07763417810201645\n",
      "##################################\n",
      "## EPOCH 1694/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07616930454969406\n",
      "##################################\n",
      "## EPOCH 1695/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025427496060729027\n",
      "##################################\n",
      "## EPOCH 1696/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014690609648823738\n",
      "##################################\n",
      "## EPOCH 1697/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10755814611911774\n",
      "##################################\n",
      "## EPOCH 1698/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0944763571023941\n",
      "##################################\n",
      "## EPOCH 1699/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.13284523785114288\n",
      "##################################\n",
      "## EPOCH 1700/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08910460025072098\n",
      "##################################\n",
      "## EPOCH 1701/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05124932527542114\n",
      "##################################\n",
      "## EPOCH 1702/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07992827892303467\n",
      "##################################\n",
      "## EPOCH 1703/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.035671256482601166\n",
      "##################################\n",
      "## EPOCH 1704/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04792729765176773\n",
      "##################################\n",
      "## EPOCH 1705/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07728596776723862\n",
      "##################################\n",
      "## EPOCH 1706/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03887122496962547\n",
      "##################################\n",
      "## EPOCH 1707/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01873256266117096\n",
      "##################################\n",
      "## EPOCH 1708/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021606002002954483\n",
      "##################################\n",
      "## EPOCH 1709/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01756853237748146\n",
      "##################################\n",
      "## EPOCH 1710/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03535471111536026\n",
      "##################################\n",
      "## EPOCH 1711/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.040206458419561386\n",
      "##################################\n",
      "## EPOCH 1712/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03561527654528618\n",
      "##################################\n",
      "## EPOCH 1713/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.10367071628570557\n",
      "##################################\n",
      "## EPOCH 1714/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027278807014226913\n",
      "##################################\n",
      "## EPOCH 1715/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0652523785829544\n",
      "##################################\n",
      "## EPOCH 1716/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09501027315855026\n",
      "##################################\n",
      "## EPOCH 1717/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020682450383901596\n",
      "##################################\n",
      "## EPOCH 1718/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0301436148583889\n",
      "##################################\n",
      "## EPOCH 1719/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.10774751007556915\n",
      "##################################\n",
      "## EPOCH 1720/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025344859808683395\n",
      "##################################\n",
      "## EPOCH 1721/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021513577550649643\n",
      "##################################\n",
      "## EPOCH 1722/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025665462017059326\n",
      "##################################\n",
      "## EPOCH 1723/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017899777740240097\n",
      "##################################\n",
      "## EPOCH 1724/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05997225642204285\n",
      "##################################\n",
      "## EPOCH 1725/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02912328764796257\n",
      "##################################\n",
      "## EPOCH 1726/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018073853105306625\n",
      "##################################\n",
      "## EPOCH 1727/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05087202787399292\n",
      "##################################\n",
      "## EPOCH 1728/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015204602852463722\n",
      "##################################\n",
      "## EPOCH 1729/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.032194655388593674\n",
      "##################################\n",
      "## EPOCH 1730/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03447349742054939\n",
      "##################################\n",
      "## EPOCH 1731/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12236011028289795\n",
      "##################################\n",
      "## EPOCH 1732/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.026738444343209267\n",
      "##################################\n",
      "## EPOCH 1733/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01445280946791172\n",
      "##################################\n",
      "## EPOCH 1734/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01714063063263893\n",
      "##################################\n",
      "## EPOCH 1735/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05149850249290466\n",
      "##################################\n",
      "## EPOCH 1736/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.019643789157271385\n",
      "##################################\n",
      "## EPOCH 1737/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013081314042210579\n",
      "##################################\n",
      "## EPOCH 1738/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027857119217514992\n",
      "##################################\n",
      "## EPOCH 1739/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.09719561040401459\n",
      "##################################\n",
      "## EPOCH 1740/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14528118073940277\n",
      "##################################\n",
      "## EPOCH 1741/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028977418318390846\n",
      "##################################\n",
      "## EPOCH 1742/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025813201442360878\n",
      "##################################\n",
      "## EPOCH 1743/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.12801390886306763\n",
      "##################################\n",
      "## EPOCH 1744/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1103501170873642\n",
      "##################################\n",
      "## EPOCH 1745/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0259883850812912\n",
      "##################################\n",
      "## EPOCH 1746/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018244782462716103\n",
      "##################################\n",
      "## EPOCH 1747/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015808172523975372\n",
      "##################################\n",
      "## EPOCH 1748/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016610048711299896\n",
      "##################################\n",
      "## EPOCH 1749/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03142404556274414\n",
      "##################################\n",
      "## EPOCH 1750/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03668120875954628\n",
      "##################################\n",
      "## EPOCH 1751/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03256707265973091\n",
      "##################################\n",
      "## EPOCH 1752/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.05461270734667778\n",
      "##################################\n",
      "## EPOCH 1753/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07914848625659943\n",
      "##################################\n",
      "## EPOCH 1754/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07915379106998444\n",
      "##################################\n",
      "## EPOCH 1755/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02532149851322174\n",
      "##################################\n",
      "## EPOCH 1756/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.045982442796230316\n",
      "##################################\n",
      "## EPOCH 1757/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.026440758258104324\n",
      "##################################\n",
      "## EPOCH 1758/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02452935092151165\n",
      "##################################\n",
      "## EPOCH 1759/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04749339073896408\n",
      "##################################\n",
      "## EPOCH 1760/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03177649900317192\n",
      "##################################\n",
      "## EPOCH 1761/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03931000083684921\n",
      "##################################\n",
      "## EPOCH 1762/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03538280352950096\n",
      "##################################\n",
      "## EPOCH 1763/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03899402171373367\n",
      "##################################\n",
      "## EPOCH 1764/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02154488116502762\n",
      "##################################\n",
      "## EPOCH 1765/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02092830464243889\n",
      "##################################\n",
      "## EPOCH 1766/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03373376280069351\n",
      "##################################\n",
      "## EPOCH 1767/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.11206861585378647\n",
      "##################################\n",
      "## EPOCH 1768/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028517212718725204\n",
      "##################################\n",
      "## EPOCH 1769/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020269425585865974\n",
      "##################################\n",
      "## EPOCH 1770/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014560846611857414\n",
      "##################################\n",
      "## EPOCH 1771/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.029359573498368263\n",
      "##################################\n",
      "## EPOCH 1772/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01908334344625473\n",
      "##################################\n",
      "## EPOCH 1773/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04023780673742294\n",
      "##################################\n",
      "## EPOCH 1774/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.030225208029150963\n",
      "##################################\n",
      "## EPOCH 1775/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02846599742770195\n",
      "##################################\n",
      "## EPOCH 1776/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012880575843155384\n",
      "##################################\n",
      "## EPOCH 1777/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.05209122225642204\n",
      "##################################\n",
      "## EPOCH 1778/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01740211248397827\n",
      "##################################\n",
      "## EPOCH 1779/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.039551589637994766\n",
      "##################################\n",
      "## EPOCH 1780/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02310037612915039\n",
      "##################################\n",
      "## EPOCH 1781/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012193648144602776\n",
      "##################################\n",
      "## EPOCH 1782/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.1280525028705597\n",
      "##################################\n",
      "## EPOCH 1783/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024208854883909225\n",
      "##################################\n",
      "## EPOCH 1784/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020544247701764107\n",
      "##################################\n",
      "## EPOCH 1785/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01931646093726158\n",
      "##################################\n",
      "## EPOCH 1786/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016943201422691345\n",
      "##################################\n",
      "## EPOCH 1787/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03925727307796478\n",
      "##################################\n",
      "## EPOCH 1788/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01534571684896946\n",
      "##################################\n",
      "## EPOCH 1789/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021355295553803444\n",
      "##################################\n",
      "## EPOCH 1790/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01596851274371147\n",
      "##################################\n",
      "## EPOCH 1791/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016206180676817894\n",
      "##################################\n",
      "## EPOCH 1792/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02384563535451889\n",
      "##################################\n",
      "## EPOCH 1793/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01896735280752182\n",
      "##################################\n",
      "## EPOCH 1794/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025989150628447533\n",
      "##################################\n",
      "## EPOCH 1795/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008296524174511433\n",
      "##################################\n",
      "## EPOCH 1796/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023791346698999405\n",
      "##################################\n",
      "## EPOCH 1797/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0841943621635437\n",
      "##################################\n",
      "## EPOCH 1798/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.044156260788440704\n",
      "##################################\n",
      "## EPOCH 1799/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03212758153676987\n",
      "##################################\n",
      "## EPOCH 1800/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016757547855377197\n",
      "##################################\n",
      "## EPOCH 1801/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009644334204494953\n",
      "##################################\n",
      "## EPOCH 1802/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009377334266901016\n",
      "##################################\n",
      "## EPOCH 1803/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013969188556075096\n",
      "##################################\n",
      "## EPOCH 1804/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012068966403603554\n",
      "##################################\n",
      "## EPOCH 1805/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021260671317577362\n",
      "##################################\n",
      "## EPOCH 1806/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01822091080248356\n",
      "##################################\n",
      "## EPOCH 1807/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009735275991261005\n",
      "##################################\n",
      "## EPOCH 1808/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.14742183685302734\n",
      "##################################\n",
      "## EPOCH 1809/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024513615295290947\n",
      "##################################\n",
      "## EPOCH 1810/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.3140537738800049\n",
      "##################################\n",
      "## EPOCH 1811/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013105526566505432\n",
      "##################################\n",
      "## EPOCH 1812/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.028743136674165726\n",
      "##################################\n",
      "## EPOCH 1813/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027243221178650856\n",
      "##################################\n",
      "## EPOCH 1814/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013242669403553009\n",
      "##################################\n",
      "## EPOCH 1815/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03577014431357384\n",
      "##################################\n",
      "## EPOCH 1816/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04060354456305504\n",
      "##################################\n",
      "## EPOCH 1817/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018978159874677658\n",
      "##################################\n",
      "## EPOCH 1818/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01290112268179655\n",
      "##################################\n",
      "## EPOCH 1819/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.062223631888628006\n",
      "##################################\n",
      "## EPOCH 1820/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014496619813144207\n",
      "##################################\n",
      "## EPOCH 1821/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.044068507850170135\n",
      "##################################\n",
      "## EPOCH 1822/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02057666704058647\n",
      "##################################\n",
      "## EPOCH 1823/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013285944238305092\n",
      "##################################\n",
      "## EPOCH 1824/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02004084177315235\n",
      "##################################\n",
      "## EPOCH 1825/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016815567389130592\n",
      "##################################\n",
      "## EPOCH 1826/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01493687741458416\n",
      "##################################\n",
      "## EPOCH 1827/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01925617828965187\n",
      "##################################\n",
      "## EPOCH 1828/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02683129534125328\n",
      "##################################\n",
      "## EPOCH 1829/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.033950161188840866\n",
      "##################################\n",
      "## EPOCH 1830/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027920380234718323\n",
      "##################################\n",
      "## EPOCH 1831/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00974988006055355\n",
      "##################################\n",
      "## EPOCH 1832/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012370630167424679\n",
      "##################################\n",
      "## EPOCH 1833/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012856828980147839\n",
      "##################################\n",
      "## EPOCH 1834/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014634022489190102\n",
      "##################################\n",
      "## EPOCH 1835/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.016587939113378525\n",
      "##################################\n",
      "## EPOCH 1836/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.016009708866477013\n",
      "##################################\n",
      "## EPOCH 1837/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024898206815123558\n",
      "##################################\n",
      "## EPOCH 1838/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00827783439308405\n",
      "##################################\n",
      "## EPOCH 1839/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020440038293600082\n",
      "##################################\n",
      "## EPOCH 1840/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.024291953071951866\n",
      "##################################\n",
      "## EPOCH 1841/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012643826194107533\n",
      "##################################\n",
      "## EPOCH 1842/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011843753978610039\n",
      "##################################\n",
      "## EPOCH 1843/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.021886300295591354\n",
      "##################################\n",
      "## EPOCH 1844/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009509026072919369\n",
      "##################################\n",
      "## EPOCH 1845/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013400517404079437\n",
      "##################################\n",
      "## EPOCH 1846/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018397308886051178\n",
      "##################################\n",
      "## EPOCH 1847/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007897521369159222\n",
      "##################################\n",
      "## EPOCH 1848/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008353464305400848\n",
      "##################################\n",
      "## EPOCH 1849/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010969233699142933\n",
      "##################################\n",
      "## EPOCH 1850/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014232630841434002\n",
      "##################################\n",
      "## EPOCH 1851/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005773687735199928\n",
      "##################################\n",
      "## EPOCH 1852/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008211765438318253\n",
      "##################################\n",
      "## EPOCH 1853/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008821194991469383\n",
      "##################################\n",
      "## EPOCH 1854/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012572239153087139\n",
      "##################################\n",
      "## EPOCH 1855/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009274832904338837\n",
      "##################################\n",
      "## EPOCH 1856/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008370013907551765\n",
      "##################################\n",
      "## EPOCH 1857/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005870412569493055\n",
      "##################################\n",
      "## EPOCH 1858/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.017503919079899788\n",
      "##################################\n",
      "## EPOCH 1859/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007752236910164356\n",
      "##################################\n",
      "## EPOCH 1860/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008855706080794334\n",
      "##################################\n",
      "## EPOCH 1861/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0098926005885005\n",
      "##################################\n",
      "## EPOCH 1862/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02336634136736393\n",
      "##################################\n",
      "## EPOCH 1863/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0074628135189414024\n",
      "##################################\n",
      "## EPOCH 1864/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006812767591327429\n",
      "##################################\n",
      "## EPOCH 1865/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0052048685029149055\n",
      "##################################\n",
      "## EPOCH 1866/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011483235284686089\n",
      "##################################\n",
      "## EPOCH 1867/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006186209619045258\n",
      "##################################\n",
      "## EPOCH 1868/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01766493171453476\n",
      "##################################\n",
      "## EPOCH 1869/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005683396942913532\n",
      "##################################\n",
      "## EPOCH 1870/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005193435586988926\n",
      "##################################\n",
      "## EPOCH 1871/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00859800260514021\n",
      "##################################\n",
      "## EPOCH 1872/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008672904223203659\n",
      "##################################\n",
      "## EPOCH 1873/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008535589091479778\n",
      "##################################\n",
      "## EPOCH 1874/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009905071929097176\n",
      "##################################\n",
      "## EPOCH 1875/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008241930976510048\n",
      "##################################\n",
      "## EPOCH 1876/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.08846507221460342\n",
      "##################################\n",
      "## EPOCH 1877/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7150256633758545\n",
      "##################################\n",
      "## EPOCH 1878/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011428147554397583\n",
      "##################################\n",
      "## EPOCH 1879/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013437831774353981\n",
      "##################################\n",
      "## EPOCH 1880/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007711390499025583\n",
      "##################################\n",
      "## EPOCH 1881/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01119334064424038\n",
      "##################################\n",
      "## EPOCH 1882/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.020478829741477966\n",
      "##################################\n",
      "## EPOCH 1883/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013383853249251842\n",
      "##################################\n",
      "## EPOCH 1884/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07373808324337006\n",
      "##################################\n",
      "## EPOCH 1885/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0202977042645216\n",
      "##################################\n",
      "## EPOCH 1886/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01187959685921669\n",
      "##################################\n",
      "## EPOCH 1887/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01754104718565941\n",
      "##################################\n",
      "## EPOCH 1888/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014434678480029106\n",
      "##################################\n",
      "## EPOCH 1889/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02355705201625824\n",
      "##################################\n",
      "## EPOCH 1890/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06322617828845978\n",
      "##################################\n",
      "## EPOCH 1891/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.06438577175140381\n",
      "##################################\n",
      "## EPOCH 1892/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03935905918478966\n",
      "##################################\n",
      "## EPOCH 1893/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.025544684380292892\n",
      "##################################\n",
      "## EPOCH 1894/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.04274987429380417\n",
      "##################################\n",
      "## EPOCH 1895/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.025964077562093735\n",
      "##################################\n",
      "## EPOCH 1896/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02027789130806923\n",
      "##################################\n",
      "## EPOCH 1897/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02180483750998974\n",
      "##################################\n",
      "## EPOCH 1898/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023926418274641037\n",
      "##################################\n",
      "## EPOCH 1899/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023855384439229965\n",
      "##################################\n",
      "## EPOCH 1900/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013393064960837364\n",
      "##################################\n",
      "## EPOCH 1901/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01297900639474392\n",
      "##################################\n",
      "## EPOCH 1902/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.03483254089951515\n",
      "##################################\n",
      "## EPOCH 1903/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011545012705028057\n",
      "##################################\n",
      "## EPOCH 1904/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014277863316237926\n",
      "##################################\n",
      "## EPOCH 1905/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02161896787583828\n",
      "##################################\n",
      "## EPOCH 1906/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009209087118506432\n",
      "##################################\n",
      "## EPOCH 1907/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011007510125637054\n",
      "##################################\n",
      "## EPOCH 1908/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012945154681801796\n",
      "##################################\n",
      "## EPOCH 1909/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.027144158259034157\n",
      "##################################\n",
      "## EPOCH 1910/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018274258822202682\n",
      "##################################\n",
      "## EPOCH 1911/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014962759800255299\n",
      "##################################\n",
      "## EPOCH 1912/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012871906161308289\n",
      "##################################\n",
      "## EPOCH 1913/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.018498431891202927\n",
      "##################################\n",
      "## EPOCH 1914/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.023556167259812355\n",
      "##################################\n",
      "## EPOCH 1915/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011666551232337952\n",
      "##################################\n",
      "## EPOCH 1916/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007340963929891586\n",
      "##################################\n",
      "## EPOCH 1917/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.033816687762737274\n",
      "##################################\n",
      "## EPOCH 1918/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02153589576482773\n",
      "##################################\n",
      "## EPOCH 1919/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005268970504403114\n",
      "##################################\n",
      "## EPOCH 1920/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01201387494802475\n",
      "##################################\n",
      "## EPOCH 1921/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.07371539622545242\n",
      "##################################\n",
      "## EPOCH 1922/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01206671167165041\n",
      "##################################\n",
      "## EPOCH 1923/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015643347054719925\n",
      "##################################\n",
      "## EPOCH 1924/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015006127767264843\n",
      "##################################\n",
      "## EPOCH 1925/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.02286224067211151\n",
      "##################################\n",
      "## EPOCH 1926/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01608981564640999\n",
      "##################################\n",
      "## EPOCH 1927/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009557552635669708\n",
      "##################################\n",
      "## EPOCH 1928/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012971008196473122\n",
      "##################################\n",
      "## EPOCH 1929/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009629658423364162\n",
      "##################################\n",
      "## EPOCH 1930/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013409962877631187\n",
      "##################################\n",
      "## EPOCH 1931/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015588710084557533\n",
      "##################################\n",
      "## EPOCH 1932/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00923681166023016\n",
      "##################################\n",
      "## EPOCH 1933/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.014664506539702415\n",
      "##################################\n",
      "## EPOCH 1934/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011881964281201363\n",
      "##################################\n",
      "## EPOCH 1935/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007965896278619766\n",
      "##################################\n",
      "## EPOCH 1936/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013332055881619453\n",
      "##################################\n",
      "## EPOCH 1937/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.015464079566299915\n",
      "##################################\n",
      "## EPOCH 1938/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008975372649729252\n",
      "##################################\n",
      "## EPOCH 1939/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006649705581367016\n",
      "##################################\n",
      "## EPOCH 1940/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009202751331031322\n",
      "##################################\n",
      "## EPOCH 1941/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012135044671595097\n",
      "##################################\n",
      "## EPOCH 1942/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010454281233251095\n",
      "##################################\n",
      "## EPOCH 1943/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011730712838470936\n",
      "##################################\n",
      "## EPOCH 1944/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010593125596642494\n",
      "##################################\n",
      "## EPOCH 1945/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006354601588100195\n",
      "##################################\n",
      "## EPOCH 1946/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01031714491546154\n",
      "##################################\n",
      "## EPOCH 1947/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006811975501477718\n",
      "##################################\n",
      "## EPOCH 1948/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008160917088389397\n",
      "##################################\n",
      "## EPOCH 1949/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007698197849094868\n",
      "##################################\n",
      "## EPOCH 1950/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007346442434936762\n",
      "##################################\n",
      "## EPOCH 1951/2000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.009715486317873001\n",
      "##################################\n",
      "## EPOCH 1952/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005977065768092871\n",
      "##################################\n",
      "## EPOCH 1953/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010756591334939003\n",
      "##################################\n",
      "## EPOCH 1954/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.004972910974174738\n",
      "##################################\n",
      "## EPOCH 1955/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0052115218713879585\n",
      "##################################\n",
      "## EPOCH 1956/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007398795336484909\n",
      "##################################\n",
      "## EPOCH 1957/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.01231884304434061\n",
      "##################################\n",
      "## EPOCH 1958/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.004328162409365177\n",
      "##################################\n",
      "## EPOCH 1959/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009220178239047527\n",
      "##################################\n",
      "## EPOCH 1960/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006833832710981369\n",
      "##################################\n",
      "## EPOCH 1961/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008187292143702507\n",
      "##################################\n",
      "## EPOCH 1962/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008826845325529575\n",
      "##################################\n",
      "## EPOCH 1963/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006314679980278015\n",
      "##################################\n",
      "## EPOCH 1964/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007261526770889759\n",
      "##################################\n",
      "## EPOCH 1965/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009175390005111694\n",
      "##################################\n",
      "## EPOCH 1966/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0075857206247746944\n",
      "##################################\n",
      "## EPOCH 1967/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.004445101134479046\n",
      "##################################\n",
      "## EPOCH 1968/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007802600506693125\n",
      "##################################\n",
      "## EPOCH 1969/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0070538632571697235\n",
      "##################################\n",
      "## EPOCH 1970/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010206039994955063\n",
      "##################################\n",
      "## EPOCH 1971/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007369340397417545\n",
      "##################################\n",
      "## EPOCH 1972/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.013633126392960548\n",
      "##################################\n",
      "## EPOCH 1973/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009334796108305454\n",
      "##################################\n",
      "## EPOCH 1974/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009061958640813828\n",
      "##################################\n",
      "## EPOCH 1975/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007944596000015736\n",
      "##################################\n",
      "## EPOCH 1976/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007494540419429541\n",
      "##################################\n",
      "## EPOCH 1977/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012455507181584835\n",
      "##################################\n",
      "## EPOCH 1978/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.011862831190228462\n",
      "##################################\n",
      "## EPOCH 1979/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0049185482785105705\n",
      "##################################\n",
      "## EPOCH 1980/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008677790872752666\n",
      "##################################\n",
      "## EPOCH 1981/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005644426681101322\n",
      "##################################\n",
      "## EPOCH 1982/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.005220641382038593\n",
      "##################################\n",
      "## EPOCH 1983/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006097032688558102\n",
      "##################################\n",
      "## EPOCH 1984/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00617481954395771\n",
      "##################################\n",
      "## EPOCH 1985/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009831259027123451\n",
      "##################################\n",
      "## EPOCH 1986/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009378215298056602\n",
      "##################################\n",
      "## EPOCH 1987/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0071222721599042416\n",
      "##################################\n",
      "## EPOCH 1988/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.0059001571498811245\n",
      "##################################\n",
      "## EPOCH 1989/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.012475404888391495\n",
      "##################################\n",
      "## EPOCH 1990/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00786339957267046\n",
      "##################################\n",
      "## EPOCH 1991/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.008559740148484707\n",
      "##################################\n",
      "## EPOCH 1992/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007946627214550972\n",
      "##################################\n",
      "## EPOCH 1993/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006544419564306736\n",
      "##################################\n",
      "## EPOCH 1994/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006864817347377539\n",
      "##################################\n",
      "## EPOCH 1995/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.006941881962120533\n",
      "##################################\n",
      "## EPOCH 1996/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.010353990830481052\n",
      "##################################\n",
      "## EPOCH 1997/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.007015743758529425\n",
      "##################################\n",
      "## EPOCH 1998/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.009863767772912979\n",
      "##################################\n",
      "## EPOCH 1999/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.00996189285069704\n",
      "##################################\n",
      "## EPOCH 2000/2000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.723410427570343\n"
     ]
    }
   ],
   "source": [
    "#%% Train network\n",
    "\n",
    "# Define Dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=training_args['batchsize'], shuffle=True, num_workers=1)\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=5e-4)\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining loss list to plot the losses\n",
    "loss_list = []\n",
    "\n",
    "# Start training\n",
    "for epoch in range(int(training_args['num_epochs'])):\n",
    "    print('##################################')\n",
    "    print('## EPOCH '+str(epoch + 1)+\"/\"+str(int(training_args['num_epochs'])))\n",
    "    print('##################################')\n",
    "    # Iterate batches\n",
    "    for batch_sample in dataloader:\n",
    "        # Extract batch\n",
    "        batch_onehot = batch_sample['encoded_onehot'].to(device)\n",
    "        # Update network\n",
    "        batch_loss = train_batch(net, batch_onehot, loss_fn, optimizer)\n",
    "        print('\\t Training loss (single batch):', batch_loss)\n",
    "        loss_list.append(batch_loss)\n",
    "\n",
    "### Save all needed parameters\n",
    "# Create output dir\n",
    "out_dir = Path(training_args['out_dir'])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Save network parameters\n",
    "torch.save(net.state_dict(), out_dir / 'net_params.pth')\n",
    "\n",
    "# Adding alphabet length\n",
    "training_args[\"alphabet_len\"] = len(dataset.alphabet)\n",
    "# Save training parameters\n",
    "with open(out_dir / 'training_args.json', 'w') as f:\n",
    "    json.dump(training_args, f, indent=4)\n",
    "# Save encoder dictionary\n",
    "with open(out_dir / 'char_to_number.json', 'w') as f:\n",
    "    json.dump(dataset.char_to_number, f, indent=4)\n",
    "# Save decoder dictionary\n",
    "with open(out_dir / 'number_to_char.json', 'w') as f:\n",
    "    json.dump(dataset.number_to_char, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_list = np.array(loss_list)\n",
    "\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: model_Wilde\n",
      "dorian was staring at his portraitm\n",
      "m.\n",
      "\n",
      " phitrsssuussenn  uu waeee oop!yemen ouaceed and oo itagtigtssyrinn oh h s u whi.. mopen phe toarvyctn p uewe he hsyyin i ahrthe  oarwhes oummeed me coarwade  uaft.ed bide\n",
      "  u 1rg..  oroseem mhem\n",
      " ued boueed  hectover th t lede oo.yp?ttttetl uuusaee  oarddppude\n",
      " aude seti th llis uh ysal; h uen wou poied ouyymeve ciuedy th i seiri hed wouetd tof t thill miued pun peee ar in bnese hel b uetthe  oarthhe  omedeppuue\n",
      "ettuntee siureee uureee  ha tovcra pouyw.hd  uaccaae  oarwhhd mude\n",
      " pe cseee lid a tofesedd oupedes ned i hetncded au tourwaae  hay porowhpe ao\"rwhem aud a uetthe  uaetaad  ta taee litos seuriled aude agtto tale oouesedd\n",
      " mumett f . :he lles yhim youen ofp tomgwhes ohyy miurwofed  h pgled huu 0fyrin bi oi wilee oi yeyvris uncctlrrgci . wher liu a uhalhd pimee hed i tetlen hiue\n",
      " au peeeryitsds uuned fg oa wtuet oh m l uses hedm otcy hhd wissssit led b toseee oueye serrtoar lh t uew.hem pemctee  oarwhctgvisvd tn cad. gotoasssmuu\"vettuncs arilppmppeectine  h f pgee  oph\"phv. mumeee pumeee pour2atetqrthrtlls auumaf urtitaee  he coagads onrttthr. hid b shalld ig oseirii hed m tet.e. oude top\"tshe. sudeb at ia wgee l ued wh i lee h uetthyp thrtls oh y umiens iudye te cille oi pieecyttu paacwise  hay wg.raacsm hen siartan surrthet ou wachit hed wouend pgmene  ua waeen cu palc.gtound yturnd semr-ine af i urwee oopesyprn lou;\n",
      " hi h .el  oh psyevv hnn a fetltdg\n",
      " aometee soueethud m uet.se puucmeee liu of yilriine momed tu cisee ou y chwel liue\n",
      " ph ilse huu pa hallw hed iy is iiiii he hide  hu regtga. l fhylldd. sude\n",
      " hed  oo waage houesyytu ciget hid b feiswdjpmuten tuiredd uudetttt  hasgs ss.ssssum whillyyiuusssett al tles liiriyyiun  of fnne ou y ugwine  oppmeppmmpu,cmmche  lamrcaf  orrthe poeed yed m tetshe xuuede thacevd uunee  f  alee ooursees muuedd puzcae t vericee  ouede toued pon p arw.cmmmmmesm med b tewees mumee hed siseey thactod b ihithe  uaccaat ofrrrhed hide  of tfey oo oa woues h urpfyy.g\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "##############################\n",
    "## PARAMETERS\n",
    "##############################\n",
    "#parser = argparse.ArgumentParser(description='Generate sonnet starting from a given text')\n",
    "\n",
    "#parser.add_argument('--sonnet_seed', type=str, default='the', help='Initial text of the sonnet')\n",
    "#parser.add_argument('--model_dir',   type=str, default='pretrained_models/model_3', help='Network model directory')\n",
    "\n",
    "##############################\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "### Parse input arguments\n",
    "#args = parser.parse_args()\n",
    "\n",
    "model_dir = Path(\"model_Wilde/\")\n",
    "sonnet_seed = \"dorian was staring at his portrait\"\n",
    "\n",
    "#%% Load training parameters\n",
    "#model_dir = Path(\"model_Wilde_2019-12-05_17:38/\")\n",
    "print ('Loading model from: %s' % model_dir)\n",
    "training_args = json.load(open(model_dir / 'training_args.json'))\n",
    "\n",
    "#%% Load encoder and decoder dictionaries\n",
    "number_to_char = json.load(open(model_dir / 'number_to_char.json'))\n",
    "char_to_number = json.load(open(model_dir / 'char_to_number.json'))\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(input_size=training_args['alphabet_len'], \n",
    "            hidden_units=training_args['hidden_units'], \n",
    "            layers_num=training_args['layers_num'])\n",
    "\n",
    "#%% Load network trained parameters\n",
    "net.load_state_dict(torch.load(model_dir / 'net_params.pth', map_location='cpu'))\n",
    "net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "\n",
    "#%% Find initial state of the RNN\n",
    "with torch.no_grad():\n",
    "    # Encode seed\n",
    "    seed_encoded = encode_text(char_to_number, sonnet_seed)\n",
    "    # One hot matrix\n",
    "    seed_onehot = create_one_hot_matrix(seed_encoded, training_args['alphabet_len'])\n",
    "    # To tensor\n",
    "    seed_onehot = torch.tensor(seed_onehot).float()\n",
    "    # Add batch axis\n",
    "    seed_onehot = seed_onehot.unsqueeze(0)\n",
    "    # Forward pass\n",
    "    net_out, net_state = net(seed_onehot)\n",
    "    # Get the most probable last output index\n",
    "    # ---------- sampling using softmax ----------\n",
    "    next_char_encoded = net_out[:, -1, :].argmax().item()\n",
    "    # Print the seed letters\n",
    "    print(sonnet_seed, end='', flush=True)\n",
    "    print(number_to_char[str(next_char_encoded)])\n",
    "\n",
    "#%% Generate sonnet\n",
    "new_line_count = 0\n",
    "tot_char_count = 0\n",
    "while True:\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # The new network input is the one hot encoding of the last chosen letter\n",
    "        net_input = create_one_hot_matrix([next_char_encoded], len(dataset.alphabet))\n",
    "        net_input = torch.tensor(net_input).float()\n",
    "        net_input = net_input.unsqueeze(0)\n",
    "        # Forward pass\n",
    "        net_out, net_state = net(net_input, net_state)\n",
    "        # Get the most probable letter index\n",
    "        \n",
    "        # Using softmax instead of argmax\n",
    "        distrib = np.array(nn.functional.softmax(net_out, dim=-1))\n",
    "        next_char_encoded = np.random.choice(len(distrib.ravel()), size=1, p=distrib.ravel())[0]\n",
    "        \n",
    "        # Get the most probable letter index\n",
    "        #next_char_encoded = net_out.argmax().item()\n",
    "        # Decode the letter\n",
    "        next_char = number_to_char[str(next_char_encoded)]\n",
    "        #next_char_encoded = net_out.argmax().item()\n",
    "        \n",
    "        # Decode the letter\n",
    "        #next_char = number_to_char[str(next_char_encoded)]\n",
    "        \n",
    "        print(next_char, end='', flush=True)\n",
    "        # Count total letters\n",
    "        tot_char_count += 1\n",
    "        # Count new lines\n",
    "        if next_char == '\\n':\n",
    "            new_line_count += 1\n",
    "        # Break if 14 lines or 2000 letters\n",
    "        if new_line_count == 14 or tot_char_count > 2000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
