{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional Neuroscience\n",
    "Accademic year 2019-2020\n",
    "Homework 3\n",
    "\n",
    "Author: Tommaso Tabarelli\n",
    "Period: december 2019\n",
    "\"\"\"\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import reduce\n",
    "from torch import optim, nn\n",
    "from network import Network, train_batch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# Importing packages to implement word2vec\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing word2vec\n",
    "\n",
    "Following the instructions at the following link: https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5746\n",
      "12519\n",
      "the studio was filled with the rich odour of roses\n",
      "['the', 'studio', 'was', 'filled', 'with', 'the', 'rich', 'odour', 'of', 'roses']\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "text = open(\"Picture_of_Dorian_Gray.txt\", 'r').read()\n",
    "\n",
    "# Removing titles\n",
    "text = re.split('\\n{7}', text)[1]\n",
    "\n",
    "# Lowering all text\n",
    "text = text.lower()\n",
    "\n",
    "sentences = re.split('[\\.,!?;:]', text)\n",
    "\n",
    "vocabulary = []\n",
    "all_words = re.split(\"[\\.,!?;:\\n -\\'-]\", text)\n",
    "for word in all_words:\n",
    "    if word not in vocabulary:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary.sort()\n",
    "\n",
    "# Removing the first 3 elements since they are numbers or empty string\n",
    "vocabulary.pop(0)\n",
    "vocabulary.pop(0)\n",
    "vocabulary.pop(0)\n",
    "\n",
    "print(len(text.split(\".\")))\n",
    "print(len(sentences))\n",
    "#print(vocabulary)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "#print(word2idx.keys())\n",
    "\n",
    "print(sentences[0])\n",
    "for sentence in sentences:\n",
    "    # saving index\n",
    "    index = sentences.index(sentence)\n",
    "    sentence = re.split(\"[\\.,!?;:\\n -\\'-]\", sentence)\n",
    "    \n",
    "    # Removing by hand ALL the unwanted characters\n",
    "    for i in range(sentence.count('')):\n",
    "        sentence.pop(sentence.index(''))\n",
    "    for i in range(sentence.count(' ')):\n",
    "        sentence.pop(sentence.index(\" \"))\n",
    "    for i in range(sentence.count('\\n')):\n",
    "        sentence.pop(sentence.index(\"\\n\"))\n",
    "    if (\"152\" in sentence):\n",
    "        sentence.pop(sentence.index(\"152\"))\n",
    "    if (\"1820\" in sentence):\n",
    "        sentence.pop(sentence.index(\"1820\"))\n",
    "    # Overwriting the sentence with a word-split one\n",
    "    sentences[index] = sentence\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"context\" environment to then train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in sentences:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make sure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to define the Neural Network structure to embed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer\n",
    "\n",
    "It should be as large as the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(len(vocabulary)).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "\n",
    "It is chosen to be only 1 hidden layer and it depends on the embedding dimension (arbitraryli chosen).\n",
    "\n",
    "W1 is the weight matrix. It has dimensions: [embedding_dims, vocabulary_size]\n",
    "\n",
    "There is no activation function â€” just plain matrix multiplication. (This will be clearer during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, len(vocabulary)).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Variable(torch.randn(len(vocabulary), embedding_dims).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 9.346418637197958\n",
      "Loss at epo 10: 7.211674061661062\n",
      "Loss at epo 20: 6.778484885877758\n",
      "Loss at epo 30: 6.574319957424175\n",
      "Loss at epo 40: 6.451177387590353\n",
      "Loss at epo 50: 6.367360884102869\n",
      "Loss at epo 60: 6.305997703103249\n",
      "Loss at epo 70: 6.258726537012349\n",
      "Loss at epo 80: 6.22101781125469\n",
      "Loss at epo 90: 6.190239186260286\n",
      "Loss at epo 100: 6.164635735624557\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 101\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(my_net.parameters(), weight_decay=5e-4)\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "\n",
    "        # Forward pass\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "        \n",
    "\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "        #optimizer.step()\n",
    "        \n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(W1, \"W1_weights\")\n",
    "torch.save(W2, \"W2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6779, 5])\n"
     ]
    }
   ],
   "source": [
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved parameters for the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6536, -0.2067, -0.2938,  ..., -0.3121, -0.6741,  1.1771],\n",
      "        [ 0.2630,  1.2787,  1.1708,  ..., -0.9615,  0.1497,  1.2112],\n",
      "        [ 0.5051,  0.5456, -0.4282,  ..., -1.2316,  0.3074,  0.5471],\n",
      "        [-0.2665, -0.3556, -0.5891,  ...,  0.3163,  1.4133,  1.0261],\n",
      "        [ 0.4310,  1.6727,  1.3909,  ...,  1.3273,  0.9603,  0.7538]],\n",
      "       requires_grad=True)\n",
      "tensor([[-4.7673,  0.7313,  1.7859, -1.6416,  3.9699],\n",
      "        [ 0.9328, -0.2321,  0.7114,  0.2274, -0.5491],\n",
      "        [-0.0083, -0.1352,  0.4440, -0.7173, -0.1907],\n",
      "        ...,\n",
      "        [ 0.1161, -1.2554,  1.0307, -1.0508, -0.2169],\n",
      "        [ 1.4459, -1.2231, -0.3555,  0.4521,  1.1497],\n",
      "        [-0.5359,  0.0490,  0.0687, -0.1203, -1.4884]], requires_grad=True)\n",
      "3.3844878673553467\n",
      "4.363911151885986\n"
     ]
    }
   ],
   "source": [
    "W1_try = torch.load(\"W1_weights\")\n",
    "W2_try = torch.load(\"W2_weights\")\n",
    "print(W1_try)\n",
    "print(W2_try)\n",
    "print(torch.max(W1_try).item())\n",
    "print(torch.max(W2_try).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking W2 as the word representation because it depends on context! In other words, I think the center is less biased term, and what I need is to be able to predict words of a given text/style/piece of paper; for this reason context representation is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM network\n",
    "\n",
    "Our network is already well implemented and it uses a LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "        # Call the parent init function (required!)\n",
    "        super().__init__()\n",
    "        # Define recurrent layer\n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                        hidden_size=hidden_units,\n",
    "                        num_layers=layers_num,\n",
    "                        dropout=dropout_prob,\n",
    "                        batch_first=True)\n",
    "        # Define output layer\n",
    "        self.out = nn.Linear(hidden_units, input_size)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        # LSTM\n",
    "        x, rnn_state = self.rnn(x, state)\n",
    "        # Linear layer\n",
    "        x = self.out(x)\n",
    "        return x, rnn_state\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_batch(net, batch_repr, loss_fn, optimizer):\n",
    "\n",
    "    ### Prepare network input and labels\n",
    "    # Get the labels (the last letter of each sequence)\n",
    "    labels_repr = batch_repr[:, -1, :]\n",
    "    \n",
    "    # This is no more useful\n",
    "    #labels_numbers = labels_onehot.argmax(dim=1)\n",
    "    \n",
    "    # Remove the labels from the input tensor\n",
    "    net_input = batch_repr[:, :-1, :]\n",
    "    # batch_repr.shape =   [50, 100, 5]\n",
    "    # labels_repr.shape =  [50, 5]\n",
    "    # net_input.shape =      [50, 99, 5]\n",
    "\n",
    "    ### Forward pass\n",
    "    # Eventually clear previous recorded gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    net_out, _ = net(net_input)\n",
    "\n",
    "    ### Update network\n",
    "    # Evaluate loss only for last output\n",
    "    loss = loss_fn(net_out[:, -1, :], labels_repr)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # Return average batch loss\n",
    "    return float(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, transform=None):\n",
    "\n",
    "        ### Load data\n",
    "        text = open(\"Picture_of_Dorian_Gray.txt\", 'r').read()\n",
    "\n",
    "        # Removing titles\n",
    "        text = re.split('\\n{7}', text)[1]\n",
    "\n",
    "        # Lowering all text\n",
    "        text = text.lower()\n",
    "\n",
    "        sentences = re.split('[\\.,!?;:]', text)\n",
    "\n",
    "        vocabulary = []\n",
    "        all_words = re.split(\"[\\.,!?;:\\n -\\'-]\", text)\n",
    "        for word in all_words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "\n",
    "        vocabulary.sort()\n",
    "\n",
    "        # Removing the first 3 elements since they are numbers or empty string\n",
    "        vocabulary.pop(0)\n",
    "        vocabulary.pop(0)\n",
    "        vocabulary.pop(0)\n",
    "\n",
    "        word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "        idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # saving index\n",
    "            index = sentences.index(sentence)\n",
    "            \n",
    "            sentence = re.split(\"[\\.,!?;:\\n -\\'-]\", sentence)\n",
    "\n",
    "            # Removing by hand ALL the unwanted characters\n",
    "            for i in range(sentence.count('')):\n",
    "                sentence.pop(sentence.index(''))\n",
    "            for i in range(sentence.count(' ')):\n",
    "                sentence.pop(sentence.index(\" \"))\n",
    "            for i in range(sentence.count('\\n')):\n",
    "                sentence.pop(sentence.index(\"\\n\"))\n",
    "            if (\"152\" in sentence):\n",
    "                sentence.pop(sentence.index(\"152\"))\n",
    "            if (\"1820\" in sentence):\n",
    "                sentence.pop(sentence.index(\"1820\"))\n",
    "            # Overwriting the sentence with a word-split one\n",
    "            sentences[index] = sentence\n",
    "\n",
    "\n",
    "        # Removing too short sentences (which are not considered sentences...)\n",
    "        large_sentences = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if (len(sent)>5):\n",
    "                large_sentences.append(sent)\n",
    "        \n",
    "        self.sentences = large_sentences\n",
    "        self.vocabulary = vocabulary\n",
    "        self.word_to_index = word2idx\n",
    "        self.index_to_word = idx2word\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sentence text\n",
    "        text = self.sentences[idx]\n",
    "        # Encode with numbers\n",
    "        encoded = encode_text(self.word_to_index, text)\n",
    "\n",
    "        # Create sample\n",
    "        sample = {'text': text, 'encoded': encoded}\n",
    "        # Transform (if defined)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "def encode_text(word_to_index, text):\n",
    "    encoded = [word_to_index[w] for w in text]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def decode_text(index_to_word, encoded):\n",
    "    text = [index_to_word[c] for c in encoded]\n",
    "    # Building proper string from a list of strings (concatenating them)\n",
    "    text = reduce(lambda s1, s2: s1 + \" \" + s2, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class RandomCrop():\n",
    "    \n",
    "    def __init__(self, crop_len):\n",
    "        self.crop_len = crop_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        text = sample['text']\n",
    "        encoded = sample['encoded']\n",
    "        # Randomly choose an index\n",
    "        tot_words = len(text)\n",
    "        if ((tot_words - self.crop_len) > 0):\n",
    "            start_idx = np.random.randint(0, tot_words - self.crop_len)\n",
    "        # if no operations can be done to sort the sentence\n",
    "        else:\n",
    "            return sample\n",
    "        end_idx = start_idx + self.crop_len\n",
    "        return {**sample,\n",
    "            'text': text[start_idx: end_idx],\n",
    "            'encoded': encoded[start_idx: end_idx]}\n",
    "\n",
    "\n",
    "def create_W2V_representation(encoded, representation):\n",
    "    # Create one hot matrix\n",
    "    #    representation is a matrix with representation row-wise, so\n",
    "    #     len(representation[0]) gives the dimension of a word\n",
    "    encoded_repr = np.zeros([len(encoded), len(representation[0])])\n",
    "    tot_words = len(encoded)\n",
    "    # Placing words vectors at the respective position of the letters: \"encoded\" indeed have numbers\n",
    "    # that encode the words, and here it is used as index dimension\n",
    "    encoded_repr[:, :] = representation.detach().numpy()[encoded]\n",
    "    return encoded_repr\n",
    "\n",
    "\n",
    "class W2V_Encoder():\n",
    "    \n",
    "    def __init__(self, W2V_representation):\n",
    "        self.representation = W2V_representation\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Load encoded text with numbers\n",
    "        encoded = np.array(sample['encoded'])\n",
    "        # Create  representation matrix\n",
    "        encoded_repr = create_W2V_representation(encoded, self.representation)\n",
    "        return {**sample,\n",
    "            'encoded_repr': encoded_repr}\n",
    "\n",
    "\n",
    "class ToTensor():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        # Convert one hot encoded text to pytorch tensor\n",
    "        encoded_repr = torch.tensor(sample['encoded_repr']).float()\n",
    "        return {'encoded_repr': encoded_repr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the sentences have at least 5 words, so to have crop_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_len = 5\n",
    "\n",
    "trans = transforms.Compose([RandomCrop(crop_len),\n",
    "                        W2V_Encoder(W2_try),\n",
    "                        ToTensor()\n",
    "                        ])\n",
    "\n",
    "dataset = word_Dataset(filepath=\"Picture_of_Dorian_Gray.txt\", \n",
    "                        transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "my_sentences = []\n",
    "lenghts = []\n",
    "for sent in dataset.sentences:\n",
    "    #print(sent)\n",
    "    lenghts.append(len(sent))\n",
    "print(min(lenghts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 5\n",
    "hidden_units = 500\n",
    "num_layers = 2\n",
    "batchsize = 20\n",
    "\n",
    "my_net = Network(embedding_dims, hidden_units, num_layers, dropout_prob=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n",
      "##################################\n",
      "## EPOCH 1\n",
      "##################################\n",
      "\t Training loss (single batch): 3.8221375942230225\n",
      "\t Training loss (single batch): 4.3480072021484375\n",
      "\t Training loss (single batch): 3.2949044704437256\n",
      "\t Training loss (single batch): 2.896714448928833\n",
      "\t Training loss (single batch): 2.490509510040283\n",
      "\t Training loss (single batch): 1.2603501081466675\n",
      "\t Training loss (single batch): 1.8766249418258667\n",
      "\t Training loss (single batch): 1.8936928510665894\n",
      "\t Training loss (single batch): 1.8663142919540405\n",
      "\t Training loss (single batch): 1.3003733158111572\n",
      "\t Training loss (single batch): 1.8950037956237793\n",
      "\t Training loss (single batch): 1.5635899305343628\n",
      "\t Training loss (single batch): 1.4965094327926636\n",
      "\t Training loss (single batch): 1.605830430984497\n",
      "\t Training loss (single batch): 1.665558099746704\n",
      "\t Training loss (single batch): 1.3614592552185059\n",
      "\t Training loss (single batch): 1.6390130519866943\n",
      "\t Training loss (single batch): 1.8574577569961548\n",
      "\t Training loss (single batch): 1.469441533088684\n",
      "\t Training loss (single batch): 1.4657237529754639\n",
      "\t Training loss (single batch): 1.308458924293518\n",
      "\t Training loss (single batch): 1.4696946144104004\n",
      "\t Training loss (single batch): 2.3712661266326904\n",
      "\t Training loss (single batch): 1.2187093496322632\n",
      "\t Training loss (single batch): 2.4697601795196533\n",
      "\t Training loss (single batch): 1.8140063285827637\n",
      "\t Training loss (single batch): 1.2661306858062744\n",
      "\t Training loss (single batch): 1.4870234727859497\n",
      "\t Training loss (single batch): 1.4808933734893799\n",
      "\t Training loss (single batch): 1.6798841953277588\n",
      "\t Training loss (single batch): 1.3550587892532349\n",
      "\t Training loss (single batch): 1.1907392740249634\n",
      "\t Training loss (single batch): 1.0220082998275757\n",
      "\t Training loss (single batch): 1.6121784448623657\n",
      "\t Training loss (single batch): 1.4887615442276\n",
      "\t Training loss (single batch): 1.7408565282821655\n",
      "\t Training loss (single batch): 1.1201008558273315\n",
      "\t Training loss (single batch): 1.0279914140701294\n",
      "\t Training loss (single batch): 1.5921801328659058\n",
      "\t Training loss (single batch): 1.1713510751724243\n",
      "\t Training loss (single batch): 0.9469754099845886\n",
      "\t Training loss (single batch): 0.7722187638282776\n",
      "\t Training loss (single batch): 1.8414502143859863\n",
      "\t Training loss (single batch): 1.8868013620376587\n",
      "\t Training loss (single batch): 0.9622449278831482\n",
      "\t Training loss (single batch): 1.3886085748672485\n",
      "\t Training loss (single batch): 1.5714398622512817\n",
      "\t Training loss (single batch): 1.2942743301391602\n",
      "\t Training loss (single batch): 0.8889880180358887\n",
      "\t Training loss (single batch): 1.442068338394165\n",
      "\t Training loss (single batch): 1.6734367609024048\n",
      "\t Training loss (single batch): 1.6085002422332764\n",
      "\t Training loss (single batch): 1.3726820945739746\n",
      "\t Training loss (single batch): 1.2212170362472534\n",
      "\t Training loss (single batch): 1.6513909101486206\n",
      "\t Training loss (single batch): 1.2373610734939575\n",
      "\t Training loss (single batch): 1.896323323249817\n",
      "\t Training loss (single batch): 1.4319531917572021\n",
      "\t Training loss (single batch): 1.3137130737304688\n",
      "\t Training loss (single batch): 1.8271313905715942\n",
      "\t Training loss (single batch): 1.4906302690505981\n",
      "\t Training loss (single batch): 1.2862814664840698\n",
      "\t Training loss (single batch): 1.511846661567688\n",
      "\t Training loss (single batch): 1.2823299169540405\n",
      "\t Training loss (single batch): 1.5202915668487549\n",
      "\t Training loss (single batch): 1.250966191291809\n",
      "\t Training loss (single batch): 1.3017082214355469\n",
      "\t Training loss (single batch): 1.2783395051956177\n",
      "\t Training loss (single batch): 1.5585124492645264\n",
      "\t Training loss (single batch): 2.036853075027466\n",
      "\t Training loss (single batch): 1.4285451173782349\n",
      "\t Training loss (single batch): 1.6542977094650269\n",
      "\t Training loss (single batch): 1.7352149486541748\n",
      "\t Training loss (single batch): 1.4633303880691528\n",
      "\t Training loss (single batch): 1.4335683584213257\n",
      "\t Training loss (single batch): 1.3706061840057373\n",
      "\t Training loss (single batch): 1.2066742181777954\n",
      "\t Training loss (single batch): 1.236661672592163\n",
      "\t Training loss (single batch): 1.4463236331939697\n",
      "\t Training loss (single batch): 1.4899951219558716\n",
      "\t Training loss (single batch): 0.9328374862670898\n",
      "\t Training loss (single batch): 1.42816162109375\n",
      "\t Training loss (single batch): 1.6884033679962158\n",
      "\t Training loss (single batch): 1.4621529579162598\n",
      "\t Training loss (single batch): 1.258744478225708\n",
      "\t Training loss (single batch): 1.3028799295425415\n",
      "\t Training loss (single batch): 1.4573135375976562\n",
      "\t Training loss (single batch): 1.7763618230819702\n",
      "\t Training loss (single batch): 1.2708206176757812\n",
      "\t Training loss (single batch): 1.768079400062561\n",
      "\t Training loss (single batch): 1.6340936422348022\n",
      "\t Training loss (single batch): 0.9938364028930664\n",
      "\t Training loss (single batch): 1.60008704662323\n",
      "\t Training loss (single batch): 1.8559571504592896\n",
      "\t Training loss (single batch): 1.7315577268600464\n",
      "\t Training loss (single batch): 1.304861307144165\n",
      "\t Training loss (single batch): 1.5510215759277344\n",
      "\t Training loss (single batch): 0.9540538191795349\n",
      "\t Training loss (single batch): 1.642805814743042\n",
      "\t Training loss (single batch): 1.5879580974578857\n",
      "\t Training loss (single batch): 1.108330488204956\n",
      "\t Training loss (single batch): 1.4178552627563477\n",
      "\t Training loss (single batch): 1.2311240434646606\n",
      "\t Training loss (single batch): 1.7834707498550415\n",
      "\t Training loss (single batch): 1.6948885917663574\n",
      "\t Training loss (single batch): 1.5120288133621216\n",
      "\t Training loss (single batch): 1.4065155982971191\n",
      "\t Training loss (single batch): 1.438244104385376\n",
      "\t Training loss (single batch): 1.5072031021118164\n",
      "\t Training loss (single batch): 1.4782929420471191\n",
      "\t Training loss (single batch): 1.413623571395874\n",
      "\t Training loss (single batch): 1.3127610683441162\n",
      "\t Training loss (single batch): 1.6904245615005493\n",
      "\t Training loss (single batch): 1.5693305730819702\n",
      "\t Training loss (single batch): 1.686572790145874\n",
      "\t Training loss (single batch): 1.1045475006103516\n",
      "\t Training loss (single batch): 1.5950231552124023\n",
      "\t Training loss (single batch): 1.161449909210205\n",
      "\t Training loss (single batch): 1.541240930557251\n",
      "\t Training loss (single batch): 1.3908830881118774\n",
      "\t Training loss (single batch): 1.435042142868042\n",
      "\t Training loss (single batch): 1.4551427364349365\n",
      "\t Training loss (single batch): 1.3587262630462646\n",
      "\t Training loss (single batch): 1.2844321727752686\n",
      "\t Training loss (single batch): 1.3809236288070679\n",
      "\t Training loss (single batch): 0.9834831953048706\n",
      "\t Training loss (single batch): 1.2648483514785767\n",
      "\t Training loss (single batch): 1.3488829135894775\n",
      "\t Training loss (single batch): 1.0992287397384644\n",
      "\t Training loss (single batch): 1.523700475692749\n",
      "\t Training loss (single batch): 1.4340126514434814\n",
      "\t Training loss (single batch): 1.5937769412994385\n",
      "\t Training loss (single batch): 1.042289137840271\n",
      "\t Training loss (single batch): 1.220626711845398\n",
      "\t Training loss (single batch): 1.3700463771820068\n",
      "\t Training loss (single batch): 1.6046650409698486\n",
      "\t Training loss (single batch): 1.4258891344070435\n",
      "\t Training loss (single batch): 1.7888779640197754\n",
      "\t Training loss (single batch): 1.3507044315338135\n",
      "\t Training loss (single batch): 0.9628626704216003\n",
      "\t Training loss (single batch): 1.2494364976882935\n",
      "\t Training loss (single batch): 1.3852338790893555\n",
      "\t Training loss (single batch): 1.865058422088623\n",
      "\t Training loss (single batch): 1.1513723134994507\n",
      "\t Training loss (single batch): 1.5735491514205933\n",
      "\t Training loss (single batch): 1.3328622579574585\n",
      "\t Training loss (single batch): 1.2803629636764526\n",
      "\t Training loss (single batch): 1.2725657224655151\n",
      "\t Training loss (single batch): 1.6300386190414429\n",
      "\t Training loss (single batch): 0.969755232334137\n",
      "\t Training loss (single batch): 1.385149359703064\n",
      "\t Training loss (single batch): 1.1830168962478638\n",
      "\t Training loss (single batch): 1.2574949264526367\n",
      "\t Training loss (single batch): 1.5000205039978027\n",
      "\t Training loss (single batch): 1.1988680362701416\n",
      "\t Training loss (single batch): 1.528682827949524\n",
      "\t Training loss (single batch): 0.9254231452941895\n",
      "\t Training loss (single batch): 1.8927395343780518\n",
      "\t Training loss (single batch): 1.9991612434387207\n",
      "\t Training loss (single batch): 1.0793218612670898\n",
      "\t Training loss (single batch): 1.3901106119155884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4891713857650757\n",
      "\t Training loss (single batch): 1.3496159315109253\n",
      "\t Training loss (single batch): 1.1127424240112305\n",
      "\t Training loss (single batch): 1.2229430675506592\n",
      "\t Training loss (single batch): 2.049783945083618\n",
      "\t Training loss (single batch): 1.5789822340011597\n",
      "\t Training loss (single batch): 1.2520272731781006\n",
      "\t Training loss (single batch): 1.0087761878967285\n",
      "\t Training loss (single batch): 1.2296509742736816\n",
      "\t Training loss (single batch): 1.4132428169250488\n",
      "\t Training loss (single batch): 1.522263765335083\n",
      "\t Training loss (single batch): 1.7031246423721313\n",
      "\t Training loss (single batch): 1.5257399082183838\n",
      "\t Training loss (single batch): 1.3766995668411255\n",
      "\t Training loss (single batch): 1.5454668998718262\n",
      "\t Training loss (single batch): 1.1475512981414795\n",
      "\t Training loss (single batch): 1.228161334991455\n",
      "\t Training loss (single batch): 1.3872756958007812\n",
      "\t Training loss (single batch): 1.612813949584961\n",
      "\t Training loss (single batch): 1.8342117071151733\n",
      "\t Training loss (single batch): 1.3612946271896362\n",
      "\t Training loss (single batch): 1.304051160812378\n",
      "\t Training loss (single batch): 1.3830755949020386\n",
      "\t Training loss (single batch): 1.6000030040740967\n",
      "\t Training loss (single batch): 1.5632898807525635\n",
      "\t Training loss (single batch): 1.4387543201446533\n",
      "\t Training loss (single batch): 1.3584285974502563\n",
      "\t Training loss (single batch): 1.4934054613113403\n",
      "\t Training loss (single batch): 1.3905613422393799\n",
      "\t Training loss (single batch): 0.9314814209938049\n",
      "\t Training loss (single batch): 1.692976713180542\n",
      "\t Training loss (single batch): 1.7016232013702393\n",
      "\t Training loss (single batch): 1.489754319190979\n",
      "\t Training loss (single batch): 1.6454321146011353\n",
      "\t Training loss (single batch): 1.0196638107299805\n",
      "\t Training loss (single batch): 1.035567283630371\n",
      "\t Training loss (single batch): 1.6551648378372192\n",
      "\t Training loss (single batch): 1.3986401557922363\n",
      "\t Training loss (single batch): 1.1331616640090942\n",
      "\t Training loss (single batch): 1.290823221206665\n",
      "\t Training loss (single batch): 1.204946756362915\n",
      "\t Training loss (single batch): 1.1125530004501343\n",
      "\t Training loss (single batch): 1.321731686592102\n",
      "\t Training loss (single batch): 1.5031763315200806\n",
      "\t Training loss (single batch): 1.2458504438400269\n",
      "\t Training loss (single batch): 1.68095862865448\n",
      "\t Training loss (single batch): 1.356791377067566\n",
      "\t Training loss (single batch): 1.8390772342681885\n",
      "\t Training loss (single batch): 1.4035038948059082\n",
      "\t Training loss (single batch): 1.7960643768310547\n",
      "\t Training loss (single batch): 1.3042620420455933\n",
      "\t Training loss (single batch): 1.3493775129318237\n",
      "\t Training loss (single batch): 1.4840484857559204\n",
      "\t Training loss (single batch): 1.6685614585876465\n",
      "\t Training loss (single batch): 1.244249939918518\n",
      "\t Training loss (single batch): 1.4453625679016113\n",
      "\t Training loss (single batch): 1.439483404159546\n",
      "\t Training loss (single batch): 1.6781105995178223\n",
      "\t Training loss (single batch): 1.1524600982666016\n",
      "\t Training loss (single batch): 1.6462732553482056\n",
      "\t Training loss (single batch): 0.953770101070404\n",
      "\t Training loss (single batch): 1.1161739826202393\n",
      "\t Training loss (single batch): 1.871004581451416\n",
      "\t Training loss (single batch): 1.7542375326156616\n",
      "\t Training loss (single batch): 1.347406268119812\n",
      "\t Training loss (single batch): 1.0613304376602173\n",
      "\t Training loss (single batch): 2.001713991165161\n",
      "\t Training loss (single batch): 2.0671093463897705\n",
      "\t Training loss (single batch): 1.7848459482192993\n",
      "\t Training loss (single batch): 1.6113191843032837\n",
      "\t Training loss (single batch): 1.6708439588546753\n",
      "\t Training loss (single batch): 1.2711280584335327\n",
      "\t Training loss (single batch): 1.3145564794540405\n",
      "\t Training loss (single batch): 1.1402270793914795\n",
      "\t Training loss (single batch): 1.630731463432312\n",
      "\t Training loss (single batch): 1.1536110639572144\n",
      "\t Training loss (single batch): 1.3996084928512573\n",
      "\t Training loss (single batch): 1.2923372983932495\n",
      "\t Training loss (single batch): 1.1001474857330322\n",
      "\t Training loss (single batch): 1.2994173765182495\n",
      "\t Training loss (single batch): 1.6535862684249878\n",
      "\t Training loss (single batch): 2.0959553718566895\n",
      "\t Training loss (single batch): 1.3602956533432007\n",
      "\t Training loss (single batch): 1.3867762088775635\n",
      "\t Training loss (single batch): 1.5819164514541626\n",
      "\t Training loss (single batch): 1.3340506553649902\n",
      "\t Training loss (single batch): 0.7445252537727356\n",
      "\t Training loss (single batch): 1.079512357711792\n",
      "\t Training loss (single batch): 1.195650339126587\n",
      "\t Training loss (single batch): 1.6513261795043945\n",
      "\t Training loss (single batch): 1.3520474433898926\n",
      "\t Training loss (single batch): 1.5499082803726196\n",
      "\t Training loss (single batch): 1.5407497882843018\n",
      "\t Training loss (single batch): 1.1892850399017334\n",
      "\t Training loss (single batch): 1.8067641258239746\n",
      "\t Training loss (single batch): 1.0040255784988403\n",
      "\t Training loss (single batch): 1.7757422924041748\n",
      "\t Training loss (single batch): 1.0715223550796509\n",
      "\t Training loss (single batch): 2.0245325565338135\n",
      "\t Training loss (single batch): 1.4951281547546387\n",
      "\t Training loss (single batch): 1.8758761882781982\n",
      "\t Training loss (single batch): 1.5563932657241821\n",
      "\t Training loss (single batch): 1.3025418519973755\n",
      "\t Training loss (single batch): 1.1570274829864502\n",
      "\t Training loss (single batch): 1.3361507654190063\n",
      "\t Training loss (single batch): 1.705336332321167\n",
      "\t Training loss (single batch): 1.1738615036010742\n",
      "\t Training loss (single batch): 1.314093828201294\n",
      "\t Training loss (single batch): 1.6941096782684326\n",
      "\t Training loss (single batch): 1.4656572341918945\n",
      "\t Training loss (single batch): 1.4954010248184204\n",
      "\t Training loss (single batch): 1.4384225606918335\n",
      "\t Training loss (single batch): 1.5680898427963257\n",
      "\t Training loss (single batch): 1.513412356376648\n",
      "\t Training loss (single batch): 1.2437416315078735\n",
      "\t Training loss (single batch): 1.3506731986999512\n",
      "\t Training loss (single batch): 1.1468266248703003\n",
      "\t Training loss (single batch): 1.3862146139144897\n",
      "\t Training loss (single batch): 1.6678024530410767\n",
      "\t Training loss (single batch): 1.5503097772598267\n",
      "\t Training loss (single batch): 1.8504599332809448\n",
      "\t Training loss (single batch): 1.633578896522522\n",
      "\t Training loss (single batch): 1.4164568185806274\n",
      "\t Training loss (single batch): 1.3209929466247559\n",
      "\t Training loss (single batch): 1.8226739168167114\n",
      "\t Training loss (single batch): 1.1612759828567505\n",
      "\t Training loss (single batch): 1.277083396911621\n",
      "\t Training loss (single batch): 1.6473802328109741\n",
      "\t Training loss (single batch): 1.585931420326233\n",
      "\t Training loss (single batch): 0.9254623055458069\n",
      "\t Training loss (single batch): 1.5490180253982544\n",
      "\t Training loss (single batch): 1.510377049446106\n",
      "\t Training loss (single batch): 1.3175325393676758\n",
      "\t Training loss (single batch): 1.0409449338912964\n",
      "\t Training loss (single batch): 1.3546770811080933\n",
      "\t Training loss (single batch): 1.1815168857574463\n",
      "\t Training loss (single batch): 1.2083348035812378\n",
      "\t Training loss (single batch): 1.1563550233840942\n",
      "\t Training loss (single batch): 1.332039713859558\n",
      "\t Training loss (single batch): 1.1020469665527344\n",
      "\t Training loss (single batch): 1.2428314685821533\n",
      "\t Training loss (single batch): 0.9992460608482361\n",
      "\t Training loss (single batch): 1.4566168785095215\n",
      "\t Training loss (single batch): 1.391591191291809\n",
      "\t Training loss (single batch): 1.248186707496643\n",
      "\t Training loss (single batch): 1.456498622894287\n",
      "\t Training loss (single batch): 1.1473498344421387\n",
      "\t Training loss (single batch): 1.9741854667663574\n",
      "\t Training loss (single batch): 1.126058578491211\n",
      "\t Training loss (single batch): 1.33464777469635\n",
      "\t Training loss (single batch): 1.17100191116333\n",
      "\t Training loss (single batch): 1.075419306755066\n",
      "\t Training loss (single batch): 1.3088468313217163\n",
      "\t Training loss (single batch): 1.0324681997299194\n",
      "\t Training loss (single batch): 1.3151156902313232\n",
      "\t Training loss (single batch): 1.2068216800689697\n",
      "##################################\n",
      "## EPOCH 2\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7684333324432373\n",
      "\t Training loss (single batch): 1.3662163019180298\n",
      "\t Training loss (single batch): 1.2611736059188843\n",
      "\t Training loss (single batch): 1.8411693572998047\n",
      "\t Training loss (single batch): 1.3840258121490479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1998635530471802\n",
      "\t Training loss (single batch): 1.1155022382736206\n",
      "\t Training loss (single batch): 1.5679799318313599\n",
      "\t Training loss (single batch): 1.2221055030822754\n",
      "\t Training loss (single batch): 1.6225379705429077\n",
      "\t Training loss (single batch): 1.5927387475967407\n",
      "\t Training loss (single batch): 1.1203464269638062\n",
      "\t Training loss (single batch): 1.4076415300369263\n",
      "\t Training loss (single batch): 1.0040093660354614\n",
      "\t Training loss (single batch): 1.5664825439453125\n",
      "\t Training loss (single batch): 1.1402521133422852\n",
      "\t Training loss (single batch): 1.2199534177780151\n",
      "\t Training loss (single batch): 1.793369174003601\n",
      "\t Training loss (single batch): 1.4587093591690063\n",
      "\t Training loss (single batch): 1.3269951343536377\n",
      "\t Training loss (single batch): 1.4837627410888672\n",
      "\t Training loss (single batch): 1.234573245048523\n",
      "\t Training loss (single batch): 1.4877716302871704\n",
      "\t Training loss (single batch): 1.3176504373550415\n",
      "\t Training loss (single batch): 1.6135808229446411\n",
      "\t Training loss (single batch): 1.399316430091858\n",
      "\t Training loss (single batch): 1.5718241930007935\n",
      "\t Training loss (single batch): 2.3813540935516357\n",
      "\t Training loss (single batch): 0.8741830587387085\n",
      "\t Training loss (single batch): 1.107493281364441\n",
      "\t Training loss (single batch): 1.7623916864395142\n",
      "\t Training loss (single batch): 1.396622657775879\n",
      "\t Training loss (single batch): 1.0147607326507568\n",
      "\t Training loss (single batch): 1.0206154584884644\n",
      "\t Training loss (single batch): 0.8064295649528503\n",
      "\t Training loss (single batch): 1.6761395931243896\n",
      "\t Training loss (single batch): 1.2678509950637817\n",
      "\t Training loss (single batch): 1.332351803779602\n",
      "\t Training loss (single batch): 0.970979630947113\n",
      "\t Training loss (single batch): 1.559212327003479\n",
      "\t Training loss (single batch): 1.3750052452087402\n",
      "\t Training loss (single batch): 1.397047996520996\n",
      "\t Training loss (single batch): 1.153368592262268\n",
      "\t Training loss (single batch): 1.205257534980774\n",
      "\t Training loss (single batch): 1.1526334285736084\n",
      "\t Training loss (single batch): 1.8900824785232544\n",
      "\t Training loss (single batch): 1.4295762777328491\n",
      "\t Training loss (single batch): 1.7527556419372559\n",
      "\t Training loss (single batch): 0.9474460482597351\n",
      "\t Training loss (single batch): 1.234569787979126\n",
      "\t Training loss (single batch): 1.2279833555221558\n",
      "\t Training loss (single batch): 0.7677276134490967\n",
      "\t Training loss (single batch): 1.7398443222045898\n",
      "\t Training loss (single batch): 1.1257848739624023\n",
      "\t Training loss (single batch): 1.6756646633148193\n",
      "\t Training loss (single batch): 1.0530362129211426\n",
      "\t Training loss (single batch): 1.4872032403945923\n",
      "\t Training loss (single batch): 1.5443669557571411\n",
      "\t Training loss (single batch): 1.3155628442764282\n",
      "\t Training loss (single batch): 1.5133053064346313\n",
      "\t Training loss (single batch): 1.5545237064361572\n",
      "\t Training loss (single batch): 1.5262317657470703\n",
      "\t Training loss (single batch): 1.6641079187393188\n",
      "\t Training loss (single batch): 1.1980690956115723\n",
      "\t Training loss (single batch): 1.561181902885437\n",
      "\t Training loss (single batch): 1.2790329456329346\n",
      "\t Training loss (single batch): 0.8595331907272339\n",
      "\t Training loss (single batch): 1.4981563091278076\n",
      "\t Training loss (single batch): 1.5111960172653198\n",
      "\t Training loss (single batch): 1.3308594226837158\n",
      "\t Training loss (single batch): 1.3828387260437012\n",
      "\t Training loss (single batch): 1.5153167247772217\n",
      "\t Training loss (single batch): 1.5446038246154785\n",
      "\t Training loss (single batch): 0.918775737285614\n",
      "\t Training loss (single batch): 0.8205685019493103\n",
      "\t Training loss (single batch): 1.3757349252700806\n",
      "\t Training loss (single batch): 1.6684976816177368\n",
      "\t Training loss (single batch): 1.7093008756637573\n",
      "\t Training loss (single batch): 1.2746213674545288\n",
      "\t Training loss (single batch): 1.2443652153015137\n",
      "\t Training loss (single batch): 1.287831425666809\n",
      "\t Training loss (single batch): 1.1093336343765259\n",
      "\t Training loss (single batch): 1.104032278060913\n",
      "\t Training loss (single batch): 1.1623611450195312\n",
      "\t Training loss (single batch): 1.9993044137954712\n",
      "\t Training loss (single batch): 1.370820164680481\n",
      "\t Training loss (single batch): 1.4643731117248535\n",
      "\t Training loss (single batch): 1.3630696535110474\n",
      "\t Training loss (single batch): 2.1051888465881348\n",
      "\t Training loss (single batch): 1.942281723022461\n",
      "\t Training loss (single batch): 1.2937005758285522\n",
      "\t Training loss (single batch): 1.3810807466506958\n",
      "\t Training loss (single batch): 1.2117770910263062\n",
      "\t Training loss (single batch): 1.0711379051208496\n",
      "\t Training loss (single batch): 1.3570321798324585\n",
      "\t Training loss (single batch): 1.680521845817566\n",
      "\t Training loss (single batch): 1.5946218967437744\n",
      "\t Training loss (single batch): 1.3382073640823364\n",
      "\t Training loss (single batch): 1.4710021018981934\n",
      "\t Training loss (single batch): 0.874025821685791\n",
      "\t Training loss (single batch): 1.49528169631958\n",
      "\t Training loss (single batch): 1.4266923666000366\n",
      "\t Training loss (single batch): 1.307911992073059\n",
      "\t Training loss (single batch): 1.125937581062317\n",
      "\t Training loss (single batch): 0.9295610785484314\n",
      "\t Training loss (single batch): 2.1008458137512207\n",
      "\t Training loss (single batch): 1.8750888109207153\n",
      "\t Training loss (single batch): 1.4233258962631226\n",
      "\t Training loss (single batch): 1.4451502561569214\n",
      "\t Training loss (single batch): 1.6024565696716309\n",
      "\t Training loss (single batch): 1.2780004739761353\n",
      "\t Training loss (single batch): 1.4028593301773071\n",
      "\t Training loss (single batch): 1.4292926788330078\n",
      "\t Training loss (single batch): 1.7569130659103394\n",
      "\t Training loss (single batch): 1.5070892572402954\n",
      "\t Training loss (single batch): 1.596753716468811\n",
      "\t Training loss (single batch): 1.5637235641479492\n",
      "\t Training loss (single batch): 1.1983529329299927\n",
      "\t Training loss (single batch): 1.3655706644058228\n",
      "\t Training loss (single batch): 1.57243812084198\n",
      "\t Training loss (single batch): 1.5953524112701416\n",
      "\t Training loss (single batch): 1.2871822118759155\n",
      "\t Training loss (single batch): 1.502028465270996\n",
      "\t Training loss (single batch): 1.4942948818206787\n",
      "\t Training loss (single batch): 1.249763011932373\n",
      "\t Training loss (single batch): 1.1653302907943726\n",
      "\t Training loss (single batch): 0.9405314922332764\n",
      "\t Training loss (single batch): 1.0328713655471802\n",
      "\t Training loss (single batch): 1.6995505094528198\n",
      "\t Training loss (single batch): 1.4887102842330933\n",
      "\t Training loss (single batch): 0.8157859444618225\n",
      "\t Training loss (single batch): 1.1503629684448242\n",
      "\t Training loss (single batch): 1.4961317777633667\n",
      "\t Training loss (single batch): 1.4879779815673828\n",
      "\t Training loss (single batch): 1.232283353805542\n",
      "\t Training loss (single batch): 1.5817818641662598\n",
      "\t Training loss (single batch): 1.1262296438217163\n",
      "\t Training loss (single batch): 1.5696814060211182\n",
      "\t Training loss (single batch): 1.3093600273132324\n",
      "\t Training loss (single batch): 1.0350531339645386\n",
      "\t Training loss (single batch): 1.5399624109268188\n",
      "\t Training loss (single batch): 1.6219384670257568\n",
      "\t Training loss (single batch): 1.1873040199279785\n",
      "\t Training loss (single batch): 1.1673271656036377\n",
      "\t Training loss (single batch): 1.5927553176879883\n",
      "\t Training loss (single batch): 1.5181427001953125\n",
      "\t Training loss (single batch): 1.528116226196289\n",
      "\t Training loss (single batch): 1.4330216646194458\n",
      "\t Training loss (single batch): 1.8557825088500977\n",
      "\t Training loss (single batch): 1.0276397466659546\n",
      "\t Training loss (single batch): 1.0955079793930054\n",
      "\t Training loss (single batch): 1.1569257974624634\n",
      "\t Training loss (single batch): 1.4413622617721558\n",
      "\t Training loss (single batch): 1.7292320728302002\n",
      "\t Training loss (single batch): 1.6347498893737793\n",
      "\t Training loss (single batch): 1.2130604982376099\n",
      "\t Training loss (single batch): 1.1757721900939941\n",
      "\t Training loss (single batch): 2.1900668144226074\n",
      "\t Training loss (single batch): 1.7213057279586792\n",
      "\t Training loss (single batch): 1.4233256578445435\n",
      "\t Training loss (single batch): 1.2594952583312988\n",
      "\t Training loss (single batch): 1.3244816064834595\n",
      "\t Training loss (single batch): 1.1946134567260742\n",
      "\t Training loss (single batch): 1.8144363164901733\n",
      "\t Training loss (single batch): 1.7105704545974731\n",
      "\t Training loss (single batch): 1.603440761566162\n",
      "\t Training loss (single batch): 1.4463931322097778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3515899181365967\n",
      "\t Training loss (single batch): 2.145787000656128\n",
      "\t Training loss (single batch): 1.8690464496612549\n",
      "\t Training loss (single batch): 1.5156370401382446\n",
      "\t Training loss (single batch): 1.2033493518829346\n",
      "\t Training loss (single batch): 1.0312904119491577\n",
      "\t Training loss (single batch): 1.0394785404205322\n",
      "\t Training loss (single batch): 1.5499213933944702\n",
      "\t Training loss (single batch): 1.4476556777954102\n",
      "\t Training loss (single batch): 1.0836308002471924\n",
      "\t Training loss (single batch): 1.0129491090774536\n",
      "\t Training loss (single batch): 1.340132474899292\n",
      "\t Training loss (single batch): 1.8998738527297974\n",
      "\t Training loss (single batch): 1.4201709032058716\n",
      "\t Training loss (single batch): 1.2257113456726074\n",
      "\t Training loss (single batch): 1.3299803733825684\n",
      "\t Training loss (single batch): 1.2208009958267212\n",
      "\t Training loss (single batch): 1.282057285308838\n",
      "\t Training loss (single batch): 1.322852373123169\n",
      "\t Training loss (single batch): 0.9981014132499695\n",
      "\t Training loss (single batch): 1.4330980777740479\n",
      "\t Training loss (single batch): 1.2820717096328735\n",
      "\t Training loss (single batch): 1.2241054773330688\n",
      "\t Training loss (single batch): 0.8625149130821228\n",
      "\t Training loss (single batch): 1.2709909677505493\n",
      "\t Training loss (single batch): 1.30845308303833\n",
      "\t Training loss (single batch): 1.3022416830062866\n",
      "\t Training loss (single batch): 1.245590329170227\n",
      "\t Training loss (single batch): 1.5312660932540894\n",
      "\t Training loss (single batch): 2.019580125808716\n",
      "\t Training loss (single batch): 1.543434739112854\n",
      "\t Training loss (single batch): 1.2088048458099365\n",
      "\t Training loss (single batch): 1.3913803100585938\n",
      "\t Training loss (single batch): 1.7822850942611694\n",
      "\t Training loss (single batch): 1.0503910779953003\n",
      "\t Training loss (single batch): 1.5076125860214233\n",
      "\t Training loss (single batch): 1.712514042854309\n",
      "\t Training loss (single batch): 2.1601269245147705\n",
      "\t Training loss (single batch): 1.3572052717208862\n",
      "\t Training loss (single batch): 1.59901762008667\n",
      "\t Training loss (single batch): 1.3081088066101074\n",
      "\t Training loss (single batch): 1.5825228691101074\n",
      "\t Training loss (single batch): 1.993371605873108\n",
      "\t Training loss (single batch): 1.4560226202011108\n",
      "\t Training loss (single batch): 2.079761028289795\n",
      "\t Training loss (single batch): 1.011589527130127\n",
      "\t Training loss (single batch): 1.7329713106155396\n",
      "\t Training loss (single batch): 1.4051008224487305\n",
      "\t Training loss (single batch): 1.640597939491272\n",
      "\t Training loss (single batch): 0.9807208776473999\n",
      "\t Training loss (single batch): 1.735986590385437\n",
      "\t Training loss (single batch): 1.109348177909851\n",
      "\t Training loss (single batch): 1.6512314081192017\n",
      "\t Training loss (single batch): 1.1043825149536133\n",
      "\t Training loss (single batch): 1.9769359827041626\n",
      "\t Training loss (single batch): 1.4483680725097656\n",
      "\t Training loss (single batch): 1.2067664861679077\n",
      "\t Training loss (single batch): 1.572628378868103\n",
      "\t Training loss (single batch): 1.2023030519485474\n",
      "\t Training loss (single batch): 1.503555417060852\n",
      "\t Training loss (single batch): 1.5425636768341064\n",
      "\t Training loss (single batch): 1.2807358503341675\n",
      "\t Training loss (single batch): 1.211489200592041\n",
      "\t Training loss (single batch): 1.2439699172973633\n",
      "\t Training loss (single batch): 1.2520169019699097\n",
      "\t Training loss (single batch): 1.1035257577896118\n",
      "\t Training loss (single batch): 1.3214629888534546\n",
      "\t Training loss (single batch): 1.052477478981018\n",
      "\t Training loss (single batch): 1.5686345100402832\n",
      "\t Training loss (single batch): 1.0775783061981201\n",
      "\t Training loss (single batch): 1.5825809240341187\n",
      "\t Training loss (single batch): 1.2689318656921387\n",
      "\t Training loss (single batch): 1.271701455116272\n",
      "\t Training loss (single batch): 1.2528544664382935\n",
      "\t Training loss (single batch): 1.3660986423492432\n",
      "\t Training loss (single batch): 1.4258137941360474\n",
      "\t Training loss (single batch): 1.289385199546814\n",
      "\t Training loss (single batch): 1.1789851188659668\n",
      "\t Training loss (single batch): 1.1848828792572021\n",
      "\t Training loss (single batch): 1.0386335849761963\n",
      "\t Training loss (single batch): 1.227325677871704\n",
      "\t Training loss (single batch): 1.6429132223129272\n",
      "\t Training loss (single batch): 1.253833532333374\n",
      "\t Training loss (single batch): 1.198804259300232\n",
      "\t Training loss (single batch): 1.3518816232681274\n",
      "\t Training loss (single batch): 1.872053623199463\n",
      "\t Training loss (single batch): 1.1432596445083618\n",
      "\t Training loss (single batch): 0.9840894341468811\n",
      "\t Training loss (single batch): 1.429301381111145\n",
      "\t Training loss (single batch): 1.4165679216384888\n",
      "\t Training loss (single batch): 2.088385820388794\n",
      "\t Training loss (single batch): 1.1625938415527344\n",
      "\t Training loss (single batch): 1.481797695159912\n",
      "\t Training loss (single batch): 1.4211773872375488\n",
      "\t Training loss (single batch): 1.0499600172042847\n",
      "\t Training loss (single batch): 1.3856383562088013\n",
      "\t Training loss (single batch): 1.1444207429885864\n",
      "\t Training loss (single batch): 1.4977060556411743\n",
      "\t Training loss (single batch): 1.6773160696029663\n",
      "\t Training loss (single batch): 1.7783629894256592\n",
      "\t Training loss (single batch): 1.2035048007965088\n",
      "\t Training loss (single batch): 1.3746020793914795\n",
      "\t Training loss (single batch): 1.3095879554748535\n",
      "\t Training loss (single batch): 1.406005859375\n",
      "\t Training loss (single batch): 1.3811969757080078\n",
      "\t Training loss (single batch): 1.6092861890792847\n",
      "\t Training loss (single batch): 0.9748063087463379\n",
      "\t Training loss (single batch): 1.2857272624969482\n",
      "\t Training loss (single batch): 1.6313081979751587\n",
      "\t Training loss (single batch): 1.2385283708572388\n",
      "\t Training loss (single batch): 1.1806249618530273\n",
      "\t Training loss (single batch): 1.227868676185608\n",
      "\t Training loss (single batch): 1.3265987634658813\n",
      "\t Training loss (single batch): 1.336466908454895\n",
      "\t Training loss (single batch): 1.716054081916809\n",
      "\t Training loss (single batch): 1.0021018981933594\n",
      "\t Training loss (single batch): 1.214870572090149\n",
      "\t Training loss (single batch): 1.1680052280426025\n",
      "\t Training loss (single batch): 1.7811522483825684\n",
      "\t Training loss (single batch): 0.9998899102210999\n",
      "\t Training loss (single batch): 1.333412528038025\n",
      "\t Training loss (single batch): 1.0504924058914185\n",
      "\t Training loss (single batch): 1.8483991622924805\n",
      "\t Training loss (single batch): 1.3221547603607178\n",
      "\t Training loss (single batch): 1.342334270477295\n",
      "\t Training loss (single batch): 1.5938726663589478\n",
      "\t Training loss (single batch): 1.9434057474136353\n",
      "\t Training loss (single batch): 1.4536601305007935\n",
      "\t Training loss (single batch): 1.4832024574279785\n",
      "\t Training loss (single batch): 1.5845434665679932\n",
      "\t Training loss (single batch): 1.411914587020874\n",
      "\t Training loss (single batch): 1.5445475578308105\n",
      "\t Training loss (single batch): 1.2081019878387451\n",
      "\t Training loss (single batch): 1.3861966133117676\n",
      "\t Training loss (single batch): 1.548123836517334\n",
      "\t Training loss (single batch): 1.2116445302963257\n",
      "\t Training loss (single batch): 1.279226303100586\n",
      "\t Training loss (single batch): 1.6796836853027344\n",
      "\t Training loss (single batch): 1.5066945552825928\n",
      "\t Training loss (single batch): 0.842854380607605\n",
      "\t Training loss (single batch): 0.820271909236908\n",
      "\t Training loss (single batch): 0.734765350818634\n",
      "\t Training loss (single batch): 1.6349884271621704\n",
      "\t Training loss (single batch): 1.6495050191879272\n",
      "\t Training loss (single batch): 1.9284865856170654\n",
      "\t Training loss (single batch): 0.6623110771179199\n",
      "\t Training loss (single batch): 1.121752142906189\n",
      "\t Training loss (single batch): 1.6466434001922607\n",
      "\t Training loss (single batch): 1.2935563325881958\n",
      "\t Training loss (single batch): 2.1471548080444336\n",
      "##################################\n",
      "## EPOCH 3\n",
      "##################################\n",
      "\t Training loss (single batch): 1.508824110031128\n",
      "\t Training loss (single batch): 1.3297038078308105\n",
      "\t Training loss (single batch): 1.4497336149215698\n",
      "\t Training loss (single batch): 1.2407057285308838\n",
      "\t Training loss (single batch): 1.1501517295837402\n",
      "\t Training loss (single batch): 1.350102186203003\n",
      "\t Training loss (single batch): 1.156365156173706\n",
      "\t Training loss (single batch): 1.1378908157348633\n",
      "\t Training loss (single batch): 1.531823992729187\n",
      "\t Training loss (single batch): 1.7098060846328735\n",
      "\t Training loss (single batch): 2.0125300884246826\n",
      "\t Training loss (single batch): 1.2939499616622925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2509607076644897\n",
      "\t Training loss (single batch): 1.4111363887786865\n",
      "\t Training loss (single batch): 1.3481616973876953\n",
      "\t Training loss (single batch): 0.9493928551673889\n",
      "\t Training loss (single batch): 1.3285434246063232\n",
      "\t Training loss (single batch): 1.3623472452163696\n",
      "\t Training loss (single batch): 1.4261010885238647\n",
      "\t Training loss (single batch): 1.0543949604034424\n",
      "\t Training loss (single batch): 1.642903447151184\n",
      "\t Training loss (single batch): 1.3596391677856445\n",
      "\t Training loss (single batch): 1.858802318572998\n",
      "\t Training loss (single batch): 1.5170643329620361\n",
      "\t Training loss (single batch): 1.4027868509292603\n",
      "\t Training loss (single batch): 1.2103443145751953\n",
      "\t Training loss (single batch): 1.2561451196670532\n",
      "\t Training loss (single batch): 1.3040435314178467\n",
      "\t Training loss (single batch): 1.2727817296981812\n",
      "\t Training loss (single batch): 1.4239567518234253\n",
      "\t Training loss (single batch): 1.5195070505142212\n",
      "\t Training loss (single batch): 1.1066358089447021\n",
      "\t Training loss (single batch): 1.2926344871520996\n",
      "\t Training loss (single batch): 1.100878357887268\n",
      "\t Training loss (single batch): 1.5385494232177734\n",
      "\t Training loss (single batch): 1.4523993730545044\n",
      "\t Training loss (single batch): 0.9771602153778076\n",
      "\t Training loss (single batch): 0.8902186155319214\n",
      "\t Training loss (single batch): 0.9128602147102356\n",
      "\t Training loss (single batch): 0.9070119857788086\n",
      "\t Training loss (single batch): 1.3190791606903076\n",
      "\t Training loss (single batch): 0.9256367087364197\n",
      "\t Training loss (single batch): 1.2771657705307007\n",
      "\t Training loss (single batch): 1.0893477201461792\n",
      "\t Training loss (single batch): 1.414165735244751\n",
      "\t Training loss (single batch): 1.4428930282592773\n",
      "\t Training loss (single batch): 0.9419736266136169\n",
      "\t Training loss (single batch): 1.8924680948257446\n",
      "\t Training loss (single batch): 1.5064359903335571\n",
      "\t Training loss (single batch): 1.4207682609558105\n",
      "\t Training loss (single batch): 1.050998568534851\n",
      "\t Training loss (single batch): 1.7156856060028076\n",
      "\t Training loss (single batch): 1.9317371845245361\n",
      "\t Training loss (single batch): 1.3451268672943115\n",
      "\t Training loss (single batch): 1.4147509336471558\n",
      "\t Training loss (single batch): 1.0519123077392578\n",
      "\t Training loss (single batch): 1.3950861692428589\n",
      "\t Training loss (single batch): 1.8286715745925903\n",
      "\t Training loss (single batch): 2.072972297668457\n",
      "\t Training loss (single batch): 1.3472667932510376\n",
      "\t Training loss (single batch): 1.2642406225204468\n",
      "\t Training loss (single batch): 0.9357957243919373\n",
      "\t Training loss (single batch): 1.2676584720611572\n",
      "\t Training loss (single batch): 1.3089507818222046\n",
      "\t Training loss (single batch): 1.1827576160430908\n",
      "\t Training loss (single batch): 1.2843613624572754\n",
      "\t Training loss (single batch): 1.4733753204345703\n",
      "\t Training loss (single batch): 1.427589774131775\n",
      "\t Training loss (single batch): 1.014209508895874\n",
      "\t Training loss (single batch): 0.9309591054916382\n",
      "\t Training loss (single batch): 1.6087533235549927\n",
      "\t Training loss (single batch): 1.6420230865478516\n",
      "\t Training loss (single batch): 1.2939831018447876\n",
      "\t Training loss (single batch): 1.6725702285766602\n",
      "\t Training loss (single batch): 1.4071903228759766\n",
      "\t Training loss (single batch): 1.2848453521728516\n",
      "\t Training loss (single batch): 1.0960209369659424\n",
      "\t Training loss (single batch): 1.4271414279937744\n",
      "\t Training loss (single batch): 1.5710155963897705\n",
      "\t Training loss (single batch): 1.6309908628463745\n",
      "\t Training loss (single batch): 1.4232547283172607\n",
      "\t Training loss (single batch): 1.2296655178070068\n",
      "\t Training loss (single batch): 1.343785047531128\n",
      "\t Training loss (single batch): 2.0967180728912354\n",
      "\t Training loss (single batch): 1.5595437288284302\n",
      "\t Training loss (single batch): 1.217861533164978\n",
      "\t Training loss (single batch): 1.48329758644104\n",
      "\t Training loss (single batch): 1.650982141494751\n",
      "\t Training loss (single batch): 1.8987305164337158\n",
      "\t Training loss (single batch): 1.396655559539795\n",
      "\t Training loss (single batch): 1.441692590713501\n",
      "\t Training loss (single batch): 1.4104481935501099\n",
      "\t Training loss (single batch): 1.0568149089813232\n",
      "\t Training loss (single batch): 1.3029383420944214\n",
      "\t Training loss (single batch): 1.3815302848815918\n",
      "\t Training loss (single batch): 1.4983144998550415\n",
      "\t Training loss (single batch): 1.1950902938842773\n",
      "\t Training loss (single batch): 1.1467288732528687\n",
      "\t Training loss (single batch): 1.6708606481552124\n",
      "\t Training loss (single batch): 1.4448000192642212\n",
      "\t Training loss (single batch): 1.5894169807434082\n",
      "\t Training loss (single batch): 1.1448928117752075\n",
      "\t Training loss (single batch): 1.1783990859985352\n",
      "\t Training loss (single batch): 1.3253620862960815\n",
      "\t Training loss (single batch): 1.2999451160430908\n",
      "\t Training loss (single batch): 1.3967822790145874\n",
      "\t Training loss (single batch): 1.0937778949737549\n",
      "\t Training loss (single batch): 1.15870201587677\n",
      "\t Training loss (single batch): 1.3760302066802979\n",
      "\t Training loss (single batch): 1.604745864868164\n",
      "\t Training loss (single batch): 1.375468373298645\n",
      "\t Training loss (single batch): 1.478895664215088\n",
      "\t Training loss (single batch): 1.6892125606536865\n",
      "\t Training loss (single batch): 1.651889681816101\n",
      "\t Training loss (single batch): 1.153075098991394\n",
      "\t Training loss (single batch): 1.3565491437911987\n",
      "\t Training loss (single batch): 1.486981987953186\n",
      "\t Training loss (single batch): 1.295387625694275\n",
      "\t Training loss (single batch): 1.6796423196792603\n",
      "\t Training loss (single batch): 1.552243709564209\n",
      "\t Training loss (single batch): 1.2305337190628052\n",
      "\t Training loss (single batch): 1.0726021528244019\n",
      "\t Training loss (single batch): 1.9164716005325317\n",
      "\t Training loss (single batch): 1.1560750007629395\n",
      "\t Training loss (single batch): 1.726802945137024\n",
      "\t Training loss (single batch): 1.395595669746399\n",
      "\t Training loss (single batch): 0.9125141501426697\n",
      "\t Training loss (single batch): 1.1875901222229004\n",
      "\t Training loss (single batch): 0.8401081562042236\n",
      "\t Training loss (single batch): 1.2595767974853516\n",
      "\t Training loss (single batch): 1.1406700611114502\n",
      "\t Training loss (single batch): 1.0576560497283936\n",
      "\t Training loss (single batch): 1.6580760478973389\n",
      "\t Training loss (single batch): 1.4300705194473267\n",
      "\t Training loss (single batch): 1.5314475297927856\n",
      "\t Training loss (single batch): 1.1273136138916016\n",
      "\t Training loss (single batch): 1.228189468383789\n",
      "\t Training loss (single batch): 1.3878564834594727\n",
      "\t Training loss (single batch): 1.4248061180114746\n",
      "\t Training loss (single batch): 1.318929672241211\n",
      "\t Training loss (single batch): 1.6959052085876465\n",
      "\t Training loss (single batch): 1.3999643325805664\n",
      "\t Training loss (single batch): 1.5181910991668701\n",
      "\t Training loss (single batch): 1.7096096277236938\n",
      "\t Training loss (single batch): 1.5055696964263916\n",
      "\t Training loss (single batch): 1.2840144634246826\n",
      "\t Training loss (single batch): 1.1094857454299927\n",
      "\t Training loss (single batch): 1.5123529434204102\n",
      "\t Training loss (single batch): 1.872023105621338\n",
      "\t Training loss (single batch): 0.9866572022438049\n",
      "\t Training loss (single batch): 1.7446129322052002\n",
      "\t Training loss (single batch): 1.6171927452087402\n",
      "\t Training loss (single batch): 1.535252571105957\n",
      "\t Training loss (single batch): 1.5342243909835815\n",
      "\t Training loss (single batch): 1.4970519542694092\n",
      "\t Training loss (single batch): 1.3243498802185059\n",
      "\t Training loss (single batch): 1.2683358192443848\n",
      "\t Training loss (single batch): 1.4459607601165771\n",
      "\t Training loss (single batch): 1.746795892715454\n",
      "\t Training loss (single batch): 1.5455329418182373\n",
      "\t Training loss (single batch): 1.593019962310791\n",
      "\t Training loss (single batch): 1.3961181640625\n",
      "\t Training loss (single batch): 1.609675407409668\n",
      "\t Training loss (single batch): 1.3595025539398193\n",
      "\t Training loss (single batch): 1.1703033447265625\n",
      "\t Training loss (single batch): 1.5823709964752197\n",
      "\t Training loss (single batch): 1.7010679244995117\n",
      "\t Training loss (single batch): 1.4540470838546753\n",
      "\t Training loss (single batch): 1.3964924812316895\n",
      "\t Training loss (single batch): 1.4065841436386108\n",
      "\t Training loss (single batch): 1.3856303691864014\n",
      "\t Training loss (single batch): 0.9797536134719849\n",
      "\t Training loss (single batch): 1.3042869567871094\n",
      "\t Training loss (single batch): 1.1324089765548706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6541557312011719\n",
      "\t Training loss (single batch): 1.264139175415039\n",
      "\t Training loss (single batch): 1.1607507467269897\n",
      "\t Training loss (single batch): 1.7031275033950806\n",
      "\t Training loss (single batch): 1.2230756282806396\n",
      "\t Training loss (single batch): 1.0337839126586914\n",
      "\t Training loss (single batch): 1.3685133457183838\n",
      "\t Training loss (single batch): 1.7722922563552856\n",
      "\t Training loss (single batch): 1.1702200174331665\n",
      "\t Training loss (single batch): 1.5397382974624634\n",
      "\t Training loss (single batch): 1.446263313293457\n",
      "\t Training loss (single batch): 1.2216819524765015\n",
      "\t Training loss (single batch): 1.3143099546432495\n",
      "\t Training loss (single batch): 1.3788782358169556\n",
      "\t Training loss (single batch): 1.713987112045288\n",
      "\t Training loss (single batch): 1.295225739479065\n",
      "\t Training loss (single batch): 1.383954644203186\n",
      "\t Training loss (single batch): 1.7116856575012207\n",
      "\t Training loss (single batch): 1.8171346187591553\n",
      "\t Training loss (single batch): 1.4933466911315918\n",
      "\t Training loss (single batch): 1.6607033014297485\n",
      "\t Training loss (single batch): 1.1768988370895386\n",
      "\t Training loss (single batch): 1.452339768409729\n",
      "\t Training loss (single batch): 1.2242424488067627\n",
      "\t Training loss (single batch): 1.196297287940979\n",
      "\t Training loss (single batch): 1.2502487897872925\n",
      "\t Training loss (single batch): 1.2214874029159546\n",
      "\t Training loss (single batch): 1.4052188396453857\n",
      "\t Training loss (single batch): 1.6413546800613403\n",
      "\t Training loss (single batch): 1.5106312036514282\n",
      "\t Training loss (single batch): 1.3364733457565308\n",
      "\t Training loss (single batch): 1.54361093044281\n",
      "\t Training loss (single batch): 1.2313734292984009\n",
      "\t Training loss (single batch): 1.5199956893920898\n",
      "\t Training loss (single batch): 1.4224464893341064\n",
      "\t Training loss (single batch): 1.3158687353134155\n",
      "\t Training loss (single batch): 1.5440133810043335\n",
      "\t Training loss (single batch): 1.9406754970550537\n",
      "\t Training loss (single batch): 1.9806427955627441\n",
      "\t Training loss (single batch): 0.9714866876602173\n",
      "\t Training loss (single batch): 1.362972378730774\n",
      "\t Training loss (single batch): 1.052016258239746\n",
      "\t Training loss (single batch): 1.45169997215271\n",
      "\t Training loss (single batch): 1.0473500490188599\n",
      "\t Training loss (single batch): 1.2003602981567383\n",
      "\t Training loss (single batch): 1.1760262250900269\n",
      "\t Training loss (single batch): 1.0986740589141846\n",
      "\t Training loss (single batch): 1.2939140796661377\n",
      "\t Training loss (single batch): 1.6015074253082275\n",
      "\t Training loss (single batch): 1.125751256942749\n",
      "\t Training loss (single batch): 1.0032382011413574\n",
      "\t Training loss (single batch): 1.7443784475326538\n",
      "\t Training loss (single batch): 1.5598889589309692\n",
      "\t Training loss (single batch): 1.316235065460205\n",
      "\t Training loss (single batch): 1.2147363424301147\n",
      "\t Training loss (single batch): 1.2849656343460083\n",
      "\t Training loss (single batch): 1.2493630647659302\n",
      "\t Training loss (single batch): 1.1572507619857788\n",
      "\t Training loss (single batch): 1.2513447999954224\n",
      "\t Training loss (single batch): 1.0743039846420288\n",
      "\t Training loss (single batch): 1.6524063348770142\n",
      "\t Training loss (single batch): 1.0527865886688232\n",
      "\t Training loss (single batch): 1.0477616786956787\n",
      "\t Training loss (single batch): 1.2747209072113037\n",
      "\t Training loss (single batch): 1.179649829864502\n",
      "\t Training loss (single batch): 1.7165725231170654\n",
      "\t Training loss (single batch): 1.058864712715149\n",
      "\t Training loss (single batch): 1.5811216831207275\n",
      "\t Training loss (single batch): 1.4452298879623413\n",
      "\t Training loss (single batch): 1.0556292533874512\n",
      "\t Training loss (single batch): 1.1161103248596191\n",
      "\t Training loss (single batch): 1.405805230140686\n",
      "\t Training loss (single batch): 1.5225670337677002\n",
      "\t Training loss (single batch): 1.534576416015625\n",
      "\t Training loss (single batch): 1.3226302862167358\n",
      "\t Training loss (single batch): 1.6966116428375244\n",
      "\t Training loss (single batch): 1.5849968194961548\n",
      "\t Training loss (single batch): 1.193731665611267\n",
      "\t Training loss (single batch): 1.6513967514038086\n",
      "\t Training loss (single batch): 1.0703506469726562\n",
      "\t Training loss (single batch): 1.3733500242233276\n",
      "\t Training loss (single batch): 1.0929255485534668\n",
      "\t Training loss (single batch): 1.2835259437561035\n",
      "\t Training loss (single batch): 1.3519338369369507\n",
      "\t Training loss (single batch): 1.1495606899261475\n",
      "\t Training loss (single batch): 1.478833794593811\n",
      "\t Training loss (single batch): 1.7072808742523193\n",
      "\t Training loss (single batch): 0.980427086353302\n",
      "\t Training loss (single batch): 1.0774742364883423\n",
      "\t Training loss (single batch): 1.1401698589324951\n",
      "\t Training loss (single batch): 1.2238794565200806\n",
      "\t Training loss (single batch): 1.3269842863082886\n",
      "\t Training loss (single batch): 1.6770520210266113\n",
      "\t Training loss (single batch): 1.4902859926223755\n",
      "\t Training loss (single batch): 1.2797656059265137\n",
      "\t Training loss (single batch): 1.0477023124694824\n",
      "\t Training loss (single batch): 1.4678744077682495\n",
      "\t Training loss (single batch): 1.2921500205993652\n",
      "\t Training loss (single batch): 1.4512883424758911\n",
      "\t Training loss (single batch): 1.4805997610092163\n",
      "\t Training loss (single batch): 1.274442434310913\n",
      "\t Training loss (single batch): 1.2525683641433716\n",
      "\t Training loss (single batch): 1.3020867109298706\n",
      "\t Training loss (single batch): 2.0122671127319336\n",
      "\t Training loss (single batch): 1.3526647090911865\n",
      "\t Training loss (single batch): 1.3487420082092285\n",
      "\t Training loss (single batch): 1.1715961694717407\n",
      "\t Training loss (single batch): 1.523943543434143\n",
      "\t Training loss (single batch): 1.4295151233673096\n",
      "\t Training loss (single batch): 1.6655927896499634\n",
      "\t Training loss (single batch): 1.2810367345809937\n",
      "\t Training loss (single batch): 1.5586695671081543\n",
      "\t Training loss (single batch): 1.6545580625534058\n",
      "\t Training loss (single batch): 1.6303927898406982\n",
      "\t Training loss (single batch): 1.4293581247329712\n",
      "\t Training loss (single batch): 1.6073225736618042\n",
      "\t Training loss (single batch): 1.7603223323822021\n",
      "\t Training loss (single batch): 1.3414376974105835\n",
      "\t Training loss (single batch): 1.511966347694397\n",
      "\t Training loss (single batch): 1.1923367977142334\n",
      "\t Training loss (single batch): 1.797713041305542\n",
      "\t Training loss (single batch): 1.1646405458450317\n",
      "\t Training loss (single batch): 1.1599382162094116\n",
      "\t Training loss (single batch): 1.6910731792449951\n",
      "\t Training loss (single batch): 1.3753035068511963\n",
      "\t Training loss (single batch): 1.7693551778793335\n",
      "\t Training loss (single batch): 1.0219043493270874\n",
      "\t Training loss (single batch): 1.3866660594940186\n",
      "\t Training loss (single batch): 1.1917859315872192\n",
      "\t Training loss (single batch): 1.4773153066635132\n",
      "\t Training loss (single batch): 1.1039761304855347\n",
      "\t Training loss (single batch): 1.1757125854492188\n",
      "\t Training loss (single batch): 1.44022536277771\n",
      "\t Training loss (single batch): 1.1580051183700562\n",
      "\t Training loss (single batch): 1.5411648750305176\n",
      "\t Training loss (single batch): 1.2834705114364624\n",
      "\t Training loss (single batch): 1.1166152954101562\n",
      "\t Training loss (single batch): 1.6839995384216309\n",
      "\t Training loss (single batch): 0.8832932710647583\n",
      "\t Training loss (single batch): 1.1637656688690186\n",
      "\t Training loss (single batch): 1.6677238941192627\n",
      "\t Training loss (single batch): 1.64799165725708\n",
      "\t Training loss (single batch): 2.9068591594696045\n",
      "##################################\n",
      "## EPOCH 4\n",
      "##################################\n",
      "\t Training loss (single batch): 1.033828854560852\n",
      "\t Training loss (single batch): 1.043018102645874\n",
      "\t Training loss (single batch): 1.338179111480713\n",
      "\t Training loss (single batch): 2.41985821723938\n",
      "\t Training loss (single batch): 2.0200490951538086\n",
      "\t Training loss (single batch): 1.0610352754592896\n",
      "\t Training loss (single batch): 0.9702677726745605\n",
      "\t Training loss (single batch): 1.819618821144104\n",
      "\t Training loss (single batch): 1.3403429985046387\n",
      "\t Training loss (single batch): 1.2246015071868896\n",
      "\t Training loss (single batch): 1.2335591316223145\n",
      "\t Training loss (single batch): 1.582526445388794\n",
      "\t Training loss (single batch): 1.542842984199524\n",
      "\t Training loss (single batch): 1.5301066637039185\n",
      "\t Training loss (single batch): 1.3095632791519165\n",
      "\t Training loss (single batch): 1.0588064193725586\n",
      "\t Training loss (single batch): 1.4114052057266235\n",
      "\t Training loss (single batch): 1.8285733461380005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7493515014648438\n",
      "\t Training loss (single batch): 1.0759050846099854\n",
      "\t Training loss (single batch): 1.2861237525939941\n",
      "\t Training loss (single batch): 1.3917913436889648\n",
      "\t Training loss (single batch): 1.2334215641021729\n",
      "\t Training loss (single batch): 1.208954095840454\n",
      "\t Training loss (single batch): 1.0059832334518433\n",
      "\t Training loss (single batch): 1.018236756324768\n",
      "\t Training loss (single batch): 2.1797308921813965\n",
      "\t Training loss (single batch): 1.3638994693756104\n",
      "\t Training loss (single batch): 1.1475878953933716\n",
      "\t Training loss (single batch): 1.1176656484603882\n",
      "\t Training loss (single batch): 1.9584122896194458\n",
      "\t Training loss (single batch): 0.8535991907119751\n",
      "\t Training loss (single batch): 1.4113584756851196\n",
      "\t Training loss (single batch): 0.8984408378601074\n",
      "\t Training loss (single batch): 1.262829303741455\n",
      "\t Training loss (single batch): 1.4284189939498901\n",
      "\t Training loss (single batch): 1.6666070222854614\n",
      "\t Training loss (single batch): 1.3661855459213257\n",
      "\t Training loss (single batch): 1.258380651473999\n",
      "\t Training loss (single batch): 1.3995260000228882\n",
      "\t Training loss (single batch): 1.8116507530212402\n",
      "\t Training loss (single batch): 1.788110613822937\n",
      "\t Training loss (single batch): 1.4787691831588745\n",
      "\t Training loss (single batch): 1.7923730611801147\n",
      "\t Training loss (single batch): 1.543514609336853\n",
      "\t Training loss (single batch): 1.5403255224227905\n",
      "\t Training loss (single batch): 1.6051623821258545\n",
      "\t Training loss (single batch): 1.0606638193130493\n",
      "\t Training loss (single batch): 1.2333093881607056\n",
      "\t Training loss (single batch): 2.0254669189453125\n",
      "\t Training loss (single batch): 0.9412064552307129\n",
      "\t Training loss (single batch): 1.332108736038208\n",
      "\t Training loss (single batch): 1.592069149017334\n",
      "\t Training loss (single batch): 1.5457336902618408\n",
      "\t Training loss (single batch): 1.0538054704666138\n",
      "\t Training loss (single batch): 0.7128413915634155\n",
      "\t Training loss (single batch): 1.1187334060668945\n",
      "\t Training loss (single batch): 1.177445888519287\n",
      "\t Training loss (single batch): 0.978713870048523\n",
      "\t Training loss (single batch): 1.3967534303665161\n",
      "\t Training loss (single batch): 1.4138226509094238\n",
      "\t Training loss (single batch): 1.6790724992752075\n",
      "\t Training loss (single batch): 1.2832682132720947\n",
      "\t Training loss (single batch): 1.882129192352295\n",
      "\t Training loss (single batch): 1.7672220468521118\n",
      "\t Training loss (single batch): 1.2371928691864014\n",
      "\t Training loss (single batch): 1.391559362411499\n",
      "\t Training loss (single batch): 1.44147789478302\n",
      "\t Training loss (single batch): 1.8017923831939697\n",
      "\t Training loss (single batch): 1.5571115016937256\n",
      "\t Training loss (single batch): 1.4884319305419922\n",
      "\t Training loss (single batch): 1.5563212633132935\n",
      "\t Training loss (single batch): 1.4180891513824463\n",
      "\t Training loss (single batch): 0.8366044163703918\n",
      "\t Training loss (single batch): 1.0876858234405518\n",
      "\t Training loss (single batch): 0.9319092035293579\n",
      "\t Training loss (single batch): 1.3011220693588257\n",
      "\t Training loss (single batch): 0.8476580381393433\n",
      "\t Training loss (single batch): 0.9874571561813354\n",
      "\t Training loss (single batch): 1.2130250930786133\n",
      "\t Training loss (single batch): 1.5995850563049316\n",
      "\t Training loss (single batch): 1.4021005630493164\n",
      "\t Training loss (single batch): 1.383461594581604\n",
      "\t Training loss (single batch): 1.8983098268508911\n",
      "\t Training loss (single batch): 1.5603816509246826\n",
      "\t Training loss (single batch): 1.294012427330017\n",
      "\t Training loss (single batch): 0.891180157661438\n",
      "\t Training loss (single batch): 1.7601423263549805\n",
      "\t Training loss (single batch): 1.3474020957946777\n",
      "\t Training loss (single batch): 1.143317461013794\n",
      "\t Training loss (single batch): 0.9370098114013672\n",
      "\t Training loss (single batch): 1.5572593212127686\n",
      "\t Training loss (single batch): 1.282307744026184\n",
      "\t Training loss (single batch): 1.674020767211914\n",
      "\t Training loss (single batch): 1.023932933807373\n",
      "\t Training loss (single batch): 1.6784006357192993\n",
      "\t Training loss (single batch): 1.0265541076660156\n",
      "\t Training loss (single batch): 1.0066816806793213\n",
      "\t Training loss (single batch): 1.0935527086257935\n",
      "\t Training loss (single batch): 1.7242653369903564\n",
      "\t Training loss (single batch): 1.1719770431518555\n",
      "\t Training loss (single batch): 1.2318466901779175\n",
      "\t Training loss (single batch): 1.4001045227050781\n",
      "\t Training loss (single batch): 0.9039186835289001\n",
      "\t Training loss (single batch): 1.5362135171890259\n",
      "\t Training loss (single batch): 1.4854825735092163\n",
      "\t Training loss (single batch): 1.1099451780319214\n",
      "\t Training loss (single batch): 1.4609969854354858\n",
      "\t Training loss (single batch): 1.4294061660766602\n",
      "\t Training loss (single batch): 1.174228549003601\n",
      "\t Training loss (single batch): 1.3735153675079346\n",
      "\t Training loss (single batch): 1.2700183391571045\n",
      "\t Training loss (single batch): 1.4161691665649414\n",
      "\t Training loss (single batch): 1.468420386314392\n",
      "\t Training loss (single batch): 0.8819615840911865\n",
      "\t Training loss (single batch): 1.2685067653656006\n",
      "\t Training loss (single batch): 1.406907320022583\n",
      "\t Training loss (single batch): 1.3316632509231567\n",
      "\t Training loss (single batch): 1.303663969039917\n",
      "\t Training loss (single batch): 1.1277592182159424\n",
      "\t Training loss (single batch): 1.0688797235488892\n",
      "\t Training loss (single batch): 0.985365629196167\n",
      "\t Training loss (single batch): 1.1476495265960693\n",
      "\t Training loss (single batch): 1.0876505374908447\n",
      "\t Training loss (single batch): 1.680599570274353\n",
      "\t Training loss (single batch): 1.3886827230453491\n",
      "\t Training loss (single batch): 1.6477806568145752\n",
      "\t Training loss (single batch): 1.8195866346359253\n",
      "\t Training loss (single batch): 1.6994819641113281\n",
      "\t Training loss (single batch): 1.650512456893921\n",
      "\t Training loss (single batch): 1.1902059316635132\n",
      "\t Training loss (single batch): 1.4089161157608032\n",
      "\t Training loss (single batch): 1.1330496072769165\n",
      "\t Training loss (single batch): 1.55620276927948\n",
      "\t Training loss (single batch): 0.8469573855400085\n",
      "\t Training loss (single batch): 1.5166189670562744\n",
      "\t Training loss (single batch): 1.2754333019256592\n",
      "\t Training loss (single batch): 1.0873076915740967\n",
      "\t Training loss (single batch): 1.830180048942566\n",
      "\t Training loss (single batch): 1.33652663230896\n",
      "\t Training loss (single batch): 1.368139624595642\n",
      "\t Training loss (single batch): 1.603718876838684\n",
      "\t Training loss (single batch): 1.3358135223388672\n",
      "\t Training loss (single batch): 1.5946029424667358\n",
      "\t Training loss (single batch): 1.432411789894104\n",
      "\t Training loss (single batch): 1.3863366842269897\n",
      "\t Training loss (single batch): 1.6076045036315918\n",
      "\t Training loss (single batch): 1.3800654411315918\n",
      "\t Training loss (single batch): 0.8475973606109619\n",
      "\t Training loss (single batch): 1.4046690464019775\n",
      "\t Training loss (single batch): 1.3473138809204102\n",
      "\t Training loss (single batch): 1.1554844379425049\n",
      "\t Training loss (single batch): 1.363625407218933\n",
      "\t Training loss (single batch): 1.4123013019561768\n",
      "\t Training loss (single batch): 1.0563076734542847\n",
      "\t Training loss (single batch): 1.6742796897888184\n",
      "\t Training loss (single batch): 1.3955906629562378\n",
      "\t Training loss (single batch): 1.8729115724563599\n",
      "\t Training loss (single batch): 1.1507397890090942\n",
      "\t Training loss (single batch): 1.0770535469055176\n",
      "\t Training loss (single batch): 1.3523838520050049\n",
      "\t Training loss (single batch): 1.162280797958374\n",
      "\t Training loss (single batch): 1.3524218797683716\n",
      "\t Training loss (single batch): 1.3790013790130615\n",
      "\t Training loss (single batch): 1.2796869277954102\n",
      "\t Training loss (single batch): 0.8959915041923523\n",
      "\t Training loss (single batch): 1.5925334692001343\n",
      "\t Training loss (single batch): 1.8636780977249146\n",
      "\t Training loss (single batch): 1.22026789188385\n",
      "\t Training loss (single batch): 1.4615167379379272\n",
      "\t Training loss (single batch): 1.3695098161697388\n",
      "\t Training loss (single batch): 1.8122886419296265\n",
      "\t Training loss (single batch): 1.2528361082077026\n",
      "\t Training loss (single batch): 1.5323268175125122\n",
      "\t Training loss (single batch): 1.8890142440795898\n",
      "\t Training loss (single batch): 1.337275505065918\n",
      "\t Training loss (single batch): 1.2803841829299927\n",
      "\t Training loss (single batch): 0.9955554008483887\n",
      "\t Training loss (single batch): 0.7418912053108215\n",
      "\t Training loss (single batch): 0.9745006561279297\n",
      "\t Training loss (single batch): 1.4509155750274658\n",
      "\t Training loss (single batch): 1.3384922742843628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2623305320739746\n",
      "\t Training loss (single batch): 1.1113252639770508\n",
      "\t Training loss (single batch): 1.455530047416687\n",
      "\t Training loss (single batch): 1.2811286449432373\n",
      "\t Training loss (single batch): 1.26845121383667\n",
      "\t Training loss (single batch): 0.980438768863678\n",
      "\t Training loss (single batch): 1.2528088092803955\n",
      "\t Training loss (single batch): 1.3985264301300049\n",
      "\t Training loss (single batch): 1.261480450630188\n",
      "\t Training loss (single batch): 1.4360222816467285\n",
      "\t Training loss (single batch): 1.1435976028442383\n",
      "\t Training loss (single batch): 1.284956455230713\n",
      "\t Training loss (single batch): 2.0428404808044434\n",
      "\t Training loss (single batch): 1.0150686502456665\n",
      "\t Training loss (single batch): 1.2019389867782593\n",
      "\t Training loss (single batch): 0.9069064855575562\n",
      "\t Training loss (single batch): 1.6760928630828857\n",
      "\t Training loss (single batch): 1.8580784797668457\n",
      "\t Training loss (single batch): 1.6831674575805664\n",
      "\t Training loss (single batch): 1.7105083465576172\n",
      "\t Training loss (single batch): 2.1931660175323486\n",
      "\t Training loss (single batch): 1.2141039371490479\n",
      "\t Training loss (single batch): 1.0095648765563965\n",
      "\t Training loss (single batch): 1.430639624595642\n",
      "\t Training loss (single batch): 1.1238632202148438\n",
      "\t Training loss (single batch): 1.5652140378952026\n",
      "\t Training loss (single batch): 1.582306981086731\n",
      "\t Training loss (single batch): 1.2783982753753662\n",
      "\t Training loss (single batch): 1.330228328704834\n",
      "\t Training loss (single batch): 1.2480109930038452\n",
      "\t Training loss (single batch): 1.5747302770614624\n",
      "\t Training loss (single batch): 1.1877537965774536\n",
      "\t Training loss (single batch): 1.4995758533477783\n",
      "\t Training loss (single batch): 1.488521933555603\n",
      "\t Training loss (single batch): 1.4861449003219604\n",
      "\t Training loss (single batch): 0.6922786235809326\n",
      "\t Training loss (single batch): 1.4876614809036255\n",
      "\t Training loss (single batch): 0.9630749821662903\n",
      "\t Training loss (single batch): 1.333736777305603\n",
      "\t Training loss (single batch): 1.0297636985778809\n",
      "\t Training loss (single batch): 1.100577473640442\n",
      "\t Training loss (single batch): 1.1702531576156616\n",
      "\t Training loss (single batch): 1.444272518157959\n",
      "\t Training loss (single batch): 1.5401277542114258\n",
      "\t Training loss (single batch): 1.1857572793960571\n",
      "\t Training loss (single batch): 1.0794934034347534\n",
      "\t Training loss (single batch): 1.1078505516052246\n",
      "\t Training loss (single batch): 1.3915233612060547\n",
      "\t Training loss (single batch): 1.4644323587417603\n",
      "\t Training loss (single batch): 1.5626107454299927\n",
      "\t Training loss (single batch): 1.538253903388977\n",
      "\t Training loss (single batch): 1.487358570098877\n",
      "\t Training loss (single batch): 1.5419381856918335\n",
      "\t Training loss (single batch): 1.5888111591339111\n",
      "\t Training loss (single batch): 1.145026445388794\n",
      "\t Training loss (single batch): 1.7039082050323486\n",
      "\t Training loss (single batch): 1.3997464179992676\n",
      "\t Training loss (single batch): 1.0370922088623047\n",
      "\t Training loss (single batch): 1.4056396484375\n",
      "\t Training loss (single batch): 1.3231658935546875\n",
      "\t Training loss (single batch): 1.3201580047607422\n",
      "\t Training loss (single batch): 1.1366626024246216\n",
      "\t Training loss (single batch): 1.1916931867599487\n",
      "\t Training loss (single batch): 1.5565911531448364\n",
      "\t Training loss (single batch): 1.2197800874710083\n",
      "\t Training loss (single batch): 1.637289047241211\n",
      "\t Training loss (single batch): 1.3242504596710205\n",
      "\t Training loss (single batch): 1.761573076248169\n",
      "\t Training loss (single batch): 1.7076163291931152\n",
      "\t Training loss (single batch): 1.6493035554885864\n",
      "\t Training loss (single batch): 1.5138514041900635\n",
      "\t Training loss (single batch): 1.638280987739563\n",
      "\t Training loss (single batch): 1.3454939126968384\n",
      "\t Training loss (single batch): 1.4658030271530151\n",
      "\t Training loss (single batch): 1.1831697225570679\n",
      "\t Training loss (single batch): 1.3973891735076904\n",
      "\t Training loss (single batch): 1.6956231594085693\n",
      "\t Training loss (single batch): 1.6115862131118774\n",
      "\t Training loss (single batch): 1.133313536643982\n",
      "\t Training loss (single batch): 1.2829937934875488\n",
      "\t Training loss (single batch): 1.2758593559265137\n",
      "\t Training loss (single batch): 1.3014744520187378\n",
      "\t Training loss (single batch): 1.702340841293335\n",
      "\t Training loss (single batch): 1.3130522966384888\n",
      "\t Training loss (single batch): 1.286137580871582\n",
      "\t Training loss (single batch): 1.2032004594802856\n",
      "\t Training loss (single batch): 1.0526247024536133\n",
      "\t Training loss (single batch): 1.3284138441085815\n",
      "\t Training loss (single batch): 1.5614649057388306\n",
      "\t Training loss (single batch): 0.883239209651947\n",
      "\t Training loss (single batch): 1.1158108711242676\n",
      "\t Training loss (single batch): 1.1825041770935059\n",
      "\t Training loss (single batch): 1.0393946170806885\n",
      "\t Training loss (single batch): 1.3170753717422485\n",
      "\t Training loss (single batch): 1.452071189880371\n",
      "\t Training loss (single batch): 1.093804955482483\n",
      "\t Training loss (single batch): 1.800108551979065\n",
      "\t Training loss (single batch): 1.3200305700302124\n",
      "\t Training loss (single batch): 1.382602334022522\n",
      "\t Training loss (single batch): 1.5288918018341064\n",
      "\t Training loss (single batch): 1.2661716938018799\n",
      "\t Training loss (single batch): 1.3246123790740967\n",
      "\t Training loss (single batch): 1.0960416793823242\n",
      "\t Training loss (single batch): 1.5706597566604614\n",
      "\t Training loss (single batch): 1.2611689567565918\n",
      "\t Training loss (single batch): 1.122101902961731\n",
      "\t Training loss (single batch): 1.2551196813583374\n",
      "\t Training loss (single batch): 1.678956389427185\n",
      "\t Training loss (single batch): 2.1494133472442627\n",
      "\t Training loss (single batch): 1.9536279439926147\n",
      "\t Training loss (single batch): 0.716250479221344\n",
      "\t Training loss (single batch): 1.6858909130096436\n",
      "\t Training loss (single batch): 1.5355052947998047\n",
      "\t Training loss (single batch): 1.07258141040802\n",
      "\t Training loss (single batch): 1.7611814737319946\n",
      "\t Training loss (single batch): 1.1089552640914917\n",
      "\t Training loss (single batch): 1.2902840375900269\n",
      "\t Training loss (single batch): 1.1986302137374878\n",
      "\t Training loss (single batch): 1.4835871458053589\n",
      "\t Training loss (single batch): 1.4783395528793335\n",
      "\t Training loss (single batch): 1.0404897928237915\n",
      "\t Training loss (single batch): 1.577906608581543\n",
      "\t Training loss (single batch): 1.0422779321670532\n",
      "\t Training loss (single batch): 1.7061223983764648\n",
      "\t Training loss (single batch): 1.0998055934906006\n",
      "\t Training loss (single batch): 2.1758506298065186\n",
      "\t Training loss (single batch): 1.2271522283554077\n",
      "\t Training loss (single batch): 1.2165008783340454\n",
      "\t Training loss (single batch): 0.8219101428985596\n",
      "\t Training loss (single batch): 0.9952093958854675\n",
      "\t Training loss (single batch): 0.8969842791557312\n",
      "\t Training loss (single batch): 1.4144214391708374\n",
      "\t Training loss (single batch): 1.2656759023666382\n",
      "\t Training loss (single batch): 1.1360349655151367\n",
      "\t Training loss (single batch): 0.19728271663188934\n",
      "##################################\n",
      "## EPOCH 5\n",
      "##################################\n",
      "\t Training loss (single batch): 1.265010118484497\n",
      "\t Training loss (single batch): 1.542137622833252\n",
      "\t Training loss (single batch): 1.0785249471664429\n",
      "\t Training loss (single batch): 1.4257171154022217\n",
      "\t Training loss (single batch): 1.6438920497894287\n",
      "\t Training loss (single batch): 1.3973701000213623\n",
      "\t Training loss (single batch): 1.149627447128296\n",
      "\t Training loss (single batch): 2.2192349433898926\n",
      "\t Training loss (single batch): 1.1376146078109741\n",
      "\t Training loss (single batch): 1.248272180557251\n",
      "\t Training loss (single batch): 1.6353557109832764\n",
      "\t Training loss (single batch): 1.5177830457687378\n",
      "\t Training loss (single batch): 1.4496068954467773\n",
      "\t Training loss (single batch): 1.281327247619629\n",
      "\t Training loss (single batch): 1.2066473960876465\n",
      "\t Training loss (single batch): 1.572746753692627\n",
      "\t Training loss (single batch): 1.3821841478347778\n",
      "\t Training loss (single batch): 1.2665057182312012\n",
      "\t Training loss (single batch): 1.5405752658843994\n",
      "\t Training loss (single batch): 0.9788928031921387\n",
      "\t Training loss (single batch): 2.0781748294830322\n",
      "\t Training loss (single batch): 1.2183585166931152\n",
      "\t Training loss (single batch): 1.1367436647415161\n",
      "\t Training loss (single batch): 1.3876793384552002\n",
      "\t Training loss (single batch): 1.4500385522842407\n",
      "\t Training loss (single batch): 1.7019062042236328\n",
      "\t Training loss (single batch): 1.3080401420593262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7425777912139893\n",
      "\t Training loss (single batch): 1.064313292503357\n",
      "\t Training loss (single batch): 1.5496082305908203\n",
      "\t Training loss (single batch): 1.2701495885849\n",
      "\t Training loss (single batch): 1.5378189086914062\n",
      "\t Training loss (single batch): 1.3266788721084595\n",
      "\t Training loss (single batch): 1.7146683931350708\n",
      "\t Training loss (single batch): 1.5095270872116089\n",
      "\t Training loss (single batch): 0.9545425176620483\n",
      "\t Training loss (single batch): 1.1763502359390259\n",
      "\t Training loss (single batch): 1.5189945697784424\n",
      "\t Training loss (single batch): 1.5777041912078857\n",
      "\t Training loss (single batch): 1.299347162246704\n",
      "\t Training loss (single batch): 1.442355990409851\n",
      "\t Training loss (single batch): 1.3261812925338745\n",
      "\t Training loss (single batch): 1.58914053440094\n",
      "\t Training loss (single batch): 1.0983541011810303\n",
      "\t Training loss (single batch): 1.2133105993270874\n",
      "\t Training loss (single batch): 1.3289148807525635\n",
      "\t Training loss (single batch): 1.3776652812957764\n",
      "\t Training loss (single batch): 1.4892361164093018\n",
      "\t Training loss (single batch): 0.9003494381904602\n",
      "\t Training loss (single batch): 1.0084861516952515\n",
      "\t Training loss (single batch): 1.619139552116394\n",
      "\t Training loss (single batch): 1.784118413925171\n",
      "\t Training loss (single batch): 1.2699058055877686\n",
      "\t Training loss (single batch): 1.4687985181808472\n",
      "\t Training loss (single batch): 1.9650086164474487\n",
      "\t Training loss (single batch): 1.715158224105835\n",
      "\t Training loss (single batch): 0.9041706323623657\n",
      "\t Training loss (single batch): 1.583500623703003\n",
      "\t Training loss (single batch): 1.0331066846847534\n",
      "\t Training loss (single batch): 1.5959633588790894\n",
      "\t Training loss (single batch): 1.539919137954712\n",
      "\t Training loss (single batch): 1.3795403242111206\n",
      "\t Training loss (single batch): 1.3686515092849731\n",
      "\t Training loss (single batch): 1.1765984296798706\n",
      "\t Training loss (single batch): 1.1251894235610962\n",
      "\t Training loss (single batch): 1.9199954271316528\n",
      "\t Training loss (single batch): 0.8768211007118225\n",
      "\t Training loss (single batch): 1.3694868087768555\n",
      "\t Training loss (single batch): 0.9696229696273804\n",
      "\t Training loss (single batch): 1.3885838985443115\n",
      "\t Training loss (single batch): 1.311953067779541\n",
      "\t Training loss (single batch): 1.332112431526184\n",
      "\t Training loss (single batch): 1.8225451707839966\n",
      "\t Training loss (single batch): 1.5803552865982056\n",
      "\t Training loss (single batch): 1.2994306087493896\n",
      "\t Training loss (single batch): 1.1743762493133545\n",
      "\t Training loss (single batch): 1.1219274997711182\n",
      "\t Training loss (single batch): 1.240498661994934\n",
      "\t Training loss (single batch): 1.468385934829712\n",
      "\t Training loss (single batch): 0.9217340350151062\n",
      "\t Training loss (single batch): 1.2511053085327148\n",
      "\t Training loss (single batch): 1.6366841793060303\n",
      "\t Training loss (single batch): 1.2310214042663574\n",
      "\t Training loss (single batch): 1.6380964517593384\n",
      "\t Training loss (single batch): 1.4693009853363037\n",
      "\t Training loss (single batch): 1.7755998373031616\n",
      "\t Training loss (single batch): 1.1309731006622314\n",
      "\t Training loss (single batch): 1.3086763620376587\n",
      "\t Training loss (single batch): 1.4775446653366089\n",
      "\t Training loss (single batch): 1.6010115146636963\n",
      "\t Training loss (single batch): 1.2410776615142822\n",
      "\t Training loss (single batch): 1.6539790630340576\n",
      "\t Training loss (single batch): 1.7506721019744873\n",
      "\t Training loss (single batch): 1.3036739826202393\n",
      "\t Training loss (single batch): 1.257499098777771\n",
      "\t Training loss (single batch): 1.6004558801651\n",
      "\t Training loss (single batch): 0.9796822667121887\n",
      "\t Training loss (single batch): 1.6167689561843872\n",
      "\t Training loss (single batch): 1.371998906135559\n",
      "\t Training loss (single batch): 1.4943956136703491\n",
      "\t Training loss (single batch): 2.3886868953704834\n",
      "\t Training loss (single batch): 1.3859039545059204\n",
      "\t Training loss (single batch): 1.4080450534820557\n",
      "\t Training loss (single batch): 1.058924674987793\n",
      "\t Training loss (single batch): 1.3457560539245605\n",
      "\t Training loss (single batch): 1.3006784915924072\n",
      "\t Training loss (single batch): 1.222719430923462\n",
      "\t Training loss (single batch): 1.1514651775360107\n",
      "\t Training loss (single batch): 1.272663950920105\n",
      "\t Training loss (single batch): 1.4114629030227661\n",
      "\t Training loss (single batch): 1.923542857170105\n",
      "\t Training loss (single batch): 0.7846084237098694\n",
      "\t Training loss (single batch): 1.5959972143173218\n",
      "\t Training loss (single batch): 1.4534802436828613\n",
      "\t Training loss (single batch): 1.2089133262634277\n",
      "\t Training loss (single batch): 1.7982066869735718\n",
      "\t Training loss (single batch): 1.155524730682373\n",
      "\t Training loss (single batch): 1.282333254814148\n",
      "\t Training loss (single batch): 1.4065847396850586\n",
      "\t Training loss (single batch): 1.2249113321304321\n",
      "\t Training loss (single batch): 1.422759771347046\n",
      "\t Training loss (single batch): 1.0518933534622192\n",
      "\t Training loss (single batch): 1.2048285007476807\n",
      "\t Training loss (single batch): 0.760090708732605\n",
      "\t Training loss (single batch): 1.2741278409957886\n",
      "\t Training loss (single batch): 1.7650692462921143\n",
      "\t Training loss (single batch): 0.9352484345436096\n",
      "\t Training loss (single batch): 1.5112648010253906\n",
      "\t Training loss (single batch): 1.486266016960144\n",
      "\t Training loss (single batch): 1.457285761833191\n",
      "\t Training loss (single batch): 1.2618387937545776\n",
      "\t Training loss (single batch): 1.3304365873336792\n",
      "\t Training loss (single batch): 1.2644991874694824\n",
      "\t Training loss (single batch): 1.53154718875885\n",
      "\t Training loss (single batch): 1.5577079057693481\n",
      "\t Training loss (single batch): 1.0795551538467407\n",
      "\t Training loss (single batch): 1.2710169553756714\n",
      "\t Training loss (single batch): 1.1002798080444336\n",
      "\t Training loss (single batch): 1.482538104057312\n",
      "\t Training loss (single batch): 1.5613023042678833\n",
      "\t Training loss (single batch): 1.0887858867645264\n",
      "\t Training loss (single batch): 1.1904410123825073\n",
      "\t Training loss (single batch): 1.7904962301254272\n",
      "\t Training loss (single batch): 1.63237726688385\n",
      "\t Training loss (single batch): 1.7427383661270142\n",
      "\t Training loss (single batch): 1.5160608291625977\n",
      "\t Training loss (single batch): 1.1391589641571045\n",
      "\t Training loss (single batch): 1.0618395805358887\n",
      "\t Training loss (single batch): 1.4738972187042236\n",
      "\t Training loss (single batch): 1.4517205953598022\n",
      "\t Training loss (single batch): 1.3058143854141235\n",
      "\t Training loss (single batch): 1.5384912490844727\n",
      "\t Training loss (single batch): 1.6287132501602173\n",
      "\t Training loss (single batch): 1.1630327701568604\n",
      "\t Training loss (single batch): 1.3150267601013184\n",
      "\t Training loss (single batch): 1.2101011276245117\n",
      "\t Training loss (single batch): 1.5034055709838867\n",
      "\t Training loss (single batch): 1.458946704864502\n",
      "\t Training loss (single batch): 1.297655463218689\n",
      "\t Training loss (single batch): 0.9996072053909302\n",
      "\t Training loss (single batch): 1.4363654851913452\n",
      "\t Training loss (single batch): 1.0225014686584473\n",
      "\t Training loss (single batch): 0.8791772127151489\n",
      "\t Training loss (single batch): 1.137247085571289\n",
      "\t Training loss (single batch): 1.2378504276275635\n",
      "\t Training loss (single batch): 1.2946463823318481\n",
      "\t Training loss (single batch): 1.0506397485733032\n",
      "\t Training loss (single batch): 1.1293264627456665\n",
      "\t Training loss (single batch): 1.4005272388458252\n",
      "\t Training loss (single batch): 1.3775358200073242\n",
      "\t Training loss (single batch): 1.4448304176330566\n",
      "\t Training loss (single batch): 0.8566837906837463\n",
      "\t Training loss (single batch): 1.2811124324798584\n",
      "\t Training loss (single batch): 1.0831522941589355\n",
      "\t Training loss (single batch): 1.214292287826538\n",
      "\t Training loss (single batch): 1.20647132396698\n",
      "\t Training loss (single batch): 1.6148751974105835\n",
      "\t Training loss (single batch): 1.3666335344314575\n",
      "\t Training loss (single batch): 1.0603439807891846\n",
      "\t Training loss (single batch): 1.091320276260376\n",
      "\t Training loss (single batch): 1.5477752685546875\n",
      "\t Training loss (single batch): 1.3292806148529053\n",
      "\t Training loss (single batch): 1.1845932006835938\n",
      "\t Training loss (single batch): 0.904853343963623\n",
      "\t Training loss (single batch): 1.1933002471923828\n",
      "\t Training loss (single batch): 1.0819613933563232\n",
      "\t Training loss (single batch): 1.3336381912231445\n",
      "\t Training loss (single batch): 1.5550614595413208\n",
      "\t Training loss (single batch): 1.6098520755767822\n",
      "\t Training loss (single batch): 0.9510949850082397\n",
      "\t Training loss (single batch): 1.247422456741333\n",
      "\t Training loss (single batch): 1.101583480834961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5651050806045532\n",
      "\t Training loss (single batch): 1.1138972043991089\n",
      "\t Training loss (single batch): 1.0264889001846313\n",
      "\t Training loss (single batch): 1.1310865879058838\n",
      "\t Training loss (single batch): 1.570828914642334\n",
      "\t Training loss (single batch): 1.3445833921432495\n",
      "\t Training loss (single batch): 1.5703208446502686\n",
      "\t Training loss (single batch): 1.4076716899871826\n",
      "\t Training loss (single batch): 1.491115689277649\n",
      "\t Training loss (single batch): 1.3339122533798218\n",
      "\t Training loss (single batch): 1.347503662109375\n",
      "\t Training loss (single batch): 1.813599705696106\n",
      "\t Training loss (single batch): 1.2035484313964844\n",
      "\t Training loss (single batch): 1.021702766418457\n",
      "\t Training loss (single batch): 0.9778752326965332\n",
      "\t Training loss (single batch): 1.2437036037445068\n",
      "\t Training loss (single batch): 1.3162206411361694\n",
      "\t Training loss (single batch): 1.4252285957336426\n",
      "\t Training loss (single batch): 1.1385459899902344\n",
      "\t Training loss (single batch): 1.3377777338027954\n",
      "\t Training loss (single batch): 1.5065497159957886\n",
      "\t Training loss (single batch): 1.2863686084747314\n",
      "\t Training loss (single batch): 2.144313097000122\n",
      "\t Training loss (single batch): 1.4582158327102661\n",
      "\t Training loss (single batch): 0.9768618941307068\n",
      "\t Training loss (single batch): 1.2999911308288574\n",
      "\t Training loss (single batch): 1.6509236097335815\n",
      "\t Training loss (single batch): 0.960905134677887\n",
      "\t Training loss (single batch): 1.3038067817687988\n",
      "\t Training loss (single batch): 1.0391360521316528\n",
      "\t Training loss (single batch): 1.3997610807418823\n",
      "\t Training loss (single batch): 1.6798101663589478\n",
      "\t Training loss (single batch): 1.488376259803772\n",
      "\t Training loss (single batch): 1.336857795715332\n",
      "\t Training loss (single batch): 1.411263346672058\n",
      "\t Training loss (single batch): 1.060631275177002\n",
      "\t Training loss (single batch): 1.1542763710021973\n",
      "\t Training loss (single batch): 1.9750248193740845\n",
      "\t Training loss (single batch): 1.659476637840271\n",
      "\t Training loss (single batch): 2.080080032348633\n",
      "\t Training loss (single batch): 0.9216127395629883\n",
      "\t Training loss (single batch): 1.4768344163894653\n",
      "\t Training loss (single batch): 1.89170241355896\n",
      "\t Training loss (single batch): 1.3525217771530151\n",
      "\t Training loss (single batch): 1.5611178874969482\n",
      "\t Training loss (single batch): 0.7828415036201477\n",
      "\t Training loss (single batch): 1.1864433288574219\n",
      "\t Training loss (single batch): 1.3302671909332275\n",
      "\t Training loss (single batch): 0.9854208827018738\n",
      "\t Training loss (single batch): 1.4291248321533203\n",
      "\t Training loss (single batch): 1.4571362733840942\n",
      "\t Training loss (single batch): 1.4296839237213135\n",
      "\t Training loss (single batch): 1.5309258699417114\n",
      "\t Training loss (single batch): 1.1571120023727417\n",
      "\t Training loss (single batch): 1.6785660982131958\n",
      "\t Training loss (single batch): 1.0028002262115479\n",
      "\t Training loss (single batch): 1.0832314491271973\n",
      "\t Training loss (single batch): 0.7337200045585632\n",
      "\t Training loss (single batch): 1.2165775299072266\n",
      "\t Training loss (single batch): 1.2412515878677368\n",
      "\t Training loss (single batch): 1.285810947418213\n",
      "\t Training loss (single batch): 1.4685925245285034\n",
      "\t Training loss (single batch): 1.4692034721374512\n",
      "\t Training loss (single batch): 0.8653371930122375\n",
      "\t Training loss (single batch): 0.8479303121566772\n",
      "\t Training loss (single batch): 1.3837920427322388\n",
      "\t Training loss (single batch): 1.303397297859192\n",
      "\t Training loss (single batch): 1.5797467231750488\n",
      "\t Training loss (single batch): 1.4422333240509033\n",
      "\t Training loss (single batch): 1.2088351249694824\n",
      "\t Training loss (single batch): 1.8804794549942017\n",
      "\t Training loss (single batch): 1.285003900527954\n",
      "\t Training loss (single batch): 1.4249753952026367\n",
      "\t Training loss (single batch): 1.3872939348220825\n",
      "\t Training loss (single batch): 1.5322954654693604\n",
      "\t Training loss (single batch): 1.3269010782241821\n",
      "\t Training loss (single batch): 1.3826466798782349\n",
      "\t Training loss (single batch): 1.3066028356552124\n",
      "\t Training loss (single batch): 1.0684901475906372\n",
      "\t Training loss (single batch): 1.3968499898910522\n",
      "\t Training loss (single batch): 1.671107292175293\n",
      "\t Training loss (single batch): 1.1952018737792969\n",
      "\t Training loss (single batch): 1.4225633144378662\n",
      "\t Training loss (single batch): 1.5019495487213135\n",
      "\t Training loss (single batch): 1.08384108543396\n",
      "\t Training loss (single batch): 1.382978916168213\n",
      "\t Training loss (single batch): 1.348854422569275\n",
      "\t Training loss (single batch): 1.450886607170105\n",
      "\t Training loss (single batch): 1.894425392150879\n",
      "\t Training loss (single batch): 1.59912109375\n",
      "\t Training loss (single batch): 1.475579023361206\n",
      "\t Training loss (single batch): 1.503957748413086\n",
      "\t Training loss (single batch): 1.3538237810134888\n",
      "\t Training loss (single batch): 1.14170503616333\n",
      "\t Training loss (single batch): 0.7298266291618347\n",
      "\t Training loss (single batch): 1.6587965488433838\n",
      "\t Training loss (single batch): 1.5548880100250244\n",
      "\t Training loss (single batch): 1.6341495513916016\n",
      "\t Training loss (single batch): 1.1803596019744873\n",
      "\t Training loss (single batch): 1.3112307786941528\n",
      "\t Training loss (single batch): 1.2194268703460693\n",
      "\t Training loss (single batch): 0.922172486782074\n",
      "\t Training loss (single batch): 1.1415972709655762\n",
      "\t Training loss (single batch): 1.3852814435958862\n",
      "\t Training loss (single batch): 2.006854772567749\n",
      "\t Training loss (single batch): 1.3536425828933716\n",
      "\t Training loss (single batch): 1.0360215902328491\n",
      "\t Training loss (single batch): 1.115096092224121\n",
      "\t Training loss (single batch): 1.0122060775756836\n",
      "\t Training loss (single batch): 1.2746294736862183\n",
      "\t Training loss (single batch): 0.9189326763153076\n",
      "\t Training loss (single batch): 0.8814234137535095\n",
      "\t Training loss (single batch): 1.4306297302246094\n",
      "\t Training loss (single batch): 1.1998913288116455\n",
      "\t Training loss (single batch): 1.378513216972351\n",
      "\t Training loss (single batch): 1.1745156049728394\n",
      "\t Training loss (single batch): 0.89980548620224\n",
      "\t Training loss (single batch): 1.4396371841430664\n",
      "\t Training loss (single batch): 1.96848726272583\n",
      "\t Training loss (single batch): 1.6135374307632446\n",
      "\t Training loss (single batch): 1.5352915525436401\n",
      "\t Training loss (single batch): 1.700006127357483\n",
      "\t Training loss (single batch): 1.6128880977630615\n",
      "\t Training loss (single batch): 1.8519917726516724\n",
      "\t Training loss (single batch): 1.1139769554138184\n",
      "##################################\n",
      "## EPOCH 6\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0479453802108765\n",
      "\t Training loss (single batch): 1.1122336387634277\n",
      "\t Training loss (single batch): 1.2629742622375488\n",
      "\t Training loss (single batch): 1.3122014999389648\n",
      "\t Training loss (single batch): 1.6683235168457031\n",
      "\t Training loss (single batch): 1.163349986076355\n",
      "\t Training loss (single batch): 1.2615578174591064\n",
      "\t Training loss (single batch): 1.100780963897705\n",
      "\t Training loss (single batch): 1.341988205909729\n",
      "\t Training loss (single batch): 1.6081339120864868\n",
      "\t Training loss (single batch): 1.6930100917816162\n",
      "\t Training loss (single batch): 1.4486361742019653\n",
      "\t Training loss (single batch): 0.8585951924324036\n",
      "\t Training loss (single batch): 1.4349002838134766\n",
      "\t Training loss (single batch): 1.2205328941345215\n",
      "\t Training loss (single batch): 1.4522194862365723\n",
      "\t Training loss (single batch): 1.1490423679351807\n",
      "\t Training loss (single batch): 1.7177077531814575\n",
      "\t Training loss (single batch): 1.1232259273529053\n",
      "\t Training loss (single batch): 1.3325042724609375\n",
      "\t Training loss (single batch): 1.0751657485961914\n",
      "\t Training loss (single batch): 1.5471134185791016\n",
      "\t Training loss (single batch): 1.0174552202224731\n",
      "\t Training loss (single batch): 0.9191094040870667\n",
      "\t Training loss (single batch): 1.4064850807189941\n",
      "\t Training loss (single batch): 1.4566905498504639\n",
      "\t Training loss (single batch): 1.5566332340240479\n",
      "\t Training loss (single batch): 1.8627196550369263\n",
      "\t Training loss (single batch): 0.7641910314559937\n",
      "\t Training loss (single batch): 1.5936397314071655\n",
      "\t Training loss (single batch): 1.0522778034210205\n",
      "\t Training loss (single batch): 1.2951018810272217\n",
      "\t Training loss (single batch): 1.4616371393203735\n",
      "\t Training loss (single batch): 0.966322124004364\n",
      "\t Training loss (single batch): 1.34371817111969\n",
      "\t Training loss (single batch): 1.5564466714859009\n",
      "\t Training loss (single batch): 1.0828553438186646\n",
      "\t Training loss (single batch): 1.147448182106018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5572099685668945\n",
      "\t Training loss (single batch): 1.1814358234405518\n",
      "\t Training loss (single batch): 1.204835295677185\n",
      "\t Training loss (single batch): 1.4752298593521118\n",
      "\t Training loss (single batch): 1.2982144355773926\n",
      "\t Training loss (single batch): 1.3032047748565674\n",
      "\t Training loss (single batch): 1.3135535717010498\n",
      "\t Training loss (single batch): 1.2866487503051758\n",
      "\t Training loss (single batch): 1.1417561769485474\n",
      "\t Training loss (single batch): 1.2002869844436646\n",
      "\t Training loss (single batch): 1.5504252910614014\n",
      "\t Training loss (single batch): 1.2439810037612915\n",
      "\t Training loss (single batch): 1.881636381149292\n",
      "\t Training loss (single batch): 1.1265500783920288\n",
      "\t Training loss (single batch): 1.3839678764343262\n",
      "\t Training loss (single batch): 1.402457356452942\n",
      "\t Training loss (single batch): 2.098276376724243\n",
      "\t Training loss (single batch): 1.2411528825759888\n",
      "\t Training loss (single batch): 1.1172518730163574\n",
      "\t Training loss (single batch): 1.1455520391464233\n",
      "\t Training loss (single batch): 1.3684537410736084\n",
      "\t Training loss (single batch): 1.180200219154358\n",
      "\t Training loss (single batch): 1.1958682537078857\n",
      "\t Training loss (single batch): 1.3969790935516357\n",
      "\t Training loss (single batch): 1.2995030879974365\n",
      "\t Training loss (single batch): 1.1148895025253296\n",
      "\t Training loss (single batch): 1.3357303142547607\n",
      "\t Training loss (single batch): 1.0237218141555786\n",
      "\t Training loss (single batch): 1.5872403383255005\n",
      "\t Training loss (single batch): 1.756583333015442\n",
      "\t Training loss (single batch): 1.5042750835418701\n",
      "\t Training loss (single batch): 1.3634557723999023\n",
      "\t Training loss (single batch): 1.401118516921997\n",
      "\t Training loss (single batch): 1.3086200952529907\n",
      "\t Training loss (single batch): 1.5588093996047974\n",
      "\t Training loss (single batch): 1.3753255605697632\n",
      "\t Training loss (single batch): 0.9714261293411255\n",
      "\t Training loss (single batch): 1.6064308881759644\n",
      "\t Training loss (single batch): 1.877284288406372\n",
      "\t Training loss (single batch): 1.5246951580047607\n",
      "\t Training loss (single batch): 1.3534042835235596\n",
      "\t Training loss (single batch): 1.1544023752212524\n",
      "\t Training loss (single batch): 1.2711939811706543\n",
      "\t Training loss (single batch): 1.1201984882354736\n",
      "\t Training loss (single batch): 1.6593939065933228\n",
      "\t Training loss (single batch): 1.348270297050476\n",
      "\t Training loss (single batch): 1.1672574281692505\n",
      "\t Training loss (single batch): 1.4250261783599854\n",
      "\t Training loss (single batch): 1.4153623580932617\n",
      "\t Training loss (single batch): 1.1830435991287231\n",
      "\t Training loss (single batch): 0.868855893611908\n",
      "\t Training loss (single batch): 1.5247042179107666\n",
      "\t Training loss (single batch): 1.5102602243423462\n",
      "\t Training loss (single batch): 1.5089476108551025\n",
      "\t Training loss (single batch): 1.5682988166809082\n",
      "\t Training loss (single batch): 1.3277556896209717\n",
      "\t Training loss (single batch): 1.1963564157485962\n",
      "\t Training loss (single batch): 1.3249571323394775\n",
      "\t Training loss (single batch): 0.9974730014801025\n",
      "\t Training loss (single batch): 1.1717530488967896\n",
      "\t Training loss (single batch): 1.4360219240188599\n",
      "\t Training loss (single batch): 0.8271389007568359\n",
      "\t Training loss (single batch): 1.4236469268798828\n",
      "\t Training loss (single batch): 1.4532192945480347\n",
      "\t Training loss (single batch): 1.7514606714248657\n",
      "\t Training loss (single batch): 1.5825843811035156\n",
      "\t Training loss (single batch): 1.182574987411499\n",
      "\t Training loss (single batch): 1.0529403686523438\n",
      "\t Training loss (single batch): 1.0427026748657227\n",
      "\t Training loss (single batch): 2.042484998703003\n",
      "\t Training loss (single batch): 1.6198500394821167\n",
      "\t Training loss (single batch): 1.2104154825210571\n",
      "\t Training loss (single batch): 1.3785697221755981\n",
      "\t Training loss (single batch): 1.305932879447937\n",
      "\t Training loss (single batch): 1.595347285270691\n",
      "\t Training loss (single batch): 1.1413018703460693\n",
      "\t Training loss (single batch): 1.2634263038635254\n",
      "\t Training loss (single batch): 1.1749932765960693\n",
      "\t Training loss (single batch): 1.4431993961334229\n",
      "\t Training loss (single batch): 1.4217838048934937\n",
      "\t Training loss (single batch): 1.0483691692352295\n",
      "\t Training loss (single batch): 1.4318733215332031\n",
      "\t Training loss (single batch): 1.2781016826629639\n",
      "\t Training loss (single batch): 1.6146650314331055\n",
      "\t Training loss (single batch): 1.6400737762451172\n",
      "\t Training loss (single batch): 1.7134662866592407\n",
      "\t Training loss (single batch): 1.1059318780899048\n",
      "\t Training loss (single batch): 1.4542604684829712\n",
      "\t Training loss (single batch): 1.4150352478027344\n",
      "\t Training loss (single batch): 1.612599492073059\n",
      "\t Training loss (single batch): 1.672995924949646\n",
      "\t Training loss (single batch): 1.2030904293060303\n",
      "\t Training loss (single batch): 1.525194764137268\n",
      "\t Training loss (single batch): 1.192894697189331\n",
      "\t Training loss (single batch): 1.3561729192733765\n",
      "\t Training loss (single batch): 0.9060057997703552\n",
      "\t Training loss (single batch): 1.1331207752227783\n",
      "\t Training loss (single batch): 1.2431392669677734\n",
      "\t Training loss (single batch): 1.6326909065246582\n",
      "\t Training loss (single batch): 1.0418763160705566\n",
      "\t Training loss (single batch): 1.6962580680847168\n",
      "\t Training loss (single batch): 1.0912116765975952\n",
      "\t Training loss (single batch): 1.4093354940414429\n",
      "\t Training loss (single batch): 1.0327192544937134\n",
      "\t Training loss (single batch): 1.2855416536331177\n",
      "\t Training loss (single batch): 1.0858376026153564\n",
      "\t Training loss (single batch): 1.4637279510498047\n",
      "\t Training loss (single batch): 1.4708374738693237\n",
      "\t Training loss (single batch): 1.257499098777771\n",
      "\t Training loss (single batch): 0.9064001441001892\n",
      "\t Training loss (single batch): 1.8949640989303589\n",
      "\t Training loss (single batch): 1.1632590293884277\n",
      "\t Training loss (single batch): 0.9325476884841919\n",
      "\t Training loss (single batch): 0.9906662106513977\n",
      "\t Training loss (single batch): 1.1915028095245361\n",
      "\t Training loss (single batch): 1.0103727579116821\n",
      "\t Training loss (single batch): 1.5805350542068481\n",
      "\t Training loss (single batch): 1.7249665260314941\n",
      "\t Training loss (single batch): 1.3498737812042236\n",
      "\t Training loss (single batch): 1.0429513454437256\n",
      "\t Training loss (single batch): 0.9378001689910889\n",
      "\t Training loss (single batch): 1.338370680809021\n",
      "\t Training loss (single batch): 1.4980534315109253\n",
      "\t Training loss (single batch): 1.5534437894821167\n",
      "\t Training loss (single batch): 1.6783161163330078\n",
      "\t Training loss (single batch): 1.5024276971817017\n",
      "\t Training loss (single batch): 1.4346579313278198\n",
      "\t Training loss (single batch): 1.5775141716003418\n",
      "\t Training loss (single batch): 1.525036096572876\n",
      "\t Training loss (single batch): 1.330580711364746\n",
      "\t Training loss (single batch): 0.8352684378623962\n",
      "\t Training loss (single batch): 1.0659552812576294\n",
      "\t Training loss (single batch): 1.0728869438171387\n",
      "\t Training loss (single batch): 1.1533746719360352\n",
      "\t Training loss (single batch): 1.18850576877594\n",
      "\t Training loss (single batch): 0.9951996803283691\n",
      "\t Training loss (single batch): 1.9186598062515259\n",
      "\t Training loss (single batch): 1.2354657649993896\n",
      "\t Training loss (single batch): 1.4457329511642456\n",
      "\t Training loss (single batch): 1.9788002967834473\n",
      "\t Training loss (single batch): 1.370003342628479\n",
      "\t Training loss (single batch): 1.4930301904678345\n",
      "\t Training loss (single batch): 1.0086355209350586\n",
      "\t Training loss (single batch): 1.0782372951507568\n",
      "\t Training loss (single batch): 0.8958082795143127\n",
      "\t Training loss (single batch): 1.4436708688735962\n",
      "\t Training loss (single batch): 1.096571445465088\n",
      "\t Training loss (single batch): 1.59200119972229\n",
      "\t Training loss (single batch): 1.3098890781402588\n",
      "\t Training loss (single batch): 1.2000514268875122\n",
      "\t Training loss (single batch): 1.4019228219985962\n",
      "\t Training loss (single batch): 1.2049938440322876\n",
      "\t Training loss (single batch): 1.340987205505371\n",
      "\t Training loss (single batch): 1.7552913427352905\n",
      "\t Training loss (single batch): 1.2391629219055176\n",
      "\t Training loss (single batch): 0.8259998559951782\n",
      "\t Training loss (single batch): 1.4863840341567993\n",
      "\t Training loss (single batch): 1.0635720491409302\n",
      "\t Training loss (single batch): 1.6966981887817383\n",
      "\t Training loss (single batch): 0.7926149964332581\n",
      "\t Training loss (single batch): 1.8410452604293823\n",
      "\t Training loss (single batch): 0.8796330094337463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3751126527786255\n",
      "\t Training loss (single batch): 1.4847419261932373\n",
      "\t Training loss (single batch): 1.3390008211135864\n",
      "\t Training loss (single batch): 1.4419935941696167\n",
      "\t Training loss (single batch): 1.3833190202713013\n",
      "\t Training loss (single batch): 1.575343132019043\n",
      "\t Training loss (single batch): 1.1626685857772827\n",
      "\t Training loss (single batch): 1.4721850156784058\n",
      "\t Training loss (single batch): 1.2313016653060913\n",
      "\t Training loss (single batch): 1.6181164979934692\n",
      "\t Training loss (single batch): 1.492221474647522\n",
      "\t Training loss (single batch): 1.1711101531982422\n",
      "\t Training loss (single batch): 1.6777340173721313\n",
      "\t Training loss (single batch): 1.371174693107605\n",
      "\t Training loss (single batch): 1.6635202169418335\n",
      "\t Training loss (single batch): 0.9916366934776306\n",
      "\t Training loss (single batch): 1.4016468524932861\n",
      "\t Training loss (single batch): 1.4544974565505981\n",
      "\t Training loss (single batch): 1.3922878503799438\n",
      "\t Training loss (single batch): 1.650229573249817\n",
      "\t Training loss (single batch): 1.4899529218673706\n",
      "\t Training loss (single batch): 1.7652570009231567\n",
      "\t Training loss (single batch): 1.726670742034912\n",
      "\t Training loss (single batch): 1.4396734237670898\n",
      "\t Training loss (single batch): 1.1713627576828003\n",
      "\t Training loss (single batch): 1.4027481079101562\n",
      "\t Training loss (single batch): 1.2187271118164062\n",
      "\t Training loss (single batch): 1.2093738317489624\n",
      "\t Training loss (single batch): 1.4245928525924683\n",
      "\t Training loss (single batch): 1.265760064125061\n",
      "\t Training loss (single batch): 1.7851037979125977\n",
      "\t Training loss (single batch): 1.4079341888427734\n",
      "\t Training loss (single batch): 1.1469773054122925\n",
      "\t Training loss (single batch): 0.8513498306274414\n",
      "\t Training loss (single batch): 1.2679023742675781\n",
      "\t Training loss (single batch): 1.3941470384597778\n",
      "\t Training loss (single batch): 0.9467712640762329\n",
      "\t Training loss (single batch): 0.9331303238868713\n",
      "\t Training loss (single batch): 1.7498148679733276\n",
      "\t Training loss (single batch): 1.116592526435852\n",
      "\t Training loss (single batch): 1.1724916696548462\n",
      "\t Training loss (single batch): 1.2560795545578003\n",
      "\t Training loss (single batch): 1.7125020027160645\n",
      "\t Training loss (single batch): 1.6004860401153564\n",
      "\t Training loss (single batch): 1.3860245943069458\n",
      "\t Training loss (single batch): 1.0767863988876343\n",
      "\t Training loss (single batch): 1.2956981658935547\n",
      "\t Training loss (single batch): 1.4037963151931763\n",
      "\t Training loss (single batch): 0.9834837317466736\n",
      "\t Training loss (single batch): 1.0753122568130493\n",
      "\t Training loss (single batch): 1.552911400794983\n",
      "\t Training loss (single batch): 0.8109349608421326\n",
      "\t Training loss (single batch): 1.19905424118042\n",
      "\t Training loss (single batch): 1.2134455442428589\n",
      "\t Training loss (single batch): 1.2759926319122314\n",
      "\t Training loss (single batch): 1.1092690229415894\n",
      "\t Training loss (single batch): 1.0712257623672485\n",
      "\t Training loss (single batch): 1.484399676322937\n",
      "\t Training loss (single batch): 1.163327932357788\n",
      "\t Training loss (single batch): 1.4826041460037231\n",
      "\t Training loss (single batch): 1.288276195526123\n",
      "\t Training loss (single batch): 0.8518561124801636\n",
      "\t Training loss (single batch): 1.470718264579773\n",
      "\t Training loss (single batch): 1.5564236640930176\n",
      "\t Training loss (single batch): 0.9127950668334961\n",
      "\t Training loss (single batch): 0.941444993019104\n",
      "\t Training loss (single batch): 1.1676160097122192\n",
      "\t Training loss (single batch): 1.4043033123016357\n",
      "\t Training loss (single batch): 1.4356157779693604\n",
      "\t Training loss (single batch): 1.2611013650894165\n",
      "\t Training loss (single batch): 1.0217677354812622\n",
      "\t Training loss (single batch): 0.9666075110435486\n",
      "\t Training loss (single batch): 1.6798627376556396\n",
      "\t Training loss (single batch): 1.1153931617736816\n",
      "\t Training loss (single batch): 1.1002637147903442\n",
      "\t Training loss (single batch): 1.0964548587799072\n",
      "\t Training loss (single batch): 1.4045120477676392\n",
      "\t Training loss (single batch): 1.1182256937026978\n",
      "\t Training loss (single batch): 0.828596293926239\n",
      "\t Training loss (single batch): 1.3813854455947876\n",
      "\t Training loss (single batch): 1.2674416303634644\n",
      "\t Training loss (single batch): 1.0851316452026367\n",
      "\t Training loss (single batch): 1.5230463743209839\n",
      "\t Training loss (single batch): 1.5194804668426514\n",
      "\t Training loss (single batch): 1.48589289188385\n",
      "\t Training loss (single batch): 1.0837304592132568\n",
      "\t Training loss (single batch): 1.2199705839157104\n",
      "\t Training loss (single batch): 1.0195465087890625\n",
      "\t Training loss (single batch): 1.3314450979232788\n",
      "\t Training loss (single batch): 1.497649908065796\n",
      "\t Training loss (single batch): 1.4845579862594604\n",
      "\t Training loss (single batch): 1.825127124786377\n",
      "\t Training loss (single batch): 1.261884093284607\n",
      "\t Training loss (single batch): 1.4671037197113037\n",
      "\t Training loss (single batch): 0.9620989561080933\n",
      "\t Training loss (single batch): 1.6171658039093018\n",
      "\t Training loss (single batch): 1.1255449056625366\n",
      "\t Training loss (single batch): 1.8123962879180908\n",
      "\t Training loss (single batch): 1.5053257942199707\n",
      "\t Training loss (single batch): 1.534570336341858\n",
      "\t Training loss (single batch): 1.2087219953536987\n",
      "\t Training loss (single batch): 1.7708877325057983\n",
      "\t Training loss (single batch): 1.2990480661392212\n",
      "\t Training loss (single batch): 1.417648196220398\n",
      "\t Training loss (single batch): 1.5301971435546875\n",
      "\t Training loss (single batch): 1.3052983283996582\n",
      "\t Training loss (single batch): 1.1216646432876587\n",
      "\t Training loss (single batch): 1.4534839391708374\n",
      "\t Training loss (single batch): 1.5323455333709717\n",
      "\t Training loss (single batch): 2.0346550941467285\n",
      "\t Training loss (single batch): 1.2309238910675049\n",
      "\t Training loss (single batch): 1.417786717414856\n",
      "\t Training loss (single batch): 0.9983096718788147\n",
      "\t Training loss (single batch): 1.2654849290847778\n",
      "\t Training loss (single batch): 1.233926773071289\n",
      "\t Training loss (single batch): 1.0147171020507812\n",
      "\t Training loss (single batch): 1.4326316118240356\n",
      "##################################\n",
      "## EPOCH 7\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4635841846466064\n",
      "\t Training loss (single batch): 1.478888750076294\n",
      "\t Training loss (single batch): 1.2660738229751587\n",
      "\t Training loss (single batch): 1.9397411346435547\n",
      "\t Training loss (single batch): 1.623794674873352\n",
      "\t Training loss (single batch): 1.6643668413162231\n",
      "\t Training loss (single batch): 1.2016974687576294\n",
      "\t Training loss (single batch): 1.5769028663635254\n",
      "\t Training loss (single batch): 1.282950520515442\n",
      "\t Training loss (single batch): 1.417271375656128\n",
      "\t Training loss (single batch): 1.1349313259124756\n",
      "\t Training loss (single batch): 1.3340778350830078\n",
      "\t Training loss (single batch): 1.1559953689575195\n",
      "\t Training loss (single batch): 1.401042103767395\n",
      "\t Training loss (single batch): 1.464753270149231\n",
      "\t Training loss (single batch): 1.6799191236495972\n",
      "\t Training loss (single batch): 1.0198346376419067\n",
      "\t Training loss (single batch): 1.5133081674575806\n",
      "\t Training loss (single batch): 0.889947235584259\n",
      "\t Training loss (single batch): 1.0504307746887207\n",
      "\t Training loss (single batch): 1.4465593099594116\n",
      "\t Training loss (single batch): 1.0798637866973877\n",
      "\t Training loss (single batch): 0.7147679328918457\n",
      "\t Training loss (single batch): 1.27835214138031\n",
      "\t Training loss (single batch): 1.0821990966796875\n",
      "\t Training loss (single batch): 1.367414116859436\n",
      "\t Training loss (single batch): 1.7387986183166504\n",
      "\t Training loss (single batch): 1.2619825601577759\n",
      "\t Training loss (single batch): 1.2947145700454712\n",
      "\t Training loss (single batch): 2.029458999633789\n",
      "\t Training loss (single batch): 1.3210728168487549\n",
      "\t Training loss (single batch): 1.7307076454162598\n",
      "\t Training loss (single batch): 1.6896532773971558\n",
      "\t Training loss (single batch): 1.0573087930679321\n",
      "\t Training loss (single batch): 1.6434741020202637\n",
      "\t Training loss (single batch): 1.318018913269043\n",
      "\t Training loss (single batch): 1.280535101890564\n",
      "\t Training loss (single batch): 1.113590121269226\n",
      "\t Training loss (single batch): 1.190711259841919\n",
      "\t Training loss (single batch): 0.9316723942756653\n",
      "\t Training loss (single batch): 1.2821671962738037\n",
      "\t Training loss (single batch): 1.1960902214050293\n",
      "\t Training loss (single batch): 1.5291101932525635\n",
      "\t Training loss (single batch): 0.8162738680839539\n",
      "\t Training loss (single batch): 0.7990548014640808\n",
      "\t Training loss (single batch): 1.6169122457504272\n",
      "\t Training loss (single batch): 1.4841333627700806\n",
      "\t Training loss (single batch): 1.421275019645691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3863770961761475\n",
      "\t Training loss (single batch): 1.0860899686813354\n",
      "\t Training loss (single batch): 1.8306668996810913\n",
      "\t Training loss (single batch): 1.8980965614318848\n",
      "\t Training loss (single batch): 1.4767937660217285\n",
      "\t Training loss (single batch): 1.2836138010025024\n",
      "\t Training loss (single batch): 1.6852974891662598\n",
      "\t Training loss (single batch): 1.3766895532608032\n",
      "\t Training loss (single batch): 1.5563361644744873\n",
      "\t Training loss (single batch): 1.2767599821090698\n",
      "\t Training loss (single batch): 1.1433006525039673\n",
      "\t Training loss (single batch): 1.1182340383529663\n",
      "\t Training loss (single batch): 0.8421965837478638\n",
      "\t Training loss (single batch): 0.8272776007652283\n",
      "\t Training loss (single batch): 1.156151294708252\n",
      "\t Training loss (single batch): 1.8743884563446045\n",
      "\t Training loss (single batch): 1.5686334371566772\n",
      "\t Training loss (single batch): 1.1138414144515991\n",
      "\t Training loss (single batch): 1.2461341619491577\n",
      "\t Training loss (single batch): 1.511902093887329\n",
      "\t Training loss (single batch): 1.0575684309005737\n",
      "\t Training loss (single batch): 1.2477810382843018\n",
      "\t Training loss (single batch): 1.5915300846099854\n",
      "\t Training loss (single batch): 1.0703352689743042\n",
      "\t Training loss (single batch): 1.1899065971374512\n",
      "\t Training loss (single batch): 1.4437483549118042\n",
      "\t Training loss (single batch): 1.8976393938064575\n",
      "\t Training loss (single batch): 1.5212692022323608\n",
      "\t Training loss (single batch): 1.2041372060775757\n",
      "\t Training loss (single batch): 1.0215964317321777\n",
      "\t Training loss (single batch): 1.3084815740585327\n",
      "\t Training loss (single batch): 1.1303380727767944\n",
      "\t Training loss (single batch): 1.1848583221435547\n",
      "\t Training loss (single batch): 0.666573703289032\n",
      "\t Training loss (single batch): 2.0220534801483154\n",
      "\t Training loss (single batch): 1.014594316482544\n",
      "\t Training loss (single batch): 1.0213619470596313\n",
      "\t Training loss (single batch): 1.9155761003494263\n",
      "\t Training loss (single batch): 1.7153247594833374\n",
      "\t Training loss (single batch): 0.9250704646110535\n",
      "\t Training loss (single batch): 1.137916088104248\n",
      "\t Training loss (single batch): 1.6162160634994507\n",
      "\t Training loss (single batch): 1.5033202171325684\n",
      "\t Training loss (single batch): 1.049411416053772\n",
      "\t Training loss (single batch): 1.3579493761062622\n",
      "\t Training loss (single batch): 1.4459667205810547\n",
      "\t Training loss (single batch): 1.1647404432296753\n",
      "\t Training loss (single batch): 0.782917857170105\n",
      "\t Training loss (single batch): 1.395733118057251\n",
      "\t Training loss (single batch): 1.1358877420425415\n",
      "\t Training loss (single batch): 1.3588974475860596\n",
      "\t Training loss (single batch): 1.6132482290267944\n",
      "\t Training loss (single batch): 1.4816871881484985\n",
      "\t Training loss (single batch): 1.4507466554641724\n",
      "\t Training loss (single batch): 1.3212834596633911\n",
      "\t Training loss (single batch): 1.1880543231964111\n",
      "\t Training loss (single batch): 1.090998888015747\n",
      "\t Training loss (single batch): 1.5746313333511353\n",
      "\t Training loss (single batch): 0.9965159296989441\n",
      "\t Training loss (single batch): 1.373569369316101\n",
      "\t Training loss (single batch): 1.1646101474761963\n",
      "\t Training loss (single batch): 1.4860984086990356\n",
      "\t Training loss (single batch): 0.9268205761909485\n",
      "\t Training loss (single batch): 1.5242950916290283\n",
      "\t Training loss (single batch): 1.2296297550201416\n",
      "\t Training loss (single batch): 1.2431939840316772\n",
      "\t Training loss (single batch): 1.4503275156021118\n",
      "\t Training loss (single batch): 1.5753930807113647\n",
      "\t Training loss (single batch): 1.532679557800293\n",
      "\t Training loss (single batch): 1.4076226949691772\n",
      "\t Training loss (single batch): 1.1893539428710938\n",
      "\t Training loss (single batch): 1.33670175075531\n",
      "\t Training loss (single batch): 1.0613378286361694\n",
      "\t Training loss (single batch): 1.5635349750518799\n",
      "\t Training loss (single batch): 1.2613370418548584\n",
      "\t Training loss (single batch): 1.6939728260040283\n",
      "\t Training loss (single batch): 1.3434714078903198\n",
      "\t Training loss (single batch): 1.9596809148788452\n",
      "\t Training loss (single batch): 1.3873220682144165\n",
      "\t Training loss (single batch): 1.1266905069351196\n",
      "\t Training loss (single batch): 1.2801870107650757\n",
      "\t Training loss (single batch): 1.5250451564788818\n",
      "\t Training loss (single batch): 0.9579570889472961\n",
      "\t Training loss (single batch): 1.349150538444519\n",
      "\t Training loss (single batch): 1.038596510887146\n",
      "\t Training loss (single batch): 1.2181471586227417\n",
      "\t Training loss (single batch): 1.5667076110839844\n",
      "\t Training loss (single batch): 1.490054726600647\n",
      "\t Training loss (single batch): 1.1717407703399658\n",
      "\t Training loss (single batch): 1.2197725772857666\n",
      "\t Training loss (single batch): 1.1149767637252808\n",
      "\t Training loss (single batch): 1.9666807651519775\n",
      "\t Training loss (single batch): 1.2612934112548828\n",
      "\t Training loss (single batch): 1.3422465324401855\n",
      "\t Training loss (single batch): 1.6053773164749146\n",
      "\t Training loss (single batch): 1.3265608549118042\n",
      "\t Training loss (single batch): 1.2616336345672607\n",
      "\t Training loss (single batch): 0.887492835521698\n",
      "\t Training loss (single batch): 1.4650808572769165\n",
      "\t Training loss (single batch): 1.3380812406539917\n",
      "\t Training loss (single batch): 0.9716243147850037\n",
      "\t Training loss (single batch): 1.6939890384674072\n",
      "\t Training loss (single batch): 1.4465892314910889\n",
      "\t Training loss (single batch): 1.1769638061523438\n",
      "\t Training loss (single batch): 1.73865807056427\n",
      "\t Training loss (single batch): 1.259304165840149\n",
      "\t Training loss (single batch): 1.181979775428772\n",
      "\t Training loss (single batch): 1.1038966178894043\n",
      "\t Training loss (single batch): 1.4240120649337769\n",
      "\t Training loss (single batch): 1.5730870962142944\n",
      "\t Training loss (single batch): 1.1616088151931763\n",
      "\t Training loss (single batch): 1.1642035245895386\n",
      "\t Training loss (single batch): 2.078937292098999\n",
      "\t Training loss (single batch): 1.0068106651306152\n",
      "\t Training loss (single batch): 1.8962289094924927\n",
      "\t Training loss (single batch): 1.2343353033065796\n",
      "\t Training loss (single batch): 1.4361428022384644\n",
      "\t Training loss (single batch): 2.0199689865112305\n",
      "\t Training loss (single batch): 1.6391687393188477\n",
      "\t Training loss (single batch): 1.2955230474472046\n",
      "\t Training loss (single batch): 1.654767632484436\n",
      "\t Training loss (single batch): 1.562246322631836\n",
      "\t Training loss (single batch): 1.4333534240722656\n",
      "\t Training loss (single batch): 0.9562643766403198\n",
      "\t Training loss (single batch): 0.9164946675300598\n",
      "\t Training loss (single batch): 1.523870587348938\n",
      "\t Training loss (single batch): 1.1854662895202637\n",
      "\t Training loss (single batch): 1.6068110466003418\n",
      "\t Training loss (single batch): 1.1467088460922241\n",
      "\t Training loss (single batch): 1.0659668445587158\n",
      "\t Training loss (single batch): 1.5085878372192383\n",
      "\t Training loss (single batch): 1.5348509550094604\n",
      "\t Training loss (single batch): 1.1190025806427002\n",
      "\t Training loss (single batch): 1.2038787603378296\n",
      "\t Training loss (single batch): 1.5754567384719849\n",
      "\t Training loss (single batch): 1.2316405773162842\n",
      "\t Training loss (single batch): 1.6752963066101074\n",
      "\t Training loss (single batch): 1.1291530132293701\n",
      "\t Training loss (single batch): 1.817459225654602\n",
      "\t Training loss (single batch): 0.9454740881919861\n",
      "\t Training loss (single batch): 0.8185219764709473\n",
      "\t Training loss (single batch): 0.8428750038146973\n",
      "\t Training loss (single batch): 1.353951096534729\n",
      "\t Training loss (single batch): 1.2728233337402344\n",
      "\t Training loss (single batch): 1.29718816280365\n",
      "\t Training loss (single batch): 0.7762730717658997\n",
      "\t Training loss (single batch): 1.0355486869812012\n",
      "\t Training loss (single batch): 1.0093328952789307\n",
      "\t Training loss (single batch): 1.320786714553833\n",
      "\t Training loss (single batch): 1.2216541767120361\n",
      "\t Training loss (single batch): 1.4857112169265747\n",
      "\t Training loss (single batch): 0.8551244735717773\n",
      "\t Training loss (single batch): 1.6390401124954224\n",
      "\t Training loss (single batch): 1.333109736442566\n",
      "\t Training loss (single batch): 1.5145339965820312\n",
      "\t Training loss (single batch): 1.4889841079711914\n",
      "\t Training loss (single batch): 1.3971937894821167\n",
      "\t Training loss (single batch): 1.2367979288101196\n",
      "\t Training loss (single batch): 1.5954389572143555\n",
      "\t Training loss (single batch): 1.7229098081588745\n",
      "\t Training loss (single batch): 0.7284402251243591\n",
      "\t Training loss (single batch): 1.4872589111328125\n",
      "\t Training loss (single batch): 1.262283444404602\n",
      "\t Training loss (single batch): 1.1653958559036255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9929804801940918\n",
      "\t Training loss (single batch): 1.4656163454055786\n",
      "\t Training loss (single batch): 1.4785959720611572\n",
      "\t Training loss (single batch): 0.9495522975921631\n",
      "\t Training loss (single batch): 1.5312384366989136\n",
      "\t Training loss (single batch): 2.0343990325927734\n",
      "\t Training loss (single batch): 1.4093327522277832\n",
      "\t Training loss (single batch): 1.426294207572937\n",
      "\t Training loss (single batch): 1.3855563402175903\n",
      "\t Training loss (single batch): 1.1866689920425415\n",
      "\t Training loss (single batch): 1.2802002429962158\n",
      "\t Training loss (single batch): 1.5909758806228638\n",
      "\t Training loss (single batch): 1.427329182624817\n",
      "\t Training loss (single batch): 1.1024798154830933\n",
      "\t Training loss (single batch): 1.2624330520629883\n",
      "\t Training loss (single batch): 1.1638365983963013\n",
      "\t Training loss (single batch): 0.9355707764625549\n",
      "\t Training loss (single batch): 1.2994128465652466\n",
      "\t Training loss (single batch): 1.5296854972839355\n",
      "\t Training loss (single batch): 1.275530219078064\n",
      "\t Training loss (single batch): 1.1301575899124146\n",
      "\t Training loss (single batch): 1.8257287740707397\n",
      "\t Training loss (single batch): 0.7614814043045044\n",
      "\t Training loss (single batch): 1.272636890411377\n",
      "\t Training loss (single batch): 0.9548396468162537\n",
      "\t Training loss (single batch): 1.1859432458877563\n",
      "\t Training loss (single batch): 1.420617938041687\n",
      "\t Training loss (single batch): 1.3677905797958374\n",
      "\t Training loss (single batch): 1.542117953300476\n",
      "\t Training loss (single batch): 1.2547327280044556\n",
      "\t Training loss (single batch): 0.8108939528465271\n",
      "\t Training loss (single batch): 1.482151985168457\n",
      "\t Training loss (single batch): 1.6038297414779663\n",
      "\t Training loss (single batch): 1.3912376165390015\n",
      "\t Training loss (single batch): 1.1832797527313232\n",
      "\t Training loss (single batch): 1.4196614027023315\n",
      "\t Training loss (single batch): 1.3481205701828003\n",
      "\t Training loss (single batch): 1.4987105131149292\n",
      "\t Training loss (single batch): 1.09806489944458\n",
      "\t Training loss (single batch): 1.2563239336013794\n",
      "\t Training loss (single batch): 1.7571604251861572\n",
      "\t Training loss (single batch): 1.1110581159591675\n",
      "\t Training loss (single batch): 1.3642446994781494\n",
      "\t Training loss (single batch): 0.9247147440910339\n",
      "\t Training loss (single batch): 1.589056372642517\n",
      "\t Training loss (single batch): 1.6428053379058838\n",
      "\t Training loss (single batch): 1.2127430438995361\n",
      "\t Training loss (single batch): 1.4281591176986694\n",
      "\t Training loss (single batch): 1.403497576713562\n",
      "\t Training loss (single batch): 1.5315136909484863\n",
      "\t Training loss (single batch): 1.0439977645874023\n",
      "\t Training loss (single batch): 1.148468017578125\n",
      "\t Training loss (single batch): 1.4720176458358765\n",
      "\t Training loss (single batch): 1.665605068206787\n",
      "\t Training loss (single batch): 1.6707375049591064\n",
      "\t Training loss (single batch): 1.67916738986969\n",
      "\t Training loss (single batch): 0.9697994589805603\n",
      "\t Training loss (single batch): 1.2031079530715942\n",
      "\t Training loss (single batch): 1.6660914421081543\n",
      "\t Training loss (single batch): 1.0327662229537964\n",
      "\t Training loss (single batch): 1.0623477697372437\n",
      "\t Training loss (single batch): 1.850372076034546\n",
      "\t Training loss (single batch): 1.395365834236145\n",
      "\t Training loss (single batch): 1.2340083122253418\n",
      "\t Training loss (single batch): 1.1512490510940552\n",
      "\t Training loss (single batch): 0.983604907989502\n",
      "\t Training loss (single batch): 1.5764596462249756\n",
      "\t Training loss (single batch): 1.757139801979065\n",
      "\t Training loss (single batch): 1.173902988433838\n",
      "\t Training loss (single batch): 1.546949863433838\n",
      "\t Training loss (single batch): 0.8049202561378479\n",
      "\t Training loss (single batch): 1.6532717943191528\n",
      "\t Training loss (single batch): 1.6472692489624023\n",
      "\t Training loss (single batch): 1.2967697381973267\n",
      "\t Training loss (single batch): 1.0822324752807617\n",
      "\t Training loss (single batch): 1.4143160581588745\n",
      "\t Training loss (single batch): 1.020246982574463\n",
      "\t Training loss (single batch): 1.3250895738601685\n",
      "\t Training loss (single batch): 0.9375976324081421\n",
      "\t Training loss (single batch): 1.2796306610107422\n",
      "\t Training loss (single batch): 1.4080617427825928\n",
      "\t Training loss (single batch): 1.2791839838027954\n",
      "\t Training loss (single batch): 0.9436364769935608\n",
      "\t Training loss (single batch): 0.9384757876396179\n",
      "\t Training loss (single batch): 1.3381671905517578\n",
      "\t Training loss (single batch): 1.0923300981521606\n",
      "\t Training loss (single batch): 1.0609019994735718\n",
      "\t Training loss (single batch): 1.1003930568695068\n",
      "\t Training loss (single batch): 1.4235188961029053\n",
      "\t Training loss (single batch): 1.3079352378845215\n",
      "\t Training loss (single batch): 1.3228583335876465\n",
      "\t Training loss (single batch): 1.891562819480896\n",
      "\t Training loss (single batch): 1.1041219234466553\n",
      "\t Training loss (single batch): 1.0013285875320435\n",
      "\t Training loss (single batch): 1.0202546119689941\n",
      "\t Training loss (single batch): 1.2073230743408203\n",
      "\t Training loss (single batch): 1.6081254482269287\n",
      "\t Training loss (single batch): 1.5641210079193115\n",
      "\t Training loss (single batch): 1.1212351322174072\n",
      "\t Training loss (single batch): 1.2781091928482056\n",
      "\t Training loss (single batch): 1.784726858139038\n",
      "\t Training loss (single batch): 1.1584526300430298\n",
      "\t Training loss (single batch): 1.1557226181030273\n",
      "\t Training loss (single batch): 1.4845882654190063\n",
      "\t Training loss (single batch): 1.5144221782684326\n",
      "##################################\n",
      "## EPOCH 8\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1754295825958252\n",
      "\t Training loss (single batch): 1.1311722993850708\n",
      "\t Training loss (single batch): 1.3432493209838867\n",
      "\t Training loss (single batch): 1.6925053596496582\n",
      "\t Training loss (single batch): 0.8258882164955139\n",
      "\t Training loss (single batch): 1.3726726770401\n",
      "\t Training loss (single batch): 1.2428622245788574\n",
      "\t Training loss (single batch): 1.3637700080871582\n",
      "\t Training loss (single batch): 1.6566494703292847\n",
      "\t Training loss (single batch): 1.6838268041610718\n",
      "\t Training loss (single batch): 0.8422298431396484\n",
      "\t Training loss (single batch): 1.9159586429595947\n",
      "\t Training loss (single batch): 1.1789038181304932\n",
      "\t Training loss (single batch): 0.8903437852859497\n",
      "\t Training loss (single batch): 1.609033226966858\n",
      "\t Training loss (single batch): 1.3517941236495972\n",
      "\t Training loss (single batch): 1.7445405721664429\n",
      "\t Training loss (single batch): 1.4496347904205322\n",
      "\t Training loss (single batch): 1.3111069202423096\n",
      "\t Training loss (single batch): 1.149868130683899\n",
      "\t Training loss (single batch): 1.172318696975708\n",
      "\t Training loss (single batch): 1.0356581211090088\n",
      "\t Training loss (single batch): 0.9289244413375854\n",
      "\t Training loss (single batch): 1.1576142311096191\n",
      "\t Training loss (single batch): 1.267502784729004\n",
      "\t Training loss (single batch): 1.174192190170288\n",
      "\t Training loss (single batch): 1.1430723667144775\n",
      "\t Training loss (single batch): 1.3870079517364502\n",
      "\t Training loss (single batch): 1.2273433208465576\n",
      "\t Training loss (single batch): 1.471638798713684\n",
      "\t Training loss (single batch): 1.1454792022705078\n",
      "\t Training loss (single batch): 1.0440829992294312\n",
      "\t Training loss (single batch): 1.0048842430114746\n",
      "\t Training loss (single batch): 0.7986392378807068\n",
      "\t Training loss (single batch): 1.579634666442871\n",
      "\t Training loss (single batch): 1.3945006132125854\n",
      "\t Training loss (single batch): 1.231785535812378\n",
      "\t Training loss (single batch): 1.3955997228622437\n",
      "\t Training loss (single batch): 1.133848786354065\n",
      "\t Training loss (single batch): 1.4256789684295654\n",
      "\t Training loss (single batch): 1.4616832733154297\n",
      "\t Training loss (single batch): 1.4635870456695557\n",
      "\t Training loss (single batch): 1.2182769775390625\n",
      "\t Training loss (single batch): 1.0486600399017334\n",
      "\t Training loss (single batch): 1.3415859937667847\n",
      "\t Training loss (single batch): 1.618914008140564\n",
      "\t Training loss (single batch): 1.2834563255310059\n",
      "\t Training loss (single batch): 1.2436914443969727\n",
      "\t Training loss (single batch): 1.4787390232086182\n",
      "\t Training loss (single batch): 0.909932017326355\n",
      "\t Training loss (single batch): 1.304950475692749\n",
      "\t Training loss (single batch): 1.217575192451477\n",
      "\t Training loss (single batch): 1.3607516288757324\n",
      "\t Training loss (single batch): 1.3972909450531006\n",
      "\t Training loss (single batch): 1.3456134796142578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5464441776275635\n",
      "\t Training loss (single batch): 1.2834113836288452\n",
      "\t Training loss (single batch): 1.1368860006332397\n",
      "\t Training loss (single batch): 1.5555249452590942\n",
      "\t Training loss (single batch): 1.4626716375350952\n",
      "\t Training loss (single batch): 1.0610129833221436\n",
      "\t Training loss (single batch): 1.1605244874954224\n",
      "\t Training loss (single batch): 1.590043544769287\n",
      "\t Training loss (single batch): 1.6121894121170044\n",
      "\t Training loss (single batch): 1.3368884325027466\n",
      "\t Training loss (single batch): 1.2164205312728882\n",
      "\t Training loss (single batch): 1.5464462041854858\n",
      "\t Training loss (single batch): 1.2669795751571655\n",
      "\t Training loss (single batch): 1.2342418432235718\n",
      "\t Training loss (single batch): 1.493949294090271\n",
      "\t Training loss (single batch): 1.3010631799697876\n",
      "\t Training loss (single batch): 1.348005771636963\n",
      "\t Training loss (single batch): 1.2633635997772217\n",
      "\t Training loss (single batch): 1.0806928873062134\n",
      "\t Training loss (single batch): 1.4603240489959717\n",
      "\t Training loss (single batch): 1.533918857574463\n",
      "\t Training loss (single batch): 0.7639742493629456\n",
      "\t Training loss (single batch): 1.1390821933746338\n",
      "\t Training loss (single batch): 1.5477464199066162\n",
      "\t Training loss (single batch): 1.3130156993865967\n",
      "\t Training loss (single batch): 1.3029239177703857\n",
      "\t Training loss (single batch): 1.7432310581207275\n",
      "\t Training loss (single batch): 0.9428628087043762\n",
      "\t Training loss (single batch): 1.5075573921203613\n",
      "\t Training loss (single batch): 1.446583867073059\n",
      "\t Training loss (single batch): 1.6930112838745117\n",
      "\t Training loss (single batch): 1.866141676902771\n",
      "\t Training loss (single batch): 0.983399510383606\n",
      "\t Training loss (single batch): 1.1397510766983032\n",
      "\t Training loss (single batch): 0.8557247519493103\n",
      "\t Training loss (single batch): 0.9159370064735413\n",
      "\t Training loss (single batch): 1.0368882417678833\n",
      "\t Training loss (single batch): 1.818148136138916\n",
      "\t Training loss (single batch): 0.9013407826423645\n",
      "\t Training loss (single batch): 1.2296503782272339\n",
      "\t Training loss (single batch): 1.4524180889129639\n",
      "\t Training loss (single batch): 1.7884783744812012\n",
      "\t Training loss (single batch): 1.345098853111267\n",
      "\t Training loss (single batch): 1.419642686843872\n",
      "\t Training loss (single batch): 1.3686023950576782\n",
      "\t Training loss (single batch): 1.3556932210922241\n",
      "\t Training loss (single batch): 1.376732349395752\n",
      "\t Training loss (single batch): 0.9059292078018188\n",
      "\t Training loss (single batch): 1.7541351318359375\n",
      "\t Training loss (single batch): 1.0056734085083008\n",
      "\t Training loss (single batch): 1.4951670169830322\n",
      "\t Training loss (single batch): 0.971876859664917\n",
      "\t Training loss (single batch): 1.2624452114105225\n",
      "\t Training loss (single batch): 1.5539124011993408\n",
      "\t Training loss (single batch): 1.292275309562683\n",
      "\t Training loss (single batch): 1.2813109159469604\n",
      "\t Training loss (single batch): 1.787925362586975\n",
      "\t Training loss (single batch): 1.0462145805358887\n",
      "\t Training loss (single batch): 1.1157286167144775\n",
      "\t Training loss (single batch): 1.1189665794372559\n",
      "\t Training loss (single batch): 0.7653648257255554\n",
      "\t Training loss (single batch): 1.2268435955047607\n",
      "\t Training loss (single batch): 1.1707422733306885\n",
      "\t Training loss (single batch): 1.174476146697998\n",
      "\t Training loss (single batch): 1.4166723489761353\n",
      "\t Training loss (single batch): 1.003377914428711\n",
      "\t Training loss (single batch): 1.3892059326171875\n",
      "\t Training loss (single batch): 1.8232967853546143\n",
      "\t Training loss (single batch): 1.3712929487228394\n",
      "\t Training loss (single batch): 1.4996280670166016\n",
      "\t Training loss (single batch): 1.526058316230774\n",
      "\t Training loss (single batch): 1.665700078010559\n",
      "\t Training loss (single batch): 1.2590148448944092\n",
      "\t Training loss (single batch): 1.2420217990875244\n",
      "\t Training loss (single batch): 1.1882164478302002\n",
      "\t Training loss (single batch): 1.5276851654052734\n",
      "\t Training loss (single batch): 1.5700409412384033\n",
      "\t Training loss (single batch): 1.7869774103164673\n",
      "\t Training loss (single batch): 1.2079657316207886\n",
      "\t Training loss (single batch): 1.2098263502120972\n",
      "\t Training loss (single batch): 1.147093653678894\n",
      "\t Training loss (single batch): 1.2970201969146729\n",
      "\t Training loss (single batch): 1.757555365562439\n",
      "\t Training loss (single batch): 1.450714111328125\n",
      "\t Training loss (single batch): 0.9118981957435608\n",
      "\t Training loss (single batch): 1.3852025270462036\n",
      "\t Training loss (single batch): 1.6754868030548096\n",
      "\t Training loss (single batch): 1.5845872163772583\n",
      "\t Training loss (single batch): 1.582973837852478\n",
      "\t Training loss (single batch): 1.691453218460083\n",
      "\t Training loss (single batch): 1.3867689371109009\n",
      "\t Training loss (single batch): 1.0802223682403564\n",
      "\t Training loss (single batch): 1.463230013847351\n",
      "\t Training loss (single batch): 1.6678025722503662\n",
      "\t Training loss (single batch): 1.3571313619613647\n",
      "\t Training loss (single batch): 1.9458065032958984\n",
      "\t Training loss (single batch): 1.5894584655761719\n",
      "\t Training loss (single batch): 1.7037930488586426\n",
      "\t Training loss (single batch): 1.120375633239746\n",
      "\t Training loss (single batch): 1.3683778047561646\n",
      "\t Training loss (single batch): 1.6704480648040771\n",
      "\t Training loss (single batch): 1.0001626014709473\n",
      "\t Training loss (single batch): 0.8726216554641724\n",
      "\t Training loss (single batch): 1.3323322534561157\n",
      "\t Training loss (single batch): 1.6626771688461304\n",
      "\t Training loss (single batch): 1.355401635169983\n",
      "\t Training loss (single batch): 1.1764558553695679\n",
      "\t Training loss (single batch): 2.086883783340454\n",
      "\t Training loss (single batch): 1.4104729890823364\n",
      "\t Training loss (single batch): 1.022063970565796\n",
      "\t Training loss (single batch): 1.4957115650177002\n",
      "\t Training loss (single batch): 1.9469882249832153\n",
      "\t Training loss (single batch): 1.8538219928741455\n",
      "\t Training loss (single batch): 1.765383243560791\n",
      "\t Training loss (single batch): 1.2024290561676025\n",
      "\t Training loss (single batch): 0.9295677542686462\n",
      "\t Training loss (single batch): 1.4460124969482422\n",
      "\t Training loss (single batch): 1.5485528707504272\n",
      "\t Training loss (single batch): 1.3599894046783447\n",
      "\t Training loss (single batch): 1.6850550174713135\n",
      "\t Training loss (single batch): 0.9718777537345886\n",
      "\t Training loss (single batch): 1.6129807233810425\n",
      "\t Training loss (single batch): 1.527817726135254\n",
      "\t Training loss (single batch): 1.5903152227401733\n",
      "\t Training loss (single batch): 1.2368413209915161\n",
      "\t Training loss (single batch): 1.2273787260055542\n",
      "\t Training loss (single batch): 1.3281956911087036\n",
      "\t Training loss (single batch): 0.7260659337043762\n",
      "\t Training loss (single batch): 1.1956923007965088\n",
      "\t Training loss (single batch): 1.312370777130127\n",
      "\t Training loss (single batch): 2.0447094440460205\n",
      "\t Training loss (single batch): 1.065048336982727\n",
      "\t Training loss (single batch): 1.343630075454712\n",
      "\t Training loss (single batch): 1.291853666305542\n",
      "\t Training loss (single batch): 1.3660808801651\n",
      "\t Training loss (single batch): 1.3478389978408813\n",
      "\t Training loss (single batch): 1.1073805093765259\n",
      "\t Training loss (single batch): 1.2558825016021729\n",
      "\t Training loss (single batch): 1.4956468343734741\n",
      "\t Training loss (single batch): 1.1822924613952637\n",
      "\t Training loss (single batch): 1.134400725364685\n",
      "\t Training loss (single batch): 1.8746962547302246\n",
      "\t Training loss (single batch): 1.2470042705535889\n",
      "\t Training loss (single batch): 1.3807061910629272\n",
      "\t Training loss (single batch): 1.8881582021713257\n",
      "\t Training loss (single batch): 1.0903568267822266\n",
      "\t Training loss (single batch): 1.207610845565796\n",
      "\t Training loss (single batch): 1.7051161527633667\n",
      "\t Training loss (single batch): 1.1690585613250732\n",
      "\t Training loss (single batch): 1.7916185855865479\n",
      "\t Training loss (single batch): 0.8389453887939453\n",
      "\t Training loss (single batch): 1.184130072593689\n",
      "\t Training loss (single batch): 1.3876516819000244\n",
      "\t Training loss (single batch): 1.7895091772079468\n",
      "\t Training loss (single batch): 1.9698666334152222\n",
      "\t Training loss (single batch): 1.3348593711853027\n",
      "\t Training loss (single batch): 0.9756604433059692\n",
      "\t Training loss (single batch): 1.410231113433838\n",
      "\t Training loss (single batch): 1.5101661682128906\n",
      "\t Training loss (single batch): 1.3562356233596802\n",
      "\t Training loss (single batch): 1.0426307916641235\n",
      "\t Training loss (single batch): 1.143195390701294\n",
      "\t Training loss (single batch): 0.9583607912063599\n",
      "\t Training loss (single batch): 1.368538737297058\n",
      "\t Training loss (single batch): 1.4172612428665161\n",
      "\t Training loss (single batch): 1.7171396017074585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3043687343597412\n",
      "\t Training loss (single batch): 1.6991854906082153\n",
      "\t Training loss (single batch): 1.6642842292785645\n",
      "\t Training loss (single batch): 1.2361961603164673\n",
      "\t Training loss (single batch): 1.7356737852096558\n",
      "\t Training loss (single batch): 1.3556581735610962\n",
      "\t Training loss (single batch): 1.6143662929534912\n",
      "\t Training loss (single batch): 1.081247329711914\n",
      "\t Training loss (single batch): 0.8902382850646973\n",
      "\t Training loss (single batch): 1.661624789237976\n",
      "\t Training loss (single batch): 1.051356315612793\n",
      "\t Training loss (single batch): 1.3011486530303955\n",
      "\t Training loss (single batch): 1.1014169454574585\n",
      "\t Training loss (single batch): 1.5536330938339233\n",
      "\t Training loss (single batch): 0.8407399654388428\n",
      "\t Training loss (single batch): 0.7749577164649963\n",
      "\t Training loss (single batch): 1.4959582090377808\n",
      "\t Training loss (single batch): 1.4004385471343994\n",
      "\t Training loss (single batch): 1.1568820476531982\n",
      "\t Training loss (single batch): 1.3621572256088257\n",
      "\t Training loss (single batch): 1.4700700044631958\n",
      "\t Training loss (single batch): 1.1741198301315308\n",
      "\t Training loss (single batch): 1.2326334714889526\n",
      "\t Training loss (single batch): 1.4274202585220337\n",
      "\t Training loss (single batch): 1.4947013854980469\n",
      "\t Training loss (single batch): 1.537861704826355\n",
      "\t Training loss (single batch): 1.5503076314926147\n",
      "\t Training loss (single batch): 1.2037960290908813\n",
      "\t Training loss (single batch): 1.1699527502059937\n",
      "\t Training loss (single batch): 1.4581986665725708\n",
      "\t Training loss (single batch): 1.0285207033157349\n",
      "\t Training loss (single batch): 1.8684520721435547\n",
      "\t Training loss (single batch): 1.4519554376602173\n",
      "\t Training loss (single batch): 1.6517674922943115\n",
      "\t Training loss (single batch): 1.401416540145874\n",
      "\t Training loss (single batch): 1.4137673377990723\n",
      "\t Training loss (single batch): 1.4480606317520142\n",
      "\t Training loss (single batch): 1.4428380727767944\n",
      "\t Training loss (single batch): 1.5960958003997803\n",
      "\t Training loss (single batch): 0.8705673217773438\n",
      "\t Training loss (single batch): 1.5121705532073975\n",
      "\t Training loss (single batch): 1.5728782415390015\n",
      "\t Training loss (single batch): 1.479820728302002\n",
      "\t Training loss (single batch): 1.3708419799804688\n",
      "\t Training loss (single batch): 1.4095067977905273\n",
      "\t Training loss (single batch): 0.9201621413230896\n",
      "\t Training loss (single batch): 1.1069095134735107\n",
      "\t Training loss (single batch): 1.3546725511550903\n",
      "\t Training loss (single batch): 1.0662857294082642\n",
      "\t Training loss (single batch): 1.4724037647247314\n",
      "\t Training loss (single batch): 1.0314604043960571\n",
      "\t Training loss (single batch): 1.7091741561889648\n",
      "\t Training loss (single batch): 1.8420741558074951\n",
      "\t Training loss (single batch): 1.6259737014770508\n",
      "\t Training loss (single batch): 1.2550873756408691\n",
      "\t Training loss (single batch): 1.377915382385254\n",
      "\t Training loss (single batch): 1.3640729188919067\n",
      "\t Training loss (single batch): 1.4631412029266357\n",
      "\t Training loss (single batch): 1.0991522073745728\n",
      "\t Training loss (single batch): 1.9074522256851196\n",
      "\t Training loss (single batch): 1.344458818435669\n",
      "\t Training loss (single batch): 1.3488795757293701\n",
      "\t Training loss (single batch): 0.7433769106864929\n",
      "\t Training loss (single batch): 1.3957107067108154\n",
      "\t Training loss (single batch): 1.0833557844161987\n",
      "\t Training loss (single batch): 1.5177103281021118\n",
      "\t Training loss (single batch): 1.6964627504348755\n",
      "\t Training loss (single batch): 1.3096208572387695\n",
      "\t Training loss (single batch): 0.953136682510376\n",
      "\t Training loss (single batch): 1.386621356010437\n",
      "\t Training loss (single batch): 1.154011845588684\n",
      "\t Training loss (single batch): 1.215924620628357\n",
      "\t Training loss (single batch): 1.3726989030838013\n",
      "\t Training loss (single batch): 0.9248595237731934\n",
      "\t Training loss (single batch): 1.3064830303192139\n",
      "\t Training loss (single batch): 1.128708839416504\n",
      "\t Training loss (single batch): 1.4079304933547974\n",
      "\t Training loss (single batch): 1.2087862491607666\n",
      "\t Training loss (single batch): 1.29929518699646\n",
      "\t Training loss (single batch): 1.1251599788665771\n",
      "\t Training loss (single batch): 1.063621997833252\n",
      "\t Training loss (single batch): 1.183733582496643\n",
      "\t Training loss (single batch): 1.1414611339569092\n",
      "\t Training loss (single batch): 0.7323294878005981\n",
      "\t Training loss (single batch): 1.7776093482971191\n",
      "\t Training loss (single batch): 1.512982726097107\n",
      "\t Training loss (single batch): 1.3864643573760986\n",
      "\t Training loss (single batch): 1.2286311388015747\n",
      "\t Training loss (single batch): 0.9668632745742798\n",
      "\t Training loss (single batch): 1.0639402866363525\n",
      "\t Training loss (single batch): 1.3519729375839233\n",
      "\t Training loss (single batch): 0.8201614618301392\n",
      "\t Training loss (single batch): 1.5649694204330444\n",
      "\t Training loss (single batch): 1.3867846727371216\n",
      "\t Training loss (single batch): 0.9871408939361572\n",
      "\t Training loss (single batch): 0.8807975053787231\n",
      "##################################\n",
      "## EPOCH 9\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1831777095794678\n",
      "\t Training loss (single batch): 1.5457885265350342\n",
      "\t Training loss (single batch): 1.1146891117095947\n",
      "\t Training loss (single batch): 0.9314796924591064\n",
      "\t Training loss (single batch): 1.1852670907974243\n",
      "\t Training loss (single batch): 1.7316594123840332\n",
      "\t Training loss (single batch): 1.7363413572311401\n",
      "\t Training loss (single batch): 1.2183057069778442\n",
      "\t Training loss (single batch): 1.6758934259414673\n",
      "\t Training loss (single batch): 1.4102245569229126\n",
      "\t Training loss (single batch): 1.542906403541565\n",
      "\t Training loss (single batch): 1.4994171857833862\n",
      "\t Training loss (single batch): 1.7699522972106934\n",
      "\t Training loss (single batch): 1.832655906677246\n",
      "\t Training loss (single batch): 1.9421523809432983\n",
      "\t Training loss (single batch): 1.2141779661178589\n",
      "\t Training loss (single batch): 1.2721291780471802\n",
      "\t Training loss (single batch): 1.6156123876571655\n",
      "\t Training loss (single batch): 1.8101158142089844\n",
      "\t Training loss (single batch): 1.3833374977111816\n",
      "\t Training loss (single batch): 1.231488585472107\n",
      "\t Training loss (single batch): 1.4753519296646118\n",
      "\t Training loss (single batch): 1.3790749311447144\n",
      "\t Training loss (single batch): 1.5925415754318237\n",
      "\t Training loss (single batch): 1.56695556640625\n",
      "\t Training loss (single batch): 1.3753292560577393\n",
      "\t Training loss (single batch): 1.4493194818496704\n",
      "\t Training loss (single batch): 0.990447461605072\n",
      "\t Training loss (single batch): 1.4418773651123047\n",
      "\t Training loss (single batch): 1.664231300354004\n",
      "\t Training loss (single batch): 1.2846397161483765\n",
      "\t Training loss (single batch): 1.4724401235580444\n",
      "\t Training loss (single batch): 1.0691591501235962\n",
      "\t Training loss (single batch): 1.2525506019592285\n",
      "\t Training loss (single batch): 1.291425108909607\n",
      "\t Training loss (single batch): 1.2638832330703735\n",
      "\t Training loss (single batch): 1.268035888671875\n",
      "\t Training loss (single batch): 1.3315075635910034\n",
      "\t Training loss (single batch): 1.00067138671875\n",
      "\t Training loss (single batch): 1.2418878078460693\n",
      "\t Training loss (single batch): 1.3420672416687012\n",
      "\t Training loss (single batch): 1.2794257402420044\n",
      "\t Training loss (single batch): 1.5928854942321777\n",
      "\t Training loss (single batch): 1.433062195777893\n",
      "\t Training loss (single batch): 1.0298173427581787\n",
      "\t Training loss (single batch): 1.903978705406189\n",
      "\t Training loss (single batch): 1.9854410886764526\n",
      "\t Training loss (single batch): 1.277276873588562\n",
      "\t Training loss (single batch): 1.5457360744476318\n",
      "\t Training loss (single batch): 1.0174893140792847\n",
      "\t Training loss (single batch): 1.0749868154525757\n",
      "\t Training loss (single batch): 1.5654942989349365\n",
      "\t Training loss (single batch): 1.1639044284820557\n",
      "\t Training loss (single batch): 1.299066185951233\n",
      "\t Training loss (single batch): 1.9680862426757812\n",
      "\t Training loss (single batch): 1.1857236623764038\n",
      "\t Training loss (single batch): 1.4936763048171997\n",
      "\t Training loss (single batch): 1.2086806297302246\n",
      "\t Training loss (single batch): 1.5871717929840088\n",
      "\t Training loss (single batch): 1.3740649223327637\n",
      "\t Training loss (single batch): 1.2963762283325195\n",
      "\t Training loss (single batch): 1.6228783130645752\n",
      "\t Training loss (single batch): 1.7848482131958008\n",
      "\t Training loss (single batch): 1.3290457725524902\n",
      "\t Training loss (single batch): 0.8703605532646179\n",
      "\t Training loss (single batch): 1.5855945348739624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2510491609573364\n",
      "\t Training loss (single batch): 1.1409249305725098\n",
      "\t Training loss (single batch): 1.0994343757629395\n",
      "\t Training loss (single batch): 1.332893967628479\n",
      "\t Training loss (single batch): 1.4190787076950073\n",
      "\t Training loss (single batch): 1.1696088314056396\n",
      "\t Training loss (single batch): 1.7628653049468994\n",
      "\t Training loss (single batch): 1.210576057434082\n",
      "\t Training loss (single batch): 1.0248892307281494\n",
      "\t Training loss (single batch): 1.134612798690796\n",
      "\t Training loss (single batch): 0.995881199836731\n",
      "\t Training loss (single batch): 1.3807289600372314\n",
      "\t Training loss (single batch): 1.4180225133895874\n",
      "\t Training loss (single batch): 1.4957484006881714\n",
      "\t Training loss (single batch): 1.2630207538604736\n",
      "\t Training loss (single batch): 1.5111318826675415\n",
      "\t Training loss (single batch): 1.4439563751220703\n",
      "\t Training loss (single batch): 0.8900653123855591\n",
      "\t Training loss (single batch): 1.2055190801620483\n",
      "\t Training loss (single batch): 1.3371076583862305\n",
      "\t Training loss (single batch): 1.4933114051818848\n",
      "\t Training loss (single batch): 1.310641884803772\n",
      "\t Training loss (single batch): 1.0343390703201294\n",
      "\t Training loss (single batch): 1.3958446979522705\n",
      "\t Training loss (single batch): 1.4415857791900635\n",
      "\t Training loss (single batch): 0.8513211011886597\n",
      "\t Training loss (single batch): 1.3787652254104614\n",
      "\t Training loss (single batch): 1.2332158088684082\n",
      "\t Training loss (single batch): 1.18454909324646\n",
      "\t Training loss (single batch): 1.439527988433838\n",
      "\t Training loss (single batch): 1.2762490510940552\n",
      "\t Training loss (single batch): 1.0064308643341064\n",
      "\t Training loss (single batch): 1.0769144296646118\n",
      "\t Training loss (single batch): 1.6202656030654907\n",
      "\t Training loss (single batch): 1.0894724130630493\n",
      "\t Training loss (single batch): 1.4003692865371704\n",
      "\t Training loss (single batch): 0.8058377504348755\n",
      "\t Training loss (single batch): 1.848207950592041\n",
      "\t Training loss (single batch): 0.9657472968101501\n",
      "\t Training loss (single batch): 1.0918121337890625\n",
      "\t Training loss (single batch): 1.698968529701233\n",
      "\t Training loss (single batch): 1.3354085683822632\n",
      "\t Training loss (single batch): 1.3736786842346191\n",
      "\t Training loss (single batch): 1.5072135925292969\n",
      "\t Training loss (single batch): 0.9930495023727417\n",
      "\t Training loss (single batch): 1.3762918710708618\n",
      "\t Training loss (single batch): 1.27327561378479\n",
      "\t Training loss (single batch): 0.9432024955749512\n",
      "\t Training loss (single batch): 1.1017985343933105\n",
      "\t Training loss (single batch): 1.2684917449951172\n",
      "\t Training loss (single batch): 1.5908972024917603\n",
      "\t Training loss (single batch): 1.2167953252792358\n",
      "\t Training loss (single batch): 0.8585804104804993\n",
      "\t Training loss (single batch): 1.5737475156784058\n",
      "\t Training loss (single batch): 0.9154099822044373\n",
      "\t Training loss (single batch): 1.807090401649475\n",
      "\t Training loss (single batch): 1.5635112524032593\n",
      "\t Training loss (single batch): 1.469834327697754\n",
      "\t Training loss (single batch): 0.9345083832740784\n",
      "\t Training loss (single batch): 1.2292709350585938\n",
      "\t Training loss (single batch): 0.944160521030426\n",
      "\t Training loss (single batch): 1.2050288915634155\n",
      "\t Training loss (single batch): 1.058860421180725\n",
      "\t Training loss (single batch): 1.1547890901565552\n",
      "\t Training loss (single batch): 1.132995367050171\n",
      "\t Training loss (single batch): 1.0561109781265259\n",
      "\t Training loss (single batch): 1.2657164335250854\n",
      "\t Training loss (single batch): 0.7405359148979187\n",
      "\t Training loss (single batch): 1.7271050214767456\n",
      "\t Training loss (single batch): 1.2972196340560913\n",
      "\t Training loss (single batch): 1.0172022581100464\n",
      "\t Training loss (single batch): 2.172346830368042\n",
      "\t Training loss (single batch): 1.3603003025054932\n",
      "\t Training loss (single batch): 0.9475284814834595\n",
      "\t Training loss (single batch): 1.5135995149612427\n",
      "\t Training loss (single batch): 1.8812215328216553\n",
      "\t Training loss (single batch): 1.4947341680526733\n",
      "\t Training loss (single batch): 1.3404581546783447\n",
      "\t Training loss (single batch): 1.4941579103469849\n",
      "\t Training loss (single batch): 1.3873944282531738\n",
      "\t Training loss (single batch): 1.1693190336227417\n",
      "\t Training loss (single batch): 1.246066689491272\n",
      "\t Training loss (single batch): 1.434848427772522\n",
      "\t Training loss (single batch): 1.7927659749984741\n",
      "\t Training loss (single batch): 1.3959749937057495\n",
      "\t Training loss (single batch): 1.20173180103302\n",
      "\t Training loss (single batch): 1.0210837125778198\n",
      "\t Training loss (single batch): 1.2850594520568848\n",
      "\t Training loss (single batch): 1.3312489986419678\n",
      "\t Training loss (single batch): 1.7483197450637817\n",
      "\t Training loss (single batch): 1.1001368761062622\n",
      "\t Training loss (single batch): 1.716347575187683\n",
      "\t Training loss (single batch): 1.0728340148925781\n",
      "\t Training loss (single batch): 1.4652633666992188\n",
      "\t Training loss (single batch): 1.4981321096420288\n",
      "\t Training loss (single batch): 1.381121277809143\n",
      "\t Training loss (single batch): 1.1809641122817993\n",
      "\t Training loss (single batch): 1.480272889137268\n",
      "\t Training loss (single batch): 1.6657192707061768\n",
      "\t Training loss (single batch): 1.3597394227981567\n",
      "\t Training loss (single batch): 1.24798583984375\n",
      "\t Training loss (single batch): 1.2654578685760498\n",
      "\t Training loss (single batch): 1.5353010892868042\n",
      "\t Training loss (single batch): 1.2523605823516846\n",
      "\t Training loss (single batch): 1.2915785312652588\n",
      "\t Training loss (single batch): 1.178229808807373\n",
      "\t Training loss (single batch): 1.7116013765335083\n",
      "\t Training loss (single batch): 1.3086551427841187\n",
      "\t Training loss (single batch): 1.1815727949142456\n",
      "\t Training loss (single batch): 1.7378894090652466\n",
      "\t Training loss (single batch): 1.2628220319747925\n",
      "\t Training loss (single batch): 1.156071424484253\n",
      "\t Training loss (single batch): 0.9123420715332031\n",
      "\t Training loss (single batch): 1.611465334892273\n",
      "\t Training loss (single batch): 1.7346601486206055\n",
      "\t Training loss (single batch): 1.1489523649215698\n",
      "\t Training loss (single batch): 1.375786304473877\n",
      "\t Training loss (single batch): 1.2469065189361572\n",
      "\t Training loss (single batch): 0.9169613718986511\n",
      "\t Training loss (single batch): 1.0357789993286133\n",
      "\t Training loss (single batch): 1.1895354986190796\n",
      "\t Training loss (single batch): 1.4716410636901855\n",
      "\t Training loss (single batch): 1.5042858123779297\n",
      "\t Training loss (single batch): 1.5768097639083862\n",
      "\t Training loss (single batch): 1.4674359560012817\n",
      "\t Training loss (single batch): 1.0499576330184937\n",
      "\t Training loss (single batch): 1.5029211044311523\n",
      "\t Training loss (single batch): 1.1107653379440308\n",
      "\t Training loss (single batch): 1.1609971523284912\n",
      "\t Training loss (single batch): 1.357511043548584\n",
      "\t Training loss (single batch): 1.3815900087356567\n",
      "\t Training loss (single batch): 1.5092864036560059\n",
      "\t Training loss (single batch): 1.2835646867752075\n",
      "\t Training loss (single batch): 1.800333857536316\n",
      "\t Training loss (single batch): 1.2953236103057861\n",
      "\t Training loss (single batch): 1.45187509059906\n",
      "\t Training loss (single batch): 1.4723190069198608\n",
      "\t Training loss (single batch): 1.2874703407287598\n",
      "\t Training loss (single batch): 1.987084984779358\n",
      "\t Training loss (single batch): 1.7877063751220703\n",
      "\t Training loss (single batch): 1.4552876949310303\n",
      "\t Training loss (single batch): 1.2524304389953613\n",
      "\t Training loss (single batch): 1.4347480535507202\n",
      "\t Training loss (single batch): 1.4201804399490356\n",
      "\t Training loss (single batch): 1.482574462890625\n",
      "\t Training loss (single batch): 1.714859127998352\n",
      "\t Training loss (single batch): 1.1485322713851929\n",
      "\t Training loss (single batch): 1.077100396156311\n",
      "\t Training loss (single batch): 1.4503055810928345\n",
      "\t Training loss (single batch): 1.170547604560852\n",
      "\t Training loss (single batch): 1.4044476747512817\n",
      "\t Training loss (single batch): 1.6568273305892944\n",
      "\t Training loss (single batch): 1.2920773029327393\n",
      "\t Training loss (single batch): 1.529573678970337\n",
      "\t Training loss (single batch): 1.5116711854934692\n",
      "\t Training loss (single batch): 1.2935220003128052\n",
      "\t Training loss (single batch): 1.4102022647857666\n",
      "\t Training loss (single batch): 1.1463711261749268\n",
      "\t Training loss (single batch): 1.612074613571167\n",
      "\t Training loss (single batch): 1.633194923400879\n",
      "\t Training loss (single batch): 1.885043740272522\n",
      "\t Training loss (single batch): 1.7308907508850098\n",
      "\t Training loss (single batch): 1.2135748863220215\n",
      "\t Training loss (single batch): 1.8834680318832397\n",
      "\t Training loss (single batch): 1.4478553533554077\n",
      "\t Training loss (single batch): 1.2449407577514648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9996652007102966\n",
      "\t Training loss (single batch): 1.8695275783538818\n",
      "\t Training loss (single batch): 0.8029956221580505\n",
      "\t Training loss (single batch): 1.6424654722213745\n",
      "\t Training loss (single batch): 0.9255833625793457\n",
      "\t Training loss (single batch): 1.2424960136413574\n",
      "\t Training loss (single batch): 1.2798190116882324\n",
      "\t Training loss (single batch): 1.4224351644515991\n",
      "\t Training loss (single batch): 1.2252318859100342\n",
      "\t Training loss (single batch): 1.558504343032837\n",
      "\t Training loss (single batch): 1.0837198495864868\n",
      "\t Training loss (single batch): 1.2959548234939575\n",
      "\t Training loss (single batch): 1.6862791776657104\n",
      "\t Training loss (single batch): 1.538177490234375\n",
      "\t Training loss (single batch): 1.571858525276184\n",
      "\t Training loss (single batch): 1.2282987833023071\n",
      "\t Training loss (single batch): 0.9066330790519714\n",
      "\t Training loss (single batch): 1.4164156913757324\n",
      "\t Training loss (single batch): 1.170509696006775\n",
      "\t Training loss (single batch): 1.710617184638977\n",
      "\t Training loss (single batch): 1.179551601409912\n",
      "\t Training loss (single batch): 1.270063042640686\n",
      "\t Training loss (single batch): 1.5216519832611084\n",
      "\t Training loss (single batch): 1.350951910018921\n",
      "\t Training loss (single batch): 1.295150637626648\n",
      "\t Training loss (single batch): 1.0876771211624146\n",
      "\t Training loss (single batch): 1.221127986907959\n",
      "\t Training loss (single batch): 1.5442675352096558\n",
      "\t Training loss (single batch): 1.2860239744186401\n",
      "\t Training loss (single batch): 0.9519155025482178\n",
      "\t Training loss (single batch): 0.9654302597045898\n",
      "\t Training loss (single batch): 1.3707975149154663\n",
      "\t Training loss (single batch): 1.0342189073562622\n",
      "\t Training loss (single batch): 1.044749140739441\n",
      "\t Training loss (single batch): 1.6099365949630737\n",
      "\t Training loss (single batch): 1.214505672454834\n",
      "\t Training loss (single batch): 1.4878261089324951\n",
      "\t Training loss (single batch): 1.336620569229126\n",
      "\t Training loss (single batch): 1.2230764627456665\n",
      "\t Training loss (single batch): 1.0568681955337524\n",
      "\t Training loss (single batch): 1.1904178857803345\n",
      "\t Training loss (single batch): 1.2890058755874634\n",
      "\t Training loss (single batch): 1.0988404750823975\n",
      "\t Training loss (single batch): 2.1450421810150146\n",
      "\t Training loss (single batch): 1.5179786682128906\n",
      "\t Training loss (single batch): 1.543416142463684\n",
      "\t Training loss (single batch): 1.0363510847091675\n",
      "\t Training loss (single batch): 1.1308693885803223\n",
      "\t Training loss (single batch): 1.3125405311584473\n",
      "\t Training loss (single batch): 1.0424219369888306\n",
      "\t Training loss (single batch): 1.5769624710083008\n",
      "\t Training loss (single batch): 1.6581965684890747\n",
      "\t Training loss (single batch): 1.3125437498092651\n",
      "\t Training loss (single batch): 1.4021050930023193\n",
      "\t Training loss (single batch): 1.4508020877838135\n",
      "\t Training loss (single batch): 1.5147279500961304\n",
      "\t Training loss (single batch): 1.6552103757858276\n",
      "\t Training loss (single batch): 1.3438255786895752\n",
      "\t Training loss (single batch): 1.0635966062545776\n",
      "\t Training loss (single batch): 1.8524643182754517\n",
      "\t Training loss (single batch): 1.003832221031189\n",
      "\t Training loss (single batch): 1.432176113128662\n",
      "\t Training loss (single batch): 1.4132604598999023\n",
      "\t Training loss (single batch): 1.5697448253631592\n",
      "\t Training loss (single batch): 1.399826169013977\n",
      "\t Training loss (single batch): 1.3237171173095703\n",
      "\t Training loss (single batch): 1.35104501247406\n",
      "\t Training loss (single batch): 1.1730929613113403\n",
      "\t Training loss (single batch): 1.3834751844406128\n",
      "\t Training loss (single batch): 1.4213165044784546\n",
      "\t Training loss (single batch): 1.2895691394805908\n",
      "\t Training loss (single batch): 1.3535429239273071\n",
      "\t Training loss (single batch): 0.9153718948364258\n",
      "\t Training loss (single batch): 1.259089469909668\n",
      "\t Training loss (single batch): 1.3224570751190186\n",
      "\t Training loss (single batch): 1.1299209594726562\n",
      "\t Training loss (single batch): 0.9092661142349243\n",
      "\t Training loss (single batch): 0.9729713797569275\n",
      "\t Training loss (single batch): 1.2450082302093506\n",
      "\t Training loss (single batch): 1.7819474935531616\n",
      "\t Training loss (single batch): 1.7177259922027588\n",
      "\t Training loss (single batch): 1.7831470966339111\n",
      "\t Training loss (single batch): 1.6158400774002075\n",
      "\t Training loss (single batch): 1.2518335580825806\n",
      "\t Training loss (single batch): 1.9301499128341675\n",
      "##################################\n",
      "## EPOCH 10\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6165947914123535\n",
      "\t Training loss (single batch): 0.9769963622093201\n",
      "\t Training loss (single batch): 0.956778347492218\n",
      "\t Training loss (single batch): 1.0861679315567017\n",
      "\t Training loss (single batch): 1.1387007236480713\n",
      "\t Training loss (single batch): 1.0476270914077759\n",
      "\t Training loss (single batch): 1.1787331104278564\n",
      "\t Training loss (single batch): 1.3395168781280518\n",
      "\t Training loss (single batch): 1.698991060256958\n",
      "\t Training loss (single batch): 1.3024020195007324\n",
      "\t Training loss (single batch): 0.6285877227783203\n",
      "\t Training loss (single batch): 1.110400915145874\n",
      "\t Training loss (single batch): 1.1010515689849854\n",
      "\t Training loss (single batch): 1.5726724863052368\n",
      "\t Training loss (single batch): 1.213922381401062\n",
      "\t Training loss (single batch): 0.9742798209190369\n",
      "\t Training loss (single batch): 1.329118251800537\n",
      "\t Training loss (single batch): 1.4660353660583496\n",
      "\t Training loss (single batch): 1.1708436012268066\n",
      "\t Training loss (single batch): 1.2041875123977661\n",
      "\t Training loss (single batch): 1.6349432468414307\n",
      "\t Training loss (single batch): 1.97629976272583\n",
      "\t Training loss (single batch): 1.3791074752807617\n",
      "\t Training loss (single batch): 1.3059693574905396\n",
      "\t Training loss (single batch): 1.1958473920822144\n",
      "\t Training loss (single batch): 1.2472755908966064\n",
      "\t Training loss (single batch): 1.0149086713790894\n",
      "\t Training loss (single batch): 0.9278003573417664\n",
      "\t Training loss (single batch): 1.7444099187850952\n",
      "\t Training loss (single batch): 0.9040020108222961\n",
      "\t Training loss (single batch): 0.9804890751838684\n",
      "\t Training loss (single batch): 1.0932090282440186\n",
      "\t Training loss (single batch): 1.295985460281372\n",
      "\t Training loss (single batch): 1.3022518157958984\n",
      "\t Training loss (single batch): 1.2793169021606445\n",
      "\t Training loss (single batch): 1.1628786325454712\n",
      "\t Training loss (single batch): 1.4003922939300537\n",
      "\t Training loss (single batch): 1.4869604110717773\n",
      "\t Training loss (single batch): 1.6618199348449707\n",
      "\t Training loss (single batch): 1.195885419845581\n",
      "\t Training loss (single batch): 1.1615935564041138\n",
      "\t Training loss (single batch): 1.0626280307769775\n",
      "\t Training loss (single batch): 1.1992731094360352\n",
      "\t Training loss (single batch): 1.0875760316848755\n",
      "\t Training loss (single batch): 0.9665433168411255\n",
      "\t Training loss (single batch): 0.8038063049316406\n",
      "\t Training loss (single batch): 1.1206591129302979\n",
      "\t Training loss (single batch): 1.3998714685440063\n",
      "\t Training loss (single batch): 1.4013726711273193\n",
      "\t Training loss (single batch): 1.5048191547393799\n",
      "\t Training loss (single batch): 1.2585029602050781\n",
      "\t Training loss (single batch): 1.1831313371658325\n",
      "\t Training loss (single batch): 1.0170058012008667\n",
      "\t Training loss (single batch): 1.3374727964401245\n",
      "\t Training loss (single batch): 1.3946205377578735\n",
      "\t Training loss (single batch): 1.0529738664627075\n",
      "\t Training loss (single batch): 1.4740346670150757\n",
      "\t Training loss (single batch): 1.40748929977417\n",
      "\t Training loss (single batch): 1.449746012687683\n",
      "\t Training loss (single batch): 1.2684153318405151\n",
      "\t Training loss (single batch): 1.3097352981567383\n",
      "\t Training loss (single batch): 1.6229969263076782\n",
      "\t Training loss (single batch): 1.1518861055374146\n",
      "\t Training loss (single batch): 1.0979083776474\n",
      "\t Training loss (single batch): 1.705942153930664\n",
      "\t Training loss (single batch): 1.1121772527694702\n",
      "\t Training loss (single batch): 1.0653213262557983\n",
      "\t Training loss (single batch): 1.4236024618148804\n",
      "\t Training loss (single batch): 1.575853705406189\n",
      "\t Training loss (single batch): 1.438820719718933\n",
      "\t Training loss (single batch): 1.1588406562805176\n",
      "\t Training loss (single batch): 0.8346933126449585\n",
      "\t Training loss (single batch): 1.6503030061721802\n",
      "\t Training loss (single batch): 0.9545652866363525\n",
      "\t Training loss (single batch): 1.0126935243606567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4063787460327148\n",
      "\t Training loss (single batch): 1.6457452774047852\n",
      "\t Training loss (single batch): 1.3739076852798462\n",
      "\t Training loss (single batch): 1.3752892017364502\n",
      "\t Training loss (single batch): 1.3655561208724976\n",
      "\t Training loss (single batch): 1.4005533456802368\n",
      "\t Training loss (single batch): 1.2532483339309692\n",
      "\t Training loss (single batch): 1.2537200450897217\n",
      "\t Training loss (single batch): 1.3278356790542603\n",
      "\t Training loss (single batch): 1.5394341945648193\n",
      "\t Training loss (single batch): 1.012454867362976\n",
      "\t Training loss (single batch): 1.5491362810134888\n",
      "\t Training loss (single batch): 1.3950588703155518\n",
      "\t Training loss (single batch): 1.3045945167541504\n",
      "\t Training loss (single batch): 1.1521992683410645\n",
      "\t Training loss (single batch): 1.753298282623291\n",
      "\t Training loss (single batch): 1.2813979387283325\n",
      "\t Training loss (single batch): 1.9938554763793945\n",
      "\t Training loss (single batch): 1.35640549659729\n",
      "\t Training loss (single batch): 1.2640808820724487\n",
      "\t Training loss (single batch): 1.6910836696624756\n",
      "\t Training loss (single batch): 1.5438636541366577\n",
      "\t Training loss (single batch): 1.0115954875946045\n",
      "\t Training loss (single batch): 1.330222249031067\n",
      "\t Training loss (single batch): 1.5037002563476562\n",
      "\t Training loss (single batch): 1.2001192569732666\n",
      "\t Training loss (single batch): 1.2002032995224\n",
      "\t Training loss (single batch): 1.4900718927383423\n",
      "\t Training loss (single batch): 1.2535182237625122\n",
      "\t Training loss (single batch): 1.508547067642212\n",
      "\t Training loss (single batch): 1.2933257818222046\n",
      "\t Training loss (single batch): 1.3521944284439087\n",
      "\t Training loss (single batch): 1.339475154876709\n",
      "\t Training loss (single batch): 1.293406367301941\n",
      "\t Training loss (single batch): 1.18814218044281\n",
      "\t Training loss (single batch): 1.2836482524871826\n",
      "\t Training loss (single batch): 1.4399569034576416\n",
      "\t Training loss (single batch): 0.8099709153175354\n",
      "\t Training loss (single batch): 1.6229630708694458\n",
      "\t Training loss (single batch): 1.1693443059921265\n",
      "\t Training loss (single batch): 1.4571443796157837\n",
      "\t Training loss (single batch): 1.2280678749084473\n",
      "\t Training loss (single batch): 1.021589994430542\n",
      "\t Training loss (single batch): 1.1925965547561646\n",
      "\t Training loss (single batch): 0.9414143562316895\n",
      "\t Training loss (single batch): 1.656078577041626\n",
      "\t Training loss (single batch): 1.2893424034118652\n",
      "\t Training loss (single batch): 1.8552486896514893\n",
      "\t Training loss (single batch): 1.664624810218811\n",
      "\t Training loss (single batch): 1.4819246530532837\n",
      "\t Training loss (single batch): 1.5441315174102783\n",
      "\t Training loss (single batch): 1.8952842950820923\n",
      "\t Training loss (single batch): 1.465499997138977\n",
      "\t Training loss (single batch): 1.2551251649856567\n",
      "\t Training loss (single batch): 1.1737593412399292\n",
      "\t Training loss (single batch): 1.5992833375930786\n",
      "\t Training loss (single batch): 1.3760982751846313\n",
      "\t Training loss (single batch): 1.3558627367019653\n",
      "\t Training loss (single batch): 1.404926061630249\n",
      "\t Training loss (single batch): 1.1261464357376099\n",
      "\t Training loss (single batch): 1.2662415504455566\n",
      "\t Training loss (single batch): 0.8331508040428162\n",
      "\t Training loss (single batch): 1.2741764783859253\n",
      "\t Training loss (single batch): 0.9709934592247009\n",
      "\t Training loss (single batch): 1.2204681634902954\n",
      "\t Training loss (single batch): 1.562395453453064\n",
      "\t Training loss (single batch): 1.3509058952331543\n",
      "\t Training loss (single batch): 1.8495919704437256\n",
      "\t Training loss (single batch): 1.2213890552520752\n",
      "\t Training loss (single batch): 1.39203941822052\n",
      "\t Training loss (single batch): 2.0001699924468994\n",
      "\t Training loss (single batch): 1.1626722812652588\n",
      "\t Training loss (single batch): 1.5736093521118164\n",
      "\t Training loss (single batch): 1.393836498260498\n",
      "\t Training loss (single batch): 1.2715098857879639\n",
      "\t Training loss (single batch): 1.0740514993667603\n",
      "\t Training loss (single batch): 1.2616965770721436\n",
      "\t Training loss (single batch): 1.3675317764282227\n",
      "\t Training loss (single batch): 1.4234158992767334\n",
      "\t Training loss (single batch): 1.1370816230773926\n",
      "\t Training loss (single batch): 1.432068943977356\n",
      "\t Training loss (single batch): 1.6066534519195557\n",
      "\t Training loss (single batch): 1.1680458784103394\n",
      "\t Training loss (single batch): 1.4218835830688477\n",
      "\t Training loss (single batch): 1.0055060386657715\n",
      "\t Training loss (single batch): 1.7190816402435303\n",
      "\t Training loss (single batch): 1.9447985887527466\n",
      "\t Training loss (single batch): 0.9625924825668335\n",
      "\t Training loss (single batch): 0.883208155632019\n",
      "\t Training loss (single batch): 1.440724492073059\n",
      "\t Training loss (single batch): 1.3542035818099976\n",
      "\t Training loss (single batch): 0.9189243912696838\n",
      "\t Training loss (single batch): 1.0737251043319702\n",
      "\t Training loss (single batch): 1.1381499767303467\n",
      "\t Training loss (single batch): 1.0677939653396606\n",
      "\t Training loss (single batch): 1.0700616836547852\n",
      "\t Training loss (single batch): 1.4961313009262085\n",
      "\t Training loss (single batch): 1.7393360137939453\n",
      "\t Training loss (single batch): 0.9248165488243103\n",
      "\t Training loss (single batch): 1.4620435237884521\n",
      "\t Training loss (single batch): 1.2521111965179443\n",
      "\t Training loss (single batch): 1.5202423334121704\n",
      "\t Training loss (single batch): 1.2271254062652588\n",
      "\t Training loss (single batch): 0.8776373267173767\n",
      "\t Training loss (single batch): 1.34096360206604\n",
      "\t Training loss (single batch): 1.0146358013153076\n",
      "\t Training loss (single batch): 0.8606680631637573\n",
      "\t Training loss (single batch): 1.0703967809677124\n",
      "\t Training loss (single batch): 0.9730626344680786\n",
      "\t Training loss (single batch): 0.6361225843429565\n",
      "\t Training loss (single batch): 2.344189405441284\n",
      "\t Training loss (single batch): 1.4175188541412354\n",
      "\t Training loss (single batch): 1.5175905227661133\n",
      "\t Training loss (single batch): 1.2978309392929077\n",
      "\t Training loss (single batch): 1.8694508075714111\n",
      "\t Training loss (single batch): 1.260610818862915\n",
      "\t Training loss (single batch): 1.086165428161621\n",
      "\t Training loss (single batch): 1.2623076438903809\n",
      "\t Training loss (single batch): 1.04849112033844\n",
      "\t Training loss (single batch): 1.5856544971466064\n",
      "\t Training loss (single batch): 1.0993282794952393\n",
      "\t Training loss (single batch): 1.831533670425415\n",
      "\t Training loss (single batch): 1.1722941398620605\n",
      "\t Training loss (single batch): 1.4885092973709106\n",
      "\t Training loss (single batch): 1.2922848463058472\n",
      "\t Training loss (single batch): 1.6445351839065552\n",
      "\t Training loss (single batch): 1.2888323068618774\n",
      "\t Training loss (single batch): 1.804647445678711\n",
      "\t Training loss (single batch): 1.2060121297836304\n",
      "\t Training loss (single batch): 1.175002098083496\n",
      "\t Training loss (single batch): 1.0161864757537842\n",
      "\t Training loss (single batch): 0.8333092927932739\n",
      "\t Training loss (single batch): 1.6412166357040405\n",
      "\t Training loss (single batch): 1.459962248802185\n",
      "\t Training loss (single batch): 1.4548861980438232\n",
      "\t Training loss (single batch): 1.7449079751968384\n",
      "\t Training loss (single batch): 1.4776650667190552\n",
      "\t Training loss (single batch): 1.4970357418060303\n",
      "\t Training loss (single batch): 1.2198559045791626\n",
      "\t Training loss (single batch): 1.320690631866455\n",
      "\t Training loss (single batch): 0.8878896832466125\n",
      "\t Training loss (single batch): 1.4471133947372437\n",
      "\t Training loss (single batch): 1.5556674003601074\n",
      "\t Training loss (single batch): 1.3163865804672241\n",
      "\t Training loss (single batch): 1.1920760869979858\n",
      "\t Training loss (single batch): 0.9078614115715027\n",
      "\t Training loss (single batch): 1.4556924104690552\n",
      "\t Training loss (single batch): 1.439985752105713\n",
      "\t Training loss (single batch): 1.1608775854110718\n",
      "\t Training loss (single batch): 1.4464219808578491\n",
      "\t Training loss (single batch): 1.256893515586853\n",
      "\t Training loss (single batch): 1.415104866027832\n",
      "\t Training loss (single batch): 1.0173190832138062\n",
      "\t Training loss (single batch): 1.538025140762329\n",
      "\t Training loss (single batch): 1.752967357635498\n",
      "\t Training loss (single batch): 1.1520887613296509\n",
      "\t Training loss (single batch): 0.9896008968353271\n",
      "\t Training loss (single batch): 1.217639446258545\n",
      "\t Training loss (single batch): 1.2572343349456787\n",
      "\t Training loss (single batch): 1.134090781211853\n",
      "\t Training loss (single batch): 0.7644826173782349\n",
      "\t Training loss (single batch): 1.0759481191635132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8448063731193542\n",
      "\t Training loss (single batch): 0.8326288461685181\n",
      "\t Training loss (single batch): 0.9800836443901062\n",
      "\t Training loss (single batch): 1.0635589361190796\n",
      "\t Training loss (single batch): 0.8424537777900696\n",
      "\t Training loss (single batch): 1.069571614265442\n",
      "\t Training loss (single batch): 1.254658579826355\n",
      "\t Training loss (single batch): 1.2843725681304932\n",
      "\t Training loss (single batch): 1.7102171182632446\n",
      "\t Training loss (single batch): 2.2020304203033447\n",
      "\t Training loss (single batch): 1.4993071556091309\n",
      "\t Training loss (single batch): 0.8881436586380005\n",
      "\t Training loss (single batch): 1.1278561353683472\n",
      "\t Training loss (single batch): 1.6241010427474976\n",
      "\t Training loss (single batch): 1.5454782247543335\n",
      "\t Training loss (single batch): 1.1034440994262695\n",
      "\t Training loss (single batch): 0.8763739466667175\n",
      "\t Training loss (single batch): 1.5491257905960083\n",
      "\t Training loss (single batch): 1.498286247253418\n",
      "\t Training loss (single batch): 1.3331053256988525\n",
      "\t Training loss (single batch): 1.4332751035690308\n",
      "\t Training loss (single batch): 1.12305748462677\n",
      "\t Training loss (single batch): 1.1342626810073853\n",
      "\t Training loss (single batch): 1.7111598253250122\n",
      "\t Training loss (single batch): 1.6584662199020386\n",
      "\t Training loss (single batch): 1.9929221868515015\n",
      "\t Training loss (single batch): 1.38277006149292\n",
      "\t Training loss (single batch): 1.2991652488708496\n",
      "\t Training loss (single batch): 1.4716625213623047\n",
      "\t Training loss (single batch): 1.6555979251861572\n",
      "\t Training loss (single batch): 1.2660914659500122\n",
      "\t Training loss (single batch): 1.240822434425354\n",
      "\t Training loss (single batch): 1.4500095844268799\n",
      "\t Training loss (single batch): 1.623064637184143\n",
      "\t Training loss (single batch): 1.509448766708374\n",
      "\t Training loss (single batch): 1.5163700580596924\n",
      "\t Training loss (single batch): 1.8567962646484375\n",
      "\t Training loss (single batch): 0.9761034250259399\n",
      "\t Training loss (single batch): 1.7016041278839111\n",
      "\t Training loss (single batch): 1.1240042448043823\n",
      "\t Training loss (single batch): 1.7041094303131104\n",
      "\t Training loss (single batch): 1.2567274570465088\n",
      "\t Training loss (single batch): 1.345104455947876\n",
      "\t Training loss (single batch): 1.2030584812164307\n",
      "\t Training loss (single batch): 1.2631356716156006\n",
      "\t Training loss (single batch): 1.0184929370880127\n",
      "\t Training loss (single batch): 1.456648826599121\n",
      "\t Training loss (single batch): 1.5536997318267822\n",
      "\t Training loss (single batch): 1.4916512966156006\n",
      "\t Training loss (single batch): 1.218576192855835\n",
      "\t Training loss (single batch): 0.8180478811264038\n",
      "\t Training loss (single batch): 1.6041125059127808\n",
      "\t Training loss (single batch): 1.1237298250198364\n",
      "\t Training loss (single batch): 1.1755012273788452\n",
      "\t Training loss (single batch): 1.6488536596298218\n",
      "\t Training loss (single batch): 1.2122533321380615\n",
      "\t Training loss (single batch): 1.8463207483291626\n",
      "\t Training loss (single batch): 1.147491216659546\n",
      "\t Training loss (single batch): 1.9929800033569336\n",
      "\t Training loss (single batch): 1.2177428007125854\n",
      "\t Training loss (single batch): 1.4568226337432861\n",
      "\t Training loss (single batch): 0.9065618515014648\n",
      "\t Training loss (single batch): 1.1711229085922241\n",
      "\t Training loss (single batch): 1.7332868576049805\n",
      "\t Training loss (single batch): 1.293944001197815\n",
      "\t Training loss (single batch): 1.197505235671997\n",
      "\t Training loss (single batch): 1.463260293006897\n",
      "\t Training loss (single batch): 1.191432237625122\n",
      "\t Training loss (single batch): 1.6263489723205566\n",
      "\t Training loss (single batch): 1.4782353639602661\n",
      "\t Training loss (single batch): 1.1454225778579712\n",
      "\t Training loss (single batch): 1.2908188104629517\n",
      "\t Training loss (single batch): 1.3778109550476074\n",
      "\t Training loss (single batch): 1.1369067430496216\n",
      "\t Training loss (single batch): 1.2742054462432861\n",
      "\t Training loss (single batch): 2.27351975440979\n",
      "\t Training loss (single batch): 1.4240541458129883\n",
      "\t Training loss (single batch): 1.7910292148590088\n",
      "\t Training loss (single batch): 1.554825782775879\n",
      "\t Training loss (single batch): 0.7508007884025574\n",
      "##################################\n",
      "## EPOCH 11\n",
      "##################################\n",
      "\t Training loss (single batch): 1.506087303161621\n",
      "\t Training loss (single batch): 0.8946702480316162\n",
      "\t Training loss (single batch): 1.249035358428955\n",
      "\t Training loss (single batch): 1.332231044769287\n",
      "\t Training loss (single batch): 1.3879824876785278\n",
      "\t Training loss (single batch): 1.2433346509933472\n",
      "\t Training loss (single batch): 1.1014959812164307\n",
      "\t Training loss (single batch): 1.0081285238265991\n",
      "\t Training loss (single batch): 1.4802621603012085\n",
      "\t Training loss (single batch): 1.1646289825439453\n",
      "\t Training loss (single batch): 1.2962796688079834\n",
      "\t Training loss (single batch): 1.5281554460525513\n",
      "\t Training loss (single batch): 1.1053614616394043\n",
      "\t Training loss (single batch): 1.0539730787277222\n",
      "\t Training loss (single batch): 1.49295175075531\n",
      "\t Training loss (single batch): 1.495096206665039\n",
      "\t Training loss (single batch): 1.021315574645996\n",
      "\t Training loss (single batch): 0.7978872656822205\n",
      "\t Training loss (single batch): 1.4431300163269043\n",
      "\t Training loss (single batch): 0.9712972640991211\n",
      "\t Training loss (single batch): 1.215460181236267\n",
      "\t Training loss (single batch): 0.9515573978424072\n",
      "\t Training loss (single batch): 0.8680064082145691\n",
      "\t Training loss (single batch): 1.2491638660430908\n",
      "\t Training loss (single batch): 1.6039916276931763\n",
      "\t Training loss (single batch): 1.434112548828125\n",
      "\t Training loss (single batch): 1.0202540159225464\n",
      "\t Training loss (single batch): 0.792016327381134\n",
      "\t Training loss (single batch): 0.6605685353279114\n",
      "\t Training loss (single batch): 1.6748905181884766\n",
      "\t Training loss (single batch): 1.7415156364440918\n",
      "\t Training loss (single batch): 1.4489983320236206\n",
      "\t Training loss (single batch): 1.1523479223251343\n",
      "\t Training loss (single batch): 1.7067338228225708\n",
      "\t Training loss (single batch): 1.0505353212356567\n",
      "\t Training loss (single batch): 1.143343448638916\n",
      "\t Training loss (single batch): 1.0276914834976196\n",
      "\t Training loss (single batch): 1.4197702407836914\n",
      "\t Training loss (single batch): 1.4218310117721558\n",
      "\t Training loss (single batch): 1.4470478296279907\n",
      "\t Training loss (single batch): 1.104790449142456\n",
      "\t Training loss (single batch): 1.1618391275405884\n",
      "\t Training loss (single batch): 1.2359124422073364\n",
      "\t Training loss (single batch): 1.6183679103851318\n",
      "\t Training loss (single batch): 1.6228197813034058\n",
      "\t Training loss (single batch): 1.2994294166564941\n",
      "\t Training loss (single batch): 0.6599434614181519\n",
      "\t Training loss (single batch): 1.3656057119369507\n",
      "\t Training loss (single batch): 1.7154440879821777\n",
      "\t Training loss (single batch): 1.4670912027359009\n",
      "\t Training loss (single batch): 1.2040748596191406\n",
      "\t Training loss (single batch): 1.797349452972412\n",
      "\t Training loss (single batch): 1.1667907238006592\n",
      "\t Training loss (single batch): 1.578775405883789\n",
      "\t Training loss (single batch): 1.1052179336547852\n",
      "\t Training loss (single batch): 1.4765660762786865\n",
      "\t Training loss (single batch): 1.2262485027313232\n",
      "\t Training loss (single batch): 1.2912307977676392\n",
      "\t Training loss (single batch): 1.0707886219024658\n",
      "\t Training loss (single batch): 1.4503505229949951\n",
      "\t Training loss (single batch): 0.8895826935768127\n",
      "\t Training loss (single batch): 1.5498132705688477\n",
      "\t Training loss (single batch): 1.5855239629745483\n",
      "\t Training loss (single batch): 1.1442790031433105\n",
      "\t Training loss (single batch): 1.2803385257720947\n",
      "\t Training loss (single batch): 1.1536691188812256\n",
      "\t Training loss (single batch): 1.1246094703674316\n",
      "\t Training loss (single batch): 1.127841830253601\n",
      "\t Training loss (single batch): 1.3859442472457886\n",
      "\t Training loss (single batch): 0.8238491415977478\n",
      "\t Training loss (single batch): 1.2504043579101562\n",
      "\t Training loss (single batch): 1.5093942880630493\n",
      "\t Training loss (single batch): 1.2901252508163452\n",
      "\t Training loss (single batch): 1.2229856252670288\n",
      "\t Training loss (single batch): 1.0673950910568237\n",
      "\t Training loss (single batch): 1.393447995185852\n",
      "\t Training loss (single batch): 1.6871249675750732\n",
      "\t Training loss (single batch): 1.0150582790374756\n",
      "\t Training loss (single batch): 1.1127424240112305\n",
      "\t Training loss (single batch): 1.2950925827026367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4621775150299072\n",
      "\t Training loss (single batch): 1.617151141166687\n",
      "\t Training loss (single batch): 1.7647509574890137\n",
      "\t Training loss (single batch): 1.4994570016860962\n",
      "\t Training loss (single batch): 1.3219951391220093\n",
      "\t Training loss (single batch): 0.8232930302619934\n",
      "\t Training loss (single batch): 1.3775525093078613\n",
      "\t Training loss (single batch): 1.0766777992248535\n",
      "\t Training loss (single batch): 1.520245909690857\n",
      "\t Training loss (single batch): 1.4255013465881348\n",
      "\t Training loss (single batch): 1.2022712230682373\n",
      "\t Training loss (single batch): 1.4302802085876465\n",
      "\t Training loss (single batch): 1.4961525201797485\n",
      "\t Training loss (single batch): 1.5131045579910278\n",
      "\t Training loss (single batch): 1.3405793905258179\n",
      "\t Training loss (single batch): 1.5986418724060059\n",
      "\t Training loss (single batch): 1.3345179557800293\n",
      "\t Training loss (single batch): 1.1240172386169434\n",
      "\t Training loss (single batch): 1.5078933238983154\n",
      "\t Training loss (single batch): 1.1831597089767456\n",
      "\t Training loss (single batch): 0.8361003398895264\n",
      "\t Training loss (single batch): 1.767695665359497\n",
      "\t Training loss (single batch): 1.3324527740478516\n",
      "\t Training loss (single batch): 1.3426767587661743\n",
      "\t Training loss (single batch): 1.325958013534546\n",
      "\t Training loss (single batch): 1.206778883934021\n",
      "\t Training loss (single batch): 1.1269358396530151\n",
      "\t Training loss (single batch): 1.729932427406311\n",
      "\t Training loss (single batch): 1.2621517181396484\n",
      "\t Training loss (single batch): 1.698668122291565\n",
      "\t Training loss (single batch): 1.0778846740722656\n",
      "\t Training loss (single batch): 1.2268651723861694\n",
      "\t Training loss (single batch): 1.3954044580459595\n",
      "\t Training loss (single batch): 1.599063515663147\n",
      "\t Training loss (single batch): 1.4961514472961426\n",
      "\t Training loss (single batch): 1.2415425777435303\n",
      "\t Training loss (single batch): 1.647743821144104\n",
      "\t Training loss (single batch): 1.4502613544464111\n",
      "\t Training loss (single batch): 1.3368165493011475\n",
      "\t Training loss (single batch): 1.2568225860595703\n",
      "\t Training loss (single batch): 1.6658982038497925\n",
      "\t Training loss (single batch): 1.5649892091751099\n",
      "\t Training loss (single batch): 1.1560217142105103\n",
      "\t Training loss (single batch): 1.3853641748428345\n",
      "\t Training loss (single batch): 1.149686574935913\n",
      "\t Training loss (single batch): 1.1220242977142334\n",
      "\t Training loss (single batch): 1.7423951625823975\n",
      "\t Training loss (single batch): 1.2899787425994873\n",
      "\t Training loss (single batch): 0.9783872365951538\n",
      "\t Training loss (single batch): 1.6118800640106201\n",
      "\t Training loss (single batch): 1.5708078145980835\n",
      "\t Training loss (single batch): 1.4943767786026\n",
      "\t Training loss (single batch): 1.1734956502914429\n",
      "\t Training loss (single batch): 1.6136654615402222\n",
      "\t Training loss (single batch): 1.1255342960357666\n",
      "\t Training loss (single batch): 1.2496100664138794\n",
      "\t Training loss (single batch): 1.3159434795379639\n",
      "\t Training loss (single batch): 1.5311683416366577\n",
      "\t Training loss (single batch): 1.5234040021896362\n",
      "\t Training loss (single batch): 1.7825136184692383\n",
      "\t Training loss (single batch): 1.0979862213134766\n",
      "\t Training loss (single batch): 1.9060425758361816\n",
      "\t Training loss (single batch): 1.300523281097412\n",
      "\t Training loss (single batch): 1.1998157501220703\n",
      "\t Training loss (single batch): 1.5518463850021362\n",
      "\t Training loss (single batch): 1.2391844987869263\n",
      "\t Training loss (single batch): 1.6035155057907104\n",
      "\t Training loss (single batch): 1.3904178142547607\n",
      "\t Training loss (single batch): 1.7054202556610107\n",
      "\t Training loss (single batch): 1.0553202629089355\n",
      "\t Training loss (single batch): 1.2914849519729614\n",
      "\t Training loss (single batch): 1.2387676239013672\n",
      "\t Training loss (single batch): 1.1388012170791626\n",
      "\t Training loss (single batch): 0.95046067237854\n",
      "\t Training loss (single batch): 1.2340331077575684\n",
      "\t Training loss (single batch): 1.4546868801116943\n",
      "\t Training loss (single batch): 1.6039882898330688\n",
      "\t Training loss (single batch): 0.8331605195999146\n",
      "\t Training loss (single batch): 1.4144344329833984\n",
      "\t Training loss (single batch): 1.4650572538375854\n",
      "\t Training loss (single batch): 1.1246923208236694\n",
      "\t Training loss (single batch): 0.7426508069038391\n",
      "\t Training loss (single batch): 1.3757436275482178\n",
      "\t Training loss (single batch): 1.3610682487487793\n",
      "\t Training loss (single batch): 1.6532334089279175\n",
      "\t Training loss (single batch): 0.9628228545188904\n",
      "\t Training loss (single batch): 0.8871512413024902\n",
      "\t Training loss (single batch): 1.1554795503616333\n",
      "\t Training loss (single batch): 1.3702166080474854\n",
      "\t Training loss (single batch): 1.6224087476730347\n",
      "\t Training loss (single batch): 1.304325819015503\n",
      "\t Training loss (single batch): 1.043543815612793\n",
      "\t Training loss (single batch): 1.2870081663131714\n",
      "\t Training loss (single batch): 1.0435731410980225\n",
      "\t Training loss (single batch): 1.2735486030578613\n",
      "\t Training loss (single batch): 1.2441911697387695\n",
      "\t Training loss (single batch): 1.2823835611343384\n",
      "\t Training loss (single batch): 1.8015064001083374\n",
      "\t Training loss (single batch): 1.1509181261062622\n",
      "\t Training loss (single batch): 1.1231554746627808\n",
      "\t Training loss (single batch): 2.1793229579925537\n",
      "\t Training loss (single batch): 0.7981938719749451\n",
      "\t Training loss (single batch): 1.0451340675354004\n",
      "\t Training loss (single batch): 1.1457269191741943\n",
      "\t Training loss (single batch): 1.6323449611663818\n",
      "\t Training loss (single batch): 1.2534223794937134\n",
      "\t Training loss (single batch): 1.421320915222168\n",
      "\t Training loss (single batch): 1.3170262575149536\n",
      "\t Training loss (single batch): 1.8502428531646729\n",
      "\t Training loss (single batch): 1.308982253074646\n",
      "\t Training loss (single batch): 0.9593239426612854\n",
      "\t Training loss (single batch): 1.5454622507095337\n",
      "\t Training loss (single batch): 1.331765055656433\n",
      "\t Training loss (single batch): 1.499923586845398\n",
      "\t Training loss (single batch): 1.5981104373931885\n",
      "\t Training loss (single batch): 1.1885157823562622\n",
      "\t Training loss (single batch): 0.9573230743408203\n",
      "\t Training loss (single batch): 1.1094838380813599\n",
      "\t Training loss (single batch): 1.5226306915283203\n",
      "\t Training loss (single batch): 1.5437431335449219\n",
      "\t Training loss (single batch): 1.1657744646072388\n",
      "\t Training loss (single batch): 1.479380488395691\n",
      "\t Training loss (single batch): 1.227098822593689\n",
      "\t Training loss (single batch): 1.3050004243850708\n",
      "\t Training loss (single batch): 1.5681861639022827\n",
      "\t Training loss (single batch): 0.901334822177887\n",
      "\t Training loss (single batch): 1.3663867712020874\n",
      "\t Training loss (single batch): 1.5202836990356445\n",
      "\t Training loss (single batch): 1.320256233215332\n",
      "\t Training loss (single batch): 0.9630083441734314\n",
      "\t Training loss (single batch): 1.1043003797531128\n",
      "\t Training loss (single batch): 1.200034260749817\n",
      "\t Training loss (single batch): 1.0151218175888062\n",
      "\t Training loss (single batch): 1.371682047843933\n",
      "\t Training loss (single batch): 1.1335482597351074\n",
      "\t Training loss (single batch): 0.7352510690689087\n",
      "\t Training loss (single batch): 1.1827284097671509\n",
      "\t Training loss (single batch): 1.385986328125\n",
      "\t Training loss (single batch): 1.4145448207855225\n",
      "\t Training loss (single batch): 1.3377076387405396\n",
      "\t Training loss (single batch): 1.6017199754714966\n",
      "\t Training loss (single batch): 0.9684544205665588\n",
      "\t Training loss (single batch): 1.6225941181182861\n",
      "\t Training loss (single batch): 1.1972758769989014\n",
      "\t Training loss (single batch): 1.2627887725830078\n",
      "\t Training loss (single batch): 1.9620472192764282\n",
      "\t Training loss (single batch): 1.278531551361084\n",
      "\t Training loss (single batch): 1.3585286140441895\n",
      "\t Training loss (single batch): 1.0102732181549072\n",
      "\t Training loss (single batch): 1.2787705659866333\n",
      "\t Training loss (single batch): 0.993026852607727\n",
      "\t Training loss (single batch): 1.2278870344161987\n",
      "\t Training loss (single batch): 1.510272741317749\n",
      "\t Training loss (single batch): 1.4602371454238892\n",
      "\t Training loss (single batch): 1.1894265413284302\n",
      "\t Training loss (single batch): 1.0370913743972778\n",
      "\t Training loss (single batch): 1.1402087211608887\n",
      "\t Training loss (single batch): 1.5982097387313843\n",
      "\t Training loss (single batch): 1.4403456449508667\n",
      "\t Training loss (single batch): 1.0571849346160889\n",
      "\t Training loss (single batch): 1.0680254697799683\n",
      "\t Training loss (single batch): 1.6428717374801636\n",
      "\t Training loss (single batch): 0.9326180815696716\n",
      "\t Training loss (single batch): 1.1122056245803833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0999644994735718\n",
      "\t Training loss (single batch): 1.7501407861709595\n",
      "\t Training loss (single batch): 1.0239149332046509\n",
      "\t Training loss (single batch): 2.0808722972869873\n",
      "\t Training loss (single batch): 1.11929452419281\n",
      "\t Training loss (single batch): 1.419498085975647\n",
      "\t Training loss (single batch): 1.3986694812774658\n",
      "\t Training loss (single batch): 1.4152148962020874\n",
      "\t Training loss (single batch): 1.0632492303848267\n",
      "\t Training loss (single batch): 1.2080937623977661\n",
      "\t Training loss (single batch): 1.0269801616668701\n",
      "\t Training loss (single batch): 0.9295490384101868\n",
      "\t Training loss (single batch): 1.2835276126861572\n",
      "\t Training loss (single batch): 1.590740442276001\n",
      "\t Training loss (single batch): 1.563652515411377\n",
      "\t Training loss (single batch): 1.2228097915649414\n",
      "\t Training loss (single batch): 1.4043705463409424\n",
      "\t Training loss (single batch): 1.4072030782699585\n",
      "\t Training loss (single batch): 1.2510020732879639\n",
      "\t Training loss (single batch): 1.122811198234558\n",
      "\t Training loss (single batch): 1.306905746459961\n",
      "\t Training loss (single batch): 1.098852515220642\n",
      "\t Training loss (single batch): 1.035279393196106\n",
      "\t Training loss (single batch): 1.5094506740570068\n",
      "\t Training loss (single batch): 1.1358143091201782\n",
      "\t Training loss (single batch): 1.3441178798675537\n",
      "\t Training loss (single batch): 1.3479236364364624\n",
      "\t Training loss (single batch): 1.5210676193237305\n",
      "\t Training loss (single batch): 1.1151227951049805\n",
      "\t Training loss (single batch): 1.5671193599700928\n",
      "\t Training loss (single batch): 1.5877405405044556\n",
      "\t Training loss (single batch): 0.8252350091934204\n",
      "\t Training loss (single batch): 1.4020756483078003\n",
      "\t Training loss (single batch): 1.5091445446014404\n",
      "\t Training loss (single batch): 1.5861438512802124\n",
      "\t Training loss (single batch): 1.1459026336669922\n",
      "\t Training loss (single batch): 1.354555368423462\n",
      "\t Training loss (single batch): 1.7782576084136963\n",
      "\t Training loss (single batch): 1.500059962272644\n",
      "\t Training loss (single batch): 1.0298303365707397\n",
      "\t Training loss (single batch): 1.5562087297439575\n",
      "\t Training loss (single batch): 1.5012574195861816\n",
      "\t Training loss (single batch): 1.2582088708877563\n",
      "\t Training loss (single batch): 1.0831905603408813\n",
      "\t Training loss (single batch): 1.341405987739563\n",
      "\t Training loss (single batch): 1.3706393241882324\n",
      "\t Training loss (single batch): 0.9714382290840149\n",
      "\t Training loss (single batch): 1.4339059591293335\n",
      "\t Training loss (single batch): 1.5144431591033936\n",
      "\t Training loss (single batch): 0.9100323915481567\n",
      "\t Training loss (single batch): 1.5506175756454468\n",
      "\t Training loss (single batch): 1.5314784049987793\n",
      "\t Training loss (single batch): 1.1084327697753906\n",
      "\t Training loss (single batch): 1.1994781494140625\n",
      "\t Training loss (single batch): 1.388191819190979\n",
      "\t Training loss (single batch): 1.3533998727798462\n",
      "\t Training loss (single batch): 1.4424422979354858\n",
      "\t Training loss (single batch): 1.113564372062683\n",
      "\t Training loss (single batch): 1.473923921585083\n",
      "\t Training loss (single batch): 1.6134966611862183\n",
      "\t Training loss (single batch): 1.1722195148468018\n",
      "\t Training loss (single batch): 1.1180466413497925\n",
      "\t Training loss (single batch): 1.51559317111969\n",
      "\t Training loss (single batch): 1.0145905017852783\n",
      "\t Training loss (single batch): 1.4515771865844727\n",
      "\t Training loss (single batch): 1.4636130332946777\n",
      "\t Training loss (single batch): 1.1574627161026\n",
      "\t Training loss (single batch): 1.8187053203582764\n",
      "\t Training loss (single batch): 1.0339093208312988\n",
      "\t Training loss (single batch): 1.2378560304641724\n",
      "\t Training loss (single batch): 1.3172837495803833\n",
      "\t Training loss (single batch): 0.7515613436698914\n",
      "\t Training loss (single batch): 1.2199820280075073\n",
      "##################################\n",
      "## EPOCH 12\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0866814851760864\n",
      "\t Training loss (single batch): 1.2404356002807617\n",
      "\t Training loss (single batch): 1.091374397277832\n",
      "\t Training loss (single batch): 1.580169439315796\n",
      "\t Training loss (single batch): 1.0105024576187134\n",
      "\t Training loss (single batch): 0.8470087051391602\n",
      "\t Training loss (single batch): 1.6461247205734253\n",
      "\t Training loss (single batch): 1.6325503587722778\n",
      "\t Training loss (single batch): 1.330986499786377\n",
      "\t Training loss (single batch): 0.9392901062965393\n",
      "\t Training loss (single batch): 1.3458166122436523\n",
      "\t Training loss (single batch): 0.9113785028457642\n",
      "\t Training loss (single batch): 1.0574167966842651\n",
      "\t Training loss (single batch): 0.8946470022201538\n",
      "\t Training loss (single batch): 1.4199094772338867\n",
      "\t Training loss (single batch): 1.3470531702041626\n",
      "\t Training loss (single batch): 1.2955788373947144\n",
      "\t Training loss (single batch): 1.0161898136138916\n",
      "\t Training loss (single batch): 1.2655645608901978\n",
      "\t Training loss (single batch): 1.7367337942123413\n",
      "\t Training loss (single batch): 0.8908632397651672\n",
      "\t Training loss (single batch): 1.3313301801681519\n",
      "\t Training loss (single batch): 0.9612314701080322\n",
      "\t Training loss (single batch): 1.1562458276748657\n",
      "\t Training loss (single batch): 1.1556222438812256\n",
      "\t Training loss (single batch): 1.646555781364441\n",
      "\t Training loss (single batch): 1.5196189880371094\n",
      "\t Training loss (single batch): 1.1153267621994019\n",
      "\t Training loss (single batch): 1.5625568628311157\n",
      "\t Training loss (single batch): 1.2086766958236694\n",
      "\t Training loss (single batch): 1.393805742263794\n",
      "\t Training loss (single batch): 1.2382723093032837\n",
      "\t Training loss (single batch): 0.972589373588562\n",
      "\t Training loss (single batch): 1.139646053314209\n",
      "\t Training loss (single batch): 1.424551248550415\n",
      "\t Training loss (single batch): 1.5505403280258179\n",
      "\t Training loss (single batch): 0.9942746162414551\n",
      "\t Training loss (single batch): 1.3228768110275269\n",
      "\t Training loss (single batch): 1.2701836824417114\n",
      "\t Training loss (single batch): 0.8837697505950928\n",
      "\t Training loss (single batch): 1.435837745666504\n",
      "\t Training loss (single batch): 1.4748069047927856\n",
      "\t Training loss (single batch): 1.2272284030914307\n",
      "\t Training loss (single batch): 1.231376051902771\n",
      "\t Training loss (single batch): 1.1744658946990967\n",
      "\t Training loss (single batch): 1.1889801025390625\n",
      "\t Training loss (single batch): 1.822077989578247\n",
      "\t Training loss (single batch): 1.455856204032898\n",
      "\t Training loss (single batch): 1.6031551361083984\n",
      "\t Training loss (single batch): 1.4892578125\n",
      "\t Training loss (single batch): 1.2086949348449707\n",
      "\t Training loss (single batch): 1.5778632164001465\n",
      "\t Training loss (single batch): 1.4683259725570679\n",
      "\t Training loss (single batch): 1.3219749927520752\n",
      "\t Training loss (single batch): 1.2383424043655396\n",
      "\t Training loss (single batch): 1.1455270051956177\n",
      "\t Training loss (single batch): 1.6090883016586304\n",
      "\t Training loss (single batch): 1.231481671333313\n",
      "\t Training loss (single batch): 1.0913933515548706\n",
      "\t Training loss (single batch): 1.6203255653381348\n",
      "\t Training loss (single batch): 1.0593990087509155\n",
      "\t Training loss (single batch): 0.8543221354484558\n",
      "\t Training loss (single batch): 1.509239673614502\n",
      "\t Training loss (single batch): 1.4305551052093506\n",
      "\t Training loss (single batch): 1.2969908714294434\n",
      "\t Training loss (single batch): 0.7816214561462402\n",
      "\t Training loss (single batch): 1.371374487876892\n",
      "\t Training loss (single batch): 1.2659016847610474\n",
      "\t Training loss (single batch): 1.1207516193389893\n",
      "\t Training loss (single batch): 1.3067035675048828\n",
      "\t Training loss (single batch): 1.5262795686721802\n",
      "\t Training loss (single batch): 1.4184774160385132\n",
      "\t Training loss (single batch): 1.0647004842758179\n",
      "\t Training loss (single batch): 0.8906592726707458\n",
      "\t Training loss (single batch): 0.9873623251914978\n",
      "\t Training loss (single batch): 1.221329689025879\n",
      "\t Training loss (single batch): 1.3987343311309814\n",
      "\t Training loss (single batch): 1.4824990034103394\n",
      "\t Training loss (single batch): 1.4887467622756958\n",
      "\t Training loss (single batch): 1.5010963678359985\n",
      "\t Training loss (single batch): 1.0664762258529663\n",
      "\t Training loss (single batch): 1.0684882402420044\n",
      "\t Training loss (single batch): 0.8778330683708191\n",
      "\t Training loss (single batch): 0.9286617636680603\n",
      "\t Training loss (single batch): 1.4124876260757446\n",
      "\t Training loss (single batch): 2.2328882217407227\n",
      "\t Training loss (single batch): 1.4751182794570923\n",
      "\t Training loss (single batch): 1.0845770835876465\n",
      "\t Training loss (single batch): 0.9570749998092651\n",
      "\t Training loss (single batch): 1.3177034854888916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2263076305389404\n",
      "\t Training loss (single batch): 1.5569257736206055\n",
      "\t Training loss (single batch): 1.417457938194275\n",
      "\t Training loss (single batch): 1.538460612297058\n",
      "\t Training loss (single batch): 1.121588110923767\n",
      "\t Training loss (single batch): 1.1384031772613525\n",
      "\t Training loss (single batch): 1.384281873703003\n",
      "\t Training loss (single batch): 0.9753918647766113\n",
      "\t Training loss (single batch): 1.3074727058410645\n",
      "\t Training loss (single batch): 1.88711416721344\n",
      "\t Training loss (single batch): 1.2883787155151367\n",
      "\t Training loss (single batch): 1.1727906465530396\n",
      "\t Training loss (single batch): 1.257157564163208\n",
      "\t Training loss (single batch): 1.1769627332687378\n",
      "\t Training loss (single batch): 1.4209303855895996\n",
      "\t Training loss (single batch): 1.6786692142486572\n",
      "\t Training loss (single batch): 1.184875726699829\n",
      "\t Training loss (single batch): 1.3217978477478027\n",
      "\t Training loss (single batch): 1.4879443645477295\n",
      "\t Training loss (single batch): 1.3250608444213867\n",
      "\t Training loss (single batch): 1.8398101329803467\n",
      "\t Training loss (single batch): 1.032550573348999\n",
      "\t Training loss (single batch): 1.4433939456939697\n",
      "\t Training loss (single batch): 1.4624367952346802\n",
      "\t Training loss (single batch): 1.344890832901001\n",
      "\t Training loss (single batch): 1.1979191303253174\n",
      "\t Training loss (single batch): 1.5145689249038696\n",
      "\t Training loss (single batch): 0.995749831199646\n",
      "\t Training loss (single batch): 1.5574136972427368\n",
      "\t Training loss (single batch): 1.2562026977539062\n",
      "\t Training loss (single batch): 1.0655107498168945\n",
      "\t Training loss (single batch): 0.8686246871948242\n",
      "\t Training loss (single batch): 1.1656571626663208\n",
      "\t Training loss (single batch): 1.1991242170333862\n",
      "\t Training loss (single batch): 1.161819338798523\n",
      "\t Training loss (single batch): 1.362615704536438\n",
      "\t Training loss (single batch): 1.2874027490615845\n",
      "\t Training loss (single batch): 1.0495303869247437\n",
      "\t Training loss (single batch): 1.1517325639724731\n",
      "\t Training loss (single batch): 1.215232253074646\n",
      "\t Training loss (single batch): 1.1726235151290894\n",
      "\t Training loss (single batch): 1.1583294868469238\n",
      "\t Training loss (single batch): 0.8969115018844604\n",
      "\t Training loss (single batch): 0.7103784680366516\n",
      "\t Training loss (single batch): 1.1281838417053223\n",
      "\t Training loss (single batch): 1.836254358291626\n",
      "\t Training loss (single batch): 1.6107984781265259\n",
      "\t Training loss (single batch): 1.0463403463363647\n",
      "\t Training loss (single batch): 0.8541967272758484\n",
      "\t Training loss (single batch): 1.230705976486206\n",
      "\t Training loss (single batch): 1.1307778358459473\n",
      "\t Training loss (single batch): 0.8913516998291016\n",
      "\t Training loss (single batch): 1.6661938428878784\n",
      "\t Training loss (single batch): 1.4067621231079102\n",
      "\t Training loss (single batch): 1.36199951171875\n",
      "\t Training loss (single batch): 0.6265407204627991\n",
      "\t Training loss (single batch): 1.4972691535949707\n",
      "\t Training loss (single batch): 1.0423134565353394\n",
      "\t Training loss (single batch): 1.2812272310256958\n",
      "\t Training loss (single batch): 1.2402586936950684\n",
      "\t Training loss (single batch): 0.943666398525238\n",
      "\t Training loss (single batch): 1.3657218217849731\n",
      "\t Training loss (single batch): 1.5714894533157349\n",
      "\t Training loss (single batch): 1.1267591714859009\n",
      "\t Training loss (single batch): 1.9456839561462402\n",
      "\t Training loss (single batch): 1.5706450939178467\n",
      "\t Training loss (single batch): 0.7968345880508423\n",
      "\t Training loss (single batch): 1.1627230644226074\n",
      "\t Training loss (single batch): 1.0692943334579468\n",
      "\t Training loss (single batch): 1.5692193508148193\n",
      "\t Training loss (single batch): 1.5100520849227905\n",
      "\t Training loss (single batch): 1.454296588897705\n",
      "\t Training loss (single batch): 1.417149305343628\n",
      "\t Training loss (single batch): 0.876949667930603\n",
      "\t Training loss (single batch): 1.002351999282837\n",
      "\t Training loss (single batch): 1.714592456817627\n",
      "\t Training loss (single batch): 1.3509770631790161\n",
      "\t Training loss (single batch): 0.9690217971801758\n",
      "\t Training loss (single batch): 1.413407325744629\n",
      "\t Training loss (single batch): 1.0985671281814575\n",
      "\t Training loss (single batch): 0.7858288884162903\n",
      "\t Training loss (single batch): 1.698891282081604\n",
      "\t Training loss (single batch): 1.1389009952545166\n",
      "\t Training loss (single batch): 1.310891032218933\n",
      "\t Training loss (single batch): 1.0964503288269043\n",
      "\t Training loss (single batch): 1.3275941610336304\n",
      "\t Training loss (single batch): 1.6221834421157837\n",
      "\t Training loss (single batch): 1.2313871383666992\n",
      "\t Training loss (single batch): 1.221468210220337\n",
      "\t Training loss (single batch): 1.2987488508224487\n",
      "\t Training loss (single batch): 0.979498565196991\n",
      "\t Training loss (single batch): 1.2790021896362305\n",
      "\t Training loss (single batch): 0.9611150026321411\n",
      "\t Training loss (single batch): 1.4349775314331055\n",
      "\t Training loss (single batch): 1.4847185611724854\n",
      "\t Training loss (single batch): 1.253016710281372\n",
      "\t Training loss (single batch): 1.1137173175811768\n",
      "\t Training loss (single batch): 1.536508321762085\n",
      "\t Training loss (single batch): 0.897338330745697\n",
      "\t Training loss (single batch): 1.8517603874206543\n",
      "\t Training loss (single batch): 0.9958102107048035\n",
      "\t Training loss (single batch): 0.9996514320373535\n",
      "\t Training loss (single batch): 1.4542229175567627\n",
      "\t Training loss (single batch): 1.3580257892608643\n",
      "\t Training loss (single batch): 0.7634485363960266\n",
      "\t Training loss (single batch): 1.3350260257720947\n",
      "\t Training loss (single batch): 1.3327966928482056\n",
      "\t Training loss (single batch): 1.5144017934799194\n",
      "\t Training loss (single batch): 1.477874517440796\n",
      "\t Training loss (single batch): 1.1972054243087769\n",
      "\t Training loss (single batch): 1.303177833557129\n",
      "\t Training loss (single batch): 1.3326219320297241\n",
      "\t Training loss (single batch): 1.2609655857086182\n",
      "\t Training loss (single batch): 1.3597261905670166\n",
      "\t Training loss (single batch): 1.7429444789886475\n",
      "\t Training loss (single batch): 1.4535516500473022\n",
      "\t Training loss (single batch): 1.4161901473999023\n",
      "\t Training loss (single batch): 1.0417723655700684\n",
      "\t Training loss (single batch): 1.2842485904693604\n",
      "\t Training loss (single batch): 1.593007206916809\n",
      "\t Training loss (single batch): 0.944029688835144\n",
      "\t Training loss (single batch): 1.6301523447036743\n",
      "\t Training loss (single batch): 1.5774716138839722\n",
      "\t Training loss (single batch): 1.0743550062179565\n",
      "\t Training loss (single batch): 0.9737257957458496\n",
      "\t Training loss (single batch): 1.0633057355880737\n",
      "\t Training loss (single batch): 1.5841988325119019\n",
      "\t Training loss (single batch): 1.2569341659545898\n",
      "\t Training loss (single batch): 1.8369193077087402\n",
      "\t Training loss (single batch): 1.254192590713501\n",
      "\t Training loss (single batch): 1.2424677610397339\n",
      "\t Training loss (single batch): 1.862013339996338\n",
      "\t Training loss (single batch): 1.6398241519927979\n",
      "\t Training loss (single batch): 1.0936460494995117\n",
      "\t Training loss (single batch): 1.350512146949768\n",
      "\t Training loss (single batch): 1.4636164903640747\n",
      "\t Training loss (single batch): 1.0014793872833252\n",
      "\t Training loss (single batch): 1.8013650178909302\n",
      "\t Training loss (single batch): 1.4234669208526611\n",
      "\t Training loss (single batch): 0.943084716796875\n",
      "\t Training loss (single batch): 1.4810186624526978\n",
      "\t Training loss (single batch): 1.390205979347229\n",
      "\t Training loss (single batch): 1.2840622663497925\n",
      "\t Training loss (single batch): 1.618983507156372\n",
      "\t Training loss (single batch): 1.7681586742401123\n",
      "\t Training loss (single batch): 1.5445302724838257\n",
      "\t Training loss (single batch): 1.3419808149337769\n",
      "\t Training loss (single batch): 1.1374485492706299\n",
      "\t Training loss (single batch): 1.5792829990386963\n",
      "\t Training loss (single batch): 0.8835505247116089\n",
      "\t Training loss (single batch): 1.1035804748535156\n",
      "\t Training loss (single batch): 0.7847303152084351\n",
      "\t Training loss (single batch): 1.6191462278366089\n",
      "\t Training loss (single batch): 1.4303560256958008\n",
      "\t Training loss (single batch): 1.096441388130188\n",
      "\t Training loss (single batch): 1.447689414024353\n",
      "\t Training loss (single batch): 1.1835689544677734\n",
      "\t Training loss (single batch): 1.2915879487991333\n",
      "\t Training loss (single batch): 1.0201833248138428\n",
      "\t Training loss (single batch): 1.6749006509780884\n",
      "\t Training loss (single batch): 1.4371974468231201\n",
      "\t Training loss (single batch): 1.3974237442016602\n",
      "\t Training loss (single batch): 1.3548309803009033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.610324501991272\n",
      "\t Training loss (single batch): 1.5495610237121582\n",
      "\t Training loss (single batch): 1.2299607992172241\n",
      "\t Training loss (single batch): 1.4081717729568481\n",
      "\t Training loss (single batch): 1.3468014001846313\n",
      "\t Training loss (single batch): 1.2152332067489624\n",
      "\t Training loss (single batch): 0.7479666471481323\n",
      "\t Training loss (single batch): 2.3655643463134766\n",
      "\t Training loss (single batch): 1.2328567504882812\n",
      "\t Training loss (single batch): 1.4035850763320923\n",
      "\t Training loss (single batch): 1.3637182712554932\n",
      "\t Training loss (single batch): 1.0852018594741821\n",
      "\t Training loss (single batch): 1.4479809999465942\n",
      "\t Training loss (single batch): 1.141714096069336\n",
      "\t Training loss (single batch): 1.5866944789886475\n",
      "\t Training loss (single batch): 1.6872917413711548\n",
      "\t Training loss (single batch): 1.1507707834243774\n",
      "\t Training loss (single batch): 1.2471245527267456\n",
      "\t Training loss (single batch): 1.1381863355636597\n",
      "\t Training loss (single batch): 2.0006399154663086\n",
      "\t Training loss (single batch): 1.2382797002792358\n",
      "\t Training loss (single batch): 1.2591979503631592\n",
      "\t Training loss (single batch): 1.1868637800216675\n",
      "\t Training loss (single batch): 0.9265105724334717\n",
      "\t Training loss (single batch): 1.1128294467926025\n",
      "\t Training loss (single batch): 1.1003355979919434\n",
      "\t Training loss (single batch): 1.7814838886260986\n",
      "\t Training loss (single batch): 1.1932474374771118\n",
      "\t Training loss (single batch): 1.5573052167892456\n",
      "\t Training loss (single batch): 1.2183970212936401\n",
      "\t Training loss (single batch): 1.8290128707885742\n",
      "\t Training loss (single batch): 1.5064723491668701\n",
      "\t Training loss (single batch): 1.0511510372161865\n",
      "\t Training loss (single batch): 1.1936352252960205\n",
      "\t Training loss (single batch): 1.1044353246688843\n",
      "\t Training loss (single batch): 1.1001770496368408\n",
      "\t Training loss (single batch): 1.2614096403121948\n",
      "\t Training loss (single batch): 1.0891398191452026\n",
      "\t Training loss (single batch): 1.3801414966583252\n",
      "\t Training loss (single batch): 0.8834890723228455\n",
      "\t Training loss (single batch): 1.0480784177780151\n",
      "\t Training loss (single batch): 1.3098523616790771\n",
      "\t Training loss (single batch): 1.718586802482605\n",
      "\t Training loss (single batch): 1.3116035461425781\n",
      "\t Training loss (single batch): 1.465798258781433\n",
      "\t Training loss (single batch): 0.9594723582267761\n",
      "\t Training loss (single batch): 0.9352341890335083\n",
      "\t Training loss (single batch): 1.2048804759979248\n",
      "\t Training loss (single batch): 1.309638500213623\n",
      "\t Training loss (single batch): 1.7052357196807861\n",
      "\t Training loss (single batch): 1.5900412797927856\n",
      "\t Training loss (single batch): 1.2303664684295654\n",
      "\t Training loss (single batch): 1.2890580892562866\n",
      "\t Training loss (single batch): 1.8817020654678345\n",
      "\t Training loss (single batch): 1.1801371574401855\n",
      "\t Training loss (single batch): 1.166943073272705\n",
      "\t Training loss (single batch): 1.1376811265945435\n",
      "\t Training loss (single batch): 2.1014204025268555\n",
      "\t Training loss (single batch): 1.2518843412399292\n",
      "\t Training loss (single batch): 1.1941232681274414\n",
      "\t Training loss (single batch): 1.1584280729293823\n",
      "\t Training loss (single batch): 0.8956375122070312\n",
      "\t Training loss (single batch): 1.8952049016952515\n",
      "\t Training loss (single batch): 0.9599489569664001\n",
      "##################################\n",
      "## EPOCH 13\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2715651988983154\n",
      "\t Training loss (single batch): 1.176552653312683\n",
      "\t Training loss (single batch): 1.2983274459838867\n",
      "\t Training loss (single batch): 1.5678879022598267\n",
      "\t Training loss (single batch): 1.113844633102417\n",
      "\t Training loss (single batch): 0.8987024426460266\n",
      "\t Training loss (single batch): 1.4324554204940796\n",
      "\t Training loss (single batch): 1.7420166730880737\n",
      "\t Training loss (single batch): 1.1563587188720703\n",
      "\t Training loss (single batch): 0.9577886462211609\n",
      "\t Training loss (single batch): 1.4680315256118774\n",
      "\t Training loss (single batch): 1.5331757068634033\n",
      "\t Training loss (single batch): 0.8520908355712891\n",
      "\t Training loss (single batch): 1.6500847339630127\n",
      "\t Training loss (single batch): 1.7163524627685547\n",
      "\t Training loss (single batch): 0.7344450950622559\n",
      "\t Training loss (single batch): 1.1161205768585205\n",
      "\t Training loss (single batch): 1.6855437755584717\n",
      "\t Training loss (single batch): 1.3760912418365479\n",
      "\t Training loss (single batch): 2.3729248046875\n",
      "\t Training loss (single batch): 1.4358423948287964\n",
      "\t Training loss (single batch): 1.4841455221176147\n",
      "\t Training loss (single batch): 1.5634739398956299\n",
      "\t Training loss (single batch): 0.8917019963264465\n",
      "\t Training loss (single batch): 0.7555155754089355\n",
      "\t Training loss (single batch): 1.0794334411621094\n",
      "\t Training loss (single batch): 1.689139485359192\n",
      "\t Training loss (single batch): 1.5979747772216797\n",
      "\t Training loss (single batch): 1.2474840879440308\n",
      "\t Training loss (single batch): 1.775888442993164\n",
      "\t Training loss (single batch): 1.7847620248794556\n",
      "\t Training loss (single batch): 1.2549598217010498\n",
      "\t Training loss (single batch): 1.0722953081130981\n",
      "\t Training loss (single batch): 1.3228861093521118\n",
      "\t Training loss (single batch): 1.0706002712249756\n",
      "\t Training loss (single batch): 0.9459680914878845\n",
      "\t Training loss (single batch): 1.0248523950576782\n",
      "\t Training loss (single batch): 1.149016261100769\n",
      "\t Training loss (single batch): 1.26288640499115\n",
      "\t Training loss (single batch): 1.293980360031128\n",
      "\t Training loss (single batch): 1.2843097448349\n",
      "\t Training loss (single batch): 1.716868281364441\n",
      "\t Training loss (single batch): 0.8190112709999084\n",
      "\t Training loss (single batch): 1.2450312376022339\n",
      "\t Training loss (single batch): 1.055100440979004\n",
      "\t Training loss (single batch): 1.2477234601974487\n",
      "\t Training loss (single batch): 1.2249923944473267\n",
      "\t Training loss (single batch): 1.2044894695281982\n",
      "\t Training loss (single batch): 1.4785826206207275\n",
      "\t Training loss (single batch): 1.5012894868850708\n",
      "\t Training loss (single batch): 1.5234545469284058\n",
      "\t Training loss (single batch): 0.964695155620575\n",
      "\t Training loss (single batch): 1.4247746467590332\n",
      "\t Training loss (single batch): 1.4940284490585327\n",
      "\t Training loss (single batch): 1.9910515546798706\n",
      "\t Training loss (single batch): 1.0095250606536865\n",
      "\t Training loss (single batch): 1.102352261543274\n",
      "\t Training loss (single batch): 1.4287004470825195\n",
      "\t Training loss (single batch): 1.4030057191848755\n",
      "\t Training loss (single batch): 1.2716575860977173\n",
      "\t Training loss (single batch): 1.3815562725067139\n",
      "\t Training loss (single batch): 1.2727152109146118\n",
      "\t Training loss (single batch): 1.7014570236206055\n",
      "\t Training loss (single batch): 0.9761965870857239\n",
      "\t Training loss (single batch): 1.4512157440185547\n",
      "\t Training loss (single batch): 1.4672003984451294\n",
      "\t Training loss (single batch): 1.3741315603256226\n",
      "\t Training loss (single batch): 1.4217283725738525\n",
      "\t Training loss (single batch): 1.493237018585205\n",
      "\t Training loss (single batch): 1.4572805166244507\n",
      "\t Training loss (single batch): 1.2547783851623535\n",
      "\t Training loss (single batch): 1.574536681175232\n",
      "\t Training loss (single batch): 1.6004630327224731\n",
      "\t Training loss (single batch): 1.546053409576416\n",
      "\t Training loss (single batch): 1.4859682321548462\n",
      "\t Training loss (single batch): 1.6185823678970337\n",
      "\t Training loss (single batch): 1.570008635520935\n",
      "\t Training loss (single batch): 1.6546415090560913\n",
      "\t Training loss (single batch): 1.3842426538467407\n",
      "\t Training loss (single batch): 1.4407497644424438\n",
      "\t Training loss (single batch): 0.959189236164093\n",
      "\t Training loss (single batch): 1.0553120374679565\n",
      "\t Training loss (single batch): 1.4647828340530396\n",
      "\t Training loss (single batch): 1.3746343851089478\n",
      "\t Training loss (single batch): 1.318198323249817\n",
      "\t Training loss (single batch): 1.3723992109298706\n",
      "\t Training loss (single batch): 1.3585909605026245\n",
      "\t Training loss (single batch): 1.3586093187332153\n",
      "\t Training loss (single batch): 1.6048234701156616\n",
      "\t Training loss (single batch): 0.8700277805328369\n",
      "\t Training loss (single batch): 1.6102004051208496\n",
      "\t Training loss (single batch): 1.3180164098739624\n",
      "\t Training loss (single batch): 1.5851223468780518\n",
      "\t Training loss (single batch): 1.4020885229110718\n",
      "\t Training loss (single batch): 1.2633470296859741\n",
      "\t Training loss (single batch): 1.0916866064071655\n",
      "\t Training loss (single batch): 1.3638670444488525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2004117965698242\n",
      "\t Training loss (single batch): 1.2516387701034546\n",
      "\t Training loss (single batch): 0.8552547693252563\n",
      "\t Training loss (single batch): 1.5592085123062134\n",
      "\t Training loss (single batch): 1.3076173067092896\n",
      "\t Training loss (single batch): 1.7102512121200562\n",
      "\t Training loss (single batch): 1.2876962423324585\n",
      "\t Training loss (single batch): 1.2478675842285156\n",
      "\t Training loss (single batch): 1.730958342552185\n",
      "\t Training loss (single batch): 1.2490220069885254\n",
      "\t Training loss (single batch): 1.2405917644500732\n",
      "\t Training loss (single batch): 1.1781445741653442\n",
      "\t Training loss (single batch): 0.9152174592018127\n",
      "\t Training loss (single batch): 1.0112676620483398\n",
      "\t Training loss (single batch): 1.022249698638916\n",
      "\t Training loss (single batch): 1.1725939512252808\n",
      "\t Training loss (single batch): 1.2458902597427368\n",
      "\t Training loss (single batch): 1.3421300649642944\n",
      "\t Training loss (single batch): 0.9726706147193909\n",
      "\t Training loss (single batch): 0.8088863492012024\n",
      "\t Training loss (single batch): 1.9900763034820557\n",
      "\t Training loss (single batch): 1.1733837127685547\n",
      "\t Training loss (single batch): 1.380008339881897\n",
      "\t Training loss (single batch): 1.4451817274093628\n",
      "\t Training loss (single batch): 1.3679131269454956\n",
      "\t Training loss (single batch): 1.7390189170837402\n",
      "\t Training loss (single batch): 1.6335445642471313\n",
      "\t Training loss (single batch): 1.1341607570648193\n",
      "\t Training loss (single batch): 1.5875627994537354\n",
      "\t Training loss (single batch): 1.0375514030456543\n",
      "\t Training loss (single batch): 0.7972745299339294\n",
      "\t Training loss (single batch): 1.3161377906799316\n",
      "\t Training loss (single batch): 1.8122098445892334\n",
      "\t Training loss (single batch): 1.4604620933532715\n",
      "\t Training loss (single batch): 1.235868215560913\n",
      "\t Training loss (single batch): 1.0223294496536255\n",
      "\t Training loss (single batch): 1.318234920501709\n",
      "\t Training loss (single batch): 1.922847867012024\n",
      "\t Training loss (single batch): 0.9579886794090271\n",
      "\t Training loss (single batch): 0.9449509382247925\n",
      "\t Training loss (single batch): 1.389772891998291\n",
      "\t Training loss (single batch): 1.2071995735168457\n",
      "\t Training loss (single batch): 1.457068681716919\n",
      "\t Training loss (single batch): 1.1672197580337524\n",
      "\t Training loss (single batch): 1.0208027362823486\n",
      "\t Training loss (single batch): 1.719154715538025\n",
      "\t Training loss (single batch): 1.220740556716919\n",
      "\t Training loss (single batch): 1.2454560995101929\n",
      "\t Training loss (single batch): 1.586760401725769\n",
      "\t Training loss (single batch): 1.1805232763290405\n",
      "\t Training loss (single batch): 1.6808006763458252\n",
      "\t Training loss (single batch): 1.4770864248275757\n",
      "\t Training loss (single batch): 1.3400877714157104\n",
      "\t Training loss (single batch): 1.4577159881591797\n",
      "\t Training loss (single batch): 1.1901051998138428\n",
      "\t Training loss (single batch): 0.949675977230072\n",
      "\t Training loss (single batch): 1.2024582624435425\n",
      "\t Training loss (single batch): 1.0939981937408447\n",
      "\t Training loss (single batch): 1.2308613061904907\n",
      "\t Training loss (single batch): 1.758744478225708\n",
      "\t Training loss (single batch): 1.607590675354004\n",
      "\t Training loss (single batch): 1.0737560987472534\n",
      "\t Training loss (single batch): 1.398884654045105\n",
      "\t Training loss (single batch): 1.4464012384414673\n",
      "\t Training loss (single batch): 1.0469985008239746\n",
      "\t Training loss (single batch): 1.3665473461151123\n",
      "\t Training loss (single batch): 1.1217278242111206\n",
      "\t Training loss (single batch): 1.2999204397201538\n",
      "\t Training loss (single batch): 2.025383710861206\n",
      "\t Training loss (single batch): 1.3889912366867065\n",
      "\t Training loss (single batch): 0.794761598110199\n",
      "\t Training loss (single batch): 1.4525790214538574\n",
      "\t Training loss (single batch): 1.2286416292190552\n",
      "\t Training loss (single batch): 1.4899057149887085\n",
      "\t Training loss (single batch): 1.5928646326065063\n",
      "\t Training loss (single batch): 1.6168051958084106\n",
      "\t Training loss (single batch): 1.3131924867630005\n",
      "\t Training loss (single batch): 0.803053081035614\n",
      "\t Training loss (single batch): 0.9489961862564087\n",
      "\t Training loss (single batch): 1.5756218433380127\n",
      "\t Training loss (single batch): 1.2267544269561768\n",
      "\t Training loss (single batch): 1.1629842519760132\n",
      "\t Training loss (single batch): 1.123450756072998\n",
      "\t Training loss (single batch): 0.9718250632286072\n",
      "\t Training loss (single batch): 1.5028364658355713\n",
      "\t Training loss (single batch): 1.3447256088256836\n",
      "\t Training loss (single batch): 1.3511179685592651\n",
      "\t Training loss (single batch): 1.120229959487915\n",
      "\t Training loss (single batch): 0.9108470678329468\n",
      "\t Training loss (single batch): 0.9688641428947449\n",
      "\t Training loss (single batch): 1.3232625722885132\n",
      "\t Training loss (single batch): 1.4888075590133667\n",
      "\t Training loss (single batch): 1.297205924987793\n",
      "\t Training loss (single batch): 1.0950599908828735\n",
      "\t Training loss (single batch): 0.9771074652671814\n",
      "\t Training loss (single batch): 1.0139992237091064\n",
      "\t Training loss (single batch): 1.3355975151062012\n",
      "\t Training loss (single batch): 1.2160297632217407\n",
      "\t Training loss (single batch): 1.4456831216812134\n",
      "\t Training loss (single batch): 1.7161544561386108\n",
      "\t Training loss (single batch): 1.683068871498108\n",
      "\t Training loss (single batch): 1.424452543258667\n",
      "\t Training loss (single batch): 1.369497537612915\n",
      "\t Training loss (single batch): 1.170699954032898\n",
      "\t Training loss (single batch): 1.7426592111587524\n",
      "\t Training loss (single batch): 0.9353205561637878\n",
      "\t Training loss (single batch): 1.1012545824050903\n",
      "\t Training loss (single batch): 1.3212381601333618\n",
      "\t Training loss (single batch): 1.557107925415039\n",
      "\t Training loss (single batch): 1.5392701625823975\n",
      "\t Training loss (single batch): 1.5931103229522705\n",
      "\t Training loss (single batch): 0.7703617811203003\n",
      "\t Training loss (single batch): 1.3255250453948975\n",
      "\t Training loss (single batch): 1.5126070976257324\n",
      "\t Training loss (single batch): 1.2704063653945923\n",
      "\t Training loss (single batch): 1.1920324563980103\n",
      "\t Training loss (single batch): 1.1149873733520508\n",
      "\t Training loss (single batch): 1.5420706272125244\n",
      "\t Training loss (single batch): 1.213766098022461\n",
      "\t Training loss (single batch): 1.3505955934524536\n",
      "\t Training loss (single batch): 1.3430074453353882\n",
      "\t Training loss (single batch): 1.4338221549987793\n",
      "\t Training loss (single batch): 0.9596384763717651\n",
      "\t Training loss (single batch): 1.0531439781188965\n",
      "\t Training loss (single batch): 1.2291587591171265\n",
      "\t Training loss (single batch): 1.546844720840454\n",
      "\t Training loss (single batch): 0.9438193440437317\n",
      "\t Training loss (single batch): 1.3562270402908325\n",
      "\t Training loss (single batch): 1.1340374946594238\n",
      "\t Training loss (single batch): 1.4292981624603271\n",
      "\t Training loss (single batch): 1.2450461387634277\n",
      "\t Training loss (single batch): 1.2220944166183472\n",
      "\t Training loss (single batch): 0.9535004496574402\n",
      "\t Training loss (single batch): 1.393343448638916\n",
      "\t Training loss (single batch): 1.1922252178192139\n",
      "\t Training loss (single batch): 1.2720260620117188\n",
      "\t Training loss (single batch): 1.650325894355774\n",
      "\t Training loss (single batch): 0.9368735551834106\n",
      "\t Training loss (single batch): 0.9224726557731628\n",
      "\t Training loss (single batch): 1.1386327743530273\n",
      "\t Training loss (single batch): 1.425038456916809\n",
      "\t Training loss (single batch): 1.785510778427124\n",
      "\t Training loss (single batch): 1.6327630281448364\n",
      "\t Training loss (single batch): 1.3768662214279175\n",
      "\t Training loss (single batch): 1.06598961353302\n",
      "\t Training loss (single batch): 1.2793034315109253\n",
      "\t Training loss (single batch): 1.3102800846099854\n",
      "\t Training loss (single batch): 1.1803866624832153\n",
      "\t Training loss (single batch): 1.1835697889328003\n",
      "\t Training loss (single batch): 1.3577584028244019\n",
      "\t Training loss (single batch): 1.2623250484466553\n",
      "\t Training loss (single batch): 1.2390475273132324\n",
      "\t Training loss (single batch): 1.4319417476654053\n",
      "\t Training loss (single batch): 1.4153329133987427\n",
      "\t Training loss (single batch): 1.2987489700317383\n",
      "\t Training loss (single batch): 1.320597767829895\n",
      "\t Training loss (single batch): 1.2771449089050293\n",
      "\t Training loss (single batch): 1.0034338235855103\n",
      "\t Training loss (single batch): 1.5455009937286377\n",
      "\t Training loss (single batch): 1.4657442569732666\n",
      "\t Training loss (single batch): 1.365903377532959\n",
      "\t Training loss (single batch): 1.4152777194976807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5999155044555664\n",
      "\t Training loss (single batch): 0.7086799144744873\n",
      "\t Training loss (single batch): 1.1642048358917236\n",
      "\t Training loss (single batch): 0.9971389770507812\n",
      "\t Training loss (single batch): 1.2523760795593262\n",
      "\t Training loss (single batch): 1.8910759687423706\n",
      "\t Training loss (single batch): 1.397161841392517\n",
      "\t Training loss (single batch): 1.2667399644851685\n",
      "\t Training loss (single batch): 1.113195538520813\n",
      "\t Training loss (single batch): 1.813428521156311\n",
      "\t Training loss (single batch): 1.2385832071304321\n",
      "\t Training loss (single batch): 1.5243041515350342\n",
      "\t Training loss (single batch): 1.4991538524627686\n",
      "\t Training loss (single batch): 1.4968359470367432\n",
      "\t Training loss (single batch): 1.40138840675354\n",
      "\t Training loss (single batch): 1.4244065284729004\n",
      "\t Training loss (single batch): 1.4795117378234863\n",
      "\t Training loss (single batch): 1.1538397073745728\n",
      "\t Training loss (single batch): 1.2338255643844604\n",
      "\t Training loss (single batch): 1.480438470840454\n",
      "\t Training loss (single batch): 1.5673346519470215\n",
      "\t Training loss (single batch): 1.2730433940887451\n",
      "\t Training loss (single batch): 1.2649115324020386\n",
      "\t Training loss (single batch): 1.2850008010864258\n",
      "\t Training loss (single batch): 1.1245228052139282\n",
      "\t Training loss (single batch): 1.6780489683151245\n",
      "\t Training loss (single batch): 1.3253766298294067\n",
      "\t Training loss (single batch): 1.2163183689117432\n",
      "\t Training loss (single batch): 1.546972393989563\n",
      "\t Training loss (single batch): 1.6208046674728394\n",
      "\t Training loss (single batch): 1.3863831758499146\n",
      "\t Training loss (single batch): 0.9618700742721558\n",
      "\t Training loss (single batch): 1.8340648412704468\n",
      "\t Training loss (single batch): 0.719918966293335\n",
      "\t Training loss (single batch): 0.754037082195282\n",
      "\t Training loss (single batch): 1.9426738023757935\n",
      "\t Training loss (single batch): 1.6262180805206299\n",
      "\t Training loss (single batch): 1.2525376081466675\n",
      "\t Training loss (single batch): 1.3536350727081299\n",
      "\t Training loss (single batch): 1.2002800703048706\n",
      "\t Training loss (single batch): 1.1624701023101807\n",
      "\t Training loss (single batch): 1.0033388137817383\n",
      "\t Training loss (single batch): 1.2802705764770508\n",
      "\t Training loss (single batch): 1.4495998620986938\n",
      "\t Training loss (single batch): 1.3760056495666504\n",
      "\t Training loss (single batch): 1.4304956197738647\n",
      "\t Training loss (single batch): 1.334527850151062\n",
      "\t Training loss (single batch): 1.4427610635757446\n",
      "\t Training loss (single batch): 1.3441038131713867\n",
      "\t Training loss (single batch): 1.5068957805633545\n",
      "\t Training loss (single batch): 1.5922731161117554\n",
      "\t Training loss (single batch): 0.8452971577644348\n",
      "\t Training loss (single batch): 1.1080801486968994\n",
      "\t Training loss (single batch): 1.1371469497680664\n",
      "\t Training loss (single batch): 1.1848335266113281\n",
      "\t Training loss (single batch): 1.3251721858978271\n",
      "\t Training loss (single batch): 1.7142093181610107\n",
      "\t Training loss (single batch): 2.2354447841644287\n",
      "##################################\n",
      "## EPOCH 14\n",
      "##################################\n",
      "\t Training loss (single batch): 1.682739019393921\n",
      "\t Training loss (single batch): 1.1428723335266113\n",
      "\t Training loss (single batch): 1.608489990234375\n",
      "\t Training loss (single batch): 1.2075952291488647\n",
      "\t Training loss (single batch): 0.8919697999954224\n",
      "\t Training loss (single batch): 1.5961107015609741\n",
      "\t Training loss (single batch): 1.5314000844955444\n",
      "\t Training loss (single batch): 1.242119312286377\n",
      "\t Training loss (single batch): 1.379042387008667\n",
      "\t Training loss (single batch): 1.5435869693756104\n",
      "\t Training loss (single batch): 1.1409519910812378\n",
      "\t Training loss (single batch): 1.0295687913894653\n",
      "\t Training loss (single batch): 1.4815680980682373\n",
      "\t Training loss (single batch): 1.2321964502334595\n",
      "\t Training loss (single batch): 1.3307219743728638\n",
      "\t Training loss (single batch): 1.57295823097229\n",
      "\t Training loss (single batch): 1.2538303136825562\n",
      "\t Training loss (single batch): 1.4526346921920776\n",
      "\t Training loss (single batch): 1.7297275066375732\n",
      "\t Training loss (single batch): 1.8366611003875732\n",
      "\t Training loss (single batch): 0.8041319251060486\n",
      "\t Training loss (single batch): 1.2565618753433228\n",
      "\t Training loss (single batch): 1.5016276836395264\n",
      "\t Training loss (single batch): 1.10862398147583\n",
      "\t Training loss (single batch): 1.0116437673568726\n",
      "\t Training loss (single batch): 1.2911837100982666\n",
      "\t Training loss (single batch): 1.5968787670135498\n",
      "\t Training loss (single batch): 1.047713279724121\n",
      "\t Training loss (single batch): 1.047556757926941\n",
      "\t Training loss (single batch): 0.9987097382545471\n",
      "\t Training loss (single batch): 1.545742154121399\n",
      "\t Training loss (single batch): 1.5058554410934448\n",
      "\t Training loss (single batch): 1.0663237571716309\n",
      "\t Training loss (single batch): 1.6606647968292236\n",
      "\t Training loss (single batch): 0.7693595886230469\n",
      "\t Training loss (single batch): 1.1650933027267456\n",
      "\t Training loss (single batch): 1.2618367671966553\n",
      "\t Training loss (single batch): 1.3805489540100098\n",
      "\t Training loss (single batch): 1.6960476636886597\n",
      "\t Training loss (single batch): 1.3593573570251465\n",
      "\t Training loss (single batch): 1.0302114486694336\n",
      "\t Training loss (single batch): 1.1347829103469849\n",
      "\t Training loss (single batch): 1.2915394306182861\n",
      "\t Training loss (single batch): 1.3427084684371948\n",
      "\t Training loss (single batch): 0.9289013743400574\n",
      "\t Training loss (single batch): 1.0288375616073608\n",
      "\t Training loss (single batch): 1.203674077987671\n",
      "\t Training loss (single batch): 1.2029589414596558\n",
      "\t Training loss (single batch): 1.2834657430648804\n",
      "\t Training loss (single batch): 1.2342058420181274\n",
      "\t Training loss (single batch): 0.8551580309867859\n",
      "\t Training loss (single batch): 1.4604603052139282\n",
      "\t Training loss (single batch): 1.4796618223190308\n",
      "\t Training loss (single batch): 1.2737202644348145\n",
      "\t Training loss (single batch): 1.4716278314590454\n",
      "\t Training loss (single batch): 1.2861560583114624\n",
      "\t Training loss (single batch): 1.5001029968261719\n",
      "\t Training loss (single batch): 1.5998910665512085\n",
      "\t Training loss (single batch): 0.97568279504776\n",
      "\t Training loss (single batch): 1.359316110610962\n",
      "\t Training loss (single batch): 1.2470155954360962\n",
      "\t Training loss (single batch): 1.1251220703125\n",
      "\t Training loss (single batch): 1.19317626953125\n",
      "\t Training loss (single batch): 1.2081423997879028\n",
      "\t Training loss (single batch): 1.0226820707321167\n",
      "\t Training loss (single batch): 1.0731943845748901\n",
      "\t Training loss (single batch): 1.334458351135254\n",
      "\t Training loss (single batch): 1.130417823791504\n",
      "\t Training loss (single batch): 1.1287940740585327\n",
      "\t Training loss (single batch): 1.1775052547454834\n",
      "\t Training loss (single batch): 1.2350960969924927\n",
      "\t Training loss (single batch): 1.2796730995178223\n",
      "\t Training loss (single batch): 1.1136934757232666\n",
      "\t Training loss (single batch): 1.0172839164733887\n",
      "\t Training loss (single batch): 0.9268193244934082\n",
      "\t Training loss (single batch): 1.8371583223342896\n",
      "\t Training loss (single batch): 1.07846999168396\n",
      "\t Training loss (single batch): 1.2328680753707886\n",
      "\t Training loss (single batch): 1.150081753730774\n",
      "\t Training loss (single batch): 1.4282728433609009\n",
      "\t Training loss (single batch): 0.9892566800117493\n",
      "\t Training loss (single batch): 1.0628000497817993\n",
      "\t Training loss (single batch): 1.017220139503479\n",
      "\t Training loss (single batch): 1.161064863204956\n",
      "\t Training loss (single batch): 1.5307384729385376\n",
      "\t Training loss (single batch): 0.7605391144752502\n",
      "\t Training loss (single batch): 1.080148696899414\n",
      "\t Training loss (single batch): 0.853266179561615\n",
      "\t Training loss (single batch): 0.9376634955406189\n",
      "\t Training loss (single batch): 1.3516875505447388\n",
      "\t Training loss (single batch): 1.2929332256317139\n",
      "\t Training loss (single batch): 1.0361359119415283\n",
      "\t Training loss (single batch): 1.2733601331710815\n",
      "\t Training loss (single batch): 1.5079175233840942\n",
      "\t Training loss (single batch): 1.4242032766342163\n",
      "\t Training loss (single batch): 1.4475903511047363\n",
      "\t Training loss (single batch): 1.0329039096832275\n",
      "\t Training loss (single batch): 1.6215988397598267\n",
      "\t Training loss (single batch): 1.2619792222976685\n",
      "\t Training loss (single batch): 1.0869778394699097\n",
      "\t Training loss (single batch): 1.6415783166885376\n",
      "\t Training loss (single batch): 1.1618173122406006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1805044412612915\n",
      "\t Training loss (single batch): 1.913529634475708\n",
      "\t Training loss (single batch): 2.0199670791625977\n",
      "\t Training loss (single batch): 1.3561291694641113\n",
      "\t Training loss (single batch): 1.211061716079712\n",
      "\t Training loss (single batch): 1.235060214996338\n",
      "\t Training loss (single batch): 1.2339503765106201\n",
      "\t Training loss (single batch): 1.3567390441894531\n",
      "\t Training loss (single batch): 1.429844617843628\n",
      "\t Training loss (single batch): 1.4557702541351318\n",
      "\t Training loss (single batch): 1.2634788751602173\n",
      "\t Training loss (single batch): 0.9138975143432617\n",
      "\t Training loss (single batch): 1.0581212043762207\n",
      "\t Training loss (single batch): 1.2768218517303467\n",
      "\t Training loss (single batch): 1.6552735567092896\n",
      "\t Training loss (single batch): 1.786605715751648\n",
      "\t Training loss (single batch): 1.2579668760299683\n",
      "\t Training loss (single batch): 1.8555101156234741\n",
      "\t Training loss (single batch): 1.0346155166625977\n",
      "\t Training loss (single batch): 1.497617244720459\n",
      "\t Training loss (single batch): 1.4646379947662354\n",
      "\t Training loss (single batch): 1.4405748844146729\n",
      "\t Training loss (single batch): 1.6182427406311035\n",
      "\t Training loss (single batch): 1.1578572988510132\n",
      "\t Training loss (single batch): 0.9595670104026794\n",
      "\t Training loss (single batch): 1.6313334703445435\n",
      "\t Training loss (single batch): 1.3442002534866333\n",
      "\t Training loss (single batch): 0.8443403840065002\n",
      "\t Training loss (single batch): 0.8292813897132874\n",
      "\t Training loss (single batch): 1.9000402688980103\n",
      "\t Training loss (single batch): 1.0840014219284058\n",
      "\t Training loss (single batch): 1.3245668411254883\n",
      "\t Training loss (single batch): 1.1068940162658691\n",
      "\t Training loss (single batch): 1.3595781326293945\n",
      "\t Training loss (single batch): 1.2083840370178223\n",
      "\t Training loss (single batch): 1.637063980102539\n",
      "\t Training loss (single batch): 1.117150902748108\n",
      "\t Training loss (single batch): 1.4017388820648193\n",
      "\t Training loss (single batch): 1.797104001045227\n",
      "\t Training loss (single batch): 1.2767140865325928\n",
      "\t Training loss (single batch): 1.1991928815841675\n",
      "\t Training loss (single batch): 1.916953206062317\n",
      "\t Training loss (single batch): 1.0708699226379395\n",
      "\t Training loss (single batch): 1.3785969018936157\n",
      "\t Training loss (single batch): 1.0423991680145264\n",
      "\t Training loss (single batch): 1.5448453426361084\n",
      "\t Training loss (single batch): 1.414787769317627\n",
      "\t Training loss (single batch): 1.3127963542938232\n",
      "\t Training loss (single batch): 0.9801538586616516\n",
      "\t Training loss (single batch): 1.029550313949585\n",
      "\t Training loss (single batch): 1.1024399995803833\n",
      "\t Training loss (single batch): 1.3325800895690918\n",
      "\t Training loss (single batch): 1.0394608974456787\n",
      "\t Training loss (single batch): 1.4509574174880981\n",
      "\t Training loss (single batch): 1.108138918876648\n",
      "\t Training loss (single batch): 1.7215150594711304\n",
      "\t Training loss (single batch): 1.2732963562011719\n",
      "\t Training loss (single batch): 1.1063005924224854\n",
      "\t Training loss (single batch): 2.093163013458252\n",
      "\t Training loss (single batch): 1.2361946105957031\n",
      "\t Training loss (single batch): 1.1441280841827393\n",
      "\t Training loss (single batch): 1.7888197898864746\n",
      "\t Training loss (single batch): 1.7161248922348022\n",
      "\t Training loss (single batch): 1.3101041316986084\n",
      "\t Training loss (single batch): 1.1680389642715454\n",
      "\t Training loss (single batch): 1.0919482707977295\n",
      "\t Training loss (single batch): 1.2090144157409668\n",
      "\t Training loss (single batch): 1.5427016019821167\n",
      "\t Training loss (single batch): 1.4143263101577759\n",
      "\t Training loss (single batch): 1.7034988403320312\n",
      "\t Training loss (single batch): 1.3512554168701172\n",
      "\t Training loss (single batch): 1.0733567476272583\n",
      "\t Training loss (single batch): 1.3005144596099854\n",
      "\t Training loss (single batch): 1.0998417139053345\n",
      "\t Training loss (single batch): 0.9518306851387024\n",
      "\t Training loss (single batch): 1.5207557678222656\n",
      "\t Training loss (single batch): 1.7536886930465698\n",
      "\t Training loss (single batch): 1.0828473567962646\n",
      "\t Training loss (single batch): 1.092613697052002\n",
      "\t Training loss (single batch): 1.3179163932800293\n",
      "\t Training loss (single batch): 1.3150312900543213\n",
      "\t Training loss (single batch): 1.779044508934021\n",
      "\t Training loss (single batch): 1.0787856578826904\n",
      "\t Training loss (single batch): 1.1371803283691406\n",
      "\t Training loss (single batch): 1.1553635597229004\n",
      "\t Training loss (single batch): 1.3374558687210083\n",
      "\t Training loss (single batch): 1.1133105754852295\n",
      "\t Training loss (single batch): 1.222744345664978\n",
      "\t Training loss (single batch): 1.9648374319076538\n",
      "\t Training loss (single batch): 1.4036915302276611\n",
      "\t Training loss (single batch): 1.6141124963760376\n",
      "\t Training loss (single batch): 1.1600311994552612\n",
      "\t Training loss (single batch): 1.7503948211669922\n",
      "\t Training loss (single batch): 1.3556525707244873\n",
      "\t Training loss (single batch): 1.2487174272537231\n",
      "\t Training loss (single batch): 1.1784040927886963\n",
      "\t Training loss (single batch): 1.4183645248413086\n",
      "\t Training loss (single batch): 1.0690007209777832\n",
      "\t Training loss (single batch): 1.6335512399673462\n",
      "\t Training loss (single batch): 1.2026174068450928\n",
      "\t Training loss (single batch): 1.3496217727661133\n",
      "\t Training loss (single batch): 1.557955026626587\n",
      "\t Training loss (single batch): 1.0773131847381592\n",
      "\t Training loss (single batch): 1.252553939819336\n",
      "\t Training loss (single batch): 1.0833288431167603\n",
      "\t Training loss (single batch): 1.2097567319869995\n",
      "\t Training loss (single batch): 1.7381815910339355\n",
      "\t Training loss (single batch): 0.983362078666687\n",
      "\t Training loss (single batch): 1.7718225717544556\n",
      "\t Training loss (single batch): 1.2148833274841309\n",
      "\t Training loss (single batch): 0.9824003577232361\n",
      "\t Training loss (single batch): 1.2003748416900635\n",
      "\t Training loss (single batch): 1.7450007200241089\n",
      "\t Training loss (single batch): 1.5025520324707031\n",
      "\t Training loss (single batch): 1.0495474338531494\n",
      "\t Training loss (single batch): 1.3290021419525146\n",
      "\t Training loss (single batch): 1.1081531047821045\n",
      "\t Training loss (single batch): 1.1267011165618896\n",
      "\t Training loss (single batch): 1.297663927078247\n",
      "\t Training loss (single batch): 1.6083805561065674\n",
      "\t Training loss (single batch): 1.1894443035125732\n",
      "\t Training loss (single batch): 0.9207843542098999\n",
      "\t Training loss (single batch): 1.404482126235962\n",
      "\t Training loss (single batch): 1.3338582515716553\n",
      "\t Training loss (single batch): 0.7512845993041992\n",
      "\t Training loss (single batch): 1.0369280576705933\n",
      "\t Training loss (single batch): 1.1733341217041016\n",
      "\t Training loss (single batch): 1.3515830039978027\n",
      "\t Training loss (single batch): 1.2675573825836182\n",
      "\t Training loss (single batch): 1.0194993019104004\n",
      "\t Training loss (single batch): 1.2085784673690796\n",
      "\t Training loss (single batch): 1.2285040616989136\n",
      "\t Training loss (single batch): 0.9905917048454285\n",
      "\t Training loss (single batch): 1.2703064680099487\n",
      "\t Training loss (single batch): 1.4448401927947998\n",
      "\t Training loss (single batch): 1.0646412372589111\n",
      "\t Training loss (single batch): 1.2875763177871704\n",
      "\t Training loss (single batch): 1.4252293109893799\n",
      "\t Training loss (single batch): 1.100022554397583\n",
      "\t Training loss (single batch): 1.0160726308822632\n",
      "\t Training loss (single batch): 1.429879903793335\n",
      "\t Training loss (single batch): 0.9314905405044556\n",
      "\t Training loss (single batch): 1.088230013847351\n",
      "\t Training loss (single batch): 1.1432961225509644\n",
      "\t Training loss (single batch): 1.4907045364379883\n",
      "\t Training loss (single batch): 1.1835401058197021\n",
      "\t Training loss (single batch): 1.426529049873352\n",
      "\t Training loss (single batch): 1.3028980493545532\n",
      "\t Training loss (single batch): 1.3129717111587524\n",
      "\t Training loss (single batch): 1.0836416482925415\n",
      "\t Training loss (single batch): 1.1708329916000366\n",
      "\t Training loss (single batch): 1.2055002450942993\n",
      "\t Training loss (single batch): 0.9982016682624817\n",
      "\t Training loss (single batch): 1.3487416505813599\n",
      "\t Training loss (single batch): 1.3605327606201172\n",
      "\t Training loss (single batch): 0.9277928471565247\n",
      "\t Training loss (single batch): 1.4163957834243774\n",
      "\t Training loss (single batch): 1.0013693571090698\n",
      "\t Training loss (single batch): 0.9901033639907837\n",
      "\t Training loss (single batch): 1.3081974983215332\n",
      "\t Training loss (single batch): 1.7244758605957031\n",
      "\t Training loss (single batch): 1.1422966718673706\n",
      "\t Training loss (single batch): 1.1943511962890625\n",
      "\t Training loss (single batch): 0.7379271388053894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5619596242904663\n",
      "\t Training loss (single batch): 1.2785381078720093\n",
      "\t Training loss (single batch): 0.9120954871177673\n",
      "\t Training loss (single batch): 1.4033902883529663\n",
      "\t Training loss (single batch): 1.236641764640808\n",
      "\t Training loss (single batch): 0.9716967940330505\n",
      "\t Training loss (single batch): 1.0802713632583618\n",
      "\t Training loss (single batch): 1.451054573059082\n",
      "\t Training loss (single batch): 1.2323977947235107\n",
      "\t Training loss (single batch): 1.0400584936141968\n",
      "\t Training loss (single batch): 1.4256887435913086\n",
      "\t Training loss (single batch): 1.1387735605239868\n",
      "\t Training loss (single batch): 1.593770146369934\n",
      "\t Training loss (single batch): 1.68263578414917\n",
      "\t Training loss (single batch): 1.6603899002075195\n",
      "\t Training loss (single batch): 1.4081416130065918\n",
      "\t Training loss (single batch): 1.2881767749786377\n",
      "\t Training loss (single batch): 1.5690767765045166\n",
      "\t Training loss (single batch): 1.2446303367614746\n",
      "\t Training loss (single batch): 1.6832518577575684\n",
      "\t Training loss (single batch): 1.1912704706192017\n",
      "\t Training loss (single batch): 1.7298741340637207\n",
      "\t Training loss (single batch): 1.1561028957366943\n",
      "\t Training loss (single batch): 1.1057428121566772\n",
      "\t Training loss (single batch): 1.4350165128707886\n",
      "\t Training loss (single batch): 1.4192633628845215\n",
      "\t Training loss (single batch): 1.0751128196716309\n",
      "\t Training loss (single batch): 0.8864628672599792\n",
      "\t Training loss (single batch): 0.7638786435127258\n",
      "\t Training loss (single batch): 1.6531424522399902\n",
      "\t Training loss (single batch): 1.0910248756408691\n",
      "\t Training loss (single batch): 1.2159268856048584\n",
      "\t Training loss (single batch): 1.8768061399459839\n",
      "\t Training loss (single batch): 1.032413363456726\n",
      "\t Training loss (single batch): 1.197945475578308\n",
      "\t Training loss (single batch): 1.2968000173568726\n",
      "\t Training loss (single batch): 1.6387404203414917\n",
      "\t Training loss (single batch): 1.248494267463684\n",
      "\t Training loss (single batch): 1.2062572240829468\n",
      "\t Training loss (single batch): 2.021644353866577\n",
      "\t Training loss (single batch): 0.9855840802192688\n",
      "\t Training loss (single batch): 1.3207106590270996\n",
      "\t Training loss (single batch): 1.76036536693573\n",
      "\t Training loss (single batch): 1.7714197635650635\n",
      "\t Training loss (single batch): 0.962945282459259\n",
      "\t Training loss (single batch): 1.4593455791473389\n",
      "\t Training loss (single batch): 1.784705638885498\n",
      "\t Training loss (single batch): 1.2000166177749634\n",
      "\t Training loss (single batch): 1.142196774482727\n",
      "\t Training loss (single batch): 1.4948456287384033\n",
      "\t Training loss (single batch): 0.6695597171783447\n",
      "##################################\n",
      "## EPOCH 15\n",
      "##################################\n",
      "\t Training loss (single batch): 1.104068636894226\n",
      "\t Training loss (single batch): 0.7323524355888367\n",
      "\t Training loss (single batch): 1.5540727376937866\n",
      "\t Training loss (single batch): 1.3299394845962524\n",
      "\t Training loss (single batch): 0.8350155353546143\n",
      "\t Training loss (single batch): 1.187334656715393\n",
      "\t Training loss (single batch): 1.1416733264923096\n",
      "\t Training loss (single batch): 1.4605621099472046\n",
      "\t Training loss (single batch): 1.8364206552505493\n",
      "\t Training loss (single batch): 1.2117418050765991\n",
      "\t Training loss (single batch): 1.4196033477783203\n",
      "\t Training loss (single batch): 1.1579185724258423\n",
      "\t Training loss (single batch): 1.0560600757598877\n",
      "\t Training loss (single batch): 1.2774078845977783\n",
      "\t Training loss (single batch): 1.5242516994476318\n",
      "\t Training loss (single batch): 1.301087737083435\n",
      "\t Training loss (single batch): 1.4481757879257202\n",
      "\t Training loss (single batch): 1.3391071557998657\n",
      "\t Training loss (single batch): 1.8887516260147095\n",
      "\t Training loss (single batch): 1.144142746925354\n",
      "\t Training loss (single batch): 1.5558363199234009\n",
      "\t Training loss (single batch): 1.0908808708190918\n",
      "\t Training loss (single batch): 1.2287193536758423\n",
      "\t Training loss (single batch): 1.2911657094955444\n",
      "\t Training loss (single batch): 1.6006072759628296\n",
      "\t Training loss (single batch): 1.1662362813949585\n",
      "\t Training loss (single batch): 1.9157977104187012\n",
      "\t Training loss (single batch): 1.1858696937561035\n",
      "\t Training loss (single batch): 1.2898927927017212\n",
      "\t Training loss (single batch): 1.1853108406066895\n",
      "\t Training loss (single batch): 0.8858392238616943\n",
      "\t Training loss (single batch): 1.084276795387268\n",
      "\t Training loss (single batch): 0.718783974647522\n",
      "\t Training loss (single batch): 1.6749279499053955\n",
      "\t Training loss (single batch): 1.3244562149047852\n",
      "\t Training loss (single batch): 0.9836449027061462\n",
      "\t Training loss (single batch): 1.5071661472320557\n",
      "\t Training loss (single batch): 1.2260383367538452\n",
      "\t Training loss (single batch): 1.132467269897461\n",
      "\t Training loss (single batch): 1.741683840751648\n",
      "\t Training loss (single batch): 1.2611128091812134\n",
      "\t Training loss (single batch): 1.2117903232574463\n",
      "\t Training loss (single batch): 1.1372005939483643\n",
      "\t Training loss (single batch): 1.3585166931152344\n",
      "\t Training loss (single batch): 1.2346926927566528\n",
      "\t Training loss (single batch): 1.6951422691345215\n",
      "\t Training loss (single batch): 1.6247750520706177\n",
      "\t Training loss (single batch): 1.1633799076080322\n",
      "\t Training loss (single batch): 1.6702545881271362\n",
      "\t Training loss (single batch): 1.3333277702331543\n",
      "\t Training loss (single batch): 1.601086139678955\n",
      "\t Training loss (single batch): 1.1342816352844238\n",
      "\t Training loss (single batch): 1.357559323310852\n",
      "\t Training loss (single batch): 1.6805427074432373\n",
      "\t Training loss (single batch): 1.0737583637237549\n",
      "\t Training loss (single batch): 1.431723713874817\n",
      "\t Training loss (single batch): 1.2482645511627197\n",
      "\t Training loss (single batch): 1.559399127960205\n",
      "\t Training loss (single batch): 1.3268351554870605\n",
      "\t Training loss (single batch): 1.3137905597686768\n",
      "\t Training loss (single batch): 0.9317427277565002\n",
      "\t Training loss (single batch): 1.366341233253479\n",
      "\t Training loss (single batch): 1.4918177127838135\n",
      "\t Training loss (single batch): 1.2134249210357666\n",
      "\t Training loss (single batch): 1.4056166410446167\n",
      "\t Training loss (single batch): 1.8066682815551758\n",
      "\t Training loss (single batch): 1.431143045425415\n",
      "\t Training loss (single batch): 1.131137490272522\n",
      "\t Training loss (single batch): 1.1543484926223755\n",
      "\t Training loss (single batch): 1.2234910726547241\n",
      "\t Training loss (single batch): 1.1639946699142456\n",
      "\t Training loss (single batch): 1.7419592142105103\n",
      "\t Training loss (single batch): 1.159938931465149\n",
      "\t Training loss (single batch): 1.1477439403533936\n",
      "\t Training loss (single batch): 1.6394472122192383\n",
      "\t Training loss (single batch): 0.9147077798843384\n",
      "\t Training loss (single batch): 1.2162929773330688\n",
      "\t Training loss (single batch): 1.4661372900009155\n",
      "\t Training loss (single batch): 1.3442972898483276\n",
      "\t Training loss (single batch): 0.8246747851371765\n",
      "\t Training loss (single batch): 1.4400224685668945\n",
      "\t Training loss (single batch): 1.198338508605957\n",
      "\t Training loss (single batch): 1.1805675029754639\n",
      "\t Training loss (single batch): 1.252954125404358\n",
      "\t Training loss (single batch): 1.16560697555542\n",
      "\t Training loss (single batch): 1.0565009117126465\n",
      "\t Training loss (single batch): 1.1264828443527222\n",
      "\t Training loss (single batch): 1.214888334274292\n",
      "\t Training loss (single batch): 1.2396018505096436\n",
      "\t Training loss (single batch): 0.8575267195701599\n",
      "\t Training loss (single batch): 1.1068884134292603\n",
      "\t Training loss (single batch): 1.4975699186325073\n",
      "\t Training loss (single batch): 0.9572073817253113\n",
      "\t Training loss (single batch): 1.1179094314575195\n",
      "\t Training loss (single batch): 1.3488844633102417\n",
      "\t Training loss (single batch): 1.4470820426940918\n",
      "\t Training loss (single batch): 1.4953157901763916\n",
      "\t Training loss (single batch): 1.1992160081863403\n",
      "\t Training loss (single batch): 1.1733136177062988\n",
      "\t Training loss (single batch): 0.9897626638412476\n",
      "\t Training loss (single batch): 2.0235142707824707\n",
      "\t Training loss (single batch): 1.174912691116333\n",
      "\t Training loss (single batch): 1.296711802482605\n",
      "\t Training loss (single batch): 0.7926754951477051\n",
      "\t Training loss (single batch): 1.7803449630737305\n",
      "\t Training loss (single batch): 1.232832431793213\n",
      "\t Training loss (single batch): 0.9418172836303711\n",
      "\t Training loss (single batch): 1.5343542098999023\n",
      "\t Training loss (single batch): 1.906855821609497\n",
      "\t Training loss (single batch): 0.9271988272666931\n",
      "\t Training loss (single batch): 1.192075490951538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2253082990646362\n",
      "\t Training loss (single batch): 1.5849000215530396\n",
      "\t Training loss (single batch): 1.2611932754516602\n",
      "\t Training loss (single batch): 1.4976296424865723\n",
      "\t Training loss (single batch): 1.253883719444275\n",
      "\t Training loss (single batch): 1.4358155727386475\n",
      "\t Training loss (single batch): 0.9187881946563721\n",
      "\t Training loss (single batch): 1.4007912874221802\n",
      "\t Training loss (single batch): 0.9848747253417969\n",
      "\t Training loss (single batch): 1.5220011472702026\n",
      "\t Training loss (single batch): 0.9230234622955322\n",
      "\t Training loss (single batch): 0.9451313614845276\n",
      "\t Training loss (single batch): 1.0185348987579346\n",
      "\t Training loss (single batch): 1.192093014717102\n",
      "\t Training loss (single batch): 1.4864331483840942\n",
      "\t Training loss (single batch): 0.9541826844215393\n",
      "\t Training loss (single batch): 1.1188302040100098\n",
      "\t Training loss (single batch): 1.025866985321045\n",
      "\t Training loss (single batch): 0.9374371767044067\n",
      "\t Training loss (single batch): 1.3564175367355347\n",
      "\t Training loss (single batch): 1.3046866655349731\n",
      "\t Training loss (single batch): 1.3625601530075073\n",
      "\t Training loss (single batch): 1.0394270420074463\n",
      "\t Training loss (single batch): 1.4461989402770996\n",
      "\t Training loss (single batch): 0.7918792366981506\n",
      "\t Training loss (single batch): 1.275913953781128\n",
      "\t Training loss (single batch): 1.0595545768737793\n",
      "\t Training loss (single batch): 1.330944538116455\n",
      "\t Training loss (single batch): 1.210087537765503\n",
      "\t Training loss (single batch): 1.6970528364181519\n",
      "\t Training loss (single batch): 0.9846806526184082\n",
      "\t Training loss (single batch): 1.2421042919158936\n",
      "\t Training loss (single batch): 0.9373643398284912\n",
      "\t Training loss (single batch): 0.9461272358894348\n",
      "\t Training loss (single batch): 1.5092754364013672\n",
      "\t Training loss (single batch): 1.0375556945800781\n",
      "\t Training loss (single batch): 1.547264814376831\n",
      "\t Training loss (single batch): 1.4385323524475098\n",
      "\t Training loss (single batch): 1.0575346946716309\n",
      "\t Training loss (single batch): 1.6032769680023193\n",
      "\t Training loss (single batch): 1.1327474117279053\n",
      "\t Training loss (single batch): 1.1185128688812256\n",
      "\t Training loss (single batch): 1.2968742847442627\n",
      "\t Training loss (single batch): 1.497518539428711\n",
      "\t Training loss (single batch): 1.4553143978118896\n",
      "\t Training loss (single batch): 1.063206434249878\n",
      "\t Training loss (single batch): 1.2430918216705322\n",
      "\t Training loss (single batch): 1.3242195844650269\n",
      "\t Training loss (single batch): 1.179029107093811\n",
      "\t Training loss (single batch): 1.37497079372406\n",
      "\t Training loss (single batch): 1.6701266765594482\n",
      "\t Training loss (single batch): 1.1416053771972656\n",
      "\t Training loss (single batch): 1.7112983465194702\n",
      "\t Training loss (single batch): 1.541164755821228\n",
      "\t Training loss (single batch): 0.9764317274093628\n",
      "\t Training loss (single batch): 2.251504898071289\n",
      "\t Training loss (single batch): 1.4156248569488525\n",
      "\t Training loss (single batch): 1.2217926979064941\n",
      "\t Training loss (single batch): 1.4411909580230713\n",
      "\t Training loss (single batch): 1.3891193866729736\n",
      "\t Training loss (single batch): 1.54092538356781\n",
      "\t Training loss (single batch): 1.3527127504348755\n",
      "\t Training loss (single batch): 1.1090153455734253\n",
      "\t Training loss (single batch): 1.2807188034057617\n",
      "\t Training loss (single batch): 1.027127981185913\n",
      "\t Training loss (single batch): 0.9723929166793823\n",
      "\t Training loss (single batch): 1.346362829208374\n",
      "\t Training loss (single batch): 1.3546168804168701\n",
      "\t Training loss (single batch): 1.5501340627670288\n",
      "\t Training loss (single batch): 1.2497589588165283\n",
      "\t Training loss (single batch): 1.2690503597259521\n",
      "\t Training loss (single batch): 1.3716431856155396\n",
      "\t Training loss (single batch): 1.2883248329162598\n",
      "\t Training loss (single batch): 1.4244790077209473\n",
      "\t Training loss (single batch): 1.1913176774978638\n",
      "\t Training loss (single batch): 1.1254241466522217\n",
      "\t Training loss (single batch): 1.3156846761703491\n",
      "\t Training loss (single batch): 1.309499740600586\n",
      "\t Training loss (single batch): 1.5389951467514038\n",
      "\t Training loss (single batch): 1.1570490598678589\n",
      "\t Training loss (single batch): 1.2599982023239136\n",
      "\t Training loss (single batch): 1.3480584621429443\n",
      "\t Training loss (single batch): 1.6698102951049805\n",
      "\t Training loss (single batch): 1.1394814252853394\n",
      "\t Training loss (single batch): 1.5907540321350098\n",
      "\t Training loss (single batch): 1.6215177774429321\n",
      "\t Training loss (single batch): 1.014959454536438\n",
      "\t Training loss (single batch): 1.2895772457122803\n",
      "\t Training loss (single batch): 1.2358198165893555\n",
      "\t Training loss (single batch): 1.0955594778060913\n",
      "\t Training loss (single batch): 1.5491647720336914\n",
      "\t Training loss (single batch): 1.0478678941726685\n",
      "\t Training loss (single batch): 1.3226978778839111\n",
      "\t Training loss (single batch): 1.263677954673767\n",
      "\t Training loss (single batch): 1.5450835227966309\n",
      "\t Training loss (single batch): 1.1983723640441895\n",
      "\t Training loss (single batch): 1.5497907400131226\n",
      "\t Training loss (single batch): 1.8404693603515625\n",
      "\t Training loss (single batch): 1.2908952236175537\n",
      "\t Training loss (single batch): 1.2079218626022339\n",
      "\t Training loss (single batch): 1.1500999927520752\n",
      "\t Training loss (single batch): 1.113560676574707\n",
      "\t Training loss (single batch): 1.2067323923110962\n",
      "\t Training loss (single batch): 1.2915199995040894\n",
      "\t Training loss (single batch): 1.348597526550293\n",
      "\t Training loss (single batch): 1.3086668252944946\n",
      "\t Training loss (single batch): 1.318930983543396\n",
      "\t Training loss (single batch): 1.194957971572876\n",
      "\t Training loss (single batch): 2.1460163593292236\n",
      "\t Training loss (single batch): 0.9811565279960632\n",
      "\t Training loss (single batch): 0.9542608857154846\n",
      "\t Training loss (single batch): 0.9348123073577881\n",
      "\t Training loss (single batch): 1.570133924484253\n",
      "\t Training loss (single batch): 1.1117783784866333\n",
      "\t Training loss (single batch): 1.6316213607788086\n",
      "\t Training loss (single batch): 1.0099382400512695\n",
      "\t Training loss (single batch): 0.9664173126220703\n",
      "\t Training loss (single batch): 1.291613221168518\n",
      "\t Training loss (single batch): 1.3395193815231323\n",
      "\t Training loss (single batch): 1.3789288997650146\n",
      "\t Training loss (single batch): 1.2715728282928467\n",
      "\t Training loss (single batch): 1.6041837930679321\n",
      "\t Training loss (single batch): 1.1938799619674683\n",
      "\t Training loss (single batch): 1.3692505359649658\n",
      "\t Training loss (single batch): 1.5960619449615479\n",
      "\t Training loss (single batch): 1.8249831199645996\n",
      "\t Training loss (single batch): 1.3612866401672363\n",
      "\t Training loss (single batch): 1.1479536294937134\n",
      "\t Training loss (single batch): 0.7524324059486389\n",
      "\t Training loss (single batch): 1.2107739448547363\n",
      "\t Training loss (single batch): 1.0187890529632568\n",
      "\t Training loss (single batch): 1.9988433122634888\n",
      "\t Training loss (single batch): 1.3024386167526245\n",
      "\t Training loss (single batch): 0.9315064549446106\n",
      "\t Training loss (single batch): 1.6174958944320679\n",
      "\t Training loss (single batch): 1.4522314071655273\n",
      "\t Training loss (single batch): 0.8299560546875\n",
      "\t Training loss (single batch): 1.078254222869873\n",
      "\t Training loss (single batch): 1.2246681451797485\n",
      "\t Training loss (single batch): 1.2225865125656128\n",
      "\t Training loss (single batch): 1.203019142150879\n",
      "\t Training loss (single batch): 1.8859843015670776\n",
      "\t Training loss (single batch): 1.4853750467300415\n",
      "\t Training loss (single batch): 1.2765032052993774\n",
      "\t Training loss (single batch): 1.6890859603881836\n",
      "\t Training loss (single batch): 1.125687837600708\n",
      "\t Training loss (single batch): 1.129567265510559\n",
      "\t Training loss (single batch): 1.111892580986023\n",
      "\t Training loss (single batch): 1.0360690355300903\n",
      "\t Training loss (single batch): 1.2178436517715454\n",
      "\t Training loss (single batch): 1.2494597434997559\n",
      "\t Training loss (single batch): 1.6168726682662964\n",
      "\t Training loss (single batch): 1.038129448890686\n",
      "\t Training loss (single batch): 1.1883734464645386\n",
      "\t Training loss (single batch): 0.9721970558166504\n",
      "\t Training loss (single batch): 1.4367012977600098\n",
      "\t Training loss (single batch): 1.186278223991394\n",
      "\t Training loss (single batch): 1.0613572597503662\n",
      "\t Training loss (single batch): 1.3431785106658936\n",
      "\t Training loss (single batch): 1.3792613744735718\n",
      "\t Training loss (single batch): 1.481786847114563\n",
      "\t Training loss (single batch): 1.6489124298095703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.429865837097168\n",
      "\t Training loss (single batch): 1.0195356607437134\n",
      "\t Training loss (single batch): 1.7191972732543945\n",
      "\t Training loss (single batch): 1.244041085243225\n",
      "\t Training loss (single batch): 1.4082081317901611\n",
      "\t Training loss (single batch): 1.153599739074707\n",
      "\t Training loss (single batch): 1.45399010181427\n",
      "\t Training loss (single batch): 1.8319828510284424\n",
      "\t Training loss (single batch): 1.8725757598876953\n",
      "\t Training loss (single batch): 1.9612438678741455\n",
      "\t Training loss (single batch): 1.0208786725997925\n",
      "\t Training loss (single batch): 1.0162643194198608\n",
      "\t Training loss (single batch): 1.094556450843811\n",
      "\t Training loss (single batch): 1.0797888040542603\n",
      "\t Training loss (single batch): 1.479862928390503\n",
      "\t Training loss (single batch): 1.239607334136963\n",
      "\t Training loss (single batch): 1.6092984676361084\n",
      "\t Training loss (single batch): 1.110908031463623\n",
      "\t Training loss (single batch): 1.238273024559021\n",
      "\t Training loss (single batch): 0.9936933517456055\n",
      "\t Training loss (single batch): 1.19600510597229\n",
      "\t Training loss (single batch): 1.0448087453842163\n",
      "\t Training loss (single batch): 1.3512091636657715\n",
      "\t Training loss (single batch): 1.2865016460418701\n",
      "\t Training loss (single batch): 1.9450236558914185\n",
      "\t Training loss (single batch): 1.222443699836731\n",
      "\t Training loss (single batch): 0.7849383354187012\n",
      "\t Training loss (single batch): 1.2550417184829712\n",
      "\t Training loss (single batch): 1.5485727787017822\n",
      "\t Training loss (single batch): 0.8727023601531982\n",
      "\t Training loss (single batch): 1.121224045753479\n",
      "\t Training loss (single batch): 0.8694008588790894\n",
      "\t Training loss (single batch): 0.8249351382255554\n",
      "\t Training loss (single batch): 1.588965892791748\n",
      "\t Training loss (single batch): 1.5369011163711548\n",
      "\t Training loss (single batch): 1.686858057975769\n",
      "\t Training loss (single batch): 1.3151233196258545\n",
      "\t Training loss (single batch): 1.2970017194747925\n",
      "\t Training loss (single batch): 1.0479069948196411\n",
      "\t Training loss (single batch): 1.2032424211502075\n",
      "\t Training loss (single batch): 1.842848539352417\n",
      "\t Training loss (single batch): 1.1598535776138306\n",
      "\t Training loss (single batch): 1.3164759874343872\n",
      "\t Training loss (single batch): 0.6689980030059814\n",
      "##################################\n",
      "## EPOCH 16\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9520622491836548\n",
      "\t Training loss (single batch): 1.2891569137573242\n",
      "\t Training loss (single batch): 1.881644368171692\n",
      "\t Training loss (single batch): 1.9475055932998657\n",
      "\t Training loss (single batch): 1.5888073444366455\n",
      "\t Training loss (single batch): 1.280286192893982\n",
      "\t Training loss (single batch): 1.456250548362732\n",
      "\t Training loss (single batch): 1.2542651891708374\n",
      "\t Training loss (single batch): 0.865343451499939\n",
      "\t Training loss (single batch): 1.4008458852767944\n",
      "\t Training loss (single batch): 1.2078897953033447\n",
      "\t Training loss (single batch): 1.6047996282577515\n",
      "\t Training loss (single batch): 1.4940286874771118\n",
      "\t Training loss (single batch): 1.2125585079193115\n",
      "\t Training loss (single batch): 1.2531746625900269\n",
      "\t Training loss (single batch): 0.9299454689025879\n",
      "\t Training loss (single batch): 1.1939799785614014\n",
      "\t Training loss (single batch): 0.9537707567214966\n",
      "\t Training loss (single batch): 1.5546218156814575\n",
      "\t Training loss (single batch): 0.9716836810112\n",
      "\t Training loss (single batch): 0.9284936785697937\n",
      "\t Training loss (single batch): 0.9104704260826111\n",
      "\t Training loss (single batch): 1.171049952507019\n",
      "\t Training loss (single batch): 1.3510571718215942\n",
      "\t Training loss (single batch): 1.3736425638198853\n",
      "\t Training loss (single batch): 1.712683916091919\n",
      "\t Training loss (single batch): 1.8812915086746216\n",
      "\t Training loss (single batch): 1.1851441860198975\n",
      "\t Training loss (single batch): 1.5371800661087036\n",
      "\t Training loss (single batch): 1.2769966125488281\n",
      "\t Training loss (single batch): 1.1918386220932007\n",
      "\t Training loss (single batch): 1.554714322090149\n",
      "\t Training loss (single batch): 1.4608479738235474\n",
      "\t Training loss (single batch): 1.443896770477295\n",
      "\t Training loss (single batch): 1.2527230978012085\n",
      "\t Training loss (single batch): 1.0142072439193726\n",
      "\t Training loss (single batch): 1.4698325395584106\n",
      "\t Training loss (single batch): 1.5686312913894653\n",
      "\t Training loss (single batch): 0.8936085104942322\n",
      "\t Training loss (single batch): 1.2784240245819092\n",
      "\t Training loss (single batch): 1.1598681211471558\n",
      "\t Training loss (single batch): 1.0176198482513428\n",
      "\t Training loss (single batch): 1.556330919265747\n",
      "\t Training loss (single batch): 1.2207646369934082\n",
      "\t Training loss (single batch): 2.1763670444488525\n",
      "\t Training loss (single batch): 1.2642072439193726\n",
      "\t Training loss (single batch): 1.3783406019210815\n",
      "\t Training loss (single batch): 1.1742023229599\n",
      "\t Training loss (single batch): 1.227876901626587\n",
      "\t Training loss (single batch): 1.341389536857605\n",
      "\t Training loss (single batch): 1.2061388492584229\n",
      "\t Training loss (single batch): 1.3909956216812134\n",
      "\t Training loss (single batch): 1.3115800619125366\n",
      "\t Training loss (single batch): 1.140171766281128\n",
      "\t Training loss (single batch): 1.606893539428711\n",
      "\t Training loss (single batch): 1.0427557229995728\n",
      "\t Training loss (single batch): 1.3528344631195068\n",
      "\t Training loss (single batch): 0.8972599506378174\n",
      "\t Training loss (single batch): 1.1833977699279785\n",
      "\t Training loss (single batch): 0.842226505279541\n",
      "\t Training loss (single batch): 1.5948704481124878\n",
      "\t Training loss (single batch): 1.8811825513839722\n",
      "\t Training loss (single batch): 0.9312552809715271\n",
      "\t Training loss (single batch): 0.7414326667785645\n",
      "\t Training loss (single batch): 1.4509921073913574\n",
      "\t Training loss (single batch): 1.0990973711013794\n",
      "\t Training loss (single batch): 1.6580321788787842\n",
      "\t Training loss (single batch): 1.345166802406311\n",
      "\t Training loss (single batch): 1.693387508392334\n",
      "\t Training loss (single batch): 1.3738001585006714\n",
      "\t Training loss (single batch): 1.3704150915145874\n",
      "\t Training loss (single batch): 0.7972744107246399\n",
      "\t Training loss (single batch): 1.1814631223678589\n",
      "\t Training loss (single batch): 0.8954729437828064\n",
      "\t Training loss (single batch): 1.4324935674667358\n",
      "\t Training loss (single batch): 1.2462108135223389\n",
      "\t Training loss (single batch): 1.2741777896881104\n",
      "\t Training loss (single batch): 1.1188468933105469\n",
      "\t Training loss (single batch): 1.1177951097488403\n",
      "\t Training loss (single batch): 1.1880426406860352\n",
      "\t Training loss (single batch): 1.076674222946167\n",
      "\t Training loss (single batch): 1.5216940641403198\n",
      "\t Training loss (single batch): 1.1208051443099976\n",
      "\t Training loss (single batch): 1.323311448097229\n",
      "\t Training loss (single batch): 1.4658178091049194\n",
      "\t Training loss (single batch): 1.3341690301895142\n",
      "\t Training loss (single batch): 0.892230749130249\n",
      "\t Training loss (single batch): 1.3982363939285278\n",
      "\t Training loss (single batch): 1.5528719425201416\n",
      "\t Training loss (single batch): 1.302211880683899\n",
      "\t Training loss (single batch): 0.9684733152389526\n",
      "\t Training loss (single batch): 1.3659942150115967\n",
      "\t Training loss (single batch): 1.6079045534133911\n",
      "\t Training loss (single batch): 0.9781072735786438\n",
      "\t Training loss (single batch): 1.0793880224227905\n",
      "\t Training loss (single batch): 1.3307102918624878\n",
      "\t Training loss (single batch): 0.7958152890205383\n",
      "\t Training loss (single batch): 1.2123943567276\n",
      "\t Training loss (single batch): 1.0025869607925415\n",
      "\t Training loss (single batch): 1.0628324747085571\n",
      "\t Training loss (single batch): 1.3837480545043945\n",
      "\t Training loss (single batch): 1.1565299034118652\n",
      "\t Training loss (single batch): 1.35570228099823\n",
      "\t Training loss (single batch): 1.4435503482818604\n",
      "\t Training loss (single batch): 1.1547513008117676\n",
      "\t Training loss (single batch): 1.1298807859420776\n",
      "\t Training loss (single batch): 1.7156550884246826\n",
      "\t Training loss (single batch): 1.343437671661377\n",
      "\t Training loss (single batch): 1.6068475246429443\n",
      "\t Training loss (single batch): 1.3726078271865845\n",
      "\t Training loss (single batch): 1.2596242427825928\n",
      "\t Training loss (single batch): 1.1127125024795532\n",
      "\t Training loss (single batch): 1.1010668277740479\n",
      "\t Training loss (single batch): 1.2913408279418945\n",
      "\t Training loss (single batch): 1.1610567569732666\n",
      "\t Training loss (single batch): 1.617485523223877\n",
      "\t Training loss (single batch): 1.3391833305358887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2995983362197876\n",
      "\t Training loss (single batch): 1.7845079898834229\n",
      "\t Training loss (single batch): 1.5587706565856934\n",
      "\t Training loss (single batch): 1.0143496990203857\n",
      "\t Training loss (single batch): 1.3304903507232666\n",
      "\t Training loss (single batch): 1.1345856189727783\n",
      "\t Training loss (single batch): 1.181640625\n",
      "\t Training loss (single batch): 1.092161774635315\n",
      "\t Training loss (single batch): 0.853418231010437\n",
      "\t Training loss (single batch): 1.1831514835357666\n",
      "\t Training loss (single batch): 1.5691429376602173\n",
      "\t Training loss (single batch): 1.0562649965286255\n",
      "\t Training loss (single batch): 1.0929841995239258\n",
      "\t Training loss (single batch): 1.2506327629089355\n",
      "\t Training loss (single batch): 1.1553970575332642\n",
      "\t Training loss (single batch): 1.3714525699615479\n",
      "\t Training loss (single batch): 0.9845412373542786\n",
      "\t Training loss (single batch): 1.5229159593582153\n",
      "\t Training loss (single batch): 1.060503363609314\n",
      "\t Training loss (single batch): 1.2613080739974976\n",
      "\t Training loss (single batch): 1.062544584274292\n",
      "\t Training loss (single batch): 1.5249686241149902\n",
      "\t Training loss (single batch): 1.23358154296875\n",
      "\t Training loss (single batch): 1.499831199645996\n",
      "\t Training loss (single batch): 1.575691819190979\n",
      "\t Training loss (single batch): 1.2141364812850952\n",
      "\t Training loss (single batch): 0.9478200674057007\n",
      "\t Training loss (single batch): 1.2725085020065308\n",
      "\t Training loss (single batch): 1.2565325498580933\n",
      "\t Training loss (single batch): 1.030193567276001\n",
      "\t Training loss (single batch): 1.1148384809494019\n",
      "\t Training loss (single batch): 1.296147108078003\n",
      "\t Training loss (single batch): 1.4424493312835693\n",
      "\t Training loss (single batch): 1.3080030679702759\n",
      "\t Training loss (single batch): 1.4642176628112793\n",
      "\t Training loss (single batch): 0.8292134404182434\n",
      "\t Training loss (single batch): 0.9892858862876892\n",
      "\t Training loss (single batch): 1.509649634361267\n",
      "\t Training loss (single batch): 1.5703169107437134\n",
      "\t Training loss (single batch): 1.1066169738769531\n",
      "\t Training loss (single batch): 1.2923444509506226\n",
      "\t Training loss (single batch): 1.1498076915740967\n",
      "\t Training loss (single batch): 1.0860583782196045\n",
      "\t Training loss (single batch): 1.5421719551086426\n",
      "\t Training loss (single batch): 1.3681062459945679\n",
      "\t Training loss (single batch): 1.775762915611267\n",
      "\t Training loss (single batch): 1.1415753364562988\n",
      "\t Training loss (single batch): 1.1436805725097656\n",
      "\t Training loss (single batch): 1.262261152267456\n",
      "\t Training loss (single batch): 1.2116938829421997\n",
      "\t Training loss (single batch): 0.736254096031189\n",
      "\t Training loss (single batch): 1.207649827003479\n",
      "\t Training loss (single batch): 1.1038706302642822\n",
      "\t Training loss (single batch): 1.4910967350006104\n",
      "\t Training loss (single batch): 1.5887644290924072\n",
      "\t Training loss (single batch): 1.0356299877166748\n",
      "\t Training loss (single batch): 1.0665756464004517\n",
      "\t Training loss (single batch): 1.235812783241272\n",
      "\t Training loss (single batch): 1.3921048641204834\n",
      "\t Training loss (single batch): 0.9325509667396545\n",
      "\t Training loss (single batch): 1.2359967231750488\n",
      "\t Training loss (single batch): 1.3623985052108765\n",
      "\t Training loss (single batch): 0.9127823114395142\n",
      "\t Training loss (single batch): 1.7046302556991577\n",
      "\t Training loss (single batch): 1.8353521823883057\n",
      "\t Training loss (single batch): 1.083423376083374\n",
      "\t Training loss (single batch): 1.550473690032959\n",
      "\t Training loss (single batch): 1.0737240314483643\n",
      "\t Training loss (single batch): 1.0057586431503296\n",
      "\t Training loss (single batch): 1.1795544624328613\n",
      "\t Training loss (single batch): 1.0188803672790527\n",
      "\t Training loss (single batch): 1.111478328704834\n",
      "\t Training loss (single batch): 0.87571120262146\n",
      "\t Training loss (single batch): 1.247787594795227\n",
      "\t Training loss (single batch): 1.0534782409667969\n",
      "\t Training loss (single batch): 1.3754174709320068\n",
      "\t Training loss (single batch): 1.5075443983078003\n",
      "\t Training loss (single batch): 1.630466341972351\n",
      "\t Training loss (single batch): 0.8116620779037476\n",
      "\t Training loss (single batch): 0.956870436668396\n",
      "\t Training loss (single batch): 1.066676378250122\n",
      "\t Training loss (single batch): 1.1571013927459717\n",
      "\t Training loss (single batch): 1.079987645149231\n",
      "\t Training loss (single batch): 1.2810851335525513\n",
      "\t Training loss (single batch): 1.1857209205627441\n",
      "\t Training loss (single batch): 1.4019912481307983\n",
      "\t Training loss (single batch): 1.4780347347259521\n",
      "\t Training loss (single batch): 1.7738982439041138\n",
      "\t Training loss (single batch): 1.500368356704712\n",
      "\t Training loss (single batch): 0.9012908339500427\n",
      "\t Training loss (single batch): 1.3151881694793701\n",
      "\t Training loss (single batch): 0.5965138673782349\n",
      "\t Training loss (single batch): 1.6735661029815674\n",
      "\t Training loss (single batch): 1.3263763189315796\n",
      "\t Training loss (single batch): 1.0338362455368042\n",
      "\t Training loss (single batch): 1.079473614692688\n",
      "\t Training loss (single batch): 1.454767107963562\n",
      "\t Training loss (single batch): 1.182722806930542\n",
      "\t Training loss (single batch): 1.58281672000885\n",
      "\t Training loss (single batch): 1.2455488443374634\n",
      "\t Training loss (single batch): 1.0719972848892212\n",
      "\t Training loss (single batch): 1.8096798658370972\n",
      "\t Training loss (single batch): 1.4001224040985107\n",
      "\t Training loss (single batch): 1.0009047985076904\n",
      "\t Training loss (single batch): 1.6397818326950073\n",
      "\t Training loss (single batch): 1.2626423835754395\n",
      "\t Training loss (single batch): 1.6056214570999146\n",
      "\t Training loss (single batch): 1.2298070192337036\n",
      "\t Training loss (single batch): 1.5856572389602661\n",
      "\t Training loss (single batch): 1.1697534322738647\n",
      "\t Training loss (single batch): 1.6388431787490845\n",
      "\t Training loss (single batch): 1.8492894172668457\n",
      "\t Training loss (single batch): 0.8856971263885498\n",
      "\t Training loss (single batch): 1.534436583518982\n",
      "\t Training loss (single batch): 1.7737308740615845\n",
      "\t Training loss (single batch): 1.294830083847046\n",
      "\t Training loss (single batch): 0.9768742322921753\n",
      "\t Training loss (single batch): 1.5031088590621948\n",
      "\t Training loss (single batch): 1.0464881658554077\n",
      "\t Training loss (single batch): 1.3561573028564453\n",
      "\t Training loss (single batch): 1.579215168952942\n",
      "\t Training loss (single batch): 1.005197286605835\n",
      "\t Training loss (single batch): 1.0931963920593262\n",
      "\t Training loss (single batch): 1.230177879333496\n",
      "\t Training loss (single batch): 1.8551939725875854\n",
      "\t Training loss (single batch): 0.9965037703514099\n",
      "\t Training loss (single batch): 1.006308674812317\n",
      "\t Training loss (single batch): 1.1886204481124878\n",
      "\t Training loss (single batch): 1.211338996887207\n",
      "\t Training loss (single batch): 1.4912302494049072\n",
      "\t Training loss (single batch): 1.1394280195236206\n",
      "\t Training loss (single batch): 1.3688609600067139\n",
      "\t Training loss (single batch): 1.108039379119873\n",
      "\t Training loss (single batch): 1.3928911685943604\n",
      "\t Training loss (single batch): 1.3140811920166016\n",
      "\t Training loss (single batch): 1.4042953252792358\n",
      "\t Training loss (single batch): 1.653956413269043\n",
      "\t Training loss (single batch): 1.1335681676864624\n",
      "\t Training loss (single batch): 1.3654817342758179\n",
      "\t Training loss (single batch): 1.0434030294418335\n",
      "\t Training loss (single batch): 1.4759538173675537\n",
      "\t Training loss (single batch): 1.5692614316940308\n",
      "\t Training loss (single batch): 1.6951189041137695\n",
      "\t Training loss (single batch): 1.0559241771697998\n",
      "\t Training loss (single batch): 1.0146211385726929\n",
      "\t Training loss (single batch): 1.1095354557037354\n",
      "\t Training loss (single batch): 1.4718773365020752\n",
      "\t Training loss (single batch): 1.2144451141357422\n",
      "\t Training loss (single batch): 1.1263689994812012\n",
      "\t Training loss (single batch): 1.262921929359436\n",
      "\t Training loss (single batch): 1.0038743019104004\n",
      "\t Training loss (single batch): 1.343870997428894\n",
      "\t Training loss (single batch): 0.7157697677612305\n",
      "\t Training loss (single batch): 1.6474283933639526\n",
      "\t Training loss (single batch): 1.2861131429672241\n",
      "\t Training loss (single batch): 1.6284974813461304\n",
      "\t Training loss (single batch): 0.7499370574951172\n",
      "\t Training loss (single batch): 1.4032450914382935\n",
      "\t Training loss (single batch): 0.9663874506950378\n",
      "\t Training loss (single batch): 1.100928783416748\n",
      "\t Training loss (single batch): 1.1073719263076782\n",
      "\t Training loss (single batch): 1.0899039506912231\n",
      "\t Training loss (single batch): 1.8051953315734863\n",
      "\t Training loss (single batch): 1.4669373035430908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8548428416252136\n",
      "\t Training loss (single batch): 1.1089049577713013\n",
      "\t Training loss (single batch): 0.9879438877105713\n",
      "\t Training loss (single batch): 1.2790080308914185\n",
      "\t Training loss (single batch): 1.4950966835021973\n",
      "\t Training loss (single batch): 1.052396297454834\n",
      "\t Training loss (single batch): 1.2375072240829468\n",
      "\t Training loss (single batch): 1.3027318716049194\n",
      "\t Training loss (single batch): 1.3523740768432617\n",
      "\t Training loss (single batch): 1.337504267692566\n",
      "\t Training loss (single batch): 1.582207441329956\n",
      "\t Training loss (single batch): 1.1843197345733643\n",
      "\t Training loss (single batch): 1.0593256950378418\n",
      "\t Training loss (single batch): 0.8266246318817139\n",
      "\t Training loss (single batch): 1.3331307172775269\n",
      "\t Training loss (single batch): 0.9788745641708374\n",
      "\t Training loss (single batch): 1.507774829864502\n",
      "\t Training loss (single batch): 1.2786059379577637\n",
      "\t Training loss (single batch): 1.8455413579940796\n",
      "\t Training loss (single batch): 1.2709693908691406\n",
      "\t Training loss (single batch): 0.9625354409217834\n",
      "\t Training loss (single batch): 1.2487713098526\n",
      "\t Training loss (single batch): 1.7227824926376343\n",
      "\t Training loss (single batch): 1.5484248399734497\n",
      "\t Training loss (single batch): 1.2066936492919922\n",
      "\t Training loss (single batch): 1.2223689556121826\n",
      "\t Training loss (single batch): 1.0625704526901245\n",
      "\t Training loss (single batch): 0.9426475763320923\n",
      "\t Training loss (single batch): 1.01844322681427\n",
      "\t Training loss (single batch): 1.4671642780303955\n",
      "\t Training loss (single batch): 1.0987012386322021\n",
      "\t Training loss (single batch): 0.8612128496170044\n",
      "\t Training loss (single batch): 1.453650712966919\n",
      "\t Training loss (single batch): 1.3366758823394775\n",
      "\t Training loss (single batch): 1.0285524129867554\n",
      "\t Training loss (single batch): 0.6027485728263855\n",
      "##################################\n",
      "## EPOCH 17\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2587285041809082\n",
      "\t Training loss (single batch): 1.462810754776001\n",
      "\t Training loss (single batch): 1.1710443496704102\n",
      "\t Training loss (single batch): 1.2910877466201782\n",
      "\t Training loss (single batch): 1.3473906517028809\n",
      "\t Training loss (single batch): 1.4649865627288818\n",
      "\t Training loss (single batch): 0.794999897480011\n",
      "\t Training loss (single batch): 1.2225161790847778\n",
      "\t Training loss (single batch): 1.2094652652740479\n",
      "\t Training loss (single batch): 1.605635166168213\n",
      "\t Training loss (single batch): 1.3071287870407104\n",
      "\t Training loss (single batch): 0.9362877011299133\n",
      "\t Training loss (single batch): 1.1606359481811523\n",
      "\t Training loss (single batch): 1.2306158542633057\n",
      "\t Training loss (single batch): 1.1753419637680054\n",
      "\t Training loss (single batch): 1.405787706375122\n",
      "\t Training loss (single batch): 1.7058367729187012\n",
      "\t Training loss (single batch): 1.4790663719177246\n",
      "\t Training loss (single batch): 1.1706609725952148\n",
      "\t Training loss (single batch): 1.248035192489624\n",
      "\t Training loss (single batch): 1.2820143699645996\n",
      "\t Training loss (single batch): 1.70012366771698\n",
      "\t Training loss (single batch): 1.4326252937316895\n",
      "\t Training loss (single batch): 1.1121667623519897\n",
      "\t Training loss (single batch): 2.061673164367676\n",
      "\t Training loss (single batch): 1.2962560653686523\n",
      "\t Training loss (single batch): 0.9633581638336182\n",
      "\t Training loss (single batch): 0.7894196510314941\n",
      "\t Training loss (single batch): 1.1401139497756958\n",
      "\t Training loss (single batch): 0.9364436864852905\n",
      "\t Training loss (single batch): 1.2466553449630737\n",
      "\t Training loss (single batch): 1.123866319656372\n",
      "\t Training loss (single batch): 0.9985606074333191\n",
      "\t Training loss (single batch): 1.3595091104507446\n",
      "\t Training loss (single batch): 1.8759639263153076\n",
      "\t Training loss (single batch): 1.2284643650054932\n",
      "\t Training loss (single batch): 1.5572443008422852\n",
      "\t Training loss (single batch): 1.4038888216018677\n",
      "\t Training loss (single batch): 1.171785593032837\n",
      "\t Training loss (single batch): 0.8919342160224915\n",
      "\t Training loss (single batch): 1.2712434530258179\n",
      "\t Training loss (single batch): 1.2445138692855835\n",
      "\t Training loss (single batch): 1.8188505172729492\n",
      "\t Training loss (single batch): 1.532089114189148\n",
      "\t Training loss (single batch): 1.3307478427886963\n",
      "\t Training loss (single batch): 1.6919904947280884\n",
      "\t Training loss (single batch): 1.353211522102356\n",
      "\t Training loss (single batch): 1.4157629013061523\n",
      "\t Training loss (single batch): 1.7439378499984741\n",
      "\t Training loss (single batch): 0.9439475536346436\n",
      "\t Training loss (single batch): 1.7309014797210693\n",
      "\t Training loss (single batch): 1.5234473943710327\n",
      "\t Training loss (single batch): 1.1217271089553833\n",
      "\t Training loss (single batch): 1.0167253017425537\n",
      "\t Training loss (single batch): 1.0515223741531372\n",
      "\t Training loss (single batch): 1.3660309314727783\n",
      "\t Training loss (single batch): 1.2057243585586548\n",
      "\t Training loss (single batch): 1.5008574724197388\n",
      "\t Training loss (single batch): 1.2282334566116333\n",
      "\t Training loss (single batch): 1.2224717140197754\n",
      "\t Training loss (single batch): 0.948060929775238\n",
      "\t Training loss (single batch): 1.728858470916748\n",
      "\t Training loss (single batch): 1.116782784461975\n",
      "\t Training loss (single batch): 1.3072259426116943\n",
      "\t Training loss (single batch): 1.5383076667785645\n",
      "\t Training loss (single batch): 1.2931733131408691\n",
      "\t Training loss (single batch): 1.3923531770706177\n",
      "\t Training loss (single batch): 1.5865448713302612\n",
      "\t Training loss (single batch): 1.0149204730987549\n",
      "\t Training loss (single batch): 1.2428961992263794\n",
      "\t Training loss (single batch): 1.760562777519226\n",
      "\t Training loss (single batch): 1.2579671144485474\n",
      "\t Training loss (single batch): 1.1299974918365479\n",
      "\t Training loss (single batch): 1.4135242700576782\n",
      "\t Training loss (single batch): 1.0131229162216187\n",
      "\t Training loss (single batch): 1.4220706224441528\n",
      "\t Training loss (single batch): 1.8445173501968384\n",
      "\t Training loss (single batch): 1.0379842519760132\n",
      "\t Training loss (single batch): 1.4270832538604736\n",
      "\t Training loss (single batch): 1.2179819345474243\n",
      "\t Training loss (single batch): 1.3125883340835571\n",
      "\t Training loss (single batch): 1.338352918624878\n",
      "\t Training loss (single batch): 1.526710867881775\n",
      "\t Training loss (single batch): 1.349258303642273\n",
      "\t Training loss (single batch): 1.0892192125320435\n",
      "\t Training loss (single batch): 1.0374373197555542\n",
      "\t Training loss (single batch): 0.9481967687606812\n",
      "\t Training loss (single batch): 1.189186930656433\n",
      "\t Training loss (single batch): 1.6488491296768188\n",
      "\t Training loss (single batch): 1.186577558517456\n",
      "\t Training loss (single batch): 1.5991055965423584\n",
      "\t Training loss (single batch): 1.1160778999328613\n",
      "\t Training loss (single batch): 1.1038297414779663\n",
      "\t Training loss (single batch): 1.4275110960006714\n",
      "\t Training loss (single batch): 1.3446892499923706\n",
      "\t Training loss (single batch): 1.1208195686340332\n",
      "\t Training loss (single batch): 1.1106237173080444\n",
      "\t Training loss (single batch): 1.6594141721725464\n",
      "\t Training loss (single batch): 1.9077820777893066\n",
      "\t Training loss (single batch): 0.9967276453971863\n",
      "\t Training loss (single batch): 1.5013009309768677\n",
      "\t Training loss (single batch): 1.579815149307251\n",
      "\t Training loss (single batch): 1.1348257064819336\n",
      "\t Training loss (single batch): 1.2270402908325195\n",
      "\t Training loss (single batch): 1.461256980895996\n",
      "\t Training loss (single batch): 1.1669723987579346\n",
      "\t Training loss (single batch): 1.2487716674804688\n",
      "\t Training loss (single batch): 1.4231337308883667\n",
      "\t Training loss (single batch): 0.9982383251190186\n",
      "\t Training loss (single batch): 0.6341577768325806\n",
      "\t Training loss (single batch): 0.9717221260070801\n",
      "\t Training loss (single batch): 0.9092285633087158\n",
      "\t Training loss (single batch): 0.8723105192184448\n",
      "\t Training loss (single batch): 1.380711317062378\n",
      "\t Training loss (single batch): 1.5992236137390137\n",
      "\t Training loss (single batch): 1.5255353450775146\n",
      "\t Training loss (single batch): 1.8310309648513794\n",
      "\t Training loss (single batch): 1.0204110145568848\n",
      "\t Training loss (single batch): 1.74122154712677\n",
      "\t Training loss (single batch): 1.1507915258407593\n",
      "\t Training loss (single batch): 0.9720368981361389\n",
      "\t Training loss (single batch): 0.9928616881370544\n",
      "\t Training loss (single batch): 1.1495264768600464\n",
      "\t Training loss (single batch): 1.2802937030792236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.156543254852295\n",
      "\t Training loss (single batch): 1.3514589071273804\n",
      "\t Training loss (single batch): 1.363647699356079\n",
      "\t Training loss (single batch): 0.9948141574859619\n",
      "\t Training loss (single batch): 1.6988180875778198\n",
      "\t Training loss (single batch): 1.2047696113586426\n",
      "\t Training loss (single batch): 1.0918934345245361\n",
      "\t Training loss (single batch): 0.9086897969245911\n",
      "\t Training loss (single batch): 1.1825178861618042\n",
      "\t Training loss (single batch): 1.4231278896331787\n",
      "\t Training loss (single batch): 1.4164421558380127\n",
      "\t Training loss (single batch): 1.638239860534668\n",
      "\t Training loss (single batch): 0.856007993221283\n",
      "\t Training loss (single batch): 1.5481650829315186\n",
      "\t Training loss (single batch): 1.5461804866790771\n",
      "\t Training loss (single batch): 0.9762758016586304\n",
      "\t Training loss (single batch): 1.3472837209701538\n",
      "\t Training loss (single batch): 0.991023063659668\n",
      "\t Training loss (single batch): 1.362121343612671\n",
      "\t Training loss (single batch): 1.5956287384033203\n",
      "\t Training loss (single batch): 1.2775150537490845\n",
      "\t Training loss (single batch): 1.0195047855377197\n",
      "\t Training loss (single batch): 1.067563533782959\n",
      "\t Training loss (single batch): 0.805330216884613\n",
      "\t Training loss (single batch): 1.1865301132202148\n",
      "\t Training loss (single batch): 1.416116714477539\n",
      "\t Training loss (single batch): 1.0443071126937866\n",
      "\t Training loss (single batch): 1.3762516975402832\n",
      "\t Training loss (single batch): 1.1126407384872437\n",
      "\t Training loss (single batch): 1.625199794769287\n",
      "\t Training loss (single batch): 1.3041048049926758\n",
      "\t Training loss (single batch): 0.831281304359436\n",
      "\t Training loss (single batch): 0.7399954795837402\n",
      "\t Training loss (single batch): 1.3488798141479492\n",
      "\t Training loss (single batch): 1.1121841669082642\n",
      "\t Training loss (single batch): 1.0049916505813599\n",
      "\t Training loss (single batch): 1.0686842203140259\n",
      "\t Training loss (single batch): 1.3823215961456299\n",
      "\t Training loss (single batch): 1.1504199504852295\n",
      "\t Training loss (single batch): 1.3441059589385986\n",
      "\t Training loss (single batch): 1.3232098817825317\n",
      "\t Training loss (single batch): 1.1265350580215454\n",
      "\t Training loss (single batch): 1.1558386087417603\n",
      "\t Training loss (single batch): 1.4873805046081543\n",
      "\t Training loss (single batch): 1.1299017667770386\n",
      "\t Training loss (single batch): 0.899215817451477\n",
      "\t Training loss (single batch): 1.1693812608718872\n",
      "\t Training loss (single batch): 1.15318763256073\n",
      "\t Training loss (single batch): 1.2121224403381348\n",
      "\t Training loss (single batch): 1.0436146259307861\n",
      "\t Training loss (single batch): 1.2957830429077148\n",
      "\t Training loss (single batch): 1.349463701248169\n",
      "\t Training loss (single batch): 1.1263008117675781\n",
      "\t Training loss (single batch): 1.1119946241378784\n",
      "\t Training loss (single batch): 1.6391407251358032\n",
      "\t Training loss (single batch): 1.9511070251464844\n",
      "\t Training loss (single batch): 1.4746981859207153\n",
      "\t Training loss (single batch): 0.9323132038116455\n",
      "\t Training loss (single batch): 1.0417228937149048\n",
      "\t Training loss (single batch): 1.4895869493484497\n",
      "\t Training loss (single batch): 1.26160728931427\n",
      "\t Training loss (single batch): 1.7891651391983032\n",
      "\t Training loss (single batch): 1.0479053258895874\n",
      "\t Training loss (single batch): 0.9271963834762573\n",
      "\t Training loss (single batch): 1.0229053497314453\n",
      "\t Training loss (single batch): 1.5044972896575928\n",
      "\t Training loss (single batch): 1.4844982624053955\n",
      "\t Training loss (single batch): 0.997822105884552\n",
      "\t Training loss (single batch): 1.2730827331542969\n",
      "\t Training loss (single batch): 0.9809097647666931\n",
      "\t Training loss (single batch): 1.084051489830017\n",
      "\t Training loss (single batch): 1.5588655471801758\n",
      "\t Training loss (single batch): 0.9707471132278442\n",
      "\t Training loss (single batch): 1.2844418287277222\n",
      "\t Training loss (single batch): 1.3461995124816895\n",
      "\t Training loss (single batch): 1.43954336643219\n",
      "\t Training loss (single batch): 1.547385811805725\n",
      "\t Training loss (single batch): 1.1967320442199707\n",
      "\t Training loss (single batch): 1.216828465461731\n",
      "\t Training loss (single batch): 1.0503422021865845\n",
      "\t Training loss (single batch): 1.0295336246490479\n",
      "\t Training loss (single batch): 1.973433256149292\n",
      "\t Training loss (single batch): 1.2389780282974243\n",
      "\t Training loss (single batch): 1.4574213027954102\n",
      "\t Training loss (single batch): 1.3594715595245361\n",
      "\t Training loss (single batch): 1.0697365999221802\n",
      "\t Training loss (single batch): 0.8518339395523071\n",
      "\t Training loss (single batch): 1.4264203310012817\n",
      "\t Training loss (single batch): 1.3282079696655273\n",
      "\t Training loss (single batch): 0.9682364463806152\n",
      "\t Training loss (single batch): 0.9737945795059204\n",
      "\t Training loss (single batch): 1.7212939262390137\n",
      "\t Training loss (single batch): 1.3589229583740234\n",
      "\t Training loss (single batch): 1.624979853630066\n",
      "\t Training loss (single batch): 1.310552954673767\n",
      "\t Training loss (single batch): 1.0609079599380493\n",
      "\t Training loss (single batch): 0.9393779039382935\n",
      "\t Training loss (single batch): 1.256671667098999\n",
      "\t Training loss (single batch): 1.3577070236206055\n",
      "\t Training loss (single batch): 0.9363494515419006\n",
      "\t Training loss (single batch): 1.2085002660751343\n",
      "\t Training loss (single batch): 1.7203081846237183\n",
      "\t Training loss (single batch): 1.0808322429656982\n",
      "\t Training loss (single batch): 1.3666311502456665\n",
      "\t Training loss (single batch): 0.8813596367835999\n",
      "\t Training loss (single batch): 1.2391570806503296\n",
      "\t Training loss (single batch): 1.5341514348983765\n",
      "\t Training loss (single batch): 1.221944808959961\n",
      "\t Training loss (single batch): 1.2975894212722778\n",
      "\t Training loss (single batch): 1.5009968280792236\n",
      "\t Training loss (single batch): 1.1758371591567993\n",
      "\t Training loss (single batch): 1.5020145177841187\n",
      "\t Training loss (single batch): 1.1021558046340942\n",
      "\t Training loss (single batch): 1.3105593919754028\n",
      "\t Training loss (single batch): 1.4441277980804443\n",
      "\t Training loss (single batch): 1.2726730108261108\n",
      "\t Training loss (single batch): 1.718610167503357\n",
      "\t Training loss (single batch): 1.3593990802764893\n",
      "\t Training loss (single batch): 1.495097041130066\n",
      "\t Training loss (single batch): 1.109255313873291\n",
      "\t Training loss (single batch): 0.9897775650024414\n",
      "\t Training loss (single batch): 1.3327348232269287\n",
      "\t Training loss (single batch): 0.8418827652931213\n",
      "\t Training loss (single batch): 1.198054313659668\n",
      "\t Training loss (single batch): 1.121258020401001\n",
      "\t Training loss (single batch): 1.0677485466003418\n",
      "\t Training loss (single batch): 1.1639996767044067\n",
      "\t Training loss (single batch): 1.2571499347686768\n",
      "\t Training loss (single batch): 0.8694652915000916\n",
      "\t Training loss (single batch): 1.3894412517547607\n",
      "\t Training loss (single batch): 1.0868265628814697\n",
      "\t Training loss (single batch): 0.8458837866783142\n",
      "\t Training loss (single batch): 1.4250158071517944\n",
      "\t Training loss (single batch): 1.3972920179367065\n",
      "\t Training loss (single batch): 1.5258790254592896\n",
      "\t Training loss (single batch): 1.3131814002990723\n",
      "\t Training loss (single batch): 1.1677006483078003\n",
      "\t Training loss (single batch): 0.8284522294998169\n",
      "\t Training loss (single batch): 1.3474992513656616\n",
      "\t Training loss (single batch): 1.160628080368042\n",
      "\t Training loss (single batch): 1.2754908800125122\n",
      "\t Training loss (single batch): 1.1058824062347412\n",
      "\t Training loss (single batch): 2.059126377105713\n",
      "\t Training loss (single batch): 0.7519687414169312\n",
      "\t Training loss (single batch): 0.9266943335533142\n",
      "\t Training loss (single batch): 1.069953203201294\n",
      "\t Training loss (single batch): 1.2032008171081543\n",
      "\t Training loss (single batch): 1.318138837814331\n",
      "\t Training loss (single batch): 1.1668193340301514\n",
      "\t Training loss (single batch): 1.2978038787841797\n",
      "\t Training loss (single batch): 1.7568473815917969\n",
      "\t Training loss (single batch): 1.422358512878418\n",
      "\t Training loss (single batch): 1.6955318450927734\n",
      "\t Training loss (single batch): 1.5804266929626465\n",
      "\t Training loss (single batch): 1.1358957290649414\n",
      "\t Training loss (single batch): 1.2046138048171997\n",
      "\t Training loss (single batch): 1.570708155632019\n",
      "\t Training loss (single batch): 1.4188858270645142\n",
      "\t Training loss (single batch): 1.4795962572097778\n",
      "\t Training loss (single batch): 1.0433189868927002\n",
      "\t Training loss (single batch): 1.3425294160842896\n",
      "\t Training loss (single batch): 0.9634482860565186\n",
      "\t Training loss (single batch): 1.2014110088348389\n",
      "\t Training loss (single batch): 1.152282476425171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0461483001708984\n",
      "\t Training loss (single batch): 1.050451397895813\n",
      "\t Training loss (single batch): 1.2856727838516235\n",
      "\t Training loss (single batch): 1.3166658878326416\n",
      "\t Training loss (single batch): 1.4629368782043457\n",
      "\t Training loss (single batch): 1.1100237369537354\n",
      "\t Training loss (single batch): 1.3256734609603882\n",
      "\t Training loss (single batch): 1.3472503423690796\n",
      "\t Training loss (single batch): 1.1750906705856323\n",
      "\t Training loss (single batch): 1.5754286050796509\n",
      "\t Training loss (single batch): 1.400653600692749\n",
      "\t Training loss (single batch): 1.1005891561508179\n",
      "\t Training loss (single batch): 0.7104164958000183\n",
      "\t Training loss (single batch): 0.9055911302566528\n",
      "\t Training loss (single batch): 1.1690363883972168\n",
      "\t Training loss (single batch): 1.0668824911117554\n",
      "\t Training loss (single batch): 1.2747101783752441\n",
      "\t Training loss (single batch): 1.064020037651062\n",
      "\t Training loss (single batch): 1.3157931566238403\n",
      "\t Training loss (single batch): 0.910517692565918\n",
      "\t Training loss (single batch): 1.3076845407485962\n",
      "\t Training loss (single batch): 1.5667667388916016\n",
      "\t Training loss (single batch): 1.205318808555603\n",
      "\t Training loss (single batch): 1.4429196119308472\n",
      "\t Training loss (single batch): 1.3204690217971802\n",
      "\t Training loss (single batch): 1.2106024026870728\n",
      "\t Training loss (single batch): 1.3051128387451172\n",
      "\t Training loss (single batch): 1.128487467765808\n",
      "\t Training loss (single batch): 1.960196614265442\n",
      "##################################\n",
      "## EPOCH 18\n",
      "##################################\n",
      "\t Training loss (single batch): 1.14602530002594\n",
      "\t Training loss (single batch): 1.7448315620422363\n",
      "\t Training loss (single batch): 1.059438705444336\n",
      "\t Training loss (single batch): 0.7187040448188782\n",
      "\t Training loss (single batch): 0.9767116904258728\n",
      "\t Training loss (single batch): 1.186432123184204\n",
      "\t Training loss (single batch): 1.4913935661315918\n",
      "\t Training loss (single batch): 0.9922583103179932\n",
      "\t Training loss (single batch): 1.2813881635665894\n",
      "\t Training loss (single batch): 1.357572317123413\n",
      "\t Training loss (single batch): 1.0448367595672607\n",
      "\t Training loss (single batch): 1.3814263343811035\n",
      "\t Training loss (single batch): 1.2091076374053955\n",
      "\t Training loss (single batch): 1.4824516773223877\n",
      "\t Training loss (single batch): 1.3266942501068115\n",
      "\t Training loss (single batch): 1.1230862140655518\n",
      "\t Training loss (single batch): 1.5205543041229248\n",
      "\t Training loss (single batch): 1.5301687717437744\n",
      "\t Training loss (single batch): 1.9459487199783325\n",
      "\t Training loss (single batch): 1.6745473146438599\n",
      "\t Training loss (single batch): 1.405174732208252\n",
      "\t Training loss (single batch): 1.547159194946289\n",
      "\t Training loss (single batch): 1.1137622594833374\n",
      "\t Training loss (single batch): 1.165747880935669\n",
      "\t Training loss (single batch): 1.1670310497283936\n",
      "\t Training loss (single batch): 1.3865296840667725\n",
      "\t Training loss (single batch): 1.0583159923553467\n",
      "\t Training loss (single batch): 1.0894204378128052\n",
      "\t Training loss (single batch): 1.3548592329025269\n",
      "\t Training loss (single batch): 1.252374529838562\n",
      "\t Training loss (single batch): 1.1813597679138184\n",
      "\t Training loss (single batch): 1.131557583808899\n",
      "\t Training loss (single batch): 1.6278303861618042\n",
      "\t Training loss (single batch): 1.676650881767273\n",
      "\t Training loss (single batch): 1.468596339225769\n",
      "\t Training loss (single batch): 1.2201752662658691\n",
      "\t Training loss (single batch): 0.9924604892730713\n",
      "\t Training loss (single batch): 1.5364536046981812\n",
      "\t Training loss (single batch): 0.9508964419364929\n",
      "\t Training loss (single batch): 1.5522345304489136\n",
      "\t Training loss (single batch): 1.3574260473251343\n",
      "\t Training loss (single batch): 0.9331101179122925\n",
      "\t Training loss (single batch): 1.131618857383728\n",
      "\t Training loss (single batch): 0.9250654578208923\n",
      "\t Training loss (single batch): 1.4853599071502686\n",
      "\t Training loss (single batch): 1.369316577911377\n",
      "\t Training loss (single batch): 1.1718714237213135\n",
      "\t Training loss (single batch): 1.0477590560913086\n",
      "\t Training loss (single batch): 1.1189483404159546\n",
      "\t Training loss (single batch): 1.2570956945419312\n",
      "\t Training loss (single batch): 1.1575026512145996\n",
      "\t Training loss (single batch): 1.2980868816375732\n",
      "\t Training loss (single batch): 1.1605783700942993\n",
      "\t Training loss (single batch): 1.375379204750061\n",
      "\t Training loss (single batch): 1.2412465810775757\n",
      "\t Training loss (single batch): 1.2564183473587036\n",
      "\t Training loss (single batch): 1.0964237451553345\n",
      "\t Training loss (single batch): 1.300366759300232\n",
      "\t Training loss (single batch): 1.2542076110839844\n",
      "\t Training loss (single batch): 1.0179818868637085\n",
      "\t Training loss (single batch): 1.5602444410324097\n",
      "\t Training loss (single batch): 1.3676789999008179\n",
      "\t Training loss (single batch): 0.8569826483726501\n",
      "\t Training loss (single batch): 1.042724609375\n",
      "\t Training loss (single batch): 1.1741244792938232\n",
      "\t Training loss (single batch): 1.0091898441314697\n",
      "\t Training loss (single batch): 1.3021904230117798\n",
      "\t Training loss (single batch): 1.222343921661377\n",
      "\t Training loss (single batch): 1.6290247440338135\n",
      "\t Training loss (single batch): 1.0649585723876953\n",
      "\t Training loss (single batch): 1.371255874633789\n",
      "\t Training loss (single batch): 2.091106653213501\n",
      "\t Training loss (single batch): 1.2141159772872925\n",
      "\t Training loss (single batch): 0.8658267259597778\n",
      "\t Training loss (single batch): 1.1142536401748657\n",
      "\t Training loss (single batch): 1.2878756523132324\n",
      "\t Training loss (single batch): 1.1195932626724243\n",
      "\t Training loss (single batch): 1.2966504096984863\n",
      "\t Training loss (single batch): 1.102393627166748\n",
      "\t Training loss (single batch): 1.4127020835876465\n",
      "\t Training loss (single batch): 1.0644397735595703\n",
      "\t Training loss (single batch): 0.9728642702102661\n",
      "\t Training loss (single batch): 1.027105450630188\n",
      "\t Training loss (single batch): 0.8520253300666809\n",
      "\t Training loss (single batch): 1.1222422122955322\n",
      "\t Training loss (single batch): 1.2888556718826294\n",
      "\t Training loss (single batch): 1.2828807830810547\n",
      "\t Training loss (single batch): 1.4260838031768799\n",
      "\t Training loss (single batch): 1.761689305305481\n",
      "\t Training loss (single batch): 1.3398106098175049\n",
      "\t Training loss (single batch): 1.3153202533721924\n",
      "\t Training loss (single batch): 1.1507785320281982\n",
      "\t Training loss (single batch): 1.2099366188049316\n",
      "\t Training loss (single batch): 1.173830270767212\n",
      "\t Training loss (single batch): 1.3939441442489624\n",
      "\t Training loss (single batch): 1.1446664333343506\n",
      "\t Training loss (single batch): 1.8223884105682373\n",
      "\t Training loss (single batch): 1.0858221054077148\n",
      "\t Training loss (single batch): 1.1670775413513184\n",
      "\t Training loss (single batch): 1.2189065217971802\n",
      "\t Training loss (single batch): 1.3724606037139893\n",
      "\t Training loss (single batch): 1.2317628860473633\n",
      "\t Training loss (single batch): 1.310760259628296\n",
      "\t Training loss (single batch): 1.1314107179641724\n",
      "\t Training loss (single batch): 1.5727537870407104\n",
      "\t Training loss (single batch): 1.3061167001724243\n",
      "\t Training loss (single batch): 1.5673261880874634\n",
      "\t Training loss (single batch): 0.7828776836395264\n",
      "\t Training loss (single batch): 0.9948285222053528\n",
      "\t Training loss (single batch): 1.2904115915298462\n",
      "\t Training loss (single batch): 1.6713463068008423\n",
      "\t Training loss (single batch): 1.2230737209320068\n",
      "\t Training loss (single batch): 1.6008394956588745\n",
      "\t Training loss (single batch): 0.9497712254524231\n",
      "\t Training loss (single batch): 1.2810801267623901\n",
      "\t Training loss (single batch): 1.2487900257110596\n",
      "\t Training loss (single batch): 1.0943057537078857\n",
      "\t Training loss (single batch): 1.2466034889221191\n",
      "\t Training loss (single batch): 1.1156032085418701\n",
      "\t Training loss (single batch): 1.4017175436019897\n",
      "\t Training loss (single batch): 1.179296612739563\n",
      "\t Training loss (single batch): 1.4465317726135254\n",
      "\t Training loss (single batch): 0.9781368374824524\n",
      "\t Training loss (single batch): 1.3138039112091064\n",
      "\t Training loss (single batch): 1.2523846626281738\n",
      "\t Training loss (single batch): 0.7335860729217529\n",
      "\t Training loss (single batch): 1.2872411012649536\n",
      "\t Training loss (single batch): 0.9552246332168579\n",
      "\t Training loss (single batch): 1.281317114830017\n",
      "\t Training loss (single batch): 0.8551281094551086\n",
      "\t Training loss (single batch): 1.6361743211746216\n",
      "\t Training loss (single batch): 1.1377779245376587\n",
      "\t Training loss (single batch): 1.211629867553711\n",
      "\t Training loss (single batch): 1.5384337902069092\n",
      "\t Training loss (single batch): 1.5477943420410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8563266396522522\n",
      "\t Training loss (single batch): 0.941447377204895\n",
      "\t Training loss (single batch): 1.2974181175231934\n",
      "\t Training loss (single batch): 0.9378030896186829\n",
      "\t Training loss (single batch): 1.278671145439148\n",
      "\t Training loss (single batch): 1.376918911933899\n",
      "\t Training loss (single batch): 1.3458856344223022\n",
      "\t Training loss (single batch): 1.4568169116973877\n",
      "\t Training loss (single batch): 1.39808988571167\n",
      "\t Training loss (single batch): 1.3818634748458862\n",
      "\t Training loss (single batch): 0.9065685868263245\n",
      "\t Training loss (single batch): 1.6766897439956665\n",
      "\t Training loss (single batch): 1.0494848489761353\n",
      "\t Training loss (single batch): 1.1721467971801758\n",
      "\t Training loss (single batch): 1.068617582321167\n",
      "\t Training loss (single batch): 1.3357385396957397\n",
      "\t Training loss (single batch): 1.0322866439819336\n",
      "\t Training loss (single batch): 1.0670568943023682\n",
      "\t Training loss (single batch): 1.4168376922607422\n",
      "\t Training loss (single batch): 1.6976425647735596\n",
      "\t Training loss (single batch): 1.0931403636932373\n",
      "\t Training loss (single batch): 1.1072458028793335\n",
      "\t Training loss (single batch): 1.1407679319381714\n",
      "\t Training loss (single batch): 1.3178859949111938\n",
      "\t Training loss (single batch): 0.8354039788246155\n",
      "\t Training loss (single batch): 1.1871352195739746\n",
      "\t Training loss (single batch): 1.7181050777435303\n",
      "\t Training loss (single batch): 0.8975662589073181\n",
      "\t Training loss (single batch): 1.4695765972137451\n",
      "\t Training loss (single batch): 1.2017629146575928\n",
      "\t Training loss (single batch): 1.213255524635315\n",
      "\t Training loss (single batch): 1.0485233068466187\n",
      "\t Training loss (single batch): 1.0170613527297974\n",
      "\t Training loss (single batch): 1.846123456954956\n",
      "\t Training loss (single batch): 1.548022985458374\n",
      "\t Training loss (single batch): 1.3278464078903198\n",
      "\t Training loss (single batch): 0.9153662919998169\n",
      "\t Training loss (single batch): 1.255068302154541\n",
      "\t Training loss (single batch): 1.4142955541610718\n",
      "\t Training loss (single batch): 1.1379390954971313\n",
      "\t Training loss (single batch): 1.3550052642822266\n",
      "\t Training loss (single batch): 1.1965687274932861\n",
      "\t Training loss (single batch): 1.0270148515701294\n",
      "\t Training loss (single batch): 1.101885437965393\n",
      "\t Training loss (single batch): 1.3276925086975098\n",
      "\t Training loss (single batch): 1.3126376867294312\n",
      "\t Training loss (single batch): 1.477915644645691\n",
      "\t Training loss (single batch): 1.5026603937149048\n",
      "\t Training loss (single batch): 1.2492765188217163\n",
      "\t Training loss (single batch): 1.4299384355545044\n",
      "\t Training loss (single batch): 0.9263186454772949\n",
      "\t Training loss (single batch): 2.0104968547821045\n",
      "\t Training loss (single batch): 1.7695250511169434\n",
      "\t Training loss (single batch): 1.0878485441207886\n",
      "\t Training loss (single batch): 1.5234140157699585\n",
      "\t Training loss (single batch): 1.3968013525009155\n",
      "\t Training loss (single batch): 1.2311265468597412\n",
      "\t Training loss (single batch): 1.2969881296157837\n",
      "\t Training loss (single batch): 1.5138789415359497\n",
      "\t Training loss (single batch): 1.490517497062683\n",
      "\t Training loss (single batch): 1.1246466636657715\n",
      "\t Training loss (single batch): 1.2978354692459106\n",
      "\t Training loss (single batch): 1.2967389822006226\n",
      "\t Training loss (single batch): 1.0902496576309204\n",
      "\t Training loss (single batch): 1.228772521018982\n",
      "\t Training loss (single batch): 1.2518233060836792\n",
      "\t Training loss (single batch): 1.500916600227356\n",
      "\t Training loss (single batch): 1.3583483695983887\n",
      "\t Training loss (single batch): 0.8290682435035706\n",
      "\t Training loss (single batch): 1.0713406801223755\n",
      "\t Training loss (single batch): 1.3916211128234863\n",
      "\t Training loss (single batch): 0.9948323965072632\n",
      "\t Training loss (single batch): 1.5523990392684937\n",
      "\t Training loss (single batch): 1.225114107131958\n",
      "\t Training loss (single batch): 0.8473097085952759\n",
      "\t Training loss (single batch): 1.4244189262390137\n",
      "\t Training loss (single batch): 1.2241520881652832\n",
      "\t Training loss (single batch): 1.2403526306152344\n",
      "\t Training loss (single batch): 1.2716026306152344\n",
      "\t Training loss (single batch): 0.9246362447738647\n",
      "\t Training loss (single batch): 1.812831163406372\n",
      "\t Training loss (single batch): 1.697525143623352\n",
      "\t Training loss (single batch): 1.6294162273406982\n",
      "\t Training loss (single batch): 1.6720021963119507\n",
      "\t Training loss (single batch): 1.3963172435760498\n",
      "\t Training loss (single batch): 1.0992200374603271\n",
      "\t Training loss (single batch): 1.6632529497146606\n",
      "\t Training loss (single batch): 0.9415029287338257\n",
      "\t Training loss (single batch): 1.1449832916259766\n",
      "\t Training loss (single batch): 0.9593681693077087\n",
      "\t Training loss (single batch): 1.551828145980835\n",
      "\t Training loss (single batch): 0.6947460770606995\n",
      "\t Training loss (single batch): 1.1316615343093872\n",
      "\t Training loss (single batch): 1.919201135635376\n",
      "\t Training loss (single batch): 1.7834585905075073\n",
      "\t Training loss (single batch): 1.544848084449768\n",
      "\t Training loss (single batch): 1.5390430688858032\n",
      "\t Training loss (single batch): 0.9457824230194092\n",
      "\t Training loss (single batch): 1.2893531322479248\n",
      "\t Training loss (single batch): 1.7846349477767944\n",
      "\t Training loss (single batch): 1.4932146072387695\n",
      "\t Training loss (single batch): 1.2740734815597534\n",
      "\t Training loss (single batch): 1.1368156671524048\n",
      "\t Training loss (single batch): 1.0947537422180176\n",
      "\t Training loss (single batch): 1.345229983329773\n",
      "\t Training loss (single batch): 1.1086281538009644\n",
      "\t Training loss (single batch): 1.1040574312210083\n",
      "\t Training loss (single batch): 1.3378196954727173\n",
      "\t Training loss (single batch): 1.100285530090332\n",
      "\t Training loss (single batch): 1.2554174661636353\n",
      "\t Training loss (single batch): 1.67606782913208\n",
      "\t Training loss (single batch): 1.6800459623336792\n",
      "\t Training loss (single batch): 1.6988109350204468\n",
      "\t Training loss (single batch): 1.2229278087615967\n",
      "\t Training loss (single batch): 1.3044981956481934\n",
      "\t Training loss (single batch): 1.2373427152633667\n",
      "\t Training loss (single batch): 1.2661867141723633\n",
      "\t Training loss (single batch): 1.0800641775131226\n",
      "\t Training loss (single batch): 1.152834415435791\n",
      "\t Training loss (single batch): 1.3187562227249146\n",
      "\t Training loss (single batch): 0.9174292683601379\n",
      "\t Training loss (single batch): 1.255005955696106\n",
      "\t Training loss (single batch): 0.9454704523086548\n",
      "\t Training loss (single batch): 1.2768750190734863\n",
      "\t Training loss (single batch): 1.2910453081130981\n",
      "\t Training loss (single batch): 1.8297315835952759\n",
      "\t Training loss (single batch): 1.4334237575531006\n",
      "\t Training loss (single batch): 0.8293612003326416\n",
      "\t Training loss (single batch): 1.2032915353775024\n",
      "\t Training loss (single batch): 0.8982079029083252\n",
      "\t Training loss (single batch): 1.5731313228607178\n",
      "\t Training loss (single batch): 1.1792176961898804\n",
      "\t Training loss (single batch): 1.1984786987304688\n",
      "\t Training loss (single batch): 1.2926477193832397\n",
      "\t Training loss (single batch): 0.8648878931999207\n",
      "\t Training loss (single batch): 1.2261370420455933\n",
      "\t Training loss (single batch): 1.074528694152832\n",
      "\t Training loss (single batch): 1.1191015243530273\n",
      "\t Training loss (single batch): 1.4000787734985352\n",
      "\t Training loss (single batch): 1.0659661293029785\n",
      "\t Training loss (single batch): 1.105270504951477\n",
      "\t Training loss (single batch): 1.2755773067474365\n",
      "\t Training loss (single batch): 1.5821099281311035\n",
      "\t Training loss (single batch): 1.4811069965362549\n",
      "\t Training loss (single batch): 1.0351241827011108\n",
      "\t Training loss (single batch): 1.739243745803833\n",
      "\t Training loss (single batch): 1.8759119510650635\n",
      "\t Training loss (single batch): 0.9288409948348999\n",
      "\t Training loss (single batch): 0.9957846999168396\n",
      "\t Training loss (single batch): 1.5788075923919678\n",
      "\t Training loss (single batch): 1.0233330726623535\n",
      "\t Training loss (single batch): 1.1951595544815063\n",
      "\t Training loss (single batch): 1.0594322681427002\n",
      "\t Training loss (single batch): 0.8815431594848633\n",
      "\t Training loss (single batch): 0.8295431137084961\n",
      "\t Training loss (single batch): 1.6296635866165161\n",
      "\t Training loss (single batch): 1.8231183290481567\n",
      "\t Training loss (single batch): 0.9928727149963379\n",
      "\t Training loss (single batch): 1.170204997062683\n",
      "\t Training loss (single batch): 1.0678060054779053\n",
      "\t Training loss (single batch): 1.3597410917282104\n",
      "\t Training loss (single batch): 1.464210033416748\n",
      "\t Training loss (single batch): 1.3278799057006836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0945327281951904\n",
      "\t Training loss (single batch): 1.1803442239761353\n",
      "\t Training loss (single batch): 1.045589566230774\n",
      "\t Training loss (single batch): 1.4518091678619385\n",
      "\t Training loss (single batch): 0.9725794196128845\n",
      "\t Training loss (single batch): 1.1965851783752441\n",
      "\t Training loss (single batch): 1.2662959098815918\n",
      "\t Training loss (single batch): 1.5108695030212402\n",
      "\t Training loss (single batch): 1.2204275131225586\n",
      "\t Training loss (single batch): 0.9518639445304871\n",
      "\t Training loss (single batch): 1.1166881322860718\n",
      "\t Training loss (single batch): 1.0322527885437012\n",
      "\t Training loss (single batch): 1.7352807521820068\n",
      "\t Training loss (single batch): 1.3070042133331299\n",
      "\t Training loss (single batch): 1.4681849479675293\n",
      "\t Training loss (single batch): 1.035645842552185\n",
      "\t Training loss (single batch): 1.3060206174850464\n",
      "\t Training loss (single batch): 1.5659136772155762\n",
      "\t Training loss (single batch): 1.064456582069397\n",
      "##################################\n",
      "## EPOCH 19\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3277881145477295\n",
      "\t Training loss (single batch): 1.4550384283065796\n",
      "\t Training loss (single batch): 1.260162115097046\n",
      "\t Training loss (single batch): 1.097531795501709\n",
      "\t Training loss (single batch): 1.4326449632644653\n",
      "\t Training loss (single batch): 1.4355310201644897\n",
      "\t Training loss (single batch): 1.1966822147369385\n",
      "\t Training loss (single batch): 1.2565257549285889\n",
      "\t Training loss (single batch): 1.0921050310134888\n",
      "\t Training loss (single batch): 0.8050717711448669\n",
      "\t Training loss (single batch): 0.9871452450752258\n",
      "\t Training loss (single batch): 1.2079499959945679\n",
      "\t Training loss (single batch): 1.5256330966949463\n",
      "\t Training loss (single batch): 1.1144707202911377\n",
      "\t Training loss (single batch): 1.4525949954986572\n",
      "\t Training loss (single batch): 1.543544054031372\n",
      "\t Training loss (single batch): 1.0600497722625732\n",
      "\t Training loss (single batch): 1.3135749101638794\n",
      "\t Training loss (single batch): 0.927024245262146\n",
      "\t Training loss (single batch): 1.263819694519043\n",
      "\t Training loss (single batch): 1.2637145519256592\n",
      "\t Training loss (single batch): 0.7086650133132935\n",
      "\t Training loss (single batch): 1.549884557723999\n",
      "\t Training loss (single batch): 1.1683282852172852\n",
      "\t Training loss (single batch): 1.0549167394638062\n",
      "\t Training loss (single batch): 0.9244020581245422\n",
      "\t Training loss (single batch): 1.2974698543548584\n",
      "\t Training loss (single batch): 1.4663171768188477\n",
      "\t Training loss (single batch): 0.8682200908660889\n",
      "\t Training loss (single batch): 1.2749102115631104\n",
      "\t Training loss (single batch): 1.0295137166976929\n",
      "\t Training loss (single batch): 1.4365458488464355\n",
      "\t Training loss (single batch): 1.4221593141555786\n",
      "\t Training loss (single batch): 1.3197219371795654\n",
      "\t Training loss (single batch): 0.9100862145423889\n",
      "\t Training loss (single batch): 1.2818560600280762\n",
      "\t Training loss (single batch): 1.1133465766906738\n",
      "\t Training loss (single batch): 0.9350175857543945\n",
      "\t Training loss (single batch): 1.3668053150177002\n",
      "\t Training loss (single batch): 1.0627429485321045\n",
      "\t Training loss (single batch): 1.06332266330719\n",
      "\t Training loss (single batch): 1.4360525608062744\n",
      "\t Training loss (single batch): 1.3018819093704224\n",
      "\t Training loss (single batch): 0.9785304665565491\n",
      "\t Training loss (single batch): 1.1932896375656128\n",
      "\t Training loss (single batch): 1.194604754447937\n",
      "\t Training loss (single batch): 1.2756074666976929\n",
      "\t Training loss (single batch): 1.0107591152191162\n",
      "\t Training loss (single batch): 1.1133185625076294\n",
      "\t Training loss (single batch): 1.4156051874160767\n",
      "\t Training loss (single batch): 1.1539710760116577\n",
      "\t Training loss (single batch): 1.5830641984939575\n",
      "\t Training loss (single batch): 1.8472161293029785\n",
      "\t Training loss (single batch): 1.3016430139541626\n",
      "\t Training loss (single batch): 1.2648816108703613\n",
      "\t Training loss (single batch): 1.1343997716903687\n",
      "\t Training loss (single batch): 1.5959193706512451\n",
      "\t Training loss (single batch): 1.1664129495620728\n",
      "\t Training loss (single batch): 1.7907413244247437\n",
      "\t Training loss (single batch): 1.2260463237762451\n",
      "\t Training loss (single batch): 1.8904021978378296\n",
      "\t Training loss (single batch): 1.4154345989227295\n",
      "\t Training loss (single batch): 1.3536285161972046\n",
      "\t Training loss (single batch): 1.2448031902313232\n",
      "\t Training loss (single batch): 1.639111042022705\n",
      "\t Training loss (single batch): 1.3374578952789307\n",
      "\t Training loss (single batch): 1.4712345600128174\n",
      "\t Training loss (single batch): 1.07505464553833\n",
      "\t Training loss (single batch): 1.5499948263168335\n",
      "\t Training loss (single batch): 1.6386213302612305\n",
      "\t Training loss (single batch): 1.3834073543548584\n",
      "\t Training loss (single batch): 1.263773798942566\n",
      "\t Training loss (single batch): 1.5854498147964478\n",
      "\t Training loss (single batch): 1.2165383100509644\n",
      "\t Training loss (single batch): 1.1898193359375\n",
      "\t Training loss (single batch): 1.121305227279663\n",
      "\t Training loss (single batch): 1.2002063989639282\n",
      "\t Training loss (single batch): 1.4137108325958252\n",
      "\t Training loss (single batch): 1.227623701095581\n",
      "\t Training loss (single batch): 0.8791943788528442\n",
      "\t Training loss (single batch): 1.7624340057373047\n",
      "\t Training loss (single batch): 1.378409504890442\n",
      "\t Training loss (single batch): 1.4584300518035889\n",
      "\t Training loss (single batch): 1.3947393894195557\n",
      "\t Training loss (single batch): 1.1940754652023315\n",
      "\t Training loss (single batch): 1.5069000720977783\n",
      "\t Training loss (single batch): 1.3393518924713135\n",
      "\t Training loss (single batch): 1.3787261247634888\n",
      "\t Training loss (single batch): 1.4643718004226685\n",
      "\t Training loss (single batch): 1.060129165649414\n",
      "\t Training loss (single batch): 1.0323735475540161\n",
      "\t Training loss (single batch): 1.4259934425354004\n",
      "\t Training loss (single batch): 1.4735820293426514\n",
      "\t Training loss (single batch): 1.114625334739685\n",
      "\t Training loss (single batch): 1.9886144399642944\n",
      "\t Training loss (single batch): 1.1670809984207153\n",
      "\t Training loss (single batch): 1.2023292779922485\n",
      "\t Training loss (single batch): 1.3681411743164062\n",
      "\t Training loss (single batch): 1.1806912422180176\n",
      "\t Training loss (single batch): 1.778869867324829\n",
      "\t Training loss (single batch): 1.3599002361297607\n",
      "\t Training loss (single batch): 1.286098837852478\n",
      "\t Training loss (single batch): 1.4641923904418945\n",
      "\t Training loss (single batch): 1.836498737335205\n",
      "\t Training loss (single batch): 0.9927290081977844\n",
      "\t Training loss (single batch): 1.479272484779358\n",
      "\t Training loss (single batch): 1.113269567489624\n",
      "\t Training loss (single batch): 1.5177003145217896\n",
      "\t Training loss (single batch): 1.4372758865356445\n",
      "\t Training loss (single batch): 1.5267987251281738\n",
      "\t Training loss (single batch): 0.9798646569252014\n",
      "\t Training loss (single batch): 1.1451138257980347\n",
      "\t Training loss (single batch): 1.0949000120162964\n",
      "\t Training loss (single batch): 1.1370574235916138\n",
      "\t Training loss (single batch): 1.7389774322509766\n",
      "\t Training loss (single batch): 1.0203126668930054\n",
      "\t Training loss (single batch): 1.4936200380325317\n",
      "\t Training loss (single batch): 1.208832025527954\n",
      "\t Training loss (single batch): 0.9203707575798035\n",
      "\t Training loss (single batch): 1.7405600547790527\n",
      "\t Training loss (single batch): 1.187265396118164\n",
      "\t Training loss (single batch): 1.1119201183319092\n",
      "\t Training loss (single batch): 0.6288056373596191\n",
      "\t Training loss (single batch): 1.1136902570724487\n",
      "\t Training loss (single batch): 1.1643269062042236\n",
      "\t Training loss (single batch): 1.3736441135406494\n",
      "\t Training loss (single batch): 0.919890820980072\n",
      "\t Training loss (single batch): 1.1175122261047363\n",
      "\t Training loss (single batch): 1.2433706521987915\n",
      "\t Training loss (single batch): 1.21959388256073\n",
      "\t Training loss (single batch): 1.0139362812042236\n",
      "\t Training loss (single batch): 0.8063153624534607\n",
      "\t Training loss (single batch): 0.9854730367660522\n",
      "\t Training loss (single batch): 1.3294755220413208\n",
      "\t Training loss (single batch): 1.2300885915756226\n",
      "\t Training loss (single batch): 1.2533516883850098\n",
      "\t Training loss (single batch): 1.4019310474395752\n",
      "\t Training loss (single batch): 1.322003722190857\n",
      "\t Training loss (single batch): 1.0727618932724\n",
      "\t Training loss (single batch): 0.9936926960945129\n",
      "\t Training loss (single batch): 1.2066552639007568\n",
      "\t Training loss (single batch): 1.7525969743728638\n",
      "\t Training loss (single batch): 1.1564003229141235\n",
      "\t Training loss (single batch): 1.2858295440673828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5807479619979858\n",
      "\t Training loss (single batch): 1.7454907894134521\n",
      "\t Training loss (single batch): 1.0526094436645508\n",
      "\t Training loss (single batch): 1.0708742141723633\n",
      "\t Training loss (single batch): 1.2809594869613647\n",
      "\t Training loss (single batch): 1.222743272781372\n",
      "\t Training loss (single batch): 1.7472389936447144\n",
      "\t Training loss (single batch): 1.0552932024002075\n",
      "\t Training loss (single batch): 1.2570443153381348\n",
      "\t Training loss (single batch): 1.2807880640029907\n",
      "\t Training loss (single batch): 1.1811282634735107\n",
      "\t Training loss (single batch): 1.0006338357925415\n",
      "\t Training loss (single batch): 1.2228258848190308\n",
      "\t Training loss (single batch): 1.449377179145813\n",
      "\t Training loss (single batch): 1.4425333738327026\n",
      "\t Training loss (single batch): 1.1161445379257202\n",
      "\t Training loss (single batch): 1.7086507081985474\n",
      "\t Training loss (single batch): 1.4455361366271973\n",
      "\t Training loss (single batch): 1.0775501728057861\n",
      "\t Training loss (single batch): 0.9490156173706055\n",
      "\t Training loss (single batch): 1.3194960355758667\n",
      "\t Training loss (single batch): 1.6550087928771973\n",
      "\t Training loss (single batch): 1.2535896301269531\n",
      "\t Training loss (single batch): 1.7899426221847534\n",
      "\t Training loss (single batch): 1.3530906438827515\n",
      "\t Training loss (single batch): 1.700303316116333\n",
      "\t Training loss (single batch): 1.250564694404602\n",
      "\t Training loss (single batch): 1.8256447315216064\n",
      "\t Training loss (single batch): 1.1896305084228516\n",
      "\t Training loss (single batch): 0.9382022023200989\n",
      "\t Training loss (single batch): 1.0095844268798828\n",
      "\t Training loss (single batch): 1.414570689201355\n",
      "\t Training loss (single batch): 1.249669075012207\n",
      "\t Training loss (single batch): 1.4084150791168213\n",
      "\t Training loss (single batch): 1.2864266633987427\n",
      "\t Training loss (single batch): 1.485837459564209\n",
      "\t Training loss (single batch): 1.2523642778396606\n",
      "\t Training loss (single batch): 1.5619066953659058\n",
      "\t Training loss (single batch): 1.595471978187561\n",
      "\t Training loss (single batch): 1.0651613473892212\n",
      "\t Training loss (single batch): 1.63197660446167\n",
      "\t Training loss (single batch): 1.0653023719787598\n",
      "\t Training loss (single batch): 0.8967380523681641\n",
      "\t Training loss (single batch): 1.2599588632583618\n",
      "\t Training loss (single batch): 1.0111435651779175\n",
      "\t Training loss (single batch): 1.5198436975479126\n",
      "\t Training loss (single batch): 1.3380756378173828\n",
      "\t Training loss (single batch): 1.5449925661087036\n",
      "\t Training loss (single batch): 1.5132747888565063\n",
      "\t Training loss (single batch): 0.9836295247077942\n",
      "\t Training loss (single batch): 1.5689417123794556\n",
      "\t Training loss (single batch): 1.1303662061691284\n",
      "\t Training loss (single batch): 1.1317596435546875\n",
      "\t Training loss (single batch): 1.4836360216140747\n",
      "\t Training loss (single batch): 1.5151145458221436\n",
      "\t Training loss (single batch): 1.7085061073303223\n",
      "\t Training loss (single batch): 1.4484130144119263\n",
      "\t Training loss (single batch): 0.8711988925933838\n",
      "\t Training loss (single batch): 1.1351436376571655\n",
      "\t Training loss (single batch): 1.2919150590896606\n",
      "\t Training loss (single batch): 1.6162986755371094\n",
      "\t Training loss (single batch): 1.7485333681106567\n",
      "\t Training loss (single batch): 1.288510799407959\n",
      "\t Training loss (single batch): 1.3752726316452026\n",
      "\t Training loss (single batch): 0.812544584274292\n",
      "\t Training loss (single batch): 1.2442916631698608\n",
      "\t Training loss (single batch): 1.1701266765594482\n",
      "\t Training loss (single batch): 1.3273718357086182\n",
      "\t Training loss (single batch): 1.2816511392593384\n",
      "\t Training loss (single batch): 0.8815926313400269\n",
      "\t Training loss (single batch): 0.9697617888450623\n",
      "\t Training loss (single batch): 1.1919575929641724\n",
      "\t Training loss (single batch): 1.0587562322616577\n",
      "\t Training loss (single batch): 1.5310572385787964\n",
      "\t Training loss (single batch): 1.4783097505569458\n",
      "\t Training loss (single batch): 1.4360384941101074\n",
      "\t Training loss (single batch): 1.3411064147949219\n",
      "\t Training loss (single batch): 1.3690844774246216\n",
      "\t Training loss (single batch): 1.4977662563323975\n",
      "\t Training loss (single batch): 1.5554553270339966\n",
      "\t Training loss (single batch): 1.7508282661437988\n",
      "\t Training loss (single batch): 0.9389204978942871\n",
      "\t Training loss (single batch): 1.0903477668762207\n",
      "\t Training loss (single batch): 1.2011849880218506\n",
      "\t Training loss (single batch): 1.3354289531707764\n",
      "\t Training loss (single batch): 1.5197054147720337\n",
      "\t Training loss (single batch): 1.029585361480713\n",
      "\t Training loss (single batch): 1.3721596002578735\n",
      "\t Training loss (single batch): 0.9200639128684998\n",
      "\t Training loss (single batch): 1.3262895345687866\n",
      "\t Training loss (single batch): 1.4627695083618164\n",
      "\t Training loss (single batch): 1.4504823684692383\n",
      "\t Training loss (single batch): 1.7572805881500244\n",
      "\t Training loss (single batch): 1.2179728746414185\n",
      "\t Training loss (single batch): 1.0921549797058105\n",
      "\t Training loss (single batch): 1.059670329093933\n",
      "\t Training loss (single batch): 1.111724853515625\n",
      "\t Training loss (single batch): 1.362438678741455\n",
      "\t Training loss (single batch): 1.166527509689331\n",
      "\t Training loss (single batch): 1.1903977394104004\n",
      "\t Training loss (single batch): 1.486680269241333\n",
      "\t Training loss (single batch): 1.3631623983383179\n",
      "\t Training loss (single batch): 1.5779681205749512\n",
      "\t Training loss (single batch): 1.0619895458221436\n",
      "\t Training loss (single batch): 1.3577041625976562\n",
      "\t Training loss (single batch): 1.4420794248580933\n",
      "\t Training loss (single batch): 1.3067104816436768\n",
      "\t Training loss (single batch): 1.3748769760131836\n",
      "\t Training loss (single batch): 1.292936086654663\n",
      "\t Training loss (single batch): 1.1589409112930298\n",
      "\t Training loss (single batch): 1.7293767929077148\n",
      "\t Training loss (single batch): 1.4894065856933594\n",
      "\t Training loss (single batch): 1.0405259132385254\n",
      "\t Training loss (single batch): 1.1112817525863647\n",
      "\t Training loss (single batch): 1.1806910037994385\n",
      "\t Training loss (single batch): 1.651379108428955\n",
      "\t Training loss (single batch): 1.217751145362854\n",
      "\t Training loss (single batch): 1.1580662727355957\n",
      "\t Training loss (single batch): 0.9379622340202332\n",
      "\t Training loss (single batch): 1.1680138111114502\n",
      "\t Training loss (single batch): 1.161753535270691\n",
      "\t Training loss (single batch): 1.353527307510376\n",
      "\t Training loss (single batch): 1.1082055568695068\n",
      "\t Training loss (single batch): 1.0407708883285522\n",
      "\t Training loss (single batch): 0.7378547191619873\n",
      "\t Training loss (single batch): 1.1325784921646118\n",
      "\t Training loss (single batch): 2.0609264373779297\n",
      "\t Training loss (single batch): 1.2462406158447266\n",
      "\t Training loss (single batch): 1.2437556982040405\n",
      "\t Training loss (single batch): 1.1025089025497437\n",
      "\t Training loss (single batch): 1.1319127082824707\n",
      "\t Training loss (single batch): 0.8496736884117126\n",
      "\t Training loss (single batch): 1.1668353080749512\n",
      "\t Training loss (single batch): 1.363896131515503\n",
      "\t Training loss (single batch): 0.8254004716873169\n",
      "\t Training loss (single batch): 1.2714823484420776\n",
      "\t Training loss (single batch): 0.9781356453895569\n",
      "\t Training loss (single batch): 0.48850929737091064\n",
      "\t Training loss (single batch): 0.834834098815918\n",
      "\t Training loss (single batch): 1.4192053079605103\n",
      "\t Training loss (single batch): 1.722247838973999\n",
      "\t Training loss (single batch): 1.4964280128479004\n",
      "\t Training loss (single batch): 1.065428376197815\n",
      "\t Training loss (single batch): 1.1064947843551636\n",
      "\t Training loss (single batch): 1.178035855293274\n",
      "\t Training loss (single batch): 1.1985803842544556\n",
      "\t Training loss (single batch): 0.9254741072654724\n",
      "\t Training loss (single batch): 1.0502521991729736\n",
      "\t Training loss (single batch): 1.0940972566604614\n",
      "\t Training loss (single batch): 1.0362837314605713\n",
      "\t Training loss (single batch): 1.640421748161316\n",
      "\t Training loss (single batch): 1.051353931427002\n",
      "\t Training loss (single batch): 0.8042893409729004\n",
      "\t Training loss (single batch): 1.2332578897476196\n",
      "\t Training loss (single batch): 1.147434949874878\n",
      "\t Training loss (single batch): 1.2604289054870605\n",
      "\t Training loss (single batch): 1.1231557130813599\n",
      "\t Training loss (single batch): 1.0364519357681274\n",
      "\t Training loss (single batch): 0.926429271697998\n",
      "\t Training loss (single batch): 2.0991098880767822\n",
      "\t Training loss (single batch): 1.3771545886993408\n",
      "\t Training loss (single batch): 1.1887315511703491\n",
      "\t Training loss (single batch): 1.429818868637085\n",
      "\t Training loss (single batch): 0.8442147374153137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.181210994720459\n",
      "\t Training loss (single batch): 1.4697421789169312\n",
      "\t Training loss (single batch): 1.0802485942840576\n",
      "\t Training loss (single batch): 0.901432991027832\n",
      "\t Training loss (single batch): 1.1868559122085571\n",
      "\t Training loss (single batch): 1.4746426343917847\n",
      "\t Training loss (single batch): 0.8259102702140808\n",
      "\t Training loss (single batch): 1.4719417095184326\n",
      "\t Training loss (single batch): 1.1700963973999023\n",
      "##################################\n",
      "## EPOCH 20\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9215772151947021\n",
      "\t Training loss (single batch): 1.661034107208252\n",
      "\t Training loss (single batch): 1.08388090133667\n",
      "\t Training loss (single batch): 1.1799179315567017\n",
      "\t Training loss (single batch): 1.0636018514633179\n",
      "\t Training loss (single batch): 1.2952516078948975\n",
      "\t Training loss (single batch): 1.350760579109192\n",
      "\t Training loss (single batch): 1.232336401939392\n",
      "\t Training loss (single batch): 1.7232537269592285\n",
      "\t Training loss (single batch): 1.0580881834030151\n",
      "\t Training loss (single batch): 1.1132913827896118\n",
      "\t Training loss (single batch): 1.1139731407165527\n",
      "\t Training loss (single batch): 1.4897836446762085\n",
      "\t Training loss (single batch): 1.081489086151123\n",
      "\t Training loss (single batch): 1.588303565979004\n",
      "\t Training loss (single batch): 0.992453396320343\n",
      "\t Training loss (single batch): 1.8434735536575317\n",
      "\t Training loss (single batch): 1.274014949798584\n",
      "\t Training loss (single batch): 1.0314967632293701\n",
      "\t Training loss (single batch): 1.588111162185669\n",
      "\t Training loss (single batch): 1.4610518217086792\n",
      "\t Training loss (single batch): 1.122755765914917\n",
      "\t Training loss (single batch): 0.9601226449012756\n",
      "\t Training loss (single batch): 1.1597094535827637\n",
      "\t Training loss (single batch): 1.2300353050231934\n",
      "\t Training loss (single batch): 1.3555080890655518\n",
      "\t Training loss (single batch): 1.585646629333496\n",
      "\t Training loss (single batch): 1.78228759765625\n",
      "\t Training loss (single batch): 1.2213656902313232\n",
      "\t Training loss (single batch): 1.3400535583496094\n",
      "\t Training loss (single batch): 1.0567400455474854\n",
      "\t Training loss (single batch): 0.993252158164978\n",
      "\t Training loss (single batch): 1.096483588218689\n",
      "\t Training loss (single batch): 1.281603217124939\n",
      "\t Training loss (single batch): 1.330698847770691\n",
      "\t Training loss (single batch): 1.034759283065796\n",
      "\t Training loss (single batch): 1.2919285297393799\n",
      "\t Training loss (single batch): 1.5579838752746582\n",
      "\t Training loss (single batch): 1.405722975730896\n",
      "\t Training loss (single batch): 1.2500807046890259\n",
      "\t Training loss (single batch): 1.4701004028320312\n",
      "\t Training loss (single batch): 0.9126348495483398\n",
      "\t Training loss (single batch): 1.222179651260376\n",
      "\t Training loss (single batch): 1.417050838470459\n",
      "\t Training loss (single batch): 1.6884255409240723\n",
      "\t Training loss (single batch): 1.4268798828125\n",
      "\t Training loss (single batch): 1.3027316331863403\n",
      "\t Training loss (single batch): 0.8718812465667725\n",
      "\t Training loss (single batch): 1.0365642309188843\n",
      "\t Training loss (single batch): 1.5400221347808838\n",
      "\t Training loss (single batch): 1.413246989250183\n",
      "\t Training loss (single batch): 1.1107454299926758\n",
      "\t Training loss (single batch): 1.2438981533050537\n",
      "\t Training loss (single batch): 0.92017662525177\n",
      "\t Training loss (single batch): 1.5279841423034668\n",
      "\t Training loss (single batch): 1.2799198627471924\n",
      "\t Training loss (single batch): 1.4996689558029175\n",
      "\t Training loss (single batch): 1.0137832164764404\n",
      "\t Training loss (single batch): 1.3155121803283691\n",
      "\t Training loss (single batch): 1.3710581064224243\n",
      "\t Training loss (single batch): 1.3058810234069824\n",
      "\t Training loss (single batch): 0.9759721159934998\n",
      "\t Training loss (single batch): 1.4786367416381836\n",
      "\t Training loss (single batch): 1.4669424295425415\n",
      "\t Training loss (single batch): 1.4065909385681152\n",
      "\t Training loss (single batch): 0.8558290600776672\n",
      "\t Training loss (single batch): 1.1531271934509277\n",
      "\t Training loss (single batch): 1.6714571714401245\n",
      "\t Training loss (single batch): 0.9549830555915833\n",
      "\t Training loss (single batch): 1.7649199962615967\n",
      "\t Training loss (single batch): 1.1884491443634033\n",
      "\t Training loss (single batch): 2.0700948238372803\n",
      "\t Training loss (single batch): 1.461158037185669\n",
      "\t Training loss (single batch): 0.874779224395752\n",
      "\t Training loss (single batch): 0.9479347467422485\n",
      "\t Training loss (single batch): 1.1049898862838745\n",
      "\t Training loss (single batch): 1.4883984327316284\n",
      "\t Training loss (single batch): 0.8739911317825317\n",
      "\t Training loss (single batch): 0.9721336960792542\n",
      "\t Training loss (single batch): 1.1181477308273315\n",
      "\t Training loss (single batch): 1.392034649848938\n",
      "\t Training loss (single batch): 1.1287754774093628\n",
      "\t Training loss (single batch): 1.3051786422729492\n",
      "\t Training loss (single batch): 1.0173591375350952\n",
      "\t Training loss (single batch): 0.9221445322036743\n",
      "\t Training loss (single batch): 1.4878617525100708\n",
      "\t Training loss (single batch): 0.8622183799743652\n",
      "\t Training loss (single batch): 0.9090211391448975\n",
      "\t Training loss (single batch): 1.265679121017456\n",
      "\t Training loss (single batch): 0.7782225012779236\n",
      "\t Training loss (single batch): 1.260244607925415\n",
      "\t Training loss (single batch): 1.449312448501587\n",
      "\t Training loss (single batch): 1.0903682708740234\n",
      "\t Training loss (single batch): 1.2021287679672241\n",
      "\t Training loss (single batch): 0.8871285319328308\n",
      "\t Training loss (single batch): 1.6592814922332764\n",
      "\t Training loss (single batch): 1.4154272079467773\n",
      "\t Training loss (single batch): 1.5464075803756714\n",
      "\t Training loss (single batch): 1.0849822759628296\n",
      "\t Training loss (single batch): 1.2594162225723267\n",
      "\t Training loss (single batch): 1.5976357460021973\n",
      "\t Training loss (single batch): 0.9825438857078552\n",
      "\t Training loss (single batch): 1.2164642810821533\n",
      "\t Training loss (single batch): 1.2521171569824219\n",
      "\t Training loss (single batch): 1.1377315521240234\n",
      "\t Training loss (single batch): 1.293825387954712\n",
      "\t Training loss (single batch): 1.6525253057479858\n",
      "\t Training loss (single batch): 1.422611951828003\n",
      "\t Training loss (single batch): 1.4594725370407104\n",
      "\t Training loss (single batch): 1.0803648233413696\n",
      "\t Training loss (single batch): 1.7045046091079712\n",
      "\t Training loss (single batch): 1.1227631568908691\n",
      "\t Training loss (single batch): 1.0146474838256836\n",
      "\t Training loss (single batch): 1.2215394973754883\n",
      "\t Training loss (single batch): 1.3866759538650513\n",
      "\t Training loss (single batch): 1.5326118469238281\n",
      "\t Training loss (single batch): 1.1748100519180298\n",
      "\t Training loss (single batch): 1.2056517601013184\n",
      "\t Training loss (single batch): 1.1913719177246094\n",
      "\t Training loss (single batch): 1.4070619344711304\n",
      "\t Training loss (single batch): 1.2366490364074707\n",
      "\t Training loss (single batch): 1.4160099029541016\n",
      "\t Training loss (single batch): 1.561824917793274\n",
      "\t Training loss (single batch): 1.0042564868927002\n",
      "\t Training loss (single batch): 1.130755066871643\n",
      "\t Training loss (single batch): 1.1735174655914307\n",
      "\t Training loss (single batch): 1.0435067415237427\n",
      "\t Training loss (single batch): 1.4021848440170288\n",
      "\t Training loss (single batch): 1.2587335109710693\n",
      "\t Training loss (single batch): 1.7817744016647339\n",
      "\t Training loss (single batch): 1.1764438152313232\n",
      "\t Training loss (single batch): 1.2334363460540771\n",
      "\t Training loss (single batch): 1.5736151933670044\n",
      "\t Training loss (single batch): 1.5959582328796387\n",
      "\t Training loss (single batch): 1.489690899848938\n",
      "\t Training loss (single batch): 1.6429396867752075\n",
      "\t Training loss (single batch): 1.5205397605895996\n",
      "\t Training loss (single batch): 1.0539097785949707\n",
      "\t Training loss (single batch): 1.5818593502044678\n",
      "\t Training loss (single batch): 1.222304344177246\n",
      "\t Training loss (single batch): 1.373008131980896\n",
      "\t Training loss (single batch): 1.6099587678909302\n",
      "\t Training loss (single batch): 1.7545020580291748\n",
      "\t Training loss (single batch): 1.2717335224151611\n",
      "\t Training loss (single batch): 1.4713685512542725\n",
      "\t Training loss (single batch): 1.2073886394500732\n",
      "\t Training loss (single batch): 1.6836133003234863\n",
      "\t Training loss (single batch): 1.0315539836883545\n",
      "\t Training loss (single batch): 1.574644923210144\n",
      "\t Training loss (single batch): 1.3497601747512817\n",
      "\t Training loss (single batch): 1.0483460426330566\n",
      "\t Training loss (single batch): 1.527756929397583\n",
      "\t Training loss (single batch): 1.2464230060577393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.382528305053711\n",
      "\t Training loss (single batch): 1.4634310007095337\n",
      "\t Training loss (single batch): 1.2731248140335083\n",
      "\t Training loss (single batch): 1.004255771636963\n",
      "\t Training loss (single batch): 1.574917197227478\n",
      "\t Training loss (single batch): 1.1606857776641846\n",
      "\t Training loss (single batch): 1.0301073789596558\n",
      "\t Training loss (single batch): 1.398828387260437\n",
      "\t Training loss (single batch): 0.8821197748184204\n",
      "\t Training loss (single batch): 1.0927934646606445\n",
      "\t Training loss (single batch): 0.8627070188522339\n",
      "\t Training loss (single batch): 1.5951640605926514\n",
      "\t Training loss (single batch): 0.9946237206459045\n",
      "\t Training loss (single batch): 1.239686369895935\n",
      "\t Training loss (single batch): 1.082914113998413\n",
      "\t Training loss (single batch): 1.054832100868225\n",
      "\t Training loss (single batch): 1.541691780090332\n",
      "\t Training loss (single batch): 1.5300700664520264\n",
      "\t Training loss (single batch): 1.083398699760437\n",
      "\t Training loss (single batch): 0.8675694465637207\n",
      "\t Training loss (single batch): 1.2765307426452637\n",
      "\t Training loss (single batch): 0.9770795106887817\n",
      "\t Training loss (single batch): 1.0839321613311768\n",
      "\t Training loss (single batch): 2.02360463142395\n",
      "\t Training loss (single batch): 1.5291643142700195\n",
      "\t Training loss (single batch): 1.1581552028656006\n",
      "\t Training loss (single batch): 1.4488364458084106\n",
      "\t Training loss (single batch): 1.540757417678833\n",
      "\t Training loss (single batch): 1.3314117193222046\n",
      "\t Training loss (single batch): 1.163741946220398\n",
      "\t Training loss (single batch): 1.3854135274887085\n",
      "\t Training loss (single batch): 1.3382996320724487\n",
      "\t Training loss (single batch): 0.7274231910705566\n",
      "\t Training loss (single batch): 1.4806536436080933\n",
      "\t Training loss (single batch): 1.5622472763061523\n",
      "\t Training loss (single batch): 1.0469980239868164\n",
      "\t Training loss (single batch): 1.2027270793914795\n",
      "\t Training loss (single batch): 1.2113611698150635\n",
      "\t Training loss (single batch): 1.486877679824829\n",
      "\t Training loss (single batch): 1.0519577264785767\n",
      "\t Training loss (single batch): 1.213495135307312\n",
      "\t Training loss (single batch): 1.5741276741027832\n",
      "\t Training loss (single batch): 0.7260797619819641\n",
      "\t Training loss (single batch): 1.1670669317245483\n",
      "\t Training loss (single batch): 1.1364754438400269\n",
      "\t Training loss (single batch): 1.468774437904358\n",
      "\t Training loss (single batch): 1.5649813413619995\n",
      "\t Training loss (single batch): 1.1239155530929565\n",
      "\t Training loss (single batch): 1.0933672189712524\n",
      "\t Training loss (single batch): 1.0245039463043213\n",
      "\t Training loss (single batch): 1.3570424318313599\n",
      "\t Training loss (single batch): 1.066666841506958\n",
      "\t Training loss (single batch): 1.6950565576553345\n",
      "\t Training loss (single batch): 1.8006001710891724\n",
      "\t Training loss (single batch): 1.212120532989502\n",
      "\t Training loss (single batch): 1.451263666152954\n",
      "\t Training loss (single batch): 1.6389422416687012\n",
      "\t Training loss (single batch): 0.9941365718841553\n",
      "\t Training loss (single batch): 1.100838541984558\n",
      "\t Training loss (single batch): 1.8551747798919678\n",
      "\t Training loss (single batch): 1.359264612197876\n",
      "\t Training loss (single batch): 1.0462138652801514\n",
      "\t Training loss (single batch): 0.8396739363670349\n",
      "\t Training loss (single batch): 1.5216246843338013\n",
      "\t Training loss (single batch): 1.865851640701294\n",
      "\t Training loss (single batch): 1.4308574199676514\n",
      "\t Training loss (single batch): 1.1887491941452026\n",
      "\t Training loss (single batch): 1.070820927619934\n",
      "\t Training loss (single batch): 1.2420315742492676\n",
      "\t Training loss (single batch): 0.7313965559005737\n",
      "\t Training loss (single batch): 1.524620771408081\n",
      "\t Training loss (single batch): 1.6538580656051636\n",
      "\t Training loss (single batch): 1.4281495809555054\n",
      "\t Training loss (single batch): 1.4484715461730957\n",
      "\t Training loss (single batch): 1.056965708732605\n",
      "\t Training loss (single batch): 1.0584503412246704\n",
      "\t Training loss (single batch): 1.1329678297042847\n",
      "\t Training loss (single batch): 1.2447102069854736\n",
      "\t Training loss (single batch): 1.4200729131698608\n",
      "\t Training loss (single batch): 1.2753570079803467\n",
      "\t Training loss (single batch): 0.9667421579360962\n",
      "\t Training loss (single batch): 1.3376439809799194\n",
      "\t Training loss (single batch): 1.3972880840301514\n",
      "\t Training loss (single batch): 1.1630083322525024\n",
      "\t Training loss (single batch): 0.9630104303359985\n",
      "\t Training loss (single batch): 1.146682620048523\n",
      "\t Training loss (single batch): 1.8189640045166016\n",
      "\t Training loss (single batch): 1.2464327812194824\n",
      "\t Training loss (single batch): 1.388458490371704\n",
      "\t Training loss (single batch): 1.11089026927948\n",
      "\t Training loss (single batch): 1.5964187383651733\n",
      "\t Training loss (single batch): 1.6791574954986572\n",
      "\t Training loss (single batch): 0.7495883107185364\n",
      "\t Training loss (single batch): 1.500690221786499\n",
      "\t Training loss (single batch): 1.0834431648254395\n",
      "\t Training loss (single batch): 1.0953150987625122\n",
      "\t Training loss (single batch): 1.360430359840393\n",
      "\t Training loss (single batch): 1.1705924272537231\n",
      "\t Training loss (single batch): 1.4815824031829834\n",
      "\t Training loss (single batch): 1.3979945182800293\n",
      "\t Training loss (single batch): 1.3925065994262695\n",
      "\t Training loss (single batch): 0.8848778009414673\n",
      "\t Training loss (single batch): 1.2302870750427246\n",
      "\t Training loss (single batch): 0.9566250443458557\n",
      "\t Training loss (single batch): 1.479681372642517\n",
      "\t Training loss (single batch): 1.516439437866211\n",
      "\t Training loss (single batch): 1.2984919548034668\n",
      "\t Training loss (single batch): 1.479960560798645\n",
      "\t Training loss (single batch): 1.1946111917495728\n",
      "\t Training loss (single batch): 1.2941546440124512\n",
      "\t Training loss (single batch): 1.520523190498352\n",
      "\t Training loss (single batch): 1.6823759078979492\n",
      "\t Training loss (single batch): 1.3465481996536255\n",
      "\t Training loss (single batch): 1.2117140293121338\n",
      "\t Training loss (single batch): 1.1299891471862793\n",
      "\t Training loss (single batch): 1.0643956661224365\n",
      "\t Training loss (single batch): 1.8420827388763428\n",
      "\t Training loss (single batch): 1.200980305671692\n",
      "\t Training loss (single batch): 1.6255805492401123\n",
      "\t Training loss (single batch): 1.2596150636672974\n",
      "\t Training loss (single batch): 1.2538609504699707\n",
      "\t Training loss (single batch): 1.1219203472137451\n",
      "\t Training loss (single batch): 1.0141065120697021\n",
      "\t Training loss (single batch): 0.8554053902626038\n",
      "\t Training loss (single batch): 1.517635464668274\n",
      "\t Training loss (single batch): 0.8580392003059387\n",
      "\t Training loss (single batch): 1.4643036127090454\n",
      "\t Training loss (single batch): 1.2817333936691284\n",
      "\t Training loss (single batch): 1.0947282314300537\n",
      "\t Training loss (single batch): 1.230284571647644\n",
      "\t Training loss (single batch): 1.7577741146087646\n",
      "\t Training loss (single batch): 0.7737196683883667\n",
      "\t Training loss (single batch): 1.1615208387374878\n",
      "\t Training loss (single batch): 1.481661319732666\n",
      "\t Training loss (single batch): 1.1301476955413818\n",
      "\t Training loss (single batch): 1.3315813541412354\n",
      "\t Training loss (single batch): 0.9885604977607727\n",
      "\t Training loss (single batch): 1.497387170791626\n",
      "\t Training loss (single batch): 1.523703694343567\n",
      "\t Training loss (single batch): 1.297743320465088\n",
      "\t Training loss (single batch): 1.208399772644043\n",
      "\t Training loss (single batch): 1.2626389265060425\n",
      "\t Training loss (single batch): 1.073119878768921\n",
      "\t Training loss (single batch): 0.8820974230766296\n",
      "\t Training loss (single batch): 1.5635840892791748\n",
      "\t Training loss (single batch): 0.9096794724464417\n",
      "\t Training loss (single batch): 1.1154818534851074\n",
      "\t Training loss (single batch): 1.0376559495925903\n",
      "\t Training loss (single batch): 0.7266173958778381\n",
      "\t Training loss (single batch): 1.1843676567077637\n",
      "\t Training loss (single batch): 0.8578089475631714\n",
      "\t Training loss (single batch): 1.1725877523422241\n",
      "\t Training loss (single batch): 1.1300139427185059\n",
      "\t Training loss (single batch): 1.6870653629302979\n",
      "\t Training loss (single batch): 1.53094482421875\n",
      "\t Training loss (single batch): 1.3427945375442505\n",
      "\t Training loss (single batch): 1.412109375\n",
      "\t Training loss (single batch): 1.1379839181900024\n",
      "\t Training loss (single batch): 1.1145827770233154\n",
      "\t Training loss (single batch): 0.7946693897247314\n",
      "\t Training loss (single batch): 1.3883366584777832\n",
      "\t Training loss (single batch): 1.4605047702789307\n",
      "\t Training loss (single batch): 1.3722039461135864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.4330191612243652\n",
      "##################################\n",
      "## EPOCH 21\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1494369506835938\n",
      "\t Training loss (single batch): 0.9972509741783142\n",
      "\t Training loss (single batch): 0.5961262583732605\n",
      "\t Training loss (single batch): 1.2310010194778442\n",
      "\t Training loss (single batch): 1.4620287418365479\n",
      "\t Training loss (single batch): 1.2371941804885864\n",
      "\t Training loss (single batch): 1.1563869714736938\n",
      "\t Training loss (single batch): 1.036853551864624\n",
      "\t Training loss (single batch): 2.0206127166748047\n",
      "\t Training loss (single batch): 1.2774198055267334\n",
      "\t Training loss (single batch): 1.3510938882827759\n",
      "\t Training loss (single batch): 1.2411624193191528\n",
      "\t Training loss (single batch): 1.0225725173950195\n",
      "\t Training loss (single batch): 1.481544852256775\n",
      "\t Training loss (single batch): 1.051291823387146\n",
      "\t Training loss (single batch): 1.0601271390914917\n",
      "\t Training loss (single batch): 1.321503758430481\n",
      "\t Training loss (single batch): 1.610520601272583\n",
      "\t Training loss (single batch): 1.4353352785110474\n",
      "\t Training loss (single batch): 1.1868071556091309\n",
      "\t Training loss (single batch): 1.12155020236969\n",
      "\t Training loss (single batch): 1.7481205463409424\n",
      "\t Training loss (single batch): 1.0420937538146973\n",
      "\t Training loss (single batch): 1.414079189300537\n",
      "\t Training loss (single batch): 1.5473747253417969\n",
      "\t Training loss (single batch): 1.3231663703918457\n",
      "\t Training loss (single batch): 1.385299563407898\n",
      "\t Training loss (single batch): 1.0868091583251953\n",
      "\t Training loss (single batch): 1.2118580341339111\n",
      "\t Training loss (single batch): 1.2226847410202026\n",
      "\t Training loss (single batch): 1.1913312673568726\n",
      "\t Training loss (single batch): 1.2446080446243286\n",
      "\t Training loss (single batch): 1.5054655075073242\n",
      "\t Training loss (single batch): 1.0406016111373901\n",
      "\t Training loss (single batch): 1.5989372730255127\n",
      "\t Training loss (single batch): 1.7794548273086548\n",
      "\t Training loss (single batch): 1.5774275064468384\n",
      "\t Training loss (single batch): 1.572105884552002\n",
      "\t Training loss (single batch): 0.8910357356071472\n",
      "\t Training loss (single batch): 1.5878956317901611\n",
      "\t Training loss (single batch): 0.7160740494728088\n",
      "\t Training loss (single batch): 1.2575637102127075\n",
      "\t Training loss (single batch): 1.0945780277252197\n",
      "\t Training loss (single batch): 1.2820872068405151\n",
      "\t Training loss (single batch): 1.363221526145935\n",
      "\t Training loss (single batch): 1.4993834495544434\n",
      "\t Training loss (single batch): 1.7453557252883911\n",
      "\t Training loss (single batch): 1.1758829355239868\n",
      "\t Training loss (single batch): 1.3401124477386475\n",
      "\t Training loss (single batch): 1.095879077911377\n",
      "\t Training loss (single batch): 1.4094184637069702\n",
      "\t Training loss (single batch): 1.406737208366394\n",
      "\t Training loss (single batch): 1.0870028734207153\n",
      "\t Training loss (single batch): 1.63913094997406\n",
      "\t Training loss (single batch): 1.3736439943313599\n",
      "\t Training loss (single batch): 1.1320552825927734\n",
      "\t Training loss (single batch): 1.1932545900344849\n",
      "\t Training loss (single batch): 1.4983879327774048\n",
      "\t Training loss (single batch): 1.367195963859558\n",
      "\t Training loss (single batch): 1.0134273767471313\n",
      "\t Training loss (single batch): 1.2362334728240967\n",
      "\t Training loss (single batch): 1.232992172241211\n",
      "\t Training loss (single batch): 1.0954506397247314\n",
      "\t Training loss (single batch): 1.257482886314392\n",
      "\t Training loss (single batch): 1.335115671157837\n",
      "\t Training loss (single batch): 1.489715814590454\n",
      "\t Training loss (single batch): 1.1796832084655762\n",
      "\t Training loss (single batch): 1.4687308073043823\n",
      "\t Training loss (single batch): 0.9677670001983643\n",
      "\t Training loss (single batch): 0.9541628956794739\n",
      "\t Training loss (single batch): 1.139599084854126\n",
      "\t Training loss (single batch): 1.0017439126968384\n",
      "\t Training loss (single batch): 1.0151634216308594\n",
      "\t Training loss (single batch): 1.0920796394348145\n",
      "\t Training loss (single batch): 1.079636812210083\n",
      "\t Training loss (single batch): 1.501920223236084\n",
      "\t Training loss (single batch): 1.347127914428711\n",
      "\t Training loss (single batch): 1.2218106985092163\n",
      "\t Training loss (single batch): 1.5775630474090576\n",
      "\t Training loss (single batch): 0.85652095079422\n",
      "\t Training loss (single batch): 1.054180383682251\n",
      "\t Training loss (single batch): 1.2068482637405396\n",
      "\t Training loss (single batch): 1.2012473344802856\n",
      "\t Training loss (single batch): 1.2495819330215454\n",
      "\t Training loss (single batch): 0.8549663424491882\n",
      "\t Training loss (single batch): 1.3256428241729736\n",
      "\t Training loss (single batch): 1.0207267999649048\n",
      "\t Training loss (single batch): 1.2313635349273682\n",
      "\t Training loss (single batch): 1.5136650800704956\n",
      "\t Training loss (single batch): 1.3319423198699951\n",
      "\t Training loss (single batch): 1.0992162227630615\n",
      "\t Training loss (single batch): 1.1035873889923096\n",
      "\t Training loss (single batch): 1.1351960897445679\n",
      "\t Training loss (single batch): 1.3261438608169556\n",
      "\t Training loss (single batch): 1.1224390268325806\n",
      "\t Training loss (single batch): 1.2450999021530151\n",
      "\t Training loss (single batch): 1.3976521492004395\n",
      "\t Training loss (single batch): 1.1112605333328247\n",
      "\t Training loss (single batch): 0.9361130595207214\n",
      "\t Training loss (single batch): 1.6182899475097656\n",
      "\t Training loss (single batch): 1.1168909072875977\n",
      "\t Training loss (single batch): 1.256774663925171\n",
      "\t Training loss (single batch): 1.7520809173583984\n",
      "\t Training loss (single batch): 1.073211431503296\n",
      "\t Training loss (single batch): 0.7213010787963867\n",
      "\t Training loss (single batch): 1.2239575386047363\n",
      "\t Training loss (single batch): 1.073380947113037\n",
      "\t Training loss (single batch): 0.9588050246238708\n",
      "\t Training loss (single batch): 0.9510982036590576\n",
      "\t Training loss (single batch): 1.1418694257736206\n",
      "\t Training loss (single batch): 1.4560290575027466\n",
      "\t Training loss (single batch): 1.167145848274231\n",
      "\t Training loss (single batch): 0.8968133330345154\n",
      "\t Training loss (single batch): 1.828894019126892\n",
      "\t Training loss (single batch): 1.4916369915008545\n",
      "\t Training loss (single batch): 1.6713435649871826\n",
      "\t Training loss (single batch): 1.3618245124816895\n",
      "\t Training loss (single batch): 1.2543580532073975\n",
      "\t Training loss (single batch): 1.0537663698196411\n",
      "\t Training loss (single batch): 1.0821068286895752\n",
      "\t Training loss (single batch): 0.9693459868431091\n",
      "\t Training loss (single batch): 1.3251680135726929\n",
      "\t Training loss (single batch): 1.2471141815185547\n",
      "\t Training loss (single batch): 1.093287706375122\n",
      "\t Training loss (single batch): 1.293416976928711\n",
      "\t Training loss (single batch): 1.4585556983947754\n",
      "\t Training loss (single batch): 1.4656038284301758\n",
      "\t Training loss (single batch): 1.2465262413024902\n",
      "\t Training loss (single batch): 1.2666078805923462\n",
      "\t Training loss (single batch): 1.1040066480636597\n",
      "\t Training loss (single batch): 1.3936535120010376\n",
      "\t Training loss (single batch): 1.2040904760360718\n",
      "\t Training loss (single batch): 1.4414668083190918\n",
      "\t Training loss (single batch): 1.443091630935669\n",
      "\t Training loss (single batch): 1.4753367900848389\n",
      "\t Training loss (single batch): 1.274944543838501\n",
      "\t Training loss (single batch): 1.1287157535552979\n",
      "\t Training loss (single batch): 1.6950470209121704\n",
      "\t Training loss (single batch): 1.1992391347885132\n",
      "\t Training loss (single batch): 1.194627285003662\n",
      "\t Training loss (single batch): 1.128083348274231\n",
      "\t Training loss (single batch): 1.3728007078170776\n",
      "\t Training loss (single batch): 1.4752190113067627\n",
      "\t Training loss (single batch): 1.0521427392959595\n",
      "\t Training loss (single batch): 1.6150474548339844\n",
      "\t Training loss (single batch): 1.2862130403518677\n",
      "\t Training loss (single batch): 1.1806284189224243\n",
      "\t Training loss (single batch): 1.3187917470932007\n",
      "\t Training loss (single batch): 1.4959567785263062\n",
      "\t Training loss (single batch): 1.0713266134262085\n",
      "\t Training loss (single batch): 0.8508111834526062\n",
      "\t Training loss (single batch): 1.4526557922363281\n",
      "\t Training loss (single batch): 0.9966811537742615\n",
      "\t Training loss (single batch): 1.4372174739837646\n",
      "\t Training loss (single batch): 1.5953139066696167\n",
      "\t Training loss (single batch): 1.0187532901763916\n",
      "\t Training loss (single batch): 1.2423886060714722\n",
      "\t Training loss (single batch): 1.5364984273910522\n",
      "\t Training loss (single batch): 1.2073382139205933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9636073112487793\n",
      "\t Training loss (single batch): 1.000654697418213\n",
      "\t Training loss (single batch): 1.0183391571044922\n",
      "\t Training loss (single batch): 1.1437979936599731\n",
      "\t Training loss (single batch): 1.2032513618469238\n",
      "\t Training loss (single batch): 1.128814458847046\n",
      "\t Training loss (single batch): 1.5422773361206055\n",
      "\t Training loss (single batch): 1.7582799196243286\n",
      "\t Training loss (single batch): 1.1564512252807617\n",
      "\t Training loss (single batch): 1.482796549797058\n",
      "\t Training loss (single batch): 1.2972201108932495\n",
      "\t Training loss (single batch): 0.8897332549095154\n",
      "\t Training loss (single batch): 1.2664498090744019\n",
      "\t Training loss (single batch): 1.4941672086715698\n",
      "\t Training loss (single batch): 1.2537522315979004\n",
      "\t Training loss (single batch): 1.0583943128585815\n",
      "\t Training loss (single batch): 1.1449459791183472\n",
      "\t Training loss (single batch): 1.4911102056503296\n",
      "\t Training loss (single batch): 0.8724164366722107\n",
      "\t Training loss (single batch): 1.1209630966186523\n",
      "\t Training loss (single batch): 0.7918485403060913\n",
      "\t Training loss (single batch): 1.0988973379135132\n",
      "\t Training loss (single batch): 1.24808669090271\n",
      "\t Training loss (single batch): 1.4614975452423096\n",
      "\t Training loss (single batch): 1.2602695226669312\n",
      "\t Training loss (single batch): 1.012306809425354\n",
      "\t Training loss (single batch): 1.7620019912719727\n",
      "\t Training loss (single batch): 1.1924840211868286\n",
      "\t Training loss (single batch): 1.1583359241485596\n",
      "\t Training loss (single batch): 1.2325067520141602\n",
      "\t Training loss (single batch): 1.393757700920105\n",
      "\t Training loss (single batch): 1.16128671169281\n",
      "\t Training loss (single batch): 1.3805345296859741\n",
      "\t Training loss (single batch): 1.3450344800949097\n",
      "\t Training loss (single batch): 1.1194466352462769\n",
      "\t Training loss (single batch): 1.513059377670288\n",
      "\t Training loss (single batch): 1.2537933588027954\n",
      "\t Training loss (single batch): 1.4602142572402954\n",
      "\t Training loss (single batch): 1.2724730968475342\n",
      "\t Training loss (single batch): 1.0482510328292847\n",
      "\t Training loss (single batch): 0.8940768241882324\n",
      "\t Training loss (single batch): 1.0209581851959229\n",
      "\t Training loss (single batch): 1.3988261222839355\n",
      "\t Training loss (single batch): 1.4737908840179443\n",
      "\t Training loss (single batch): 1.193717122077942\n",
      "\t Training loss (single batch): 1.1743017435073853\n",
      "\t Training loss (single batch): 1.0581440925598145\n",
      "\t Training loss (single batch): 1.1767199039459229\n",
      "\t Training loss (single batch): 1.3300435543060303\n",
      "\t Training loss (single batch): 1.57691490650177\n",
      "\t Training loss (single batch): 1.2360247373580933\n",
      "\t Training loss (single batch): 1.2195719480514526\n",
      "\t Training loss (single batch): 1.586012601852417\n",
      "\t Training loss (single batch): 1.1302963495254517\n",
      "\t Training loss (single batch): 1.5428476333618164\n",
      "\t Training loss (single batch): 1.1020039319992065\n",
      "\t Training loss (single batch): 1.100174903869629\n",
      "\t Training loss (single batch): 1.0259002447128296\n",
      "\t Training loss (single batch): 1.3877954483032227\n",
      "\t Training loss (single batch): 1.4963023662567139\n",
      "\t Training loss (single batch): 1.655124545097351\n",
      "\t Training loss (single batch): 1.1843156814575195\n",
      "\t Training loss (single batch): 1.378932237625122\n",
      "\t Training loss (single batch): 1.055781364440918\n",
      "\t Training loss (single batch): 1.2131768465042114\n",
      "\t Training loss (single batch): 1.1334689855575562\n",
      "\t Training loss (single batch): 0.7494255900382996\n",
      "\t Training loss (single batch): 1.3418822288513184\n",
      "\t Training loss (single batch): 1.931840419769287\n",
      "\t Training loss (single batch): 1.4220566749572754\n",
      "\t Training loss (single batch): 1.1487646102905273\n",
      "\t Training loss (single batch): 0.9832344055175781\n",
      "\t Training loss (single batch): 1.582701325416565\n",
      "\t Training loss (single batch): 1.0924901962280273\n",
      "\t Training loss (single batch): 1.3119280338287354\n",
      "\t Training loss (single batch): 1.4310015439987183\n",
      "\t Training loss (single batch): 1.334790825843811\n",
      "\t Training loss (single batch): 1.2373342514038086\n",
      "\t Training loss (single batch): 1.1376324892044067\n",
      "\t Training loss (single batch): 1.6867587566375732\n",
      "\t Training loss (single batch): 1.207645058631897\n",
      "\t Training loss (single batch): 1.4346956014633179\n",
      "\t Training loss (single batch): 1.453560471534729\n",
      "\t Training loss (single batch): 1.2695714235305786\n",
      "\t Training loss (single batch): 1.3665359020233154\n",
      "\t Training loss (single batch): 1.0419387817382812\n",
      "\t Training loss (single batch): 1.3399147987365723\n",
      "\t Training loss (single batch): 0.9136566519737244\n",
      "\t Training loss (single batch): 1.814704418182373\n",
      "\t Training loss (single batch): 1.3730535507202148\n",
      "\t Training loss (single batch): 0.9831196069717407\n",
      "\t Training loss (single batch): 1.6221712827682495\n",
      "\t Training loss (single batch): 1.0719443559646606\n",
      "\t Training loss (single batch): 0.9148091673851013\n",
      "\t Training loss (single batch): 1.3966929912567139\n",
      "\t Training loss (single batch): 1.304018497467041\n",
      "\t Training loss (single batch): 1.2193198204040527\n",
      "\t Training loss (single batch): 1.4274126291275024\n",
      "\t Training loss (single batch): 1.091570258140564\n",
      "\t Training loss (single batch): 1.5512343645095825\n",
      "\t Training loss (single batch): 1.5751731395721436\n",
      "\t Training loss (single batch): 1.1406923532485962\n",
      "\t Training loss (single batch): 1.1998611688613892\n",
      "\t Training loss (single batch): 1.0992094278335571\n",
      "\t Training loss (single batch): 1.1714645624160767\n",
      "\t Training loss (single batch): 1.3232964277267456\n",
      "\t Training loss (single batch): 1.1978007555007935\n",
      "\t Training loss (single batch): 1.6214677095413208\n",
      "\t Training loss (single batch): 1.4587152004241943\n",
      "\t Training loss (single batch): 1.030800700187683\n",
      "\t Training loss (single batch): 1.4901961088180542\n",
      "\t Training loss (single batch): 1.576012372970581\n",
      "\t Training loss (single batch): 1.5790659189224243\n",
      "\t Training loss (single batch): 1.0685856342315674\n",
      "\t Training loss (single batch): 0.954012930393219\n",
      "\t Training loss (single batch): 1.075540542602539\n",
      "\t Training loss (single batch): 1.042802095413208\n",
      "\t Training loss (single batch): 0.8040650486946106\n",
      "\t Training loss (single batch): 1.0796343088150024\n",
      "\t Training loss (single batch): 1.0922132730484009\n",
      "\t Training loss (single batch): 0.7375807166099548\n",
      "\t Training loss (single batch): 1.0940536260604858\n",
      "\t Training loss (single batch): 0.9564116597175598\n",
      "\t Training loss (single batch): 1.1929419040679932\n",
      "\t Training loss (single batch): 1.5503157377243042\n",
      "\t Training loss (single batch): 0.9225718975067139\n",
      "\t Training loss (single batch): 1.515360951423645\n",
      "\t Training loss (single batch): 1.3508776426315308\n",
      "\t Training loss (single batch): 1.2797126770019531\n",
      "\t Training loss (single batch): 1.8348883390426636\n",
      "\t Training loss (single batch): 1.1049952507019043\n",
      "\t Training loss (single batch): 0.8990254402160645\n",
      "\t Training loss (single batch): 1.0565320253372192\n",
      "\t Training loss (single batch): 1.517993450164795\n",
      "\t Training loss (single batch): 1.5545029640197754\n",
      "\t Training loss (single batch): 1.3571971654891968\n",
      "\t Training loss (single batch): 1.0273473262786865\n",
      "\t Training loss (single batch): 1.3070276975631714\n",
      "\t Training loss (single batch): 1.2741886377334595\n",
      "\t Training loss (single batch): 1.2897586822509766\n",
      "\t Training loss (single batch): 1.3863030672073364\n",
      "\t Training loss (single batch): 1.440817952156067\n",
      "\t Training loss (single batch): 1.3829946517944336\n",
      "\t Training loss (single batch): 1.1449028253555298\n",
      "\t Training loss (single batch): 1.0313485860824585\n",
      "\t Training loss (single batch): 0.9359009861946106\n",
      "\t Training loss (single batch): 1.214239478111267\n",
      "\t Training loss (single batch): 1.418691635131836\n",
      "\t Training loss (single batch): 0.8886429667472839\n",
      "\t Training loss (single batch): 1.2539764642715454\n",
      "\t Training loss (single batch): 1.8553316593170166\n",
      "\t Training loss (single batch): 1.263227939605713\n",
      "\t Training loss (single batch): 0.9636989235877991\n",
      "\t Training loss (single batch): 1.8304147720336914\n",
      "\t Training loss (single batch): 1.7501459121704102\n",
      "\t Training loss (single batch): 1.0249691009521484\n",
      "\t Training loss (single batch): 0.7368212938308716\n",
      "\t Training loss (single batch): 0.32669010758399963\n",
      "##################################\n",
      "## EPOCH 22\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2408722639083862\n",
      "\t Training loss (single batch): 1.2953749895095825\n",
      "\t Training loss (single batch): 0.6964596509933472\n",
      "\t Training loss (single batch): 1.2544410228729248\n",
      "\t Training loss (single batch): 1.0780497789382935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4324277639389038\n",
      "\t Training loss (single batch): 0.8632604479789734\n",
      "\t Training loss (single batch): 0.7556402683258057\n",
      "\t Training loss (single batch): 1.5777132511138916\n",
      "\t Training loss (single batch): 1.4942142963409424\n",
      "\t Training loss (single batch): 1.7070233821868896\n",
      "\t Training loss (single batch): 1.3868778944015503\n",
      "\t Training loss (single batch): 1.3878718614578247\n",
      "\t Training loss (single batch): 1.0161867141723633\n",
      "\t Training loss (single batch): 1.712961196899414\n",
      "\t Training loss (single batch): 1.0287915468215942\n",
      "\t Training loss (single batch): 1.089982032775879\n",
      "\t Training loss (single batch): 0.9860517978668213\n",
      "\t Training loss (single batch): 0.9834049344062805\n",
      "\t Training loss (single batch): 1.1010159254074097\n",
      "\t Training loss (single batch): 1.0081696510314941\n",
      "\t Training loss (single batch): 1.066930890083313\n",
      "\t Training loss (single batch): 0.9512038826942444\n",
      "\t Training loss (single batch): 1.0443705320358276\n",
      "\t Training loss (single batch): 0.7435169816017151\n",
      "\t Training loss (single batch): 1.1134144067764282\n",
      "\t Training loss (single batch): 1.2331123352050781\n",
      "\t Training loss (single batch): 0.9858844876289368\n",
      "\t Training loss (single batch): 1.324910044670105\n",
      "\t Training loss (single batch): 1.6610294580459595\n",
      "\t Training loss (single batch): 1.0043622255325317\n",
      "\t Training loss (single batch): 1.408502221107483\n",
      "\t Training loss (single batch): 1.4488266706466675\n",
      "\t Training loss (single batch): 1.6669442653656006\n",
      "\t Training loss (single batch): 1.2277387380599976\n",
      "\t Training loss (single batch): 1.2694566249847412\n",
      "\t Training loss (single batch): 1.0217077732086182\n",
      "\t Training loss (single batch): 1.420778751373291\n",
      "\t Training loss (single batch): 1.2234399318695068\n",
      "\t Training loss (single batch): 1.0479658842086792\n",
      "\t Training loss (single batch): 1.3609681129455566\n",
      "\t Training loss (single batch): 1.30870521068573\n",
      "\t Training loss (single batch): 1.1932011842727661\n",
      "\t Training loss (single batch): 1.3483768701553345\n",
      "\t Training loss (single batch): 2.0969526767730713\n",
      "\t Training loss (single batch): 1.252495288848877\n",
      "\t Training loss (single batch): 1.1715259552001953\n",
      "\t Training loss (single batch): 1.230876088142395\n",
      "\t Training loss (single batch): 1.4120045900344849\n",
      "\t Training loss (single batch): 1.6630080938339233\n",
      "\t Training loss (single batch): 1.0746536254882812\n",
      "\t Training loss (single batch): 1.475294589996338\n",
      "\t Training loss (single batch): 1.4299825429916382\n",
      "\t Training loss (single batch): 1.5772124528884888\n",
      "\t Training loss (single batch): 0.9329925179481506\n",
      "\t Training loss (single batch): 1.2470322847366333\n",
      "\t Training loss (single batch): 1.223423957824707\n",
      "\t Training loss (single batch): 0.9948897957801819\n",
      "\t Training loss (single batch): 0.8725767731666565\n",
      "\t Training loss (single batch): 1.3697290420532227\n",
      "\t Training loss (single batch): 0.8935397863388062\n",
      "\t Training loss (single batch): 1.1087226867675781\n",
      "\t Training loss (single batch): 1.3335962295532227\n",
      "\t Training loss (single batch): 0.9459982514381409\n",
      "\t Training loss (single batch): 1.0571184158325195\n",
      "\t Training loss (single batch): 1.6566407680511475\n",
      "\t Training loss (single batch): 1.606662392616272\n",
      "\t Training loss (single batch): 1.5370203256607056\n",
      "\t Training loss (single batch): 1.0890252590179443\n",
      "\t Training loss (single batch): 1.120749592781067\n",
      "\t Training loss (single batch): 1.2898896932601929\n",
      "\t Training loss (single batch): 1.0028592348098755\n",
      "\t Training loss (single batch): 1.3413931131362915\n",
      "\t Training loss (single batch): 1.871385097503662\n",
      "\t Training loss (single batch): 1.015665054321289\n",
      "\t Training loss (single batch): 1.5117374658584595\n",
      "\t Training loss (single batch): 1.4790263175964355\n",
      "\t Training loss (single batch): 1.1107046604156494\n",
      "\t Training loss (single batch): 1.3359642028808594\n",
      "\t Training loss (single batch): 1.625935435295105\n",
      "\t Training loss (single batch): 1.2112407684326172\n",
      "\t Training loss (single batch): 1.323407769203186\n",
      "\t Training loss (single batch): 1.2934074401855469\n",
      "\t Training loss (single batch): 1.233185052871704\n",
      "\t Training loss (single batch): 1.0142751932144165\n",
      "\t Training loss (single batch): 1.139359474182129\n",
      "\t Training loss (single batch): 0.9498830437660217\n",
      "\t Training loss (single batch): 1.132979154586792\n",
      "\t Training loss (single batch): 0.9987899661064148\n",
      "\t Training loss (single batch): 1.2892998456954956\n",
      "\t Training loss (single batch): 0.8023459315299988\n",
      "\t Training loss (single batch): 1.3713608980178833\n",
      "\t Training loss (single batch): 1.3635770082473755\n",
      "\t Training loss (single batch): 1.3422120809555054\n",
      "\t Training loss (single batch): 1.3130518198013306\n",
      "\t Training loss (single batch): 0.9055442214012146\n",
      "\t Training loss (single batch): 1.2502155303955078\n",
      "\t Training loss (single batch): 0.7999471426010132\n",
      "\t Training loss (single batch): 0.9084777235984802\n",
      "\t Training loss (single batch): 1.1240767240524292\n",
      "\t Training loss (single batch): 1.3461495637893677\n",
      "\t Training loss (single batch): 1.3420742750167847\n",
      "\t Training loss (single batch): 1.0404725074768066\n",
      "\t Training loss (single batch): 1.1580244302749634\n",
      "\t Training loss (single batch): 1.10016930103302\n",
      "\t Training loss (single batch): 1.3996411561965942\n",
      "\t Training loss (single batch): 1.4361604452133179\n",
      "\t Training loss (single batch): 0.8828557133674622\n",
      "\t Training loss (single batch): 0.7953259944915771\n",
      "\t Training loss (single batch): 1.3504486083984375\n",
      "\t Training loss (single batch): 1.277888298034668\n",
      "\t Training loss (single batch): 1.113120675086975\n",
      "\t Training loss (single batch): 1.0006225109100342\n",
      "\t Training loss (single batch): 1.536300778388977\n",
      "\t Training loss (single batch): 0.952422022819519\n",
      "\t Training loss (single batch): 1.4396382570266724\n",
      "\t Training loss (single batch): 1.331735610961914\n",
      "\t Training loss (single batch): 1.3612244129180908\n",
      "\t Training loss (single batch): 1.5922855138778687\n",
      "\t Training loss (single batch): 1.2210314273834229\n",
      "\t Training loss (single batch): 1.778014063835144\n",
      "\t Training loss (single batch): 0.9831846356391907\n",
      "\t Training loss (single batch): 1.1534464359283447\n",
      "\t Training loss (single batch): 1.07089364528656\n",
      "\t Training loss (single batch): 1.4402215480804443\n",
      "\t Training loss (single batch): 1.343743085861206\n",
      "\t Training loss (single batch): 1.2834885120391846\n",
      "\t Training loss (single batch): 1.82770836353302\n",
      "\t Training loss (single batch): 1.7683427333831787\n",
      "\t Training loss (single batch): 1.4180248975753784\n",
      "\t Training loss (single batch): 1.212525725364685\n",
      "\t Training loss (single batch): 1.6871825456619263\n",
      "\t Training loss (single batch): 1.9185547828674316\n",
      "\t Training loss (single batch): 1.459112524986267\n",
      "\t Training loss (single batch): 1.3321857452392578\n",
      "\t Training loss (single batch): 1.2662137746810913\n",
      "\t Training loss (single batch): 1.5218945741653442\n",
      "\t Training loss (single batch): 1.5183912515640259\n",
      "\t Training loss (single batch): 1.3738563060760498\n",
      "\t Training loss (single batch): 0.8956692814826965\n",
      "\t Training loss (single batch): 1.4095288515090942\n",
      "\t Training loss (single batch): 1.2881683111190796\n",
      "\t Training loss (single batch): 1.4062877893447876\n",
      "\t Training loss (single batch): 1.479119896888733\n",
      "\t Training loss (single batch): 1.1180224418640137\n",
      "\t Training loss (single batch): 1.1882175207138062\n",
      "\t Training loss (single batch): 1.5779858827590942\n",
      "\t Training loss (single batch): 0.98578280210495\n",
      "\t Training loss (single batch): 1.2299764156341553\n",
      "\t Training loss (single batch): 1.5302324295043945\n",
      "\t Training loss (single batch): 1.2674578428268433\n",
      "\t Training loss (single batch): 0.9463324546813965\n",
      "\t Training loss (single batch): 1.4644571542739868\n",
      "\t Training loss (single batch): 1.2503939867019653\n",
      "\t Training loss (single batch): 1.8328019380569458\n",
      "\t Training loss (single batch): 1.0614246129989624\n",
      "\t Training loss (single batch): 1.081207275390625\n",
      "\t Training loss (single batch): 1.1197763681411743\n",
      "\t Training loss (single batch): 0.778400719165802\n",
      "\t Training loss (single batch): 1.1560641527175903\n",
      "\t Training loss (single batch): 1.063839077949524\n",
      "\t Training loss (single batch): 1.1241742372512817\n",
      "\t Training loss (single batch): 0.997451663017273\n",
      "\t Training loss (single batch): 1.3093394041061401\n",
      "\t Training loss (single batch): 1.212941288948059\n",
      "\t Training loss (single batch): 1.3449867963790894\n",
      "\t Training loss (single batch): 0.7393768429756165\n",
      "\t Training loss (single batch): 1.0282907485961914\n",
      "\t Training loss (single batch): 1.2249666452407837\n",
      "\t Training loss (single batch): 1.4949631690979004\n",
      "\t Training loss (single batch): 1.42318594455719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.8715555667877197\n",
      "\t Training loss (single batch): 1.707106351852417\n",
      "\t Training loss (single batch): 1.3519411087036133\n",
      "\t Training loss (single batch): 1.0881738662719727\n",
      "\t Training loss (single batch): 1.3719494342803955\n",
      "\t Training loss (single batch): 0.9026641845703125\n",
      "\t Training loss (single batch): 1.2166950702667236\n",
      "\t Training loss (single batch): 1.1726665496826172\n",
      "\t Training loss (single batch): 1.480946660041809\n",
      "\t Training loss (single batch): 1.425285816192627\n",
      "\t Training loss (single batch): 1.5596905946731567\n",
      "\t Training loss (single batch): 1.1659075021743774\n",
      "\t Training loss (single batch): 1.0927722454071045\n",
      "\t Training loss (single batch): 0.9503552913665771\n",
      "\t Training loss (single batch): 1.425821304321289\n",
      "\t Training loss (single batch): 1.5399346351623535\n",
      "\t Training loss (single batch): 0.9843177199363708\n",
      "\t Training loss (single batch): 1.1255452632904053\n",
      "\t Training loss (single batch): 1.250015139579773\n",
      "\t Training loss (single batch): 0.9801558256149292\n",
      "\t Training loss (single batch): 1.0937947034835815\n",
      "\t Training loss (single batch): 0.6920441389083862\n",
      "\t Training loss (single batch): 1.5288313627243042\n",
      "\t Training loss (single batch): 1.2153164148330688\n",
      "\t Training loss (single batch): 1.0534452199935913\n",
      "\t Training loss (single batch): 0.9241045117378235\n",
      "\t Training loss (single batch): 1.2236000299453735\n",
      "\t Training loss (single batch): 1.058483600616455\n",
      "\t Training loss (single batch): 1.1599624156951904\n",
      "\t Training loss (single batch): 1.5178899765014648\n",
      "\t Training loss (single batch): 0.8835557103157043\n",
      "\t Training loss (single batch): 1.626857876777649\n",
      "\t Training loss (single batch): 1.7744321823120117\n",
      "\t Training loss (single batch): 0.9871880412101746\n",
      "\t Training loss (single batch): 1.8106011152267456\n",
      "\t Training loss (single batch): 0.7945172190666199\n",
      "\t Training loss (single batch): 1.0031155347824097\n",
      "\t Training loss (single batch): 1.1188281774520874\n",
      "\t Training loss (single batch): 0.9864542484283447\n",
      "\t Training loss (single batch): 1.3419616222381592\n",
      "\t Training loss (single batch): 1.431383490562439\n",
      "\t Training loss (single batch): 1.1507457494735718\n",
      "\t Training loss (single batch): 1.2947880029678345\n",
      "\t Training loss (single batch): 1.2854880094528198\n",
      "\t Training loss (single batch): 1.0329654216766357\n",
      "\t Training loss (single batch): 1.2221705913543701\n",
      "\t Training loss (single batch): 1.406831979751587\n",
      "\t Training loss (single batch): 1.1188894510269165\n",
      "\t Training loss (single batch): 1.5219653844833374\n",
      "\t Training loss (single batch): 1.2000036239624023\n",
      "\t Training loss (single batch): 1.3014512062072754\n",
      "\t Training loss (single batch): 1.6412736177444458\n",
      "\t Training loss (single batch): 1.2938472032546997\n",
      "\t Training loss (single batch): 1.7423133850097656\n",
      "\t Training loss (single batch): 1.0372369289398193\n",
      "\t Training loss (single batch): 1.2644187211990356\n",
      "\t Training loss (single batch): 1.28560471534729\n",
      "\t Training loss (single batch): 1.1652061939239502\n",
      "\t Training loss (single batch): 1.0982956886291504\n",
      "\t Training loss (single batch): 1.3944013118743896\n",
      "\t Training loss (single batch): 1.5673437118530273\n",
      "\t Training loss (single batch): 1.0972541570663452\n",
      "\t Training loss (single batch): 1.545383095741272\n",
      "\t Training loss (single batch): 1.4596258401870728\n",
      "\t Training loss (single batch): 0.9423419833183289\n",
      "\t Training loss (single batch): 1.2939661741256714\n",
      "\t Training loss (single batch): 1.9036312103271484\n",
      "\t Training loss (single batch): 1.0510404109954834\n",
      "\t Training loss (single batch): 0.8311331272125244\n",
      "\t Training loss (single batch): 1.7655583620071411\n",
      "\t Training loss (single batch): 1.0977214574813843\n",
      "\t Training loss (single batch): 1.4946174621582031\n",
      "\t Training loss (single batch): 1.316841721534729\n",
      "\t Training loss (single batch): 1.3806114196777344\n",
      "\t Training loss (single batch): 1.165648341178894\n",
      "\t Training loss (single batch): 1.3911588191986084\n",
      "\t Training loss (single batch): 0.9600600600242615\n",
      "\t Training loss (single batch): 1.2476855516433716\n",
      "\t Training loss (single batch): 1.137805700302124\n",
      "\t Training loss (single batch): 0.8772290349006653\n",
      "\t Training loss (single batch): 1.376145839691162\n",
      "\t Training loss (single batch): 1.0282429456710815\n",
      "\t Training loss (single batch): 1.1592596769332886\n",
      "\t Training loss (single batch): 0.9042184352874756\n",
      "\t Training loss (single batch): 1.146084189414978\n",
      "\t Training loss (single batch): 1.1490743160247803\n",
      "\t Training loss (single batch): 1.146676778793335\n",
      "\t Training loss (single batch): 1.3537687063217163\n",
      "\t Training loss (single batch): 1.3208870887756348\n",
      "\t Training loss (single batch): 1.3059674501419067\n",
      "\t Training loss (single batch): 1.4182970523834229\n",
      "\t Training loss (single batch): 1.5228195190429688\n",
      "\t Training loss (single batch): 0.9387490153312683\n",
      "\t Training loss (single batch): 1.557887315750122\n",
      "\t Training loss (single batch): 0.9733086228370667\n",
      "\t Training loss (single batch): 1.5053634643554688\n",
      "\t Training loss (single batch): 1.2198432683944702\n",
      "\t Training loss (single batch): 1.2662863731384277\n",
      "\t Training loss (single batch): 1.4003794193267822\n",
      "\t Training loss (single batch): 1.1485481262207031\n",
      "\t Training loss (single batch): 0.7466588616371155\n",
      "\t Training loss (single batch): 1.0121986865997314\n",
      "\t Training loss (single batch): 0.9588212370872498\n",
      "\t Training loss (single batch): 1.5570403337478638\n",
      "\t Training loss (single batch): 1.702541708946228\n",
      "\t Training loss (single batch): 1.2270234823226929\n",
      "\t Training loss (single batch): 1.278244972229004\n",
      "\t Training loss (single batch): 1.022551417350769\n",
      "\t Training loss (single batch): 1.279771089553833\n",
      "\t Training loss (single batch): 1.7040174007415771\n",
      "\t Training loss (single batch): 1.110189437866211\n",
      "\t Training loss (single batch): 1.7830528020858765\n",
      "\t Training loss (single batch): 0.6430866122245789\n",
      "\t Training loss (single batch): 1.8057881593704224\n",
      "\t Training loss (single batch): 1.6836353540420532\n",
      "\t Training loss (single batch): 1.3417569398880005\n",
      "\t Training loss (single batch): 1.4898403882980347\n",
      "\t Training loss (single batch): 1.066953182220459\n",
      "\t Training loss (single batch): 1.5959138870239258\n",
      "\t Training loss (single batch): 1.4950305223464966\n",
      "\t Training loss (single batch): 1.3299753665924072\n",
      "\t Training loss (single batch): 1.1463847160339355\n",
      "\t Training loss (single batch): 1.0523327589035034\n",
      "\t Training loss (single batch): 1.1799153089523315\n",
      "\t Training loss (single batch): 1.4360418319702148\n",
      "\t Training loss (single batch): 1.0980885028839111\n",
      "\t Training loss (single batch): 0.8256033658981323\n",
      "\t Training loss (single batch): 1.1462182998657227\n",
      "\t Training loss (single batch): 1.1212024688720703\n",
      "\t Training loss (single batch): 1.232696533203125\n",
      "\t Training loss (single batch): 0.923425018787384\n",
      "\t Training loss (single batch): 1.6497609615325928\n",
      "\t Training loss (single batch): 1.405055284500122\n",
      "\t Training loss (single batch): 1.345971941947937\n",
      "\t Training loss (single batch): 1.0412565469741821\n",
      "\t Training loss (single batch): 1.1872092485427856\n",
      "\t Training loss (single batch): 1.524799108505249\n",
      "\t Training loss (single batch): 1.5403896570205688\n",
      "\t Training loss (single batch): 1.55332350730896\n",
      "\t Training loss (single batch): 1.4243069887161255\n",
      "\t Training loss (single batch): 1.4711689949035645\n",
      "\t Training loss (single batch): 1.1155394315719604\n",
      "\t Training loss (single batch): 0.7765018343925476\n",
      "\t Training loss (single batch): 0.761681318283081\n",
      "\t Training loss (single batch): 1.7491718530654907\n",
      "\t Training loss (single batch): 0.578822910785675\n",
      "##################################\n",
      "## EPOCH 23\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8875089287757874\n",
      "\t Training loss (single batch): 1.010880470275879\n",
      "\t Training loss (single batch): 1.397796869277954\n",
      "\t Training loss (single batch): 1.7422692775726318\n",
      "\t Training loss (single batch): 1.5658811330795288\n",
      "\t Training loss (single batch): 1.1327040195465088\n",
      "\t Training loss (single batch): 1.0891746282577515\n",
      "\t Training loss (single batch): 1.6312212944030762\n",
      "\t Training loss (single batch): 0.928611159324646\n",
      "\t Training loss (single batch): 1.4780921936035156\n",
      "\t Training loss (single batch): 0.9796318411827087\n",
      "\t Training loss (single batch): 1.295584797859192\n",
      "\t Training loss (single batch): 1.4753901958465576\n",
      "\t Training loss (single batch): 1.3423205614089966\n",
      "\t Training loss (single batch): 1.4035050868988037\n",
      "\t Training loss (single batch): 1.5849249362945557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9124817848205566\n",
      "\t Training loss (single batch): 1.1260242462158203\n",
      "\t Training loss (single batch): 1.0520739555358887\n",
      "\t Training loss (single batch): 1.364147424697876\n",
      "\t Training loss (single batch): 1.260555624961853\n",
      "\t Training loss (single batch): 1.2908838987350464\n",
      "\t Training loss (single batch): 1.4110549688339233\n",
      "\t Training loss (single batch): 1.5850296020507812\n",
      "\t Training loss (single batch): 1.0366218090057373\n",
      "\t Training loss (single batch): 1.619208574295044\n",
      "\t Training loss (single batch): 0.9255942106246948\n",
      "\t Training loss (single batch): 0.9862262010574341\n",
      "\t Training loss (single batch): 1.0894769430160522\n",
      "\t Training loss (single batch): 1.3149937391281128\n",
      "\t Training loss (single batch): 0.8822517395019531\n",
      "\t Training loss (single batch): 1.31647527217865\n",
      "\t Training loss (single batch): 1.063744306564331\n",
      "\t Training loss (single batch): 0.9875537157058716\n",
      "\t Training loss (single batch): 1.3737295866012573\n",
      "\t Training loss (single batch): 0.906600296497345\n",
      "\t Training loss (single batch): 2.026827096939087\n",
      "\t Training loss (single batch): 1.4987554550170898\n",
      "\t Training loss (single batch): 1.3356813192367554\n",
      "\t Training loss (single batch): 1.1287490129470825\n",
      "\t Training loss (single batch): 1.255037546157837\n",
      "\t Training loss (single batch): 0.8295716047286987\n",
      "\t Training loss (single batch): 0.987834632396698\n",
      "\t Training loss (single batch): 1.4823658466339111\n",
      "\t Training loss (single batch): 1.4214969873428345\n",
      "\t Training loss (single batch): 0.7929396629333496\n",
      "\t Training loss (single batch): 0.8296019434928894\n",
      "\t Training loss (single batch): 0.9055763483047485\n",
      "\t Training loss (single batch): 1.5505186319351196\n",
      "\t Training loss (single batch): 1.3336464166641235\n",
      "\t Training loss (single batch): 1.4361705780029297\n",
      "\t Training loss (single batch): 1.8644968271255493\n",
      "\t Training loss (single batch): 1.1339104175567627\n",
      "\t Training loss (single batch): 1.3538923263549805\n",
      "\t Training loss (single batch): 0.9625095725059509\n",
      "\t Training loss (single batch): 1.4640508890151978\n",
      "\t Training loss (single batch): 1.3331407308578491\n",
      "\t Training loss (single batch): 1.782660961151123\n",
      "\t Training loss (single batch): 1.1360549926757812\n",
      "\t Training loss (single batch): 1.5744448900222778\n",
      "\t Training loss (single batch): 1.3170278072357178\n",
      "\t Training loss (single batch): 1.2722963094711304\n",
      "\t Training loss (single batch): 1.7353790998458862\n",
      "\t Training loss (single batch): 0.9174990057945251\n",
      "\t Training loss (single batch): 1.6935791969299316\n",
      "\t Training loss (single batch): 1.3217148780822754\n",
      "\t Training loss (single batch): 1.7516340017318726\n",
      "\t Training loss (single batch): 1.2533422708511353\n",
      "\t Training loss (single batch): 1.0411889553070068\n",
      "\t Training loss (single batch): 1.7222992181777954\n",
      "\t Training loss (single batch): 1.319357991218567\n",
      "\t Training loss (single batch): 1.5595901012420654\n",
      "\t Training loss (single batch): 1.2776691913604736\n",
      "\t Training loss (single batch): 1.1731492280960083\n",
      "\t Training loss (single batch): 0.9959501624107361\n",
      "\t Training loss (single batch): 0.9327359795570374\n",
      "\t Training loss (single batch): 1.7327475547790527\n",
      "\t Training loss (single batch): 1.1546778678894043\n",
      "\t Training loss (single batch): 1.5424144268035889\n",
      "\t Training loss (single batch): 1.0716606378555298\n",
      "\t Training loss (single batch): 1.37490975856781\n",
      "\t Training loss (single batch): 0.9999732971191406\n",
      "\t Training loss (single batch): 1.168704628944397\n",
      "\t Training loss (single batch): 1.3900634050369263\n",
      "\t Training loss (single batch): 1.1390230655670166\n",
      "\t Training loss (single batch): 1.0671148300170898\n",
      "\t Training loss (single batch): 1.150347113609314\n",
      "\t Training loss (single batch): 1.070960521697998\n",
      "\t Training loss (single batch): 1.1089736223220825\n",
      "\t Training loss (single batch): 1.138717532157898\n",
      "\t Training loss (single batch): 0.7949106097221375\n",
      "\t Training loss (single batch): 1.0014504194259644\n",
      "\t Training loss (single batch): 1.4462116956710815\n",
      "\t Training loss (single batch): 1.061825156211853\n",
      "\t Training loss (single batch): 1.1213440895080566\n",
      "\t Training loss (single batch): 1.1882472038269043\n",
      "\t Training loss (single batch): 1.1819040775299072\n",
      "\t Training loss (single batch): 1.1973642110824585\n",
      "\t Training loss (single batch): 1.1407212018966675\n",
      "\t Training loss (single batch): 0.9133187532424927\n",
      "\t Training loss (single batch): 1.2183740139007568\n",
      "\t Training loss (single batch): 1.0415527820587158\n",
      "\t Training loss (single batch): 0.944452166557312\n",
      "\t Training loss (single batch): 1.3128981590270996\n",
      "\t Training loss (single batch): 1.5017280578613281\n",
      "\t Training loss (single batch): 1.839123010635376\n",
      "\t Training loss (single batch): 1.1667468547821045\n",
      "\t Training loss (single batch): 1.161641240119934\n",
      "\t Training loss (single batch): 1.0272387266159058\n",
      "\t Training loss (single batch): 1.4612059593200684\n",
      "\t Training loss (single batch): 2.3435375690460205\n",
      "\t Training loss (single batch): 1.3353514671325684\n",
      "\t Training loss (single batch): 1.392546534538269\n",
      "\t Training loss (single batch): 1.1272425651550293\n",
      "\t Training loss (single batch): 1.1845687627792358\n",
      "\t Training loss (single batch): 1.170506477355957\n",
      "\t Training loss (single batch): 1.0256881713867188\n",
      "\t Training loss (single batch): 1.0788496732711792\n",
      "\t Training loss (single batch): 1.316810965538025\n",
      "\t Training loss (single batch): 0.7363793849945068\n",
      "\t Training loss (single batch): 1.3366360664367676\n",
      "\t Training loss (single batch): 1.3795418739318848\n",
      "\t Training loss (single batch): 1.0689687728881836\n",
      "\t Training loss (single batch): 1.2090026140213013\n",
      "\t Training loss (single batch): 1.2700848579406738\n",
      "\t Training loss (single batch): 0.9476945996284485\n",
      "\t Training loss (single batch): 1.316407322883606\n",
      "\t Training loss (single batch): 0.9280548095703125\n",
      "\t Training loss (single batch): 0.9803243279457092\n",
      "\t Training loss (single batch): 1.6379847526550293\n",
      "\t Training loss (single batch): 1.6727467775344849\n",
      "\t Training loss (single batch): 1.1971781253814697\n",
      "\t Training loss (single batch): 1.6239303350448608\n",
      "\t Training loss (single batch): 1.1923085451126099\n",
      "\t Training loss (single batch): 0.912381649017334\n",
      "\t Training loss (single batch): 1.430588960647583\n",
      "\t Training loss (single batch): 1.1852387189865112\n",
      "\t Training loss (single batch): 1.138319969177246\n",
      "\t Training loss (single batch): 1.0707811117172241\n",
      "\t Training loss (single batch): 1.3887556791305542\n",
      "\t Training loss (single batch): 1.6348135471343994\n",
      "\t Training loss (single batch): 1.4612040519714355\n",
      "\t Training loss (single batch): 1.1565855741500854\n",
      "\t Training loss (single batch): 1.2976446151733398\n",
      "\t Training loss (single batch): 0.9093047976493835\n",
      "\t Training loss (single batch): 1.0293892621994019\n",
      "\t Training loss (single batch): 1.0907082557678223\n",
      "\t Training loss (single batch): 1.3339654207229614\n",
      "\t Training loss (single batch): 1.4448250532150269\n",
      "\t Training loss (single batch): 1.1171455383300781\n",
      "\t Training loss (single batch): 1.073356032371521\n",
      "\t Training loss (single batch): 1.2257020473480225\n",
      "\t Training loss (single batch): 1.1707428693771362\n",
      "\t Training loss (single batch): 0.6108912825584412\n",
      "\t Training loss (single batch): 0.8872354030609131\n",
      "\t Training loss (single batch): 1.1355592012405396\n",
      "\t Training loss (single batch): 0.934836208820343\n",
      "\t Training loss (single batch): 1.2760719060897827\n",
      "\t Training loss (single batch): 1.326306700706482\n",
      "\t Training loss (single batch): 1.1871581077575684\n",
      "\t Training loss (single batch): 1.322739601135254\n",
      "\t Training loss (single batch): 1.260155200958252\n",
      "\t Training loss (single batch): 1.6196006536483765\n",
      "\t Training loss (single batch): 1.031491756439209\n",
      "\t Training loss (single batch): 1.3236849308013916\n",
      "\t Training loss (single batch): 1.1966054439544678\n",
      "\t Training loss (single batch): 1.4952466487884521\n",
      "\t Training loss (single batch): 1.2538952827453613\n",
      "\t Training loss (single batch): 1.323146104812622\n",
      "\t Training loss (single batch): 1.0174051523208618\n",
      "\t Training loss (single batch): 1.0329679250717163\n",
      "\t Training loss (single batch): 1.8036596775054932\n",
      "\t Training loss (single batch): 1.4802148342132568\n",
      "\t Training loss (single batch): 0.7428126931190491\n",
      "\t Training loss (single batch): 1.4659032821655273\n",
      "\t Training loss (single batch): 1.097662091255188\n",
      "\t Training loss (single batch): 1.8248975276947021\n",
      "\t Training loss (single batch): 1.0214632749557495\n",
      "\t Training loss (single batch): 1.4224690198898315\n",
      "\t Training loss (single batch): 1.3899399042129517\n",
      "\t Training loss (single batch): 0.9991695284843445\n",
      "\t Training loss (single batch): 1.3876913785934448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5408875942230225\n",
      "\t Training loss (single batch): 1.6022303104400635\n",
      "\t Training loss (single batch): 1.2876613140106201\n",
      "\t Training loss (single batch): 1.1324037313461304\n",
      "\t Training loss (single batch): 1.3371554613113403\n",
      "\t Training loss (single batch): 1.0765897035598755\n",
      "\t Training loss (single batch): 1.3024625778198242\n",
      "\t Training loss (single batch): 1.2669588327407837\n",
      "\t Training loss (single batch): 1.664939522743225\n",
      "\t Training loss (single batch): 1.2385995388031006\n",
      "\t Training loss (single batch): 1.2328672409057617\n",
      "\t Training loss (single batch): 1.7857539653778076\n",
      "\t Training loss (single batch): 1.3719650506973267\n",
      "\t Training loss (single batch): 1.1816765069961548\n",
      "\t Training loss (single batch): 0.8434669375419617\n",
      "\t Training loss (single batch): 1.2905782461166382\n",
      "\t Training loss (single batch): 1.0074002742767334\n",
      "\t Training loss (single batch): 1.4878464937210083\n",
      "\t Training loss (single batch): 1.6806068420410156\n",
      "\t Training loss (single batch): 1.0911383628845215\n",
      "\t Training loss (single batch): 1.0391677618026733\n",
      "\t Training loss (single batch): 1.024134874343872\n",
      "\t Training loss (single batch): 1.3218843936920166\n",
      "\t Training loss (single batch): 1.0743080377578735\n",
      "\t Training loss (single batch): 1.3914721012115479\n",
      "\t Training loss (single batch): 1.1036213636398315\n",
      "\t Training loss (single batch): 1.4862444400787354\n",
      "\t Training loss (single batch): 1.2340292930603027\n",
      "\t Training loss (single batch): 1.4806817770004272\n",
      "\t Training loss (single batch): 1.117717981338501\n",
      "\t Training loss (single batch): 1.4359731674194336\n",
      "\t Training loss (single batch): 1.3963520526885986\n",
      "\t Training loss (single batch): 1.8562443256378174\n",
      "\t Training loss (single batch): 1.0495051145553589\n",
      "\t Training loss (single batch): 1.548357367515564\n",
      "\t Training loss (single batch): 1.6156284809112549\n",
      "\t Training loss (single batch): 1.2813332080841064\n",
      "\t Training loss (single batch): 1.2523014545440674\n",
      "\t Training loss (single batch): 1.2020282745361328\n",
      "\t Training loss (single batch): 1.3063143491744995\n",
      "\t Training loss (single batch): 1.1337149143218994\n",
      "\t Training loss (single batch): 1.3790658712387085\n",
      "\t Training loss (single batch): 1.532088041305542\n",
      "\t Training loss (single batch): 1.3694859743118286\n",
      "\t Training loss (single batch): 1.4379253387451172\n",
      "\t Training loss (single batch): 1.3018462657928467\n",
      "\t Training loss (single batch): 0.9051812291145325\n",
      "\t Training loss (single batch): 1.0537742376327515\n",
      "\t Training loss (single batch): 1.5798319578170776\n",
      "\t Training loss (single batch): 1.209124207496643\n",
      "\t Training loss (single batch): 1.1699352264404297\n",
      "\t Training loss (single batch): 1.2906599044799805\n",
      "\t Training loss (single batch): 0.8782872557640076\n",
      "\t Training loss (single batch): 1.2170318365097046\n",
      "\t Training loss (single batch): 1.5760576725006104\n",
      "\t Training loss (single batch): 0.9053628444671631\n",
      "\t Training loss (single batch): 1.0727967023849487\n",
      "\t Training loss (single batch): 1.3747177124023438\n",
      "\t Training loss (single batch): 1.0418727397918701\n",
      "\t Training loss (single batch): 1.186635971069336\n",
      "\t Training loss (single batch): 1.374457597732544\n",
      "\t Training loss (single batch): 1.147039771080017\n",
      "\t Training loss (single batch): 1.3070669174194336\n",
      "\t Training loss (single batch): 1.8608595132827759\n",
      "\t Training loss (single batch): 0.8969753384590149\n",
      "\t Training loss (single batch): 1.5832778215408325\n",
      "\t Training loss (single batch): 1.4128336906433105\n",
      "\t Training loss (single batch): 1.4183217287063599\n",
      "\t Training loss (single batch): 1.1001566648483276\n",
      "\t Training loss (single batch): 1.6811366081237793\n",
      "\t Training loss (single batch): 1.5019009113311768\n",
      "\t Training loss (single batch): 1.21583890914917\n",
      "\t Training loss (single batch): 1.6034759283065796\n",
      "\t Training loss (single batch): 1.4281595945358276\n",
      "\t Training loss (single batch): 1.0556926727294922\n",
      "\t Training loss (single batch): 1.1266831159591675\n",
      "\t Training loss (single batch): 1.2623273134231567\n",
      "\t Training loss (single batch): 2.092369794845581\n",
      "\t Training loss (single batch): 1.0256919860839844\n",
      "\t Training loss (single batch): 1.2370918989181519\n",
      "\t Training loss (single batch): 0.9282961487770081\n",
      "\t Training loss (single batch): 1.0490696430206299\n",
      "\t Training loss (single batch): 1.594398856163025\n",
      "\t Training loss (single batch): 1.2916852235794067\n",
      "\t Training loss (single batch): 1.458463430404663\n",
      "\t Training loss (single batch): 1.5037630796432495\n",
      "\t Training loss (single batch): 1.5499330759048462\n",
      "\t Training loss (single batch): 1.146714210510254\n",
      "\t Training loss (single batch): 1.4314383268356323\n",
      "\t Training loss (single batch): 1.1168137788772583\n",
      "\t Training loss (single batch): 1.455989956855774\n",
      "\t Training loss (single batch): 1.5594589710235596\n",
      "\t Training loss (single batch): 1.4578922986984253\n",
      "\t Training loss (single batch): 1.6685395240783691\n",
      "\t Training loss (single batch): 1.0642895698547363\n",
      "\t Training loss (single batch): 1.0437052249908447\n",
      "\t Training loss (single batch): 1.8686320781707764\n",
      "\t Training loss (single batch): 1.021543264389038\n",
      "\t Training loss (single batch): 0.9696268439292908\n",
      "\t Training loss (single batch): 1.3273521661758423\n",
      "\t Training loss (single batch): 1.2659246921539307\n",
      "\t Training loss (single batch): 1.2459558248519897\n",
      "\t Training loss (single batch): 1.4631483554840088\n",
      "\t Training loss (single batch): 0.9058268070220947\n",
      "\t Training loss (single batch): 1.705927848815918\n",
      "\t Training loss (single batch): 1.1445246934890747\n",
      "\t Training loss (single batch): 1.6342527866363525\n",
      "\t Training loss (single batch): 1.0962485074996948\n",
      "\t Training loss (single batch): 1.3785890340805054\n",
      "\t Training loss (single batch): 1.1941264867782593\n",
      "\t Training loss (single batch): 1.1014409065246582\n",
      "\t Training loss (single batch): 1.1176189184188843\n",
      "\t Training loss (single batch): 0.9751951694488525\n",
      "\t Training loss (single batch): 1.2641239166259766\n",
      "\t Training loss (single batch): 0.7564359903335571\n",
      "\t Training loss (single batch): 1.4306068420410156\n",
      "\t Training loss (single batch): 1.2128247022628784\n",
      "\t Training loss (single batch): 0.7857275605201721\n",
      "\t Training loss (single batch): 0.6694523096084595\n",
      "\t Training loss (single batch): 1.2312045097351074\n",
      "\t Training loss (single batch): 1.4326677322387695\n",
      "\t Training loss (single batch): 1.2684926986694336\n",
      "\t Training loss (single batch): 0.8739705681800842\n",
      "\t Training loss (single batch): 1.151991367340088\n",
      "\t Training loss (single batch): 1.3031188249588013\n",
      "\t Training loss (single batch): 1.5948435068130493\n",
      "\t Training loss (single batch): 1.0823923349380493\n",
      "\t Training loss (single batch): 1.3828622102737427\n",
      "\t Training loss (single batch): 1.5513838529586792\n",
      "\t Training loss (single batch): 0.8344081044197083\n",
      "\t Training loss (single batch): 0.8784247636795044\n",
      "\t Training loss (single batch): 1.3233202695846558\n",
      "\t Training loss (single batch): 0.9807398319244385\n",
      "\t Training loss (single batch): 1.6098231077194214\n",
      "\t Training loss (single batch): 0.4379922151565552\n",
      "##################################\n",
      "## EPOCH 24\n",
      "##################################\n",
      "\t Training loss (single batch): 1.956729769706726\n",
      "\t Training loss (single batch): 1.4171555042266846\n",
      "\t Training loss (single batch): 1.0504159927368164\n",
      "\t Training loss (single batch): 1.412677526473999\n",
      "\t Training loss (single batch): 1.0257478952407837\n",
      "\t Training loss (single batch): 1.1217026710510254\n",
      "\t Training loss (single batch): 0.9630542397499084\n",
      "\t Training loss (single batch): 1.5162442922592163\n",
      "\t Training loss (single batch): 1.727030634880066\n",
      "\t Training loss (single batch): 1.451743721961975\n",
      "\t Training loss (single batch): 1.0778409242630005\n",
      "\t Training loss (single batch): 1.4754191637039185\n",
      "\t Training loss (single batch): 1.471022367477417\n",
      "\t Training loss (single batch): 1.189355731010437\n",
      "\t Training loss (single batch): 1.2624469995498657\n",
      "\t Training loss (single batch): 1.1273740530014038\n",
      "\t Training loss (single batch): 1.024848461151123\n",
      "\t Training loss (single batch): 1.4214463233947754\n",
      "\t Training loss (single batch): 1.5717270374298096\n",
      "\t Training loss (single batch): 1.2947155237197876\n",
      "\t Training loss (single batch): 0.9519161581993103\n",
      "\t Training loss (single batch): 1.2175343036651611\n",
      "\t Training loss (single batch): 1.8385837078094482\n",
      "\t Training loss (single batch): 1.7097272872924805\n",
      "\t Training loss (single batch): 1.1469345092773438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6934716701507568\n",
      "\t Training loss (single batch): 1.5000724792480469\n",
      "\t Training loss (single batch): 1.113585114479065\n",
      "\t Training loss (single batch): 1.0554604530334473\n",
      "\t Training loss (single batch): 1.1820183992385864\n",
      "\t Training loss (single batch): 0.9753835797309875\n",
      "\t Training loss (single batch): 1.323229193687439\n",
      "\t Training loss (single batch): 1.0551948547363281\n",
      "\t Training loss (single batch): 1.3474520444869995\n",
      "\t Training loss (single batch): 1.2389864921569824\n",
      "\t Training loss (single batch): 1.3722996711730957\n",
      "\t Training loss (single batch): 1.2506128549575806\n",
      "\t Training loss (single batch): 1.4037257432937622\n",
      "\t Training loss (single batch): 1.7569392919540405\n",
      "\t Training loss (single batch): 1.0812695026397705\n",
      "\t Training loss (single batch): 1.1600977182388306\n",
      "\t Training loss (single batch): 1.4436920881271362\n",
      "\t Training loss (single batch): 1.321552038192749\n",
      "\t Training loss (single batch): 1.3978667259216309\n",
      "\t Training loss (single batch): 0.8989160656929016\n",
      "\t Training loss (single batch): 1.4241526126861572\n",
      "\t Training loss (single batch): 1.314080834388733\n",
      "\t Training loss (single batch): 0.8742097616195679\n",
      "\t Training loss (single batch): 1.258630394935608\n",
      "\t Training loss (single batch): 1.1198863983154297\n",
      "\t Training loss (single batch): 0.7873305082321167\n",
      "\t Training loss (single batch): 1.4218966960906982\n",
      "\t Training loss (single batch): 1.3861969709396362\n",
      "\t Training loss (single batch): 1.130088210105896\n",
      "\t Training loss (single batch): 1.2426868677139282\n",
      "\t Training loss (single batch): 1.2683497667312622\n",
      "\t Training loss (single batch): 0.9601525664329529\n",
      "\t Training loss (single batch): 1.1094213724136353\n",
      "\t Training loss (single batch): 1.0507339239120483\n",
      "\t Training loss (single batch): 0.9566303491592407\n",
      "\t Training loss (single batch): 1.526526689529419\n",
      "\t Training loss (single batch): 1.047472357749939\n",
      "\t Training loss (single batch): 1.3918485641479492\n",
      "\t Training loss (single batch): 1.1738605499267578\n",
      "\t Training loss (single batch): 1.247228980064392\n",
      "\t Training loss (single batch): 1.1662390232086182\n",
      "\t Training loss (single batch): 1.3871300220489502\n",
      "\t Training loss (single batch): 1.0639796257019043\n",
      "\t Training loss (single batch): 1.2791260480880737\n",
      "\t Training loss (single batch): 1.1676697731018066\n",
      "\t Training loss (single batch): 1.061546802520752\n",
      "\t Training loss (single batch): 1.174604058265686\n",
      "\t Training loss (single batch): 1.274179220199585\n",
      "\t Training loss (single batch): 1.2318845987319946\n",
      "\t Training loss (single batch): 2.1254212856292725\n",
      "\t Training loss (single batch): 1.2077754735946655\n",
      "\t Training loss (single batch): 1.1777615547180176\n",
      "\t Training loss (single batch): 1.4170159101486206\n",
      "\t Training loss (single batch): 1.418953776359558\n",
      "\t Training loss (single batch): 1.4830915927886963\n",
      "\t Training loss (single batch): 0.9269983172416687\n",
      "\t Training loss (single batch): 1.0239856243133545\n",
      "\t Training loss (single batch): 1.0929712057113647\n",
      "\t Training loss (single batch): 1.1990118026733398\n",
      "\t Training loss (single batch): 1.2455464601516724\n",
      "\t Training loss (single batch): 1.3795965909957886\n",
      "\t Training loss (single batch): 1.0424730777740479\n",
      "\t Training loss (single batch): 1.1022955179214478\n",
      "\t Training loss (single batch): 1.3136810064315796\n",
      "\t Training loss (single batch): 1.337485432624817\n",
      "\t Training loss (single batch): 1.60532546043396\n",
      "\t Training loss (single batch): 1.17124605178833\n",
      "\t Training loss (single batch): 0.8905781507492065\n",
      "\t Training loss (single batch): 1.0676000118255615\n",
      "\t Training loss (single batch): 0.9215592741966248\n",
      "\t Training loss (single batch): 1.032486081123352\n",
      "\t Training loss (single batch): 1.3568384647369385\n",
      "\t Training loss (single batch): 1.0474785566329956\n",
      "\t Training loss (single batch): 1.1540299654006958\n",
      "\t Training loss (single batch): 1.6313222646713257\n",
      "\t Training loss (single batch): 1.4658997058868408\n",
      "\t Training loss (single batch): 0.8136724233627319\n",
      "\t Training loss (single batch): 0.8433713316917419\n",
      "\t Training loss (single batch): 1.1392484903335571\n",
      "\t Training loss (single batch): 0.9143623113632202\n",
      "\t Training loss (single batch): 1.1251133680343628\n",
      "\t Training loss (single batch): 1.2181510925292969\n",
      "\t Training loss (single batch): 1.329178810119629\n",
      "\t Training loss (single batch): 1.172891616821289\n",
      "\t Training loss (single batch): 1.092507243156433\n",
      "\t Training loss (single batch): 1.178465485572815\n",
      "\t Training loss (single batch): 1.067109227180481\n",
      "\t Training loss (single batch): 1.4220190048217773\n",
      "\t Training loss (single batch): 1.1010708808898926\n",
      "\t Training loss (single batch): 1.7269818782806396\n",
      "\t Training loss (single batch): 0.9016028642654419\n",
      "\t Training loss (single batch): 1.2293055057525635\n",
      "\t Training loss (single batch): 1.5399543046951294\n",
      "\t Training loss (single batch): 1.203582525253296\n",
      "\t Training loss (single batch): 1.3072822093963623\n",
      "\t Training loss (single batch): 0.9686413407325745\n",
      "\t Training loss (single batch): 0.8873039484024048\n",
      "\t Training loss (single batch): 1.4276164770126343\n",
      "\t Training loss (single batch): 0.9569824934005737\n",
      "\t Training loss (single batch): 1.7299811840057373\n",
      "\t Training loss (single batch): 0.8679417371749878\n",
      "\t Training loss (single batch): 1.2762911319732666\n",
      "\t Training loss (single batch): 1.660210371017456\n",
      "\t Training loss (single batch): 0.9476970434188843\n",
      "\t Training loss (single batch): 1.2607815265655518\n",
      "\t Training loss (single batch): 1.4491571187973022\n",
      "\t Training loss (single batch): 1.479446530342102\n",
      "\t Training loss (single batch): 1.2580183744430542\n",
      "\t Training loss (single batch): 1.0117700099945068\n",
      "\t Training loss (single batch): 1.3889901638031006\n",
      "\t Training loss (single batch): 1.2819796800613403\n",
      "\t Training loss (single batch): 1.0335453748703003\n",
      "\t Training loss (single batch): 1.0903005599975586\n",
      "\t Training loss (single batch): 1.183439016342163\n",
      "\t Training loss (single batch): 1.0962368249893188\n",
      "\t Training loss (single batch): 1.1732605695724487\n",
      "\t Training loss (single batch): 1.68299400806427\n",
      "\t Training loss (single batch): 1.4248296022415161\n",
      "\t Training loss (single batch): 0.8500610589981079\n",
      "\t Training loss (single batch): 1.2097766399383545\n",
      "\t Training loss (single batch): 1.4468103647232056\n",
      "\t Training loss (single batch): 1.0421985387802124\n",
      "\t Training loss (single batch): 1.0450985431671143\n",
      "\t Training loss (single batch): 0.9132901430130005\n",
      "\t Training loss (single batch): 0.93437659740448\n",
      "\t Training loss (single batch): 1.5408490896224976\n",
      "\t Training loss (single batch): 1.437367558479309\n",
      "\t Training loss (single batch): 1.2882492542266846\n",
      "\t Training loss (single batch): 1.2482399940490723\n",
      "\t Training loss (single batch): 1.197631597518921\n",
      "\t Training loss (single batch): 1.5074151754379272\n",
      "\t Training loss (single batch): 1.2598295211791992\n",
      "\t Training loss (single batch): 1.1669038534164429\n",
      "\t Training loss (single batch): 1.5161480903625488\n",
      "\t Training loss (single batch): 1.2771837711334229\n",
      "\t Training loss (single batch): 1.091753602027893\n",
      "\t Training loss (single batch): 1.205188512802124\n",
      "\t Training loss (single batch): 1.0361337661743164\n",
      "\t Training loss (single batch): 0.9144520163536072\n",
      "\t Training loss (single batch): 1.4702180624008179\n",
      "\t Training loss (single batch): 1.4311326742172241\n",
      "\t Training loss (single batch): 1.3115818500518799\n",
      "\t Training loss (single batch): 1.1720476150512695\n",
      "\t Training loss (single batch): 1.5192313194274902\n",
      "\t Training loss (single batch): 1.1857197284698486\n",
      "\t Training loss (single batch): 1.1174856424331665\n",
      "\t Training loss (single batch): 1.4043329954147339\n",
      "\t Training loss (single batch): 0.9784250259399414\n",
      "\t Training loss (single batch): 0.8809034824371338\n",
      "\t Training loss (single batch): 1.14561927318573\n",
      "\t Training loss (single batch): 1.2381033897399902\n",
      "\t Training loss (single batch): 1.2172011137008667\n",
      "\t Training loss (single batch): 1.04676353931427\n",
      "\t Training loss (single batch): 1.050689697265625\n",
      "\t Training loss (single batch): 1.208877682685852\n",
      "\t Training loss (single batch): 0.9734197854995728\n",
      "\t Training loss (single batch): 1.153944492340088\n",
      "\t Training loss (single batch): 1.2111353874206543\n",
      "\t Training loss (single batch): 0.7098784446716309\n",
      "\t Training loss (single batch): 1.8245195150375366\n",
      "\t Training loss (single batch): 1.0727185010910034\n",
      "\t Training loss (single batch): 1.1406549215316772\n",
      "\t Training loss (single batch): 1.1094162464141846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9262616634368896\n",
      "\t Training loss (single batch): 1.4155012369155884\n",
      "\t Training loss (single batch): 1.0915162563323975\n",
      "\t Training loss (single batch): 1.9220272302627563\n",
      "\t Training loss (single batch): 1.064598798751831\n",
      "\t Training loss (single batch): 1.3576122522354126\n",
      "\t Training loss (single batch): 0.808807373046875\n",
      "\t Training loss (single batch): 1.5373636484146118\n",
      "\t Training loss (single batch): 0.9593482613563538\n",
      "\t Training loss (single batch): 1.4494352340698242\n",
      "\t Training loss (single batch): 1.2854423522949219\n",
      "\t Training loss (single batch): 1.392956018447876\n",
      "\t Training loss (single batch): 0.9982659220695496\n",
      "\t Training loss (single batch): 1.4297451972961426\n",
      "\t Training loss (single batch): 1.3849512338638306\n",
      "\t Training loss (single batch): 0.8667211532592773\n",
      "\t Training loss (single batch): 1.3013811111450195\n",
      "\t Training loss (single batch): 0.9971959590911865\n",
      "\t Training loss (single batch): 1.5756787061691284\n",
      "\t Training loss (single batch): 0.9968997836112976\n",
      "\t Training loss (single batch): 1.1001574993133545\n",
      "\t Training loss (single batch): 1.0170707702636719\n",
      "\t Training loss (single batch): 1.5123318433761597\n",
      "\t Training loss (single batch): 1.4075114727020264\n",
      "\t Training loss (single batch): 1.6722233295440674\n",
      "\t Training loss (single batch): 1.5803709030151367\n",
      "\t Training loss (single batch): 1.0132997035980225\n",
      "\t Training loss (single batch): 1.7683041095733643\n",
      "\t Training loss (single batch): 1.9198458194732666\n",
      "\t Training loss (single batch): 0.9637402296066284\n",
      "\t Training loss (single batch): 1.116331696510315\n",
      "\t Training loss (single batch): 1.0664303302764893\n",
      "\t Training loss (single batch): 1.4074268341064453\n",
      "\t Training loss (single batch): 1.591786503791809\n",
      "\t Training loss (single batch): 1.749445915222168\n",
      "\t Training loss (single batch): 0.7677584290504456\n",
      "\t Training loss (single batch): 1.1385568380355835\n",
      "\t Training loss (single batch): 1.1644521951675415\n",
      "\t Training loss (single batch): 1.4644578695297241\n",
      "\t Training loss (single batch): 0.9818213582038879\n",
      "\t Training loss (single batch): 1.8573468923568726\n",
      "\t Training loss (single batch): 1.2653303146362305\n",
      "\t Training loss (single batch): 1.1023688316345215\n",
      "\t Training loss (single batch): 0.8898748755455017\n",
      "\t Training loss (single batch): 1.1718471050262451\n",
      "\t Training loss (single batch): 1.3346048593521118\n",
      "\t Training loss (single batch): 1.0364875793457031\n",
      "\t Training loss (single batch): 0.954735517501831\n",
      "\t Training loss (single batch): 0.957332193851471\n",
      "\t Training loss (single batch): 1.669051170349121\n",
      "\t Training loss (single batch): 1.206408977508545\n",
      "\t Training loss (single batch): 0.6598042845726013\n",
      "\t Training loss (single batch): 1.3040651082992554\n",
      "\t Training loss (single batch): 1.2556486129760742\n",
      "\t Training loss (single batch): 1.5295923948287964\n",
      "\t Training loss (single batch): 1.178178071975708\n",
      "\t Training loss (single batch): 1.5227447748184204\n",
      "\t Training loss (single batch): 1.2999423742294312\n",
      "\t Training loss (single batch): 1.4711112976074219\n",
      "\t Training loss (single batch): 1.329824686050415\n",
      "\t Training loss (single batch): 1.1213929653167725\n",
      "\t Training loss (single batch): 1.0245084762573242\n",
      "\t Training loss (single batch): 1.1384670734405518\n",
      "\t Training loss (single batch): 1.3136961460113525\n",
      "\t Training loss (single batch): 1.929396152496338\n",
      "\t Training loss (single batch): 2.107487201690674\n",
      "\t Training loss (single batch): 1.0080695152282715\n",
      "\t Training loss (single batch): 1.1601929664611816\n",
      "\t Training loss (single batch): 1.823334813117981\n",
      "\t Training loss (single batch): 1.4238460063934326\n",
      "\t Training loss (single batch): 1.1866121292114258\n",
      "\t Training loss (single batch): 1.295931339263916\n",
      "\t Training loss (single batch): 1.6949737071990967\n",
      "\t Training loss (single batch): 1.5399268865585327\n",
      "\t Training loss (single batch): 1.138122797012329\n",
      "\t Training loss (single batch): 1.685163974761963\n",
      "\t Training loss (single batch): 1.15738046169281\n",
      "\t Training loss (single batch): 1.4055821895599365\n",
      "\t Training loss (single batch): 1.2907170057296753\n",
      "\t Training loss (single batch): 1.0022947788238525\n",
      "\t Training loss (single batch): 1.2430403232574463\n",
      "\t Training loss (single batch): 1.3362213373184204\n",
      "\t Training loss (single batch): 1.3233643770217896\n",
      "\t Training loss (single batch): 1.2800486087799072\n",
      "\t Training loss (single batch): 1.3169047832489014\n",
      "\t Training loss (single batch): 1.3156851530075073\n",
      "\t Training loss (single batch): 1.1097068786621094\n",
      "\t Training loss (single batch): 0.8505100011825562\n",
      "\t Training loss (single batch): 1.5232762098312378\n",
      "\t Training loss (single batch): 1.2911680936813354\n",
      "\t Training loss (single batch): 0.9486306309700012\n",
      "\t Training loss (single batch): 1.228344202041626\n",
      "\t Training loss (single batch): 1.4265142679214478\n",
      "\t Training loss (single batch): 1.3319644927978516\n",
      "\t Training loss (single batch): 1.1023120880126953\n",
      "\t Training loss (single batch): 1.3452664613723755\n",
      "\t Training loss (single batch): 1.4386783838272095\n",
      "\t Training loss (single batch): 1.0918089151382446\n",
      "\t Training loss (single batch): 0.9978416562080383\n",
      "\t Training loss (single batch): 0.7761057615280151\n",
      "\t Training loss (single batch): 1.745269775390625\n",
      "\t Training loss (single batch): 1.2243250608444214\n",
      "\t Training loss (single batch): 0.9396680593490601\n",
      "\t Training loss (single batch): 1.3757696151733398\n",
      "\t Training loss (single batch): 1.5383425951004028\n",
      "\t Training loss (single batch): 0.8261247873306274\n",
      "\t Training loss (single batch): 0.8104119300842285\n",
      "\t Training loss (single batch): 1.6071549654006958\n",
      "\t Training loss (single batch): 1.5834654569625854\n",
      "\t Training loss (single batch): 1.5456717014312744\n",
      "\t Training loss (single batch): 1.3335319757461548\n",
      "\t Training loss (single batch): 1.06011962890625\n",
      "\t Training loss (single batch): 1.6414769887924194\n",
      "\t Training loss (single batch): 1.2870107889175415\n",
      "\t Training loss (single batch): 1.3685827255249023\n",
      "\t Training loss (single batch): 1.1433736085891724\n",
      "\t Training loss (single batch): 1.3827719688415527\n",
      "\t Training loss (single batch): 1.3699324131011963\n",
      "\t Training loss (single batch): 1.4668009281158447\n",
      "\t Training loss (single batch): 1.0695407390594482\n",
      "\t Training loss (single batch): 1.280111312866211\n",
      "\t Training loss (single batch): 1.061800479888916\n",
      "\t Training loss (single batch): 1.2218451499938965\n",
      "\t Training loss (single batch): 1.1499056816101074\n",
      "\t Training loss (single batch): 0.9749064445495605\n",
      "\t Training loss (single batch): 1.0154626369476318\n",
      "\t Training loss (single batch): 1.0657511949539185\n",
      "\t Training loss (single batch): 1.2096645832061768\n",
      "\t Training loss (single batch): 0.7990884780883789\n",
      "##################################\n",
      "## EPOCH 25\n",
      "##################################\n",
      "\t Training loss (single batch): 1.080776333808899\n",
      "\t Training loss (single batch): 1.0020735263824463\n",
      "\t Training loss (single batch): 0.8938454985618591\n",
      "\t Training loss (single batch): 1.1119277477264404\n",
      "\t Training loss (single batch): 2.1571667194366455\n",
      "\t Training loss (single batch): 1.1620980501174927\n",
      "\t Training loss (single batch): 1.1944960355758667\n",
      "\t Training loss (single batch): 1.2681348323822021\n",
      "\t Training loss (single batch): 1.3696843385696411\n",
      "\t Training loss (single batch): 1.024587631225586\n",
      "\t Training loss (single batch): 1.4363172054290771\n",
      "\t Training loss (single batch): 1.1711924076080322\n",
      "\t Training loss (single batch): 0.7856184840202332\n",
      "\t Training loss (single batch): 0.9870179891586304\n",
      "\t Training loss (single batch): 0.9346314668655396\n",
      "\t Training loss (single batch): 1.244344711303711\n",
      "\t Training loss (single batch): 1.5301121473312378\n",
      "\t Training loss (single batch): 1.132269263267517\n",
      "\t Training loss (single batch): 1.393814206123352\n",
      "\t Training loss (single batch): 1.0177441835403442\n",
      "\t Training loss (single batch): 1.273409128189087\n",
      "\t Training loss (single batch): 1.3345030546188354\n",
      "\t Training loss (single batch): 1.3580107688903809\n",
      "\t Training loss (single batch): 0.9933413863182068\n",
      "\t Training loss (single batch): 1.1406536102294922\n",
      "\t Training loss (single batch): 2.0512781143188477\n",
      "\t Training loss (single batch): 1.1748744249343872\n",
      "\t Training loss (single batch): 1.528680443763733\n",
      "\t Training loss (single batch): 1.6164612770080566\n",
      "\t Training loss (single batch): 0.8814011216163635\n",
      "\t Training loss (single batch): 1.3055732250213623\n",
      "\t Training loss (single batch): 1.005588412284851\n",
      "\t Training loss (single batch): 1.1682168245315552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.191862940788269\n",
      "\t Training loss (single batch): 1.6531074047088623\n",
      "\t Training loss (single batch): 0.9737522006034851\n",
      "\t Training loss (single batch): 1.380786418914795\n",
      "\t Training loss (single batch): 1.4257760047912598\n",
      "\t Training loss (single batch): 1.4459279775619507\n",
      "\t Training loss (single batch): 1.3028984069824219\n",
      "\t Training loss (single batch): 1.3741263151168823\n",
      "\t Training loss (single batch): 1.2438921928405762\n",
      "\t Training loss (single batch): 0.8157239556312561\n",
      "\t Training loss (single batch): 1.0288549661636353\n",
      "\t Training loss (single batch): 1.065976858139038\n",
      "\t Training loss (single batch): 1.045469045639038\n",
      "\t Training loss (single batch): 1.3998684883117676\n",
      "\t Training loss (single batch): 1.4096758365631104\n",
      "\t Training loss (single batch): 0.6940171718597412\n",
      "\t Training loss (single batch): 1.5127220153808594\n",
      "\t Training loss (single batch): 1.1427087783813477\n",
      "\t Training loss (single batch): 1.0883671045303345\n",
      "\t Training loss (single batch): 1.6528689861297607\n",
      "\t Training loss (single batch): 1.103488802909851\n",
      "\t Training loss (single batch): 1.3474892377853394\n",
      "\t Training loss (single batch): 1.1551413536071777\n",
      "\t Training loss (single batch): 1.104225754737854\n",
      "\t Training loss (single batch): 1.2351489067077637\n",
      "\t Training loss (single batch): 1.9259411096572876\n",
      "\t Training loss (single batch): 1.0351247787475586\n",
      "\t Training loss (single batch): 1.0506378412246704\n",
      "\t Training loss (single batch): 0.9411613345146179\n",
      "\t Training loss (single batch): 1.1976748704910278\n",
      "\t Training loss (single batch): 1.169238805770874\n",
      "\t Training loss (single batch): 1.393506646156311\n",
      "\t Training loss (single batch): 1.4190425872802734\n",
      "\t Training loss (single batch): 1.3547065258026123\n",
      "\t Training loss (single batch): 1.0749759674072266\n",
      "\t Training loss (single batch): 1.5071568489074707\n",
      "\t Training loss (single batch): 1.1163768768310547\n",
      "\t Training loss (single batch): 0.9049438238143921\n",
      "\t Training loss (single batch): 1.120862364768982\n",
      "\t Training loss (single batch): 0.8721471428871155\n",
      "\t Training loss (single batch): 1.0156292915344238\n",
      "\t Training loss (single batch): 1.2699388265609741\n",
      "\t Training loss (single batch): 1.1477999687194824\n",
      "\t Training loss (single batch): 1.343889832496643\n",
      "\t Training loss (single batch): 1.0517221689224243\n",
      "\t Training loss (single batch): 0.8880529999732971\n",
      "\t Training loss (single batch): 1.1758557558059692\n",
      "\t Training loss (single batch): 1.2592254877090454\n",
      "\t Training loss (single batch): 1.4456288814544678\n",
      "\t Training loss (single batch): 1.5178502798080444\n",
      "\t Training loss (single batch): 1.0218778848648071\n",
      "\t Training loss (single batch): 0.8577312231063843\n",
      "\t Training loss (single batch): 1.213796854019165\n",
      "\t Training loss (single batch): 0.9087912440299988\n",
      "\t Training loss (single batch): 1.4268953800201416\n",
      "\t Training loss (single batch): 1.385438323020935\n",
      "\t Training loss (single batch): 1.0235600471496582\n",
      "\t Training loss (single batch): 1.142338752746582\n",
      "\t Training loss (single batch): 1.0487303733825684\n",
      "\t Training loss (single batch): 1.4771167039871216\n",
      "\t Training loss (single batch): 1.1495623588562012\n",
      "\t Training loss (single batch): 1.2833839654922485\n",
      "\t Training loss (single batch): 2.11323881149292\n",
      "\t Training loss (single batch): 1.249570608139038\n",
      "\t Training loss (single batch): 0.923721432685852\n",
      "\t Training loss (single batch): 0.9409877061843872\n",
      "\t Training loss (single batch): 1.475640058517456\n",
      "\t Training loss (single batch): 0.915187418460846\n",
      "\t Training loss (single batch): 1.542117953300476\n",
      "\t Training loss (single batch): 1.4944429397583008\n",
      "\t Training loss (single batch): 1.1475564241409302\n",
      "\t Training loss (single batch): 1.094803810119629\n",
      "\t Training loss (single batch): 1.6320582628250122\n",
      "\t Training loss (single batch): 0.9347726106643677\n",
      "\t Training loss (single batch): 0.8257567286491394\n",
      "\t Training loss (single batch): 1.0505998134613037\n",
      "\t Training loss (single batch): 1.140863060951233\n",
      "\t Training loss (single batch): 0.9443442821502686\n",
      "\t Training loss (single batch): 1.0482136011123657\n",
      "\t Training loss (single batch): 1.0911990404129028\n",
      "\t Training loss (single batch): 0.9185821413993835\n",
      "\t Training loss (single batch): 1.0893000364303589\n",
      "\t Training loss (single batch): 1.3184149265289307\n",
      "\t Training loss (single batch): 0.948057234287262\n",
      "\t Training loss (single batch): 1.0125094652175903\n",
      "\t Training loss (single batch): 0.9571731090545654\n",
      "\t Training loss (single batch): 1.1385542154312134\n",
      "\t Training loss (single batch): 1.1391284465789795\n",
      "\t Training loss (single batch): 1.5268642902374268\n",
      "\t Training loss (single batch): 1.3929574489593506\n",
      "\t Training loss (single batch): 1.0004023313522339\n",
      "\t Training loss (single batch): 1.0320580005645752\n",
      "\t Training loss (single batch): 1.3241750001907349\n",
      "\t Training loss (single batch): 1.3395636081695557\n",
      "\t Training loss (single batch): 1.025633454322815\n",
      "\t Training loss (single batch): 1.119699478149414\n",
      "\t Training loss (single batch): 1.2459772825241089\n",
      "\t Training loss (single batch): 1.6660038232803345\n",
      "\t Training loss (single batch): 1.8386913537979126\n",
      "\t Training loss (single batch): 1.1527373790740967\n",
      "\t Training loss (single batch): 1.4834415912628174\n",
      "\t Training loss (single batch): 1.300944209098816\n",
      "\t Training loss (single batch): 1.491578221321106\n",
      "\t Training loss (single batch): 1.1373841762542725\n",
      "\t Training loss (single batch): 1.2561981678009033\n",
      "\t Training loss (single batch): 1.0169862508773804\n",
      "\t Training loss (single batch): 1.0517218112945557\n",
      "\t Training loss (single batch): 1.463174819946289\n",
      "\t Training loss (single batch): 1.2064111232757568\n",
      "\t Training loss (single batch): 1.313002586364746\n",
      "\t Training loss (single batch): 1.6139634847640991\n",
      "\t Training loss (single batch): 1.1164861917495728\n",
      "\t Training loss (single batch): 1.2020787000656128\n",
      "\t Training loss (single batch): 1.2928142547607422\n",
      "\t Training loss (single batch): 1.6913251876831055\n",
      "\t Training loss (single batch): 0.9714176058769226\n",
      "\t Training loss (single batch): 0.9372004866600037\n",
      "\t Training loss (single batch): 1.6413722038269043\n",
      "\t Training loss (single batch): 1.3083558082580566\n",
      "\t Training loss (single batch): 0.9664432406425476\n",
      "\t Training loss (single batch): 1.001280426979065\n",
      "\t Training loss (single batch): 1.3595616817474365\n",
      "\t Training loss (single batch): 1.211033582687378\n",
      "\t Training loss (single batch): 1.0699658393859863\n",
      "\t Training loss (single batch): 0.9932464957237244\n",
      "\t Training loss (single batch): 1.696146845817566\n",
      "\t Training loss (single batch): 1.141655683517456\n",
      "\t Training loss (single batch): 1.0678558349609375\n",
      "\t Training loss (single batch): 0.8582732081413269\n",
      "\t Training loss (single batch): 1.222798466682434\n",
      "\t Training loss (single batch): 1.4230033159255981\n",
      "\t Training loss (single batch): 1.2400108575820923\n",
      "\t Training loss (single batch): 1.0390574932098389\n",
      "\t Training loss (single batch): 1.6035716533660889\n",
      "\t Training loss (single batch): 1.0762648582458496\n",
      "\t Training loss (single batch): 1.2505825757980347\n",
      "\t Training loss (single batch): 1.2175897359848022\n",
      "\t Training loss (single batch): 1.7067643404006958\n",
      "\t Training loss (single batch): 1.1459612846374512\n",
      "\t Training loss (single batch): 1.388048768043518\n",
      "\t Training loss (single batch): 1.2627232074737549\n",
      "\t Training loss (single batch): 1.2284713983535767\n",
      "\t Training loss (single batch): 1.0422873497009277\n",
      "\t Training loss (single batch): 1.257093071937561\n",
      "\t Training loss (single batch): 1.0361602306365967\n",
      "\t Training loss (single batch): 1.3798846006393433\n",
      "\t Training loss (single batch): 1.2302418947219849\n",
      "\t Training loss (single batch): 1.2056291103363037\n",
      "\t Training loss (single batch): 1.1745153665542603\n",
      "\t Training loss (single batch): 1.07781183719635\n",
      "\t Training loss (single batch): 1.094366192817688\n",
      "\t Training loss (single batch): 1.3780730962753296\n",
      "\t Training loss (single batch): 0.968665599822998\n",
      "\t Training loss (single batch): 1.6644914150238037\n",
      "\t Training loss (single batch): 1.46864652633667\n",
      "\t Training loss (single batch): 1.201784610748291\n",
      "\t Training loss (single batch): 1.4104976654052734\n",
      "\t Training loss (single batch): 1.2869994640350342\n",
      "\t Training loss (single batch): 1.4816536903381348\n",
      "\t Training loss (single batch): 1.3773715496063232\n",
      "\t Training loss (single batch): 1.5522648096084595\n",
      "\t Training loss (single batch): 1.2322709560394287\n",
      "\t Training loss (single batch): 1.3399207592010498\n",
      "\t Training loss (single batch): 1.4611908197402954\n",
      "\t Training loss (single batch): 1.379688024520874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2554315328598022\n",
      "\t Training loss (single batch): 1.043223261833191\n",
      "\t Training loss (single batch): 1.225024700164795\n",
      "\t Training loss (single batch): 1.0130681991577148\n",
      "\t Training loss (single batch): 1.4131189584732056\n",
      "\t Training loss (single batch): 0.9172658324241638\n",
      "\t Training loss (single batch): 1.05083429813385\n",
      "\t Training loss (single batch): 0.9715693593025208\n",
      "\t Training loss (single batch): 0.8214700222015381\n",
      "\t Training loss (single batch): 1.0305469036102295\n",
      "\t Training loss (single batch): 1.2158269882202148\n",
      "\t Training loss (single batch): 1.4176956415176392\n",
      "\t Training loss (single batch): 0.9713600277900696\n",
      "\t Training loss (single batch): 1.571177363395691\n",
      "\t Training loss (single batch): 1.2227108478546143\n",
      "\t Training loss (single batch): 1.1139090061187744\n",
      "\t Training loss (single batch): 1.317284345626831\n",
      "\t Training loss (single batch): 1.290996789932251\n",
      "\t Training loss (single batch): 1.672836422920227\n",
      "\t Training loss (single batch): 1.4408479928970337\n",
      "\t Training loss (single batch): 1.4638898372650146\n",
      "\t Training loss (single batch): 1.0443353652954102\n",
      "\t Training loss (single batch): 0.9013434052467346\n",
      "\t Training loss (single batch): 1.3932037353515625\n",
      "\t Training loss (single batch): 1.4416462182998657\n",
      "\t Training loss (single batch): 1.0586975812911987\n",
      "\t Training loss (single batch): 1.3041040897369385\n",
      "\t Training loss (single batch): 1.5501235723495483\n",
      "\t Training loss (single batch): 1.350053310394287\n",
      "\t Training loss (single batch): 1.4974793195724487\n",
      "\t Training loss (single batch): 1.075940489768982\n",
      "\t Training loss (single batch): 1.7171053886413574\n",
      "\t Training loss (single batch): 1.445504903793335\n",
      "\t Training loss (single batch): 1.2199831008911133\n",
      "\t Training loss (single batch): 1.2025259733200073\n",
      "\t Training loss (single batch): 1.8480056524276733\n",
      "\t Training loss (single batch): 1.301493763923645\n",
      "\t Training loss (single batch): 1.0508522987365723\n",
      "\t Training loss (single batch): 1.2662938833236694\n",
      "\t Training loss (single batch): 1.2807735204696655\n",
      "\t Training loss (single batch): 1.075387954711914\n",
      "\t Training loss (single batch): 1.0332874059677124\n",
      "\t Training loss (single batch): 0.9929916858673096\n",
      "\t Training loss (single batch): 0.9604679346084595\n",
      "\t Training loss (single batch): 1.2451503276824951\n",
      "\t Training loss (single batch): 0.7471495866775513\n",
      "\t Training loss (single batch): 1.6829580068588257\n",
      "\t Training loss (single batch): 2.106274127960205\n",
      "\t Training loss (single batch): 1.400885820388794\n",
      "\t Training loss (single batch): 0.9145169854164124\n",
      "\t Training loss (single batch): 1.7293583154678345\n",
      "\t Training loss (single batch): 1.6025511026382446\n",
      "\t Training loss (single batch): 1.0915725231170654\n",
      "\t Training loss (single batch): 1.0952532291412354\n",
      "\t Training loss (single batch): 1.8259291648864746\n",
      "\t Training loss (single batch): 1.5064891576766968\n",
      "\t Training loss (single batch): 1.3980218172073364\n",
      "\t Training loss (single batch): 1.9323408603668213\n",
      "\t Training loss (single batch): 1.336164116859436\n",
      "\t Training loss (single batch): 1.100406289100647\n",
      "\t Training loss (single batch): 1.2603851556777954\n",
      "\t Training loss (single batch): 1.3819189071655273\n",
      "\t Training loss (single batch): 1.370214819908142\n",
      "\t Training loss (single batch): 1.0623070001602173\n",
      "\t Training loss (single batch): 1.2938379049301147\n",
      "\t Training loss (single batch): 1.5506489276885986\n",
      "\t Training loss (single batch): 1.1713340282440186\n",
      "\t Training loss (single batch): 1.2242183685302734\n",
      "\t Training loss (single batch): 1.0920220613479614\n",
      "\t Training loss (single batch): 1.3301509618759155\n",
      "\t Training loss (single batch): 1.133946418762207\n",
      "\t Training loss (single batch): 0.7709346413612366\n",
      "\t Training loss (single batch): 1.1970832347869873\n",
      "\t Training loss (single batch): 1.4300196170806885\n",
      "\t Training loss (single batch): 1.2378259897232056\n",
      "\t Training loss (single batch): 0.8374148607254028\n",
      "\t Training loss (single batch): 1.2735862731933594\n",
      "\t Training loss (single batch): 1.0975245237350464\n",
      "\t Training loss (single batch): 0.776097297668457\n",
      "\t Training loss (single batch): 1.405810832977295\n",
      "\t Training loss (single batch): 0.9911944270133972\n",
      "\t Training loss (single batch): 0.6757938265800476\n",
      "\t Training loss (single batch): 1.4954637289047241\n",
      "\t Training loss (single batch): 0.8441920280456543\n",
      "\t Training loss (single batch): 1.3060524463653564\n",
      "\t Training loss (single batch): 1.4408307075500488\n",
      "\t Training loss (single batch): 1.22971773147583\n",
      "\t Training loss (single batch): 1.021931767463684\n",
      "\t Training loss (single batch): 0.9981852769851685\n",
      "\t Training loss (single batch): 1.939833164215088\n",
      "\t Training loss (single batch): 1.1157647371292114\n",
      "\t Training loss (single batch): 1.1850652694702148\n",
      "\t Training loss (single batch): 1.629980444908142\n",
      "\t Training loss (single batch): 1.0503782033920288\n",
      "\t Training loss (single batch): 1.0495306253433228\n",
      "\t Training loss (single batch): 1.2846832275390625\n",
      "\t Training loss (single batch): 1.0937645435333252\n",
      "\t Training loss (single batch): 1.7827332019805908\n",
      "\t Training loss (single batch): 1.6657674312591553\n",
      "\t Training loss (single batch): 1.4683889150619507\n",
      "\t Training loss (single batch): 0.879915714263916\n",
      "\t Training loss (single batch): 1.3728828430175781\n",
      "\t Training loss (single batch): 1.1487007141113281\n",
      "\t Training loss (single batch): 1.134945034980774\n",
      "\t Training loss (single batch): 1.192633032798767\n",
      "\t Training loss (single batch): 0.8250129818916321\n",
      "\t Training loss (single batch): 1.0134457349777222\n",
      "\t Training loss (single batch): 1.1550805568695068\n",
      "\t Training loss (single batch): 1.1805824041366577\n",
      "\t Training loss (single batch): 1.259168267250061\n",
      "\t Training loss (single batch): 1.6156541109085083\n",
      "\t Training loss (single batch): 1.0991908311843872\n",
      "\t Training loss (single batch): 1.2513935565948486\n",
      "\t Training loss (single batch): 1.5888131856918335\n",
      "\t Training loss (single batch): 1.4011260271072388\n",
      "\t Training loss (single batch): 1.4024111032485962\n",
      "\t Training loss (single batch): 1.161720633506775\n",
      "\t Training loss (single batch): 1.318400502204895\n",
      "\t Training loss (single batch): 0.8244693875312805\n",
      "##################################\n",
      "## EPOCH 26\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2246366739273071\n",
      "\t Training loss (single batch): 0.985939621925354\n",
      "\t Training loss (single batch): 1.33572256565094\n",
      "\t Training loss (single batch): 0.82448410987854\n",
      "\t Training loss (single batch): 1.2126219272613525\n",
      "\t Training loss (single batch): 1.0645438432693481\n",
      "\t Training loss (single batch): 1.6092182397842407\n",
      "\t Training loss (single batch): 1.178633451461792\n",
      "\t Training loss (single batch): 1.1534769535064697\n",
      "\t Training loss (single batch): 0.9107828140258789\n",
      "\t Training loss (single batch): 1.1887496709823608\n",
      "\t Training loss (single batch): 1.212803840637207\n",
      "\t Training loss (single batch): 0.9200676083564758\n",
      "\t Training loss (single batch): 1.4330040216445923\n",
      "\t Training loss (single batch): 0.9517810344696045\n",
      "\t Training loss (single batch): 1.4749269485473633\n",
      "\t Training loss (single batch): 0.9520219564437866\n",
      "\t Training loss (single batch): 1.050866961479187\n",
      "\t Training loss (single batch): 1.1609551906585693\n",
      "\t Training loss (single batch): 1.471014142036438\n",
      "\t Training loss (single batch): 0.9018726348876953\n",
      "\t Training loss (single batch): 1.8970280885696411\n",
      "\t Training loss (single batch): 1.115296721458435\n",
      "\t Training loss (single batch): 1.348449945449829\n",
      "\t Training loss (single batch): 1.3506202697753906\n",
      "\t Training loss (single batch): 1.3287328481674194\n",
      "\t Training loss (single batch): 1.0568486452102661\n",
      "\t Training loss (single batch): 1.0533477067947388\n",
      "\t Training loss (single batch): 1.4766125679016113\n",
      "\t Training loss (single batch): 1.4682356119155884\n",
      "\t Training loss (single batch): 1.2293121814727783\n",
      "\t Training loss (single batch): 1.2224504947662354\n",
      "\t Training loss (single batch): 1.3272819519042969\n",
      "\t Training loss (single batch): 1.186571478843689\n",
      "\t Training loss (single batch): 1.5579859018325806\n",
      "\t Training loss (single batch): 0.9578939080238342\n",
      "\t Training loss (single batch): 1.244005799293518\n",
      "\t Training loss (single batch): 1.259609341621399\n",
      "\t Training loss (single batch): 0.8552812933921814\n",
      "\t Training loss (single batch): 0.8916198015213013\n",
      "\t Training loss (single batch): 1.4456255435943604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.920307457447052\n",
      "\t Training loss (single batch): 1.1558313369750977\n",
      "\t Training loss (single batch): 1.4279249906539917\n",
      "\t Training loss (single batch): 0.961456835269928\n",
      "\t Training loss (single batch): 1.4572904109954834\n",
      "\t Training loss (single batch): 1.2706102132797241\n",
      "\t Training loss (single batch): 0.9431828856468201\n",
      "\t Training loss (single batch): 0.9032485485076904\n",
      "\t Training loss (single batch): 0.9783477187156677\n",
      "\t Training loss (single batch): 1.17512047290802\n",
      "\t Training loss (single batch): 0.9823998212814331\n",
      "\t Training loss (single batch): 1.8715134859085083\n",
      "\t Training loss (single batch): 0.9790125489234924\n",
      "\t Training loss (single batch): 1.167592167854309\n",
      "\t Training loss (single batch): 1.1761360168457031\n",
      "\t Training loss (single batch): 1.3665698766708374\n",
      "\t Training loss (single batch): 0.7045274972915649\n",
      "\t Training loss (single batch): 1.1410460472106934\n",
      "\t Training loss (single batch): 1.5284687280654907\n",
      "\t Training loss (single batch): 0.9766427874565125\n",
      "\t Training loss (single batch): 0.770689070224762\n",
      "\t Training loss (single batch): 1.6925883293151855\n",
      "\t Training loss (single batch): 1.4483689069747925\n",
      "\t Training loss (single batch): 1.7411904335021973\n",
      "\t Training loss (single batch): 1.402536392211914\n",
      "\t Training loss (single batch): 1.1722862720489502\n",
      "\t Training loss (single batch): 1.0773922204971313\n",
      "\t Training loss (single batch): 1.0088701248168945\n",
      "\t Training loss (single batch): 1.3638854026794434\n",
      "\t Training loss (single batch): 1.160697102546692\n",
      "\t Training loss (single batch): 1.275618076324463\n",
      "\t Training loss (single batch): 0.8951050043106079\n",
      "\t Training loss (single batch): 1.1936984062194824\n",
      "\t Training loss (single batch): 1.327103853225708\n",
      "\t Training loss (single batch): 0.9591543078422546\n",
      "\t Training loss (single batch): 1.4992257356643677\n",
      "\t Training loss (single batch): 1.3170000314712524\n",
      "\t Training loss (single batch): 1.5332226753234863\n",
      "\t Training loss (single batch): 1.39297616481781\n",
      "\t Training loss (single batch): 0.9620739221572876\n",
      "\t Training loss (single batch): 0.995461642742157\n",
      "\t Training loss (single batch): 1.0164955854415894\n",
      "\t Training loss (single batch): 1.0607365369796753\n",
      "\t Training loss (single batch): 1.21786367893219\n",
      "\t Training loss (single batch): 1.3910273313522339\n",
      "\t Training loss (single batch): 1.6020915508270264\n",
      "\t Training loss (single batch): 1.2893140316009521\n",
      "\t Training loss (single batch): 0.8500820398330688\n",
      "\t Training loss (single batch): 1.32123863697052\n",
      "\t Training loss (single batch): 1.0578540563583374\n",
      "\t Training loss (single batch): 1.3527377843856812\n",
      "\t Training loss (single batch): 1.2504266500473022\n",
      "\t Training loss (single batch): 1.2215591669082642\n",
      "\t Training loss (single batch): 0.9391664266586304\n",
      "\t Training loss (single batch): 1.1683293581008911\n",
      "\t Training loss (single batch): 0.9982104301452637\n",
      "\t Training loss (single batch): 1.393670916557312\n",
      "\t Training loss (single batch): 1.2158838510513306\n",
      "\t Training loss (single batch): 0.9392446875572205\n",
      "\t Training loss (single batch): 1.4012185335159302\n",
      "\t Training loss (single batch): 1.3957483768463135\n",
      "\t Training loss (single batch): 1.5494658946990967\n",
      "\t Training loss (single batch): 1.1889935731887817\n",
      "\t Training loss (single batch): 0.8374456763267517\n",
      "\t Training loss (single batch): 2.170926570892334\n",
      "\t Training loss (single batch): 0.9319443702697754\n",
      "\t Training loss (single batch): 1.0186195373535156\n",
      "\t Training loss (single batch): 1.3268516063690186\n",
      "\t Training loss (single batch): 1.0372951030731201\n",
      "\t Training loss (single batch): 1.1762819290161133\n",
      "\t Training loss (single batch): 1.1034549474716187\n",
      "\t Training loss (single batch): 1.7258645296096802\n",
      "\t Training loss (single batch): 1.5152822732925415\n",
      "\t Training loss (single batch): 1.1111351251602173\n",
      "\t Training loss (single batch): 1.233209252357483\n",
      "\t Training loss (single batch): 1.2137802839279175\n",
      "\t Training loss (single batch): 0.9330925941467285\n",
      "\t Training loss (single batch): 1.5081263780593872\n",
      "\t Training loss (single batch): 1.5382100343704224\n",
      "\t Training loss (single batch): 0.873505175113678\n",
      "\t Training loss (single batch): 1.404199481010437\n",
      "\t Training loss (single batch): 1.2698750495910645\n",
      "\t Training loss (single batch): 1.579028606414795\n",
      "\t Training loss (single batch): 1.3126530647277832\n",
      "\t Training loss (single batch): 1.1668004989624023\n",
      "\t Training loss (single batch): 1.0962319374084473\n",
      "\t Training loss (single batch): 1.7794419527053833\n",
      "\t Training loss (single batch): 1.5727030038833618\n",
      "\t Training loss (single batch): 1.055771827697754\n",
      "\t Training loss (single batch): 1.7000453472137451\n",
      "\t Training loss (single batch): 1.3770005702972412\n",
      "\t Training loss (single batch): 1.2776848077774048\n",
      "\t Training loss (single batch): 1.1658672094345093\n",
      "\t Training loss (single batch): 1.1150041818618774\n",
      "\t Training loss (single batch): 1.285194754600525\n",
      "\t Training loss (single batch): 1.0791456699371338\n",
      "\t Training loss (single batch): 0.9871220588684082\n",
      "\t Training loss (single batch): 1.1543277502059937\n",
      "\t Training loss (single batch): 1.561987042427063\n",
      "\t Training loss (single batch): 0.8419595956802368\n",
      "\t Training loss (single batch): 1.6913865804672241\n",
      "\t Training loss (single batch): 1.239385724067688\n",
      "\t Training loss (single batch): 1.0450387001037598\n",
      "\t Training loss (single batch): 1.3243318796157837\n",
      "\t Training loss (single batch): 1.0475269556045532\n",
      "\t Training loss (single batch): 1.5716854333877563\n",
      "\t Training loss (single batch): 1.6722984313964844\n",
      "\t Training loss (single batch): 1.0781573057174683\n",
      "\t Training loss (single batch): 1.3921068906784058\n",
      "\t Training loss (single batch): 1.2403314113616943\n",
      "\t Training loss (single batch): 1.1243705749511719\n",
      "\t Training loss (single batch): 1.4063605070114136\n",
      "\t Training loss (single batch): 1.0979889631271362\n",
      "\t Training loss (single batch): 1.2074742317199707\n",
      "\t Training loss (single batch): 1.504980444908142\n",
      "\t Training loss (single batch): 1.4681202173233032\n",
      "\t Training loss (single batch): 1.1373405456542969\n",
      "\t Training loss (single batch): 1.335700511932373\n",
      "\t Training loss (single batch): 1.3538086414337158\n",
      "\t Training loss (single batch): 1.0609242916107178\n",
      "\t Training loss (single batch): 1.0514092445373535\n",
      "\t Training loss (single batch): 1.0622773170471191\n",
      "\t Training loss (single batch): 1.1941983699798584\n",
      "\t Training loss (single batch): 1.0562891960144043\n",
      "\t Training loss (single batch): 1.0685231685638428\n",
      "\t Training loss (single batch): 1.0677663087844849\n",
      "\t Training loss (single batch): 1.109170913696289\n",
      "\t Training loss (single batch): 0.948554277420044\n",
      "\t Training loss (single batch): 1.7481871843338013\n",
      "\t Training loss (single batch): 0.8471101522445679\n",
      "\t Training loss (single batch): 1.5477129220962524\n",
      "\t Training loss (single batch): 1.53586745262146\n",
      "\t Training loss (single batch): 0.8222546577453613\n",
      "\t Training loss (single batch): 1.611541748046875\n",
      "\t Training loss (single batch): 0.9328606128692627\n",
      "\t Training loss (single batch): 1.2701795101165771\n",
      "\t Training loss (single batch): 1.3023936748504639\n",
      "\t Training loss (single batch): 1.229379415512085\n",
      "\t Training loss (single batch): 1.0630544424057007\n",
      "\t Training loss (single batch): 1.39383864402771\n",
      "\t Training loss (single batch): 1.7497471570968628\n",
      "\t Training loss (single batch): 1.0196419954299927\n",
      "\t Training loss (single batch): 1.441672921180725\n",
      "\t Training loss (single batch): 1.419309377670288\n",
      "\t Training loss (single batch): 1.060238003730774\n",
      "\t Training loss (single batch): 0.9988148212432861\n",
      "\t Training loss (single batch): 0.8212130665779114\n",
      "\t Training loss (single batch): 1.6416151523590088\n",
      "\t Training loss (single batch): 1.8607003688812256\n",
      "\t Training loss (single batch): 1.2441314458847046\n",
      "\t Training loss (single batch): 1.5948954820632935\n",
      "\t Training loss (single batch): 1.1003005504608154\n",
      "\t Training loss (single batch): 1.2496074438095093\n",
      "\t Training loss (single batch): 1.4462770223617554\n",
      "\t Training loss (single batch): 1.5332547426223755\n",
      "\t Training loss (single batch): 1.425421953201294\n",
      "\t Training loss (single batch): 0.9318000674247742\n",
      "\t Training loss (single batch): 1.0879954099655151\n",
      "\t Training loss (single batch): 1.5043997764587402\n",
      "\t Training loss (single batch): 1.6946475505828857\n",
      "\t Training loss (single batch): 1.5107208490371704\n",
      "\t Training loss (single batch): 1.3427324295043945\n",
      "\t Training loss (single batch): 1.3526043891906738\n",
      "\t Training loss (single batch): 1.3215112686157227\n",
      "\t Training loss (single batch): 1.2615153789520264\n",
      "\t Training loss (single batch): 0.6643199920654297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.278754472732544\n",
      "\t Training loss (single batch): 1.3016051054000854\n",
      "\t Training loss (single batch): 1.6319397687911987\n",
      "\t Training loss (single batch): 1.105013132095337\n",
      "\t Training loss (single batch): 0.8867210745811462\n",
      "\t Training loss (single batch): 1.447184681892395\n",
      "\t Training loss (single batch): 1.1130738258361816\n",
      "\t Training loss (single batch): 1.3269504308700562\n",
      "\t Training loss (single batch): 1.3077031373977661\n",
      "\t Training loss (single batch): 1.1587120294570923\n",
      "\t Training loss (single batch): 1.3516448736190796\n",
      "\t Training loss (single batch): 0.9867340922355652\n",
      "\t Training loss (single batch): 1.224015235900879\n",
      "\t Training loss (single batch): 1.1609489917755127\n",
      "\t Training loss (single batch): 1.4588958024978638\n",
      "\t Training loss (single batch): 1.3886600732803345\n",
      "\t Training loss (single batch): 1.048689603805542\n",
      "\t Training loss (single batch): 1.3645809888839722\n",
      "\t Training loss (single batch): 1.1706575155258179\n",
      "\t Training loss (single batch): 1.2445200681686401\n",
      "\t Training loss (single batch): 1.3974043130874634\n",
      "\t Training loss (single batch): 1.2209978103637695\n",
      "\t Training loss (single batch): 0.9137101173400879\n",
      "\t Training loss (single batch): 1.223975658416748\n",
      "\t Training loss (single batch): 1.0626736879348755\n",
      "\t Training loss (single batch): 1.2866055965423584\n",
      "\t Training loss (single batch): 1.1934914588928223\n",
      "\t Training loss (single batch): 1.650404930114746\n",
      "\t Training loss (single batch): 1.1399730443954468\n",
      "\t Training loss (single batch): 1.1467641592025757\n",
      "\t Training loss (single batch): 1.2353050708770752\n",
      "\t Training loss (single batch): 1.015797734260559\n",
      "\t Training loss (single batch): 0.9561724066734314\n",
      "\t Training loss (single batch): 1.5878418684005737\n",
      "\t Training loss (single batch): 1.652226209640503\n",
      "\t Training loss (single batch): 0.7243301272392273\n",
      "\t Training loss (single batch): 1.2355886697769165\n",
      "\t Training loss (single batch): 1.2843393087387085\n",
      "\t Training loss (single batch): 1.1123613119125366\n",
      "\t Training loss (single batch): 1.012837290763855\n",
      "\t Training loss (single batch): 1.6860318183898926\n",
      "\t Training loss (single batch): 1.388471245765686\n",
      "\t Training loss (single batch): 1.0228867530822754\n",
      "\t Training loss (single batch): 1.5940157175064087\n",
      "\t Training loss (single batch): 1.6004527807235718\n",
      "\t Training loss (single batch): 1.1690675020217896\n",
      "\t Training loss (single batch): 1.2164002656936646\n",
      "\t Training loss (single batch): 1.3742244243621826\n",
      "\t Training loss (single batch): 0.9579179883003235\n",
      "\t Training loss (single batch): 1.513215184211731\n",
      "\t Training loss (single batch): 1.261104702949524\n",
      "\t Training loss (single batch): 1.2550677061080933\n",
      "\t Training loss (single batch): 0.9232349395751953\n",
      "\t Training loss (single batch): 1.2640535831451416\n",
      "\t Training loss (single batch): 1.6048378944396973\n",
      "\t Training loss (single batch): 1.0124588012695312\n",
      "\t Training loss (single batch): 1.0272966623306274\n",
      "\t Training loss (single batch): 1.2113220691680908\n",
      "\t Training loss (single batch): 1.5322787761688232\n",
      "\t Training loss (single batch): 1.2178633213043213\n",
      "\t Training loss (single batch): 0.977670431137085\n",
      "\t Training loss (single batch): 1.5261170864105225\n",
      "\t Training loss (single batch): 0.8396931886672974\n",
      "\t Training loss (single batch): 1.0808565616607666\n",
      "\t Training loss (single batch): 1.0700303316116333\n",
      "\t Training loss (single batch): 0.9529269933700562\n",
      "\t Training loss (single batch): 1.523877501487732\n",
      "\t Training loss (single batch): 1.1053024530410767\n",
      "\t Training loss (single batch): 1.4017173051834106\n",
      "\t Training loss (single batch): 1.4099435806274414\n",
      "\t Training loss (single batch): 1.2183152437210083\n",
      "\t Training loss (single batch): 1.3716188669204712\n",
      "\t Training loss (single batch): 1.0722193717956543\n",
      "\t Training loss (single batch): 1.6992733478546143\n",
      "\t Training loss (single batch): 1.6227449178695679\n",
      "\t Training loss (single batch): 1.129380464553833\n",
      "\t Training loss (single batch): 1.1315348148345947\n",
      "\t Training loss (single batch): 1.4684298038482666\n",
      "\t Training loss (single batch): 1.2780126333236694\n",
      "\t Training loss (single batch): 1.3874181509017944\n",
      "\t Training loss (single batch): 1.3817832469940186\n",
      "\t Training loss (single batch): 1.5923405885696411\n",
      "\t Training loss (single batch): 1.7079991102218628\n",
      "\t Training loss (single batch): 1.058553695678711\n",
      "\t Training loss (single batch): 1.0461622476577759\n",
      "\t Training loss (single batch): 0.7641885876655579\n",
      "\t Training loss (single batch): 1.4943368434906006\n",
      "\t Training loss (single batch): 1.5037410259246826\n",
      "\t Training loss (single batch): 1.014127254486084\n",
      "\t Training loss (single batch): 1.514761209487915\n",
      "\t Training loss (single batch): 1.5057592391967773\n",
      "\t Training loss (single batch): 1.5705139636993408\n",
      "\t Training loss (single batch): 0.8592894077301025\n",
      "\t Training loss (single batch): 1.3099766969680786\n",
      "\t Training loss (single batch): 1.4109041690826416\n",
      "\t Training loss (single batch): 1.2058898210525513\n",
      "\t Training loss (single batch): 1.3347506523132324\n",
      "\t Training loss (single batch): 1.0083091259002686\n",
      "\t Training loss (single batch): 1.2266920804977417\n",
      "\t Training loss (single batch): 1.3285138607025146\n",
      "\t Training loss (single batch): 1.2765812873840332\n",
      "\t Training loss (single batch): 0.8987132906913757\n",
      "\t Training loss (single batch): 1.101706624031067\n",
      "\t Training loss (single batch): 1.1173325777053833\n",
      "\t Training loss (single batch): 0.7481153607368469\n",
      "\t Training loss (single batch): 1.1796355247497559\n",
      "\t Training loss (single batch): 1.0229694843292236\n",
      "\t Training loss (single batch): 0.9219216704368591\n",
      "\t Training loss (single batch): 1.0478222370147705\n",
      "\t Training loss (single batch): 0.9762227535247803\n",
      "##################################\n",
      "## EPOCH 27\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2415200471878052\n",
      "\t Training loss (single batch): 0.9192525744438171\n",
      "\t Training loss (single batch): 1.3062593936920166\n",
      "\t Training loss (single batch): 0.9470381736755371\n",
      "\t Training loss (single batch): 1.0979137420654297\n",
      "\t Training loss (single batch): 0.8345911502838135\n",
      "\t Training loss (single batch): 1.1022430658340454\n",
      "\t Training loss (single batch): 1.0498777627944946\n",
      "\t Training loss (single batch): 1.4760898351669312\n",
      "\t Training loss (single batch): 0.9921358227729797\n",
      "\t Training loss (single batch): 1.5005682706832886\n",
      "\t Training loss (single batch): 1.7447417974472046\n",
      "\t Training loss (single batch): 1.3031500577926636\n",
      "\t Training loss (single batch): 1.2845327854156494\n",
      "\t Training loss (single batch): 1.5977567434310913\n",
      "\t Training loss (single batch): 1.64081871509552\n",
      "\t Training loss (single batch): 0.7683334946632385\n",
      "\t Training loss (single batch): 1.6227972507476807\n",
      "\t Training loss (single batch): 1.0689122676849365\n",
      "\t Training loss (single batch): 1.5843640565872192\n",
      "\t Training loss (single batch): 1.413872241973877\n",
      "\t Training loss (single batch): 0.9448454976081848\n",
      "\t Training loss (single batch): 0.8806381821632385\n",
      "\t Training loss (single batch): 1.498290777206421\n",
      "\t Training loss (single batch): 1.0182902812957764\n",
      "\t Training loss (single batch): 1.4540587663650513\n",
      "\t Training loss (single batch): 1.2456467151641846\n",
      "\t Training loss (single batch): 1.1220062971115112\n",
      "\t Training loss (single batch): 1.5656033754348755\n",
      "\t Training loss (single batch): 1.5827839374542236\n",
      "\t Training loss (single batch): 1.6449333429336548\n",
      "\t Training loss (single batch): 1.2092771530151367\n",
      "\t Training loss (single batch): 1.2937339544296265\n",
      "\t Training loss (single batch): 1.1402379274368286\n",
      "\t Training loss (single batch): 1.1080412864685059\n",
      "\t Training loss (single batch): 1.1324821710586548\n",
      "\t Training loss (single batch): 0.9644855260848999\n",
      "\t Training loss (single batch): 1.0297837257385254\n",
      "\t Training loss (single batch): 1.3813025951385498\n",
      "\t Training loss (single batch): 1.0828986167907715\n",
      "\t Training loss (single batch): 1.2484886646270752\n",
      "\t Training loss (single batch): 1.2364261150360107\n",
      "\t Training loss (single batch): 1.4717837572097778\n",
      "\t Training loss (single batch): 1.4480204582214355\n",
      "\t Training loss (single batch): 0.8852388858795166\n",
      "\t Training loss (single batch): 1.3662246465682983\n",
      "\t Training loss (single batch): 1.4878219366073608\n",
      "\t Training loss (single batch): 0.8905606865882874\n",
      "\t Training loss (single batch): 1.3158138990402222\n",
      "\t Training loss (single batch): 1.2050340175628662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.736318826675415\n",
      "\t Training loss (single batch): 1.3157429695129395\n",
      "\t Training loss (single batch): 1.3866709470748901\n",
      "\t Training loss (single batch): 0.9381563663482666\n",
      "\t Training loss (single batch): 1.2203154563903809\n",
      "\t Training loss (single batch): 1.3506742715835571\n",
      "\t Training loss (single batch): 1.2826440334320068\n",
      "\t Training loss (single batch): 1.5825384855270386\n",
      "\t Training loss (single batch): 1.5226469039916992\n",
      "\t Training loss (single batch): 1.4763224124908447\n",
      "\t Training loss (single batch): 1.1975691318511963\n",
      "\t Training loss (single batch): 0.9850720167160034\n",
      "\t Training loss (single batch): 1.1706266403198242\n",
      "\t Training loss (single batch): 1.0844342708587646\n",
      "\t Training loss (single batch): 1.3343324661254883\n",
      "\t Training loss (single batch): 1.4794063568115234\n",
      "\t Training loss (single batch): 0.9632067680358887\n",
      "\t Training loss (single batch): 2.0463433265686035\n",
      "\t Training loss (single batch): 1.0938855409622192\n",
      "\t Training loss (single batch): 1.1168986558914185\n",
      "\t Training loss (single batch): 1.3874962329864502\n",
      "\t Training loss (single batch): 1.1902374029159546\n",
      "\t Training loss (single batch): 1.3103985786437988\n",
      "\t Training loss (single batch): 1.7170889377593994\n",
      "\t Training loss (single batch): 1.27345871925354\n",
      "\t Training loss (single batch): 1.216524362564087\n",
      "\t Training loss (single batch): 0.9616696834564209\n",
      "\t Training loss (single batch): 1.3812227249145508\n",
      "\t Training loss (single batch): 0.8753385543823242\n",
      "\t Training loss (single batch): 1.229987382888794\n",
      "\t Training loss (single batch): 1.242265224456787\n",
      "\t Training loss (single batch): 1.2962857484817505\n",
      "\t Training loss (single batch): 1.5571694374084473\n",
      "\t Training loss (single batch): 0.8861787915229797\n",
      "\t Training loss (single batch): 1.1016044616699219\n",
      "\t Training loss (single batch): 1.1553221940994263\n",
      "\t Training loss (single batch): 1.0109846591949463\n",
      "\t Training loss (single batch): 1.6238470077514648\n",
      "\t Training loss (single batch): 1.1833922863006592\n",
      "\t Training loss (single batch): 0.8602142930030823\n",
      "\t Training loss (single batch): 1.0334666967391968\n",
      "\t Training loss (single batch): 1.2075130939483643\n",
      "\t Training loss (single batch): 1.2492618560791016\n",
      "\t Training loss (single batch): 1.8550928831100464\n",
      "\t Training loss (single batch): 1.1601678133010864\n",
      "\t Training loss (single batch): 1.2883011102676392\n",
      "\t Training loss (single batch): 2.120676279067993\n",
      "\t Training loss (single batch): 1.0587888956069946\n",
      "\t Training loss (single batch): 1.2682665586471558\n",
      "\t Training loss (single batch): 1.9913794994354248\n",
      "\t Training loss (single batch): 1.3595715761184692\n",
      "\t Training loss (single batch): 1.0945847034454346\n",
      "\t Training loss (single batch): 1.056857705116272\n",
      "\t Training loss (single batch): 1.2178071737289429\n",
      "\t Training loss (single batch): 1.1943753957748413\n",
      "\t Training loss (single batch): 1.0987149477005005\n",
      "\t Training loss (single batch): 1.318841814994812\n",
      "\t Training loss (single batch): 1.113079309463501\n",
      "\t Training loss (single batch): 0.9624929428100586\n",
      "\t Training loss (single batch): 1.1714884042739868\n",
      "\t Training loss (single batch): 1.1975510120391846\n",
      "\t Training loss (single batch): 1.0876893997192383\n",
      "\t Training loss (single batch): 1.2877482175827026\n",
      "\t Training loss (single batch): 0.9069613814353943\n",
      "\t Training loss (single batch): 1.9717966318130493\n",
      "\t Training loss (single batch): 1.130883812904358\n",
      "\t Training loss (single batch): 1.5680965185165405\n",
      "\t Training loss (single batch): 1.0312796831130981\n",
      "\t Training loss (single batch): 1.0316029787063599\n",
      "\t Training loss (single batch): 1.3106606006622314\n",
      "\t Training loss (single batch): 1.8523564338684082\n",
      "\t Training loss (single batch): 1.4165819883346558\n",
      "\t Training loss (single batch): 1.1599555015563965\n",
      "\t Training loss (single batch): 1.0845757722854614\n",
      "\t Training loss (single batch): 1.5527336597442627\n",
      "\t Training loss (single batch): 1.657800555229187\n",
      "\t Training loss (single batch): 1.5181968212127686\n",
      "\t Training loss (single batch): 1.3863584995269775\n",
      "\t Training loss (single batch): 1.2219325304031372\n",
      "\t Training loss (single batch): 1.6113855838775635\n",
      "\t Training loss (single batch): 0.9655632376670837\n",
      "\t Training loss (single batch): 1.2884122133255005\n",
      "\t Training loss (single batch): 1.7148184776306152\n",
      "\t Training loss (single batch): 0.8115589618682861\n",
      "\t Training loss (single batch): 1.0600767135620117\n",
      "\t Training loss (single batch): 0.832231342792511\n",
      "\t Training loss (single batch): 1.1941637992858887\n",
      "\t Training loss (single batch): 1.026349425315857\n",
      "\t Training loss (single batch): 0.8810238242149353\n",
      "\t Training loss (single batch): 1.437069058418274\n",
      "\t Training loss (single batch): 1.5240097045898438\n",
      "\t Training loss (single batch): 1.5997414588928223\n",
      "\t Training loss (single batch): 1.140195608139038\n",
      "\t Training loss (single batch): 1.2637016773223877\n",
      "\t Training loss (single batch): 1.6661046743392944\n",
      "\t Training loss (single batch): 1.2223283052444458\n",
      "\t Training loss (single batch): 1.0610140562057495\n",
      "\t Training loss (single batch): 1.5476531982421875\n",
      "\t Training loss (single batch): 0.845491886138916\n",
      "\t Training loss (single batch): 0.9807794094085693\n",
      "\t Training loss (single batch): 1.4399452209472656\n",
      "\t Training loss (single batch): 1.440745234489441\n",
      "\t Training loss (single batch): 1.247353434562683\n",
      "\t Training loss (single batch): 0.9587084054946899\n",
      "\t Training loss (single batch): 1.7482305765151978\n",
      "\t Training loss (single batch): 0.9256551861763\n",
      "\t Training loss (single batch): 1.1862844228744507\n",
      "\t Training loss (single batch): 1.0098085403442383\n",
      "\t Training loss (single batch): 1.0304582118988037\n",
      "\t Training loss (single batch): 1.107540488243103\n",
      "\t Training loss (single batch): 1.1000566482543945\n",
      "\t Training loss (single batch): 0.8009019494056702\n",
      "\t Training loss (single batch): 1.0173648595809937\n",
      "\t Training loss (single batch): 1.3236675262451172\n",
      "\t Training loss (single batch): 1.3069915771484375\n",
      "\t Training loss (single batch): 0.9678524136543274\n",
      "\t Training loss (single batch): 0.9934865236282349\n",
      "\t Training loss (single batch): 0.9745981097221375\n",
      "\t Training loss (single batch): 1.2264186143875122\n",
      "\t Training loss (single batch): 0.9469941258430481\n",
      "\t Training loss (single batch): 1.2515758275985718\n",
      "\t Training loss (single batch): 1.5730820894241333\n",
      "\t Training loss (single batch): 1.1217901706695557\n",
      "\t Training loss (single batch): 0.9389076828956604\n",
      "\t Training loss (single batch): 1.266100525856018\n",
      "\t Training loss (single batch): 0.8963763117790222\n",
      "\t Training loss (single batch): 1.2808407545089722\n",
      "\t Training loss (single batch): 1.3263320922851562\n",
      "\t Training loss (single batch): 1.3299816846847534\n",
      "\t Training loss (single batch): 2.0440502166748047\n",
      "\t Training loss (single batch): 1.4933695793151855\n",
      "\t Training loss (single batch): 1.4481512308120728\n",
      "\t Training loss (single batch): 1.3882744312286377\n",
      "\t Training loss (single batch): 1.4480026960372925\n",
      "\t Training loss (single batch): 1.0487630367279053\n",
      "\t Training loss (single batch): 1.526757836341858\n",
      "\t Training loss (single batch): 1.286829948425293\n",
      "\t Training loss (single batch): 1.4925099611282349\n",
      "\t Training loss (single batch): 1.557808518409729\n",
      "\t Training loss (single batch): 1.2560290098190308\n",
      "\t Training loss (single batch): 1.0174418687820435\n",
      "\t Training loss (single batch): 1.3195606470108032\n",
      "\t Training loss (single batch): 1.500329613685608\n",
      "\t Training loss (single batch): 1.0621849298477173\n",
      "\t Training loss (single batch): 0.957065761089325\n",
      "\t Training loss (single batch): 1.0322082042694092\n",
      "\t Training loss (single batch): 1.070051670074463\n",
      "\t Training loss (single batch): 1.0166035890579224\n",
      "\t Training loss (single batch): 1.3390089273452759\n",
      "\t Training loss (single batch): 1.5208474397659302\n",
      "\t Training loss (single batch): 1.3743425607681274\n",
      "\t Training loss (single batch): 1.192865252494812\n",
      "\t Training loss (single batch): 1.4815577268600464\n",
      "\t Training loss (single batch): 1.2216241359710693\n",
      "\t Training loss (single batch): 1.1280473470687866\n",
      "\t Training loss (single batch): 1.2594939470291138\n",
      "\t Training loss (single batch): 1.479626178741455\n",
      "\t Training loss (single batch): 0.8941400051116943\n",
      "\t Training loss (single batch): 1.1431852579116821\n",
      "\t Training loss (single batch): 1.154545783996582\n",
      "\t Training loss (single batch): 1.0516248941421509\n",
      "\t Training loss (single batch): 0.9573967456817627\n",
      "\t Training loss (single batch): 0.9676443338394165\n",
      "\t Training loss (single batch): 1.070539116859436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4689366817474365\n",
      "\t Training loss (single batch): 1.596013069152832\n",
      "\t Training loss (single batch): 1.4284051656723022\n",
      "\t Training loss (single batch): 1.2750874757766724\n",
      "\t Training loss (single batch): 0.9963989853858948\n",
      "\t Training loss (single batch): 1.177080512046814\n",
      "\t Training loss (single batch): 1.1159923076629639\n",
      "\t Training loss (single batch): 1.302762508392334\n",
      "\t Training loss (single batch): 1.2771645784378052\n",
      "\t Training loss (single batch): 1.7790837287902832\n",
      "\t Training loss (single batch): 1.3810126781463623\n",
      "\t Training loss (single batch): 1.4047317504882812\n",
      "\t Training loss (single batch): 1.338325023651123\n",
      "\t Training loss (single batch): 1.2942203283309937\n",
      "\t Training loss (single batch): 1.3300317525863647\n",
      "\t Training loss (single batch): 1.0829921960830688\n",
      "\t Training loss (single batch): 1.332969069480896\n",
      "\t Training loss (single batch): 1.2628772258758545\n",
      "\t Training loss (single batch): 1.145203948020935\n",
      "\t Training loss (single batch): 0.7452425360679626\n",
      "\t Training loss (single batch): 1.4602302312850952\n",
      "\t Training loss (single batch): 1.1558703184127808\n",
      "\t Training loss (single batch): 1.1812506914138794\n",
      "\t Training loss (single batch): 1.6109381914138794\n",
      "\t Training loss (single batch): 0.8942500352859497\n",
      "\t Training loss (single batch): 1.1688494682312012\n",
      "\t Training loss (single batch): 0.9228439331054688\n",
      "\t Training loss (single batch): 1.0987786054611206\n",
      "\t Training loss (single batch): 1.2128762006759644\n",
      "\t Training loss (single batch): 1.2589231729507446\n",
      "\t Training loss (single batch): 1.0213736295700073\n",
      "\t Training loss (single batch): 0.9563785791397095\n",
      "\t Training loss (single batch): 1.5193321704864502\n",
      "\t Training loss (single batch): 1.0800602436065674\n",
      "\t Training loss (single batch): 1.5257389545440674\n",
      "\t Training loss (single batch): 1.0690257549285889\n",
      "\t Training loss (single batch): 0.9957178831100464\n",
      "\t Training loss (single batch): 1.1536717414855957\n",
      "\t Training loss (single batch): 1.1367312669754028\n",
      "\t Training loss (single batch): 1.172262191772461\n",
      "\t Training loss (single batch): 0.9723837971687317\n",
      "\t Training loss (single batch): 0.9885552525520325\n",
      "\t Training loss (single batch): 1.004752516746521\n",
      "\t Training loss (single batch): 1.5396676063537598\n",
      "\t Training loss (single batch): 0.7845950722694397\n",
      "\t Training loss (single batch): 1.16994309425354\n",
      "\t Training loss (single batch): 1.0236486196517944\n",
      "\t Training loss (single batch): 1.493293046951294\n",
      "\t Training loss (single batch): 1.5667681694030762\n",
      "\t Training loss (single batch): 0.909598708152771\n",
      "\t Training loss (single batch): 1.6573930978775024\n",
      "\t Training loss (single batch): 1.6795772314071655\n",
      "\t Training loss (single batch): 1.6295208930969238\n",
      "\t Training loss (single batch): 1.2555081844329834\n",
      "\t Training loss (single batch): 1.87261962890625\n",
      "\t Training loss (single batch): 1.163361668586731\n",
      "\t Training loss (single batch): 1.141718864440918\n",
      "\t Training loss (single batch): 1.2286827564239502\n",
      "\t Training loss (single batch): 1.3711674213409424\n",
      "\t Training loss (single batch): 1.6071802377700806\n",
      "\t Training loss (single batch): 1.253844976425171\n",
      "\t Training loss (single batch): 1.2301238775253296\n",
      "\t Training loss (single batch): 1.028609037399292\n",
      "\t Training loss (single batch): 1.3154360055923462\n",
      "\t Training loss (single batch): 0.9839047193527222\n",
      "\t Training loss (single batch): 1.701235055923462\n",
      "\t Training loss (single batch): 1.213381290435791\n",
      "\t Training loss (single batch): 1.0131057500839233\n",
      "\t Training loss (single batch): 1.5808340311050415\n",
      "\t Training loss (single batch): 1.335742712020874\n",
      "\t Training loss (single batch): 1.0411514043807983\n",
      "\t Training loss (single batch): 1.416995882987976\n",
      "\t Training loss (single batch): 1.5948976278305054\n",
      "\t Training loss (single batch): 1.271109700202942\n",
      "\t Training loss (single batch): 1.5115636587142944\n",
      "\t Training loss (single batch): 1.3642079830169678\n",
      "\t Training loss (single batch): 1.092717170715332\n",
      "\t Training loss (single batch): 1.1952351331710815\n",
      "\t Training loss (single batch): 1.235499382019043\n",
      "\t Training loss (single batch): 1.5190430879592896\n",
      "\t Training loss (single batch): 0.968966543674469\n",
      "\t Training loss (single batch): 1.4930546283721924\n",
      "\t Training loss (single batch): 0.7731105089187622\n",
      "\t Training loss (single batch): 0.887445330619812\n",
      "\t Training loss (single batch): 1.678160548210144\n",
      "\t Training loss (single batch): 1.0545099973678589\n",
      "\t Training loss (single batch): 1.2649866342544556\n",
      "\t Training loss (single batch): 0.867926836013794\n",
      "\t Training loss (single batch): 1.1798700094223022\n",
      "\t Training loss (single batch): 1.219954013824463\n",
      "\t Training loss (single batch): 1.2830997705459595\n",
      "\t Training loss (single batch): 1.209301471710205\n",
      "\t Training loss (single batch): 1.2279976606369019\n",
      "\t Training loss (single batch): 1.1922730207443237\n",
      "\t Training loss (single batch): 0.8806273937225342\n",
      "\t Training loss (single batch): 1.1674362421035767\n",
      "\t Training loss (single batch): 1.214068055152893\n",
      "\t Training loss (single batch): 1.0620522499084473\n",
      "\t Training loss (single batch): 0.7495555877685547\n",
      "\t Training loss (single batch): 1.0400933027267456\n",
      "\t Training loss (single batch): 1.3233575820922852\n",
      "\t Training loss (single batch): 1.4655742645263672\n",
      "\t Training loss (single batch): 0.9894503951072693\n",
      "##################################\n",
      "## EPOCH 28\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2928950786590576\n",
      "\t Training loss (single batch): 1.4498319625854492\n",
      "\t Training loss (single batch): 1.279827356338501\n",
      "\t Training loss (single batch): 1.6846158504486084\n",
      "\t Training loss (single batch): 1.3625153303146362\n",
      "\t Training loss (single batch): 1.102752923965454\n",
      "\t Training loss (single batch): 1.1859383583068848\n",
      "\t Training loss (single batch): 1.532001256942749\n",
      "\t Training loss (single batch): 1.1655809879302979\n",
      "\t Training loss (single batch): 1.2901034355163574\n",
      "\t Training loss (single batch): 1.2287648916244507\n",
      "\t Training loss (single batch): 1.1326994895935059\n",
      "\t Training loss (single batch): 0.9457885026931763\n",
      "\t Training loss (single batch): 1.0094664096832275\n",
      "\t Training loss (single batch): 1.2473899126052856\n",
      "\t Training loss (single batch): 1.373508334159851\n",
      "\t Training loss (single batch): 1.00657320022583\n",
      "\t Training loss (single batch): 1.1601896286010742\n",
      "\t Training loss (single batch): 1.2347228527069092\n",
      "\t Training loss (single batch): 1.2203789949417114\n",
      "\t Training loss (single batch): 1.4437538385391235\n",
      "\t Training loss (single batch): 1.1444847583770752\n",
      "\t Training loss (single batch): 1.0116078853607178\n",
      "\t Training loss (single batch): 0.8143196105957031\n",
      "\t Training loss (single batch): 1.092672348022461\n",
      "\t Training loss (single batch): 1.3771299123764038\n",
      "\t Training loss (single batch): 2.151097536087036\n",
      "\t Training loss (single batch): 0.9659739136695862\n",
      "\t Training loss (single batch): 1.404894232749939\n",
      "\t Training loss (single batch): 1.3951438665390015\n",
      "\t Training loss (single batch): 1.4718397855758667\n",
      "\t Training loss (single batch): 1.1944624185562134\n",
      "\t Training loss (single batch): 1.1940672397613525\n",
      "\t Training loss (single batch): 1.4327642917633057\n",
      "\t Training loss (single batch): 1.8539400100708008\n",
      "\t Training loss (single batch): 1.2638590335845947\n",
      "\t Training loss (single batch): 1.2850341796875\n",
      "\t Training loss (single batch): 1.0057241916656494\n",
      "\t Training loss (single batch): 1.1335076093673706\n",
      "\t Training loss (single batch): 0.9947920441627502\n",
      "\t Training loss (single batch): 1.4800763130187988\n",
      "\t Training loss (single batch): 1.572946310043335\n",
      "\t Training loss (single batch): 1.0096888542175293\n",
      "\t Training loss (single batch): 1.1272815465927124\n",
      "\t Training loss (single batch): 1.6388932466506958\n",
      "\t Training loss (single batch): 1.823110818862915\n",
      "\t Training loss (single batch): 1.0253148078918457\n",
      "\t Training loss (single batch): 1.1274943351745605\n",
      "\t Training loss (single batch): 1.0337363481521606\n",
      "\t Training loss (single batch): 1.0395888090133667\n",
      "\t Training loss (single batch): 1.2003778219223022\n",
      "\t Training loss (single batch): 1.0254806280136108\n",
      "\t Training loss (single batch): 0.9435277581214905\n",
      "\t Training loss (single batch): 1.2478169202804565\n",
      "\t Training loss (single batch): 1.1327513456344604\n",
      "\t Training loss (single batch): 1.275607943534851\n",
      "\t Training loss (single batch): 0.947101891040802\n",
      "\t Training loss (single batch): 1.3397948741912842\n",
      "\t Training loss (single batch): 1.7716152667999268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.562994360923767\n",
      "\t Training loss (single batch): 0.998978316783905\n",
      "\t Training loss (single batch): 1.0922091007232666\n",
      "\t Training loss (single batch): 1.3309156894683838\n",
      "\t Training loss (single batch): 1.316634178161621\n",
      "\t Training loss (single batch): 1.4758480787277222\n",
      "\t Training loss (single batch): 1.1970298290252686\n",
      "\t Training loss (single batch): 1.5028883218765259\n",
      "\t Training loss (single batch): 1.386443018913269\n",
      "\t Training loss (single batch): 1.2949753999710083\n",
      "\t Training loss (single batch): 1.371512532234192\n",
      "\t Training loss (single batch): 1.168664574623108\n",
      "\t Training loss (single batch): 0.9860849976539612\n",
      "\t Training loss (single batch): 1.1621588468551636\n",
      "\t Training loss (single batch): 1.1129510402679443\n",
      "\t Training loss (single batch): 1.440450668334961\n",
      "\t Training loss (single batch): 1.0325251817703247\n",
      "\t Training loss (single batch): 1.2168937921524048\n",
      "\t Training loss (single batch): 1.6254172325134277\n",
      "\t Training loss (single batch): 1.0724486112594604\n",
      "\t Training loss (single batch): 1.1501744985580444\n",
      "\t Training loss (single batch): 1.0055550336837769\n",
      "\t Training loss (single batch): 1.1026835441589355\n",
      "\t Training loss (single batch): 1.6191121339797974\n",
      "\t Training loss (single batch): 1.255792260169983\n",
      "\t Training loss (single batch): 0.8844292163848877\n",
      "\t Training loss (single batch): 1.2375580072402954\n",
      "\t Training loss (single batch): 1.0606789588928223\n",
      "\t Training loss (single batch): 1.4166570901870728\n",
      "\t Training loss (single batch): 0.811233639717102\n",
      "\t Training loss (single batch): 0.9901601672172546\n",
      "\t Training loss (single batch): 1.3273406028747559\n",
      "\t Training loss (single batch): 1.1009159088134766\n",
      "\t Training loss (single batch): 1.409415602684021\n",
      "\t Training loss (single batch): 1.7798800468444824\n",
      "\t Training loss (single batch): 1.3691346645355225\n",
      "\t Training loss (single batch): 1.0992255210876465\n",
      "\t Training loss (single batch): 1.120367169380188\n",
      "\t Training loss (single batch): 1.5520349740982056\n",
      "\t Training loss (single batch): 1.249420404434204\n",
      "\t Training loss (single batch): 1.3071165084838867\n",
      "\t Training loss (single batch): 1.0223720073699951\n",
      "\t Training loss (single batch): 1.3186029195785522\n",
      "\t Training loss (single batch): 1.1873348951339722\n",
      "\t Training loss (single batch): 1.219813346862793\n",
      "\t Training loss (single batch): 1.1655848026275635\n",
      "\t Training loss (single batch): 1.5762543678283691\n",
      "\t Training loss (single batch): 1.2770971059799194\n",
      "\t Training loss (single batch): 1.124561071395874\n",
      "\t Training loss (single batch): 1.336391806602478\n",
      "\t Training loss (single batch): 1.344547986984253\n",
      "\t Training loss (single batch): 1.46882164478302\n",
      "\t Training loss (single batch): 1.1030827760696411\n",
      "\t Training loss (single batch): 0.7691279649734497\n",
      "\t Training loss (single batch): 1.3033311367034912\n",
      "\t Training loss (single batch): 1.2690443992614746\n",
      "\t Training loss (single batch): 1.243797779083252\n",
      "\t Training loss (single batch): 1.1103323698043823\n",
      "\t Training loss (single batch): 1.2480876445770264\n",
      "\t Training loss (single batch): 0.8290959596633911\n",
      "\t Training loss (single batch): 0.8654611110687256\n",
      "\t Training loss (single batch): 1.6018686294555664\n",
      "\t Training loss (single batch): 1.1553866863250732\n",
      "\t Training loss (single batch): 1.494149923324585\n",
      "\t Training loss (single batch): 1.7186832427978516\n",
      "\t Training loss (single batch): 1.5150867700576782\n",
      "\t Training loss (single batch): 1.0951346158981323\n",
      "\t Training loss (single batch): 1.621383547782898\n",
      "\t Training loss (single batch): 1.5866142511367798\n",
      "\t Training loss (single batch): 1.0156769752502441\n",
      "\t Training loss (single batch): 1.2801669836044312\n",
      "\t Training loss (single batch): 1.1887887716293335\n",
      "\t Training loss (single batch): 0.7885760068893433\n",
      "\t Training loss (single batch): 1.0763075351715088\n",
      "\t Training loss (single batch): 1.5529978275299072\n",
      "\t Training loss (single batch): 1.4522836208343506\n",
      "\t Training loss (single batch): 0.9124240279197693\n",
      "\t Training loss (single batch): 1.2022432088851929\n",
      "\t Training loss (single batch): 1.2254445552825928\n",
      "\t Training loss (single batch): 1.0024299621582031\n",
      "\t Training loss (single batch): 1.191702127456665\n",
      "\t Training loss (single batch): 0.7779030799865723\n",
      "\t Training loss (single batch): 1.5539591312408447\n",
      "\t Training loss (single batch): 1.1126922369003296\n",
      "\t Training loss (single batch): 1.093919277191162\n",
      "\t Training loss (single batch): 0.9630877375602722\n",
      "\t Training loss (single batch): 1.6535191535949707\n",
      "\t Training loss (single batch): 0.8862853050231934\n",
      "\t Training loss (single batch): 1.4557576179504395\n",
      "\t Training loss (single batch): 0.9482175707817078\n",
      "\t Training loss (single batch): 1.2316292524337769\n",
      "\t Training loss (single batch): 1.3178191184997559\n",
      "\t Training loss (single batch): 1.54341721534729\n",
      "\t Training loss (single batch): 1.4224307537078857\n",
      "\t Training loss (single batch): 0.991945743560791\n",
      "\t Training loss (single batch): 1.0185651779174805\n",
      "\t Training loss (single batch): 1.0496735572814941\n",
      "\t Training loss (single batch): 1.935021162033081\n",
      "\t Training loss (single batch): 1.4329721927642822\n",
      "\t Training loss (single batch): 1.6795988082885742\n",
      "\t Training loss (single batch): 1.1592304706573486\n",
      "\t Training loss (single batch): 1.0693652629852295\n",
      "\t Training loss (single batch): 1.6288059949874878\n",
      "\t Training loss (single batch): 2.1445915699005127\n",
      "\t Training loss (single batch): 1.5818854570388794\n",
      "\t Training loss (single batch): 1.0823171138763428\n",
      "\t Training loss (single batch): 1.2338508367538452\n",
      "\t Training loss (single batch): 1.3217670917510986\n",
      "\t Training loss (single batch): 0.9677456617355347\n",
      "\t Training loss (single batch): 1.4758398532867432\n",
      "\t Training loss (single batch): 1.106390118598938\n",
      "\t Training loss (single batch): 1.1358284950256348\n",
      "\t Training loss (single batch): 1.0248701572418213\n",
      "\t Training loss (single batch): 1.420388102531433\n",
      "\t Training loss (single batch): 1.1030604839324951\n",
      "\t Training loss (single batch): 1.3951972723007202\n",
      "\t Training loss (single batch): 1.2981010675430298\n",
      "\t Training loss (single batch): 1.0501164197921753\n",
      "\t Training loss (single batch): 1.0199341773986816\n",
      "\t Training loss (single batch): 1.0318304300308228\n",
      "\t Training loss (single batch): 1.2179362773895264\n",
      "\t Training loss (single batch): 1.6060166358947754\n",
      "\t Training loss (single batch): 1.8340097665786743\n",
      "\t Training loss (single batch): 1.0013134479522705\n",
      "\t Training loss (single batch): 1.7528592348098755\n",
      "\t Training loss (single batch): 1.342212200164795\n",
      "\t Training loss (single batch): 1.3474699258804321\n",
      "\t Training loss (single batch): 1.2497000694274902\n",
      "\t Training loss (single batch): 1.4846627712249756\n",
      "\t Training loss (single batch): 1.0311293601989746\n",
      "\t Training loss (single batch): 1.5391336679458618\n",
      "\t Training loss (single batch): 0.9894044995307922\n",
      "\t Training loss (single batch): 1.551425576210022\n",
      "\t Training loss (single batch): 1.2051985263824463\n",
      "\t Training loss (single batch): 1.3161838054656982\n",
      "\t Training loss (single batch): 1.2281150817871094\n",
      "\t Training loss (single batch): 1.4240270853042603\n",
      "\t Training loss (single batch): 0.9578993320465088\n",
      "\t Training loss (single batch): 1.894249439239502\n",
      "\t Training loss (single batch): 1.034988284111023\n",
      "\t Training loss (single batch): 1.4855984449386597\n",
      "\t Training loss (single batch): 0.9203426241874695\n",
      "\t Training loss (single batch): 1.7926137447357178\n",
      "\t Training loss (single batch): 1.0121668577194214\n",
      "\t Training loss (single batch): 1.023366928100586\n",
      "\t Training loss (single batch): 1.1490288972854614\n",
      "\t Training loss (single batch): 1.1247198581695557\n",
      "\t Training loss (single batch): 1.6536314487457275\n",
      "\t Training loss (single batch): 1.320058822631836\n",
      "\t Training loss (single batch): 0.8843787312507629\n",
      "\t Training loss (single batch): 0.8467788696289062\n",
      "\t Training loss (single batch): 1.5359992980957031\n",
      "\t Training loss (single batch): 1.3871690034866333\n",
      "\t Training loss (single batch): 1.5498520135879517\n",
      "\t Training loss (single batch): 1.2270244359970093\n",
      "\t Training loss (single batch): 1.3021949529647827\n",
      "\t Training loss (single batch): 1.3287841081619263\n",
      "\t Training loss (single batch): 1.189453125\n",
      "\t Training loss (single batch): 1.243248701095581\n",
      "\t Training loss (single batch): 1.0996557474136353\n",
      "\t Training loss (single batch): 1.3927055597305298\n",
      "\t Training loss (single batch): 1.0438430309295654\n",
      "\t Training loss (single batch): 0.9723841547966003\n",
      "\t Training loss (single batch): 1.0340558290481567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.9216489791870117\n",
      "\t Training loss (single batch): 0.9399682283401489\n",
      "\t Training loss (single batch): 0.9847252368927002\n",
      "\t Training loss (single batch): 1.7409539222717285\n",
      "\t Training loss (single batch): 1.1559306383132935\n",
      "\t Training loss (single batch): 1.1683927774429321\n",
      "\t Training loss (single batch): 1.1910574436187744\n",
      "\t Training loss (single batch): 1.2571885585784912\n",
      "\t Training loss (single batch): 1.4098950624465942\n",
      "\t Training loss (single batch): 1.0035154819488525\n",
      "\t Training loss (single batch): 1.147952675819397\n",
      "\t Training loss (single batch): 0.7934998869895935\n",
      "\t Training loss (single batch): 1.079028606414795\n",
      "\t Training loss (single batch): 1.4171111583709717\n",
      "\t Training loss (single batch): 1.320853352546692\n",
      "\t Training loss (single batch): 0.9596830606460571\n",
      "\t Training loss (single batch): 1.0352975130081177\n",
      "\t Training loss (single batch): 0.9238245487213135\n",
      "\t Training loss (single batch): 1.3886568546295166\n",
      "\t Training loss (single batch): 1.540921688079834\n",
      "\t Training loss (single batch): 1.3728783130645752\n",
      "\t Training loss (single batch): 0.8534634709358215\n",
      "\t Training loss (single batch): 0.9126691818237305\n",
      "\t Training loss (single batch): 0.9961820244789124\n",
      "\t Training loss (single batch): 1.4121441841125488\n",
      "\t Training loss (single batch): 1.0997272729873657\n",
      "\t Training loss (single batch): 0.9103385210037231\n",
      "\t Training loss (single batch): 1.261361002922058\n",
      "\t Training loss (single batch): 1.0711795091629028\n",
      "\t Training loss (single batch): 1.0227612257003784\n",
      "\t Training loss (single batch): 1.4053897857666016\n",
      "\t Training loss (single batch): 1.3356752395629883\n",
      "\t Training loss (single batch): 2.115154504776001\n",
      "\t Training loss (single batch): 0.9395699501037598\n",
      "\t Training loss (single batch): 0.8141337633132935\n",
      "\t Training loss (single batch): 1.0088552236557007\n",
      "\t Training loss (single batch): 1.1209667921066284\n",
      "\t Training loss (single batch): 1.0899161100387573\n",
      "\t Training loss (single batch): 1.2989656925201416\n",
      "\t Training loss (single batch): 1.1696252822875977\n",
      "\t Training loss (single batch): 0.9201581478118896\n",
      "\t Training loss (single batch): 1.3750609159469604\n",
      "\t Training loss (single batch): 1.3978309631347656\n",
      "\t Training loss (single batch): 1.0070544481277466\n",
      "\t Training loss (single batch): 1.2179887294769287\n",
      "\t Training loss (single batch): 1.1686367988586426\n",
      "\t Training loss (single batch): 1.1710247993469238\n",
      "\t Training loss (single batch): 1.202233910560608\n",
      "\t Training loss (single batch): 1.2129942178726196\n",
      "\t Training loss (single batch): 1.6883575916290283\n",
      "\t Training loss (single batch): 1.6165857315063477\n",
      "\t Training loss (single batch): 1.494091272354126\n",
      "\t Training loss (single batch): 1.149621844291687\n",
      "\t Training loss (single batch): 1.1994448900222778\n",
      "\t Training loss (single batch): 1.4783458709716797\n",
      "\t Training loss (single batch): 1.4960983991622925\n",
      "\t Training loss (single batch): 1.1517845392227173\n",
      "\t Training loss (single batch): 1.216610312461853\n",
      "\t Training loss (single batch): 1.2023870944976807\n",
      "\t Training loss (single batch): 1.3714091777801514\n",
      "\t Training loss (single batch): 0.927105724811554\n",
      "\t Training loss (single batch): 1.1975609064102173\n",
      "\t Training loss (single batch): 0.9285833835601807\n",
      "\t Training loss (single batch): 0.8242599368095398\n",
      "\t Training loss (single batch): 1.152335524559021\n",
      "\t Training loss (single batch): 0.7325160503387451\n",
      "\t Training loss (single batch): 1.5076154470443726\n",
      "\t Training loss (single batch): 1.5047131776809692\n",
      "\t Training loss (single batch): 0.8892419934272766\n",
      "\t Training loss (single batch): 1.553029179573059\n",
      "\t Training loss (single batch): 1.4555555582046509\n",
      "\t Training loss (single batch): 1.8603724241256714\n",
      "\t Training loss (single batch): 0.79423987865448\n",
      "\t Training loss (single batch): 1.3568733930587769\n",
      "\t Training loss (single batch): 1.4697208404541016\n",
      "\t Training loss (single batch): 1.0301331281661987\n",
      "\t Training loss (single batch): 1.2654838562011719\n",
      "\t Training loss (single batch): 1.4981951713562012\n",
      "\t Training loss (single batch): 1.058196783065796\n",
      "\t Training loss (single batch): 1.0227043628692627\n",
      "\t Training loss (single batch): 0.9732899069786072\n",
      "\t Training loss (single batch): 0.9690529108047485\n",
      "\t Training loss (single batch): 1.188873052597046\n",
      "\t Training loss (single batch): 1.3986461162567139\n",
      "\t Training loss (single batch): 1.0955764055252075\n",
      "\t Training loss (single batch): 1.3972928524017334\n",
      "\t Training loss (single batch): 1.1149293184280396\n",
      "\t Training loss (single batch): 1.1116621494293213\n",
      "\t Training loss (single batch): 1.5687612295150757\n",
      "\t Training loss (single batch): 0.665462076663971\n",
      "\t Training loss (single batch): 1.1368948221206665\n",
      "\t Training loss (single batch): 1.370966911315918\n",
      "\t Training loss (single batch): 1.7275245189666748\n",
      "\t Training loss (single batch): 0.4818240702152252\n",
      "##################################\n",
      "## EPOCH 29\n",
      "##################################\n",
      "\t Training loss (single batch): 1.447322964668274\n",
      "\t Training loss (single batch): 1.0910214185714722\n",
      "\t Training loss (single batch): 1.291128396987915\n",
      "\t Training loss (single batch): 0.7038263082504272\n",
      "\t Training loss (single batch): 0.7674521207809448\n",
      "\t Training loss (single batch): 1.6428766250610352\n",
      "\t Training loss (single batch): 1.3569674491882324\n",
      "\t Training loss (single batch): 1.2911211252212524\n",
      "\t Training loss (single batch): 1.1303913593292236\n",
      "\t Training loss (single batch): 1.6202014684677124\n",
      "\t Training loss (single batch): 1.1722520589828491\n",
      "\t Training loss (single batch): 1.203145146369934\n",
      "\t Training loss (single batch): 1.2611491680145264\n",
      "\t Training loss (single batch): 1.6309905052185059\n",
      "\t Training loss (single batch): 1.0134090185165405\n",
      "\t Training loss (single batch): 1.1694415807724\n",
      "\t Training loss (single batch): 1.1820316314697266\n",
      "\t Training loss (single batch): 1.016289234161377\n",
      "\t Training loss (single batch): 1.6082334518432617\n",
      "\t Training loss (single batch): 1.2192449569702148\n",
      "\t Training loss (single batch): 1.3701144456863403\n",
      "\t Training loss (single batch): 1.1673150062561035\n",
      "\t Training loss (single batch): 0.6747597455978394\n",
      "\t Training loss (single batch): 0.9006161689758301\n",
      "\t Training loss (single batch): 1.0483068227767944\n",
      "\t Training loss (single batch): 1.5006365776062012\n",
      "\t Training loss (single batch): 0.9075038433074951\n",
      "\t Training loss (single batch): 1.5022828578948975\n",
      "\t Training loss (single batch): 0.880422830581665\n",
      "\t Training loss (single batch): 1.0254323482513428\n",
      "\t Training loss (single batch): 1.1917002201080322\n",
      "\t Training loss (single batch): 0.8588639497756958\n",
      "\t Training loss (single batch): 0.837289571762085\n",
      "\t Training loss (single batch): 1.2421945333480835\n",
      "\t Training loss (single batch): 1.0568795204162598\n",
      "\t Training loss (single batch): 1.0712918043136597\n",
      "\t Training loss (single batch): 1.8551490306854248\n",
      "\t Training loss (single batch): 1.4436569213867188\n",
      "\t Training loss (single batch): 1.1438943147659302\n",
      "\t Training loss (single batch): 1.0586434602737427\n",
      "\t Training loss (single batch): 0.9868292212486267\n",
      "\t Training loss (single batch): 1.2729995250701904\n",
      "\t Training loss (single batch): 1.1336177587509155\n",
      "\t Training loss (single batch): 1.6354552507400513\n",
      "\t Training loss (single batch): 0.6447243690490723\n",
      "\t Training loss (single batch): 1.7582807540893555\n",
      "\t Training loss (single batch): 1.020290732383728\n",
      "\t Training loss (single batch): 1.1671189069747925\n",
      "\t Training loss (single batch): 1.322841763496399\n",
      "\t Training loss (single batch): 1.6593068838119507\n",
      "\t Training loss (single batch): 1.1737208366394043\n",
      "\t Training loss (single batch): 1.1272941827774048\n",
      "\t Training loss (single batch): 1.085984230041504\n",
      "\t Training loss (single batch): 1.194605827331543\n",
      "\t Training loss (single batch): 1.446917176246643\n",
      "\t Training loss (single batch): 1.3165138959884644\n",
      "\t Training loss (single batch): 1.1061599254608154\n",
      "\t Training loss (single batch): 1.2872347831726074\n",
      "\t Training loss (single batch): 1.6871886253356934\n",
      "\t Training loss (single batch): 0.8878775238990784\n",
      "\t Training loss (single batch): 0.9159207940101624\n",
      "\t Training loss (single batch): 1.5502275228500366\n",
      "\t Training loss (single batch): 1.3686155080795288\n",
      "\t Training loss (single batch): 1.6011329889297485\n",
      "\t Training loss (single batch): 0.9554514288902283\n",
      "\t Training loss (single batch): 1.0272527933120728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8646589517593384\n",
      "\t Training loss (single batch): 1.5139973163604736\n",
      "\t Training loss (single batch): 1.2604713439941406\n",
      "\t Training loss (single batch): 1.8194292783737183\n",
      "\t Training loss (single batch): 1.7563838958740234\n",
      "\t Training loss (single batch): 1.1712827682495117\n",
      "\t Training loss (single batch): 1.6467649936676025\n",
      "\t Training loss (single batch): 1.6688511371612549\n",
      "\t Training loss (single batch): 1.7889991998672485\n",
      "\t Training loss (single batch): 1.4298487901687622\n",
      "\t Training loss (single batch): 1.1935802698135376\n",
      "\t Training loss (single batch): 1.3851791620254517\n",
      "\t Training loss (single batch): 1.059388518333435\n",
      "\t Training loss (single batch): 1.3565739393234253\n",
      "\t Training loss (single batch): 1.5895673036575317\n",
      "\t Training loss (single batch): 1.6278945207595825\n",
      "\t Training loss (single batch): 0.8757365942001343\n",
      "\t Training loss (single batch): 1.1679954528808594\n",
      "\t Training loss (single batch): 1.4287183284759521\n",
      "\t Training loss (single batch): 1.1029062271118164\n",
      "\t Training loss (single batch): 1.0082948207855225\n",
      "\t Training loss (single batch): 1.22921621799469\n",
      "\t Training loss (single batch): 1.2661004066467285\n",
      "\t Training loss (single batch): 0.9605211615562439\n",
      "\t Training loss (single batch): 1.210348129272461\n",
      "\t Training loss (single batch): 1.1267281770706177\n",
      "\t Training loss (single batch): 1.3048759698867798\n",
      "\t Training loss (single batch): 1.1682847738265991\n",
      "\t Training loss (single batch): 1.1546483039855957\n",
      "\t Training loss (single batch): 1.2395631074905396\n",
      "\t Training loss (single batch): 1.2598539590835571\n",
      "\t Training loss (single batch): 0.9708274006843567\n",
      "\t Training loss (single batch): 1.4832046031951904\n",
      "\t Training loss (single batch): 1.207749605178833\n",
      "\t Training loss (single batch): 1.793535828590393\n",
      "\t Training loss (single batch): 0.9761834144592285\n",
      "\t Training loss (single batch): 1.3188070058822632\n",
      "\t Training loss (single batch): 1.4855440855026245\n",
      "\t Training loss (single batch): 1.3732478618621826\n",
      "\t Training loss (single batch): 1.4980990886688232\n",
      "\t Training loss (single batch): 1.1163424253463745\n",
      "\t Training loss (single batch): 1.1686325073242188\n",
      "\t Training loss (single batch): 1.4029386043548584\n",
      "\t Training loss (single batch): 0.8790594339370728\n",
      "\t Training loss (single batch): 1.4290601015090942\n",
      "\t Training loss (single batch): 0.9161809682846069\n",
      "\t Training loss (single batch): 1.5388259887695312\n",
      "\t Training loss (single batch): 1.3855143785476685\n",
      "\t Training loss (single batch): 1.2797231674194336\n",
      "\t Training loss (single batch): 1.0422073602676392\n",
      "\t Training loss (single batch): 1.3124594688415527\n",
      "\t Training loss (single batch): 1.6087560653686523\n",
      "\t Training loss (single batch): 1.3768283128738403\n",
      "\t Training loss (single batch): 1.1420308351516724\n",
      "\t Training loss (single batch): 0.9451974630355835\n",
      "\t Training loss (single batch): 1.2576221227645874\n",
      "\t Training loss (single batch): 1.2561098337173462\n",
      "\t Training loss (single batch): 1.1161985397338867\n",
      "\t Training loss (single batch): 0.6932995915412903\n",
      "\t Training loss (single batch): 1.0730199813842773\n",
      "\t Training loss (single batch): 0.9116015434265137\n",
      "\t Training loss (single batch): 1.3874632120132446\n",
      "\t Training loss (single batch): 1.3310720920562744\n",
      "\t Training loss (single batch): 1.155552864074707\n",
      "\t Training loss (single batch): 0.9783115386962891\n",
      "\t Training loss (single batch): 0.9742419123649597\n",
      "\t Training loss (single batch): 1.3268638849258423\n",
      "\t Training loss (single batch): 1.6268336772918701\n",
      "\t Training loss (single batch): 1.3737441301345825\n",
      "\t Training loss (single batch): 1.6096630096435547\n",
      "\t Training loss (single batch): 1.255386233329773\n",
      "\t Training loss (single batch): 1.3659725189208984\n",
      "\t Training loss (single batch): 0.9625964760780334\n",
      "\t Training loss (single batch): 1.192805528640747\n",
      "\t Training loss (single batch): 0.9928591847419739\n",
      "\t Training loss (single batch): 1.2372413873672485\n",
      "\t Training loss (single batch): 1.461047887802124\n",
      "\t Training loss (single batch): 1.3440221548080444\n",
      "\t Training loss (single batch): 0.7545047402381897\n",
      "\t Training loss (single batch): 1.4206751585006714\n",
      "\t Training loss (single batch): 1.3142492771148682\n",
      "\t Training loss (single batch): 1.1799026727676392\n",
      "\t Training loss (single batch): 0.9091941714286804\n",
      "\t Training loss (single batch): 1.2489171028137207\n",
      "\t Training loss (single batch): 0.9599173069000244\n",
      "\t Training loss (single batch): 1.7271084785461426\n",
      "\t Training loss (single batch): 1.2097804546356201\n",
      "\t Training loss (single batch): 1.1760050058364868\n",
      "\t Training loss (single batch): 1.0468825101852417\n",
      "\t Training loss (single batch): 1.2581833600997925\n",
      "\t Training loss (single batch): 1.090982437133789\n",
      "\t Training loss (single batch): 1.2741835117340088\n",
      "\t Training loss (single batch): 0.8251979351043701\n",
      "\t Training loss (single batch): 1.5696150064468384\n",
      "\t Training loss (single batch): 1.6997524499893188\n",
      "\t Training loss (single batch): 0.9359737038612366\n",
      "\t Training loss (single batch): 1.2625491619110107\n",
      "\t Training loss (single batch): 1.1284105777740479\n",
      "\t Training loss (single batch): 1.2400226593017578\n",
      "\t Training loss (single batch): 1.3921127319335938\n",
      "\t Training loss (single batch): 1.3443365097045898\n",
      "\t Training loss (single batch): 1.2004854679107666\n",
      "\t Training loss (single batch): 1.0714255571365356\n",
      "\t Training loss (single batch): 1.2783030271530151\n",
      "\t Training loss (single batch): 1.1031297445297241\n",
      "\t Training loss (single batch): 1.3314135074615479\n",
      "\t Training loss (single batch): 0.8490317463874817\n",
      "\t Training loss (single batch): 1.0750309228897095\n",
      "\t Training loss (single batch): 0.9817996025085449\n",
      "\t Training loss (single batch): 1.578325867652893\n",
      "\t Training loss (single batch): 1.1296195983886719\n",
      "\t Training loss (single batch): 1.5545666217803955\n",
      "\t Training loss (single batch): 0.9892270565032959\n",
      "\t Training loss (single batch): 1.2084945440292358\n",
      "\t Training loss (single batch): 1.5974807739257812\n",
      "\t Training loss (single batch): 1.0502985715866089\n",
      "\t Training loss (single batch): 1.1921391487121582\n",
      "\t Training loss (single batch): 0.7646154165267944\n",
      "\t Training loss (single batch): 1.1500475406646729\n",
      "\t Training loss (single batch): 1.4705123901367188\n",
      "\t Training loss (single batch): 1.2129478454589844\n",
      "\t Training loss (single batch): 1.2384015321731567\n",
      "\t Training loss (single batch): 1.1002706289291382\n",
      "\t Training loss (single batch): 1.1909492015838623\n",
      "\t Training loss (single batch): 1.6127800941467285\n",
      "\t Training loss (single batch): 1.0397709608078003\n",
      "\t Training loss (single batch): 1.396385669708252\n",
      "\t Training loss (single batch): 1.2183431386947632\n",
      "\t Training loss (single batch): 1.058742880821228\n",
      "\t Training loss (single batch): 1.1012564897537231\n",
      "\t Training loss (single batch): 1.2793636322021484\n",
      "\t Training loss (single batch): 1.1168110370635986\n",
      "\t Training loss (single batch): 1.0057778358459473\n",
      "\t Training loss (single batch): 1.1469660997390747\n",
      "\t Training loss (single batch): 1.1759880781173706\n",
      "\t Training loss (single batch): 1.544599175453186\n",
      "\t Training loss (single batch): 1.3853759765625\n",
      "\t Training loss (single batch): 1.7628746032714844\n",
      "\t Training loss (single batch): 1.337152361869812\n",
      "\t Training loss (single batch): 1.0619763135910034\n",
      "\t Training loss (single batch): 0.9818546175956726\n",
      "\t Training loss (single batch): 1.5893326997756958\n",
      "\t Training loss (single batch): 1.3141783475875854\n",
      "\t Training loss (single batch): 1.1880316734313965\n",
      "\t Training loss (single batch): 1.20039701461792\n",
      "\t Training loss (single batch): 1.4057809114456177\n",
      "\t Training loss (single batch): 0.856999397277832\n",
      "\t Training loss (single batch): 1.034784197807312\n",
      "\t Training loss (single batch): 1.1370872259140015\n",
      "\t Training loss (single batch): 1.1038211584091187\n",
      "\t Training loss (single batch): 1.3578057289123535\n",
      "\t Training loss (single batch): 1.2753456830978394\n",
      "\t Training loss (single batch): 0.8748841285705566\n",
      "\t Training loss (single batch): 1.2538708448410034\n",
      "\t Training loss (single batch): 1.234505534172058\n",
      "\t Training loss (single batch): 1.1939491033554077\n",
      "\t Training loss (single batch): 1.5740864276885986\n",
      "\t Training loss (single batch): 1.351009726524353\n",
      "\t Training loss (single batch): 1.290266990661621\n",
      "\t Training loss (single batch): 0.6215118169784546\n",
      "\t Training loss (single batch): 1.1195224523544312\n",
      "\t Training loss (single batch): 1.1072806119918823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4746291637420654\n",
      "\t Training loss (single batch): 1.4462378025054932\n",
      "\t Training loss (single batch): 1.109545350074768\n",
      "\t Training loss (single batch): 1.0625661611557007\n",
      "\t Training loss (single batch): 1.048753261566162\n",
      "\t Training loss (single batch): 1.1380306482315063\n",
      "\t Training loss (single batch): 1.2640347480773926\n",
      "\t Training loss (single batch): 0.9685878753662109\n",
      "\t Training loss (single batch): 0.7079827189445496\n",
      "\t Training loss (single batch): 1.5360047817230225\n",
      "\t Training loss (single batch): 0.8483339548110962\n",
      "\t Training loss (single batch): 0.978236734867096\n",
      "\t Training loss (single batch): 1.048919677734375\n",
      "\t Training loss (single batch): 1.670799732208252\n",
      "\t Training loss (single batch): 0.98811936378479\n",
      "\t Training loss (single batch): 1.414137363433838\n",
      "\t Training loss (single batch): 1.084352970123291\n",
      "\t Training loss (single batch): 1.5838472843170166\n",
      "\t Training loss (single batch): 1.2442227602005005\n",
      "\t Training loss (single batch): 1.215541958808899\n",
      "\t Training loss (single batch): 1.070046067237854\n",
      "\t Training loss (single batch): 1.0388394594192505\n",
      "\t Training loss (single batch): 1.3230396509170532\n",
      "\t Training loss (single batch): 1.1467469930648804\n",
      "\t Training loss (single batch): 1.1046347618103027\n",
      "\t Training loss (single batch): 1.0294654369354248\n",
      "\t Training loss (single batch): 1.5993365049362183\n",
      "\t Training loss (single batch): 1.334564208984375\n",
      "\t Training loss (single batch): 1.3803550004959106\n",
      "\t Training loss (single batch): 0.8490881323814392\n",
      "\t Training loss (single batch): 1.225643277168274\n",
      "\t Training loss (single batch): 1.2456424236297607\n",
      "\t Training loss (single batch): 1.1425153017044067\n",
      "\t Training loss (single batch): 1.011730432510376\n",
      "\t Training loss (single batch): 0.9203697443008423\n",
      "\t Training loss (single batch): 0.9685986042022705\n",
      "\t Training loss (single batch): 1.3500422239303589\n",
      "\t Training loss (single batch): 1.09528648853302\n",
      "\t Training loss (single batch): 1.229284644126892\n",
      "\t Training loss (single batch): 1.7186187505722046\n",
      "\t Training loss (single batch): 1.3550529479980469\n",
      "\t Training loss (single batch): 1.3620487451553345\n",
      "\t Training loss (single batch): 1.4017335176467896\n",
      "\t Training loss (single batch): 1.4529714584350586\n",
      "\t Training loss (single batch): 1.3039696216583252\n",
      "\t Training loss (single batch): 1.4146901369094849\n",
      "\t Training loss (single batch): 1.2270793914794922\n",
      "\t Training loss (single batch): 1.2162244319915771\n",
      "\t Training loss (single batch): 1.3146510124206543\n",
      "\t Training loss (single batch): 1.1688047647476196\n",
      "\t Training loss (single batch): 1.4200465679168701\n",
      "\t Training loss (single batch): 1.4085747003555298\n",
      "\t Training loss (single batch): 1.0348902940750122\n",
      "\t Training loss (single batch): 1.2657748460769653\n",
      "\t Training loss (single batch): 1.1727691888809204\n",
      "\t Training loss (single batch): 1.5258890390396118\n",
      "\t Training loss (single batch): 1.0860790014266968\n",
      "\t Training loss (single batch): 1.0918112993240356\n",
      "\t Training loss (single batch): 1.7487093210220337\n",
      "\t Training loss (single batch): 1.2448309659957886\n",
      "\t Training loss (single batch): 1.3327305316925049\n",
      "\t Training loss (single batch): 0.889945924282074\n",
      "\t Training loss (single batch): 1.0622072219848633\n",
      "\t Training loss (single batch): 1.2499159574508667\n",
      "\t Training loss (single batch): 0.885944664478302\n",
      "\t Training loss (single batch): 2.025239944458008\n",
      "\t Training loss (single batch): 1.624468445777893\n",
      "\t Training loss (single batch): 1.2022933959960938\n",
      "\t Training loss (single batch): 1.7123064994812012\n",
      "\t Training loss (single batch): 1.45578932762146\n",
      "\t Training loss (single batch): 1.5489236116409302\n",
      "\t Training loss (single batch): 1.5952645540237427\n",
      "\t Training loss (single batch): 1.5219976902008057\n",
      "\t Training loss (single batch): 1.1943283081054688\n",
      "\t Training loss (single batch): 1.7018266916275024\n",
      "\t Training loss (single batch): 1.4791067838668823\n",
      "\t Training loss (single batch): 1.209935188293457\n",
      "\t Training loss (single batch): 1.2454407215118408\n",
      "\t Training loss (single batch): 1.6408939361572266\n",
      "\t Training loss (single batch): 1.523740530014038\n",
      "\t Training loss (single batch): 1.4398530721664429\n",
      "\t Training loss (single batch): 1.1919457912445068\n",
      "\t Training loss (single batch): 1.525131106376648\n",
      "\t Training loss (single batch): 1.1128041744232178\n",
      "\t Training loss (single batch): 1.4517930746078491\n",
      "\t Training loss (single batch): 0.965008020401001\n",
      "\t Training loss (single batch): 0.9918692708015442\n",
      "\t Training loss (single batch): 1.9006128311157227\n",
      "\t Training loss (single batch): 3.6708266735076904\n",
      "##################################\n",
      "## EPOCH 30\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3873049020767212\n",
      "\t Training loss (single batch): 1.4276005029678345\n",
      "\t Training loss (single batch): 0.9684314727783203\n",
      "\t Training loss (single batch): 1.5759197473526\n",
      "\t Training loss (single batch): 1.5768201351165771\n",
      "\t Training loss (single batch): 1.0738271474838257\n",
      "\t Training loss (single batch): 1.6699159145355225\n",
      "\t Training loss (single batch): 1.2850637435913086\n",
      "\t Training loss (single batch): 1.0836784839630127\n",
      "\t Training loss (single batch): 1.3004828691482544\n",
      "\t Training loss (single batch): 1.501942753791809\n",
      "\t Training loss (single batch): 1.5371626615524292\n",
      "\t Training loss (single batch): 1.2063899040222168\n",
      "\t Training loss (single batch): 1.3192836046218872\n",
      "\t Training loss (single batch): 1.4364523887634277\n",
      "\t Training loss (single batch): 1.4987890720367432\n",
      "\t Training loss (single batch): 1.3150898218154907\n",
      "\t Training loss (single batch): 1.3950114250183105\n",
      "\t Training loss (single batch): 0.8689101338386536\n",
      "\t Training loss (single batch): 1.3434760570526123\n",
      "\t Training loss (single batch): 1.55853271484375\n",
      "\t Training loss (single batch): 1.1579971313476562\n",
      "\t Training loss (single batch): 1.3042839765548706\n",
      "\t Training loss (single batch): 1.0184205770492554\n",
      "\t Training loss (single batch): 1.420033574104309\n",
      "\t Training loss (single batch): 1.18903386592865\n",
      "\t Training loss (single batch): 1.1438215970993042\n",
      "\t Training loss (single batch): 1.4091639518737793\n",
      "\t Training loss (single batch): 1.2842625379562378\n",
      "\t Training loss (single batch): 1.0146894454956055\n",
      "\t Training loss (single batch): 1.202183723449707\n",
      "\t Training loss (single batch): 1.4230819940567017\n",
      "\t Training loss (single batch): 1.3041332960128784\n",
      "\t Training loss (single batch): 0.992145299911499\n",
      "\t Training loss (single batch): 0.8218151926994324\n",
      "\t Training loss (single batch): 0.8938201665878296\n",
      "\t Training loss (single batch): 0.7358822822570801\n",
      "\t Training loss (single batch): 1.1276909112930298\n",
      "\t Training loss (single batch): 1.5991617441177368\n",
      "\t Training loss (single batch): 1.375038504600525\n",
      "\t Training loss (single batch): 0.622146487236023\n",
      "\t Training loss (single batch): 1.9624791145324707\n",
      "\t Training loss (single batch): 1.307471513748169\n",
      "\t Training loss (single batch): 1.1388236284255981\n",
      "\t Training loss (single batch): 1.0948700904846191\n",
      "\t Training loss (single batch): 0.9154189229011536\n",
      "\t Training loss (single batch): 0.7416391372680664\n",
      "\t Training loss (single batch): 1.3864623308181763\n",
      "\t Training loss (single batch): 0.9960642457008362\n",
      "\t Training loss (single batch): 1.3255667686462402\n",
      "\t Training loss (single batch): 1.019779920578003\n",
      "\t Training loss (single batch): 1.1705909967422485\n",
      "\t Training loss (single batch): 1.2662372589111328\n",
      "\t Training loss (single batch): 1.0839552879333496\n",
      "\t Training loss (single batch): 1.367942452430725\n",
      "\t Training loss (single batch): 1.2334554195404053\n",
      "\t Training loss (single batch): 1.0734363794326782\n",
      "\t Training loss (single batch): 0.9130899310112\n",
      "\t Training loss (single batch): 1.0413827896118164\n",
      "\t Training loss (single batch): 1.1428203582763672\n",
      "\t Training loss (single batch): 0.9866698384284973\n",
      "\t Training loss (single batch): 1.4261292219161987\n",
      "\t Training loss (single batch): 1.0627728700637817\n",
      "\t Training loss (single batch): 0.9811645746231079\n",
      "\t Training loss (single batch): 1.3909956216812134\n",
      "\t Training loss (single batch): 1.1151129007339478\n",
      "\t Training loss (single batch): 1.051161527633667\n",
      "\t Training loss (single batch): 1.3581606149673462\n",
      "\t Training loss (single batch): 1.0123542547225952\n",
      "\t Training loss (single batch): 1.1116915941238403\n",
      "\t Training loss (single batch): 0.8130818605422974\n",
      "\t Training loss (single batch): 0.7783005833625793\n",
      "\t Training loss (single batch): 1.0878547430038452\n",
      "\t Training loss (single batch): 1.1162235736846924\n",
      "\t Training loss (single batch): 1.721667766571045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2588272094726562\n",
      "\t Training loss (single batch): 1.3544557094573975\n",
      "\t Training loss (single batch): 1.4567712545394897\n",
      "\t Training loss (single batch): 1.3191717863082886\n",
      "\t Training loss (single batch): 0.9248427748680115\n",
      "\t Training loss (single batch): 1.1908313035964966\n",
      "\t Training loss (single batch): 1.1905946731567383\n",
      "\t Training loss (single batch): 1.7638276815414429\n",
      "\t Training loss (single batch): 0.9590513706207275\n",
      "\t Training loss (single batch): 1.0208245515823364\n",
      "\t Training loss (single batch): 1.544180989265442\n",
      "\t Training loss (single batch): 0.7859953045845032\n",
      "\t Training loss (single batch): 0.9495332837104797\n",
      "\t Training loss (single batch): 0.7515546679496765\n",
      "\t Training loss (single batch): 0.9525044560432434\n",
      "\t Training loss (single batch): 1.081982970237732\n",
      "\t Training loss (single batch): 1.6597031354904175\n",
      "\t Training loss (single batch): 1.2819457054138184\n",
      "\t Training loss (single batch): 2.1031923294067383\n",
      "\t Training loss (single batch): 1.0516252517700195\n",
      "\t Training loss (single batch): 1.601120948791504\n",
      "\t Training loss (single batch): 1.348483920097351\n",
      "\t Training loss (single batch): 1.437307357788086\n",
      "\t Training loss (single batch): 0.7692583203315735\n",
      "\t Training loss (single batch): 0.9348222017288208\n",
      "\t Training loss (single batch): 1.580450415611267\n",
      "\t Training loss (single batch): 1.599105715751648\n",
      "\t Training loss (single batch): 1.3109219074249268\n",
      "\t Training loss (single batch): 1.598238229751587\n",
      "\t Training loss (single batch): 1.3236750364303589\n",
      "\t Training loss (single batch): 0.9809209704399109\n",
      "\t Training loss (single batch): 0.8012951016426086\n",
      "\t Training loss (single batch): 1.525068998336792\n",
      "\t Training loss (single batch): 1.2389047145843506\n",
      "\t Training loss (single batch): 1.030043125152588\n",
      "\t Training loss (single batch): 1.7730989456176758\n",
      "\t Training loss (single batch): 1.6826614141464233\n",
      "\t Training loss (single batch): 1.2725023031234741\n",
      "\t Training loss (single batch): 1.0610607862472534\n",
      "\t Training loss (single batch): 1.0031204223632812\n",
      "\t Training loss (single batch): 1.2736438512802124\n",
      "\t Training loss (single batch): 1.1247444152832031\n",
      "\t Training loss (single batch): 0.9590969085693359\n",
      "\t Training loss (single batch): 0.9234631061553955\n",
      "\t Training loss (single batch): 1.11600923538208\n",
      "\t Training loss (single batch): 0.8685243725776672\n",
      "\t Training loss (single batch): 1.0855365991592407\n",
      "\t Training loss (single batch): 1.1456780433654785\n",
      "\t Training loss (single batch): 1.5459750890731812\n",
      "\t Training loss (single batch): 1.5840706825256348\n",
      "\t Training loss (single batch): 1.062816858291626\n",
      "\t Training loss (single batch): 1.5656956434249878\n",
      "\t Training loss (single batch): 1.0392946004867554\n",
      "\t Training loss (single batch): 1.3393399715423584\n",
      "\t Training loss (single batch): 1.2404521703720093\n",
      "\t Training loss (single batch): 1.3120182752609253\n",
      "\t Training loss (single batch): 1.5294398069381714\n",
      "\t Training loss (single batch): 1.663855791091919\n",
      "\t Training loss (single batch): 1.06331467628479\n",
      "\t Training loss (single batch): 0.9762625098228455\n",
      "\t Training loss (single batch): 1.776167631149292\n",
      "\t Training loss (single batch): 1.1547000408172607\n",
      "\t Training loss (single batch): 1.5252530574798584\n",
      "\t Training loss (single batch): 0.962550699710846\n",
      "\t Training loss (single batch): 1.3586963415145874\n",
      "\t Training loss (single batch): 1.6544634103775024\n",
      "\t Training loss (single batch): 1.2856707572937012\n",
      "\t Training loss (single batch): 0.9903318881988525\n",
      "\t Training loss (single batch): 1.3566021919250488\n",
      "\t Training loss (single batch): 1.0672346353530884\n",
      "\t Training loss (single batch): 1.005342960357666\n",
      "\t Training loss (single batch): 1.136332631111145\n",
      "\t Training loss (single batch): 1.0553475618362427\n",
      "\t Training loss (single batch): 1.4010776281356812\n",
      "\t Training loss (single batch): 1.4851047992706299\n",
      "\t Training loss (single batch): 1.3256993293762207\n",
      "\t Training loss (single batch): 0.882826030254364\n",
      "\t Training loss (single batch): 1.0547922849655151\n",
      "\t Training loss (single batch): 1.5586793422698975\n",
      "\t Training loss (single batch): 1.4905128479003906\n",
      "\t Training loss (single batch): 1.2993942499160767\n",
      "\t Training loss (single batch): 1.5981396436691284\n",
      "\t Training loss (single batch): 1.7473483085632324\n",
      "\t Training loss (single batch): 1.0895923376083374\n",
      "\t Training loss (single batch): 1.5903574228286743\n",
      "\t Training loss (single batch): 0.9884002208709717\n",
      "\t Training loss (single batch): 1.6967289447784424\n",
      "\t Training loss (single batch): 1.5411475896835327\n",
      "\t Training loss (single batch): 0.6637230515480042\n",
      "\t Training loss (single batch): 1.1679418087005615\n",
      "\t Training loss (single batch): 0.958016574382782\n",
      "\t Training loss (single batch): 0.9304408431053162\n",
      "\t Training loss (single batch): 1.2841300964355469\n",
      "\t Training loss (single batch): 1.7514338493347168\n",
      "\t Training loss (single batch): 1.2303521633148193\n",
      "\t Training loss (single batch): 0.8444462418556213\n",
      "\t Training loss (single batch): 1.3477504253387451\n",
      "\t Training loss (single batch): 1.4162688255310059\n",
      "\t Training loss (single batch): 1.5262144804000854\n",
      "\t Training loss (single batch): 1.2669278383255005\n",
      "\t Training loss (single batch): 1.271030306816101\n",
      "\t Training loss (single batch): 1.7189719676971436\n",
      "\t Training loss (single batch): 1.0073115825653076\n",
      "\t Training loss (single batch): 1.104723572731018\n",
      "\t Training loss (single batch): 1.3317039012908936\n",
      "\t Training loss (single batch): 0.9130330085754395\n",
      "\t Training loss (single batch): 1.6986522674560547\n",
      "\t Training loss (single batch): 1.3218858242034912\n",
      "\t Training loss (single batch): 1.0996888875961304\n",
      "\t Training loss (single batch): 1.1692523956298828\n",
      "\t Training loss (single batch): 1.198914647102356\n",
      "\t Training loss (single batch): 1.0902777910232544\n",
      "\t Training loss (single batch): 1.660749912261963\n",
      "\t Training loss (single batch): 0.707305371761322\n",
      "\t Training loss (single batch): 1.2165029048919678\n",
      "\t Training loss (single batch): 1.285866618156433\n",
      "\t Training loss (single batch): 1.3862224817276\n",
      "\t Training loss (single batch): 1.2608247995376587\n",
      "\t Training loss (single batch): 0.8750923275947571\n",
      "\t Training loss (single batch): 1.2976385354995728\n",
      "\t Training loss (single batch): 1.3049695491790771\n",
      "\t Training loss (single batch): 1.4439923763275146\n",
      "\t Training loss (single batch): 1.2829965353012085\n",
      "\t Training loss (single batch): 1.0261213779449463\n",
      "\t Training loss (single batch): 0.7558414340019226\n",
      "\t Training loss (single batch): 1.1121703386306763\n",
      "\t Training loss (single batch): 0.9926044344902039\n",
      "\t Training loss (single batch): 1.236552119255066\n",
      "\t Training loss (single batch): 1.2093398571014404\n",
      "\t Training loss (single batch): 0.9030516147613525\n",
      "\t Training loss (single batch): 1.362368106842041\n",
      "\t Training loss (single batch): 1.0431222915649414\n",
      "\t Training loss (single batch): 1.266764521598816\n",
      "\t Training loss (single batch): 2.2421133518218994\n",
      "\t Training loss (single batch): 1.6592652797698975\n",
      "\t Training loss (single batch): 1.0558280944824219\n",
      "\t Training loss (single batch): 1.473161220550537\n",
      "\t Training loss (single batch): 1.3253532648086548\n",
      "\t Training loss (single batch): 1.2488651275634766\n",
      "\t Training loss (single batch): 0.8251070380210876\n",
      "\t Training loss (single batch): 1.135811448097229\n",
      "\t Training loss (single batch): 1.0750938653945923\n",
      "\t Training loss (single batch): 1.0947316884994507\n",
      "\t Training loss (single batch): 1.2168618440628052\n",
      "\t Training loss (single batch): 0.8622879385948181\n",
      "\t Training loss (single batch): 1.017649531364441\n",
      "\t Training loss (single batch): 0.9103416204452515\n",
      "\t Training loss (single batch): 1.0617293119430542\n",
      "\t Training loss (single batch): 1.6556240320205688\n",
      "\t Training loss (single batch): 1.3894203901290894\n",
      "\t Training loss (single batch): 1.6123219728469849\n",
      "\t Training loss (single batch): 1.2987456321716309\n",
      "\t Training loss (single batch): 0.7749232053756714\n",
      "\t Training loss (single batch): 1.067884922027588\n",
      "\t Training loss (single batch): 1.2912837266921997\n",
      "\t Training loss (single batch): 0.9024733304977417\n",
      "\t Training loss (single batch): 0.9889339208602905\n",
      "\t Training loss (single batch): 1.0900903940200806\n",
      "\t Training loss (single batch): 1.2652865648269653\n",
      "\t Training loss (single batch): 0.7871911525726318\n",
      "\t Training loss (single batch): 0.9679967164993286\n",
      "\t Training loss (single batch): 1.5764487981796265\n",
      "\t Training loss (single batch): 0.9998428225517273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5967063903808594\n",
      "\t Training loss (single batch): 1.0854418277740479\n",
      "\t Training loss (single batch): 1.4522184133529663\n",
      "\t Training loss (single batch): 1.1358511447906494\n",
      "\t Training loss (single batch): 1.8807190656661987\n",
      "\t Training loss (single batch): 1.39774751663208\n",
      "\t Training loss (single batch): 1.22345769405365\n",
      "\t Training loss (single batch): 1.4394729137420654\n",
      "\t Training loss (single batch): 0.8597050309181213\n",
      "\t Training loss (single batch): 1.1778961420059204\n",
      "\t Training loss (single batch): 1.1642351150512695\n",
      "\t Training loss (single batch): 1.0162220001220703\n",
      "\t Training loss (single batch): 1.394478678703308\n",
      "\t Training loss (single batch): 0.9643265604972839\n",
      "\t Training loss (single batch): 1.2646597623825073\n",
      "\t Training loss (single batch): 1.4541661739349365\n",
      "\t Training loss (single batch): 0.8201363682746887\n",
      "\t Training loss (single batch): 1.069275140762329\n",
      "\t Training loss (single batch): 1.1799070835113525\n",
      "\t Training loss (single batch): 1.4982227087020874\n",
      "\t Training loss (single batch): 1.346364140510559\n",
      "\t Training loss (single batch): 1.6943193674087524\n",
      "\t Training loss (single batch): 0.9016995429992676\n",
      "\t Training loss (single batch): 1.1839317083358765\n",
      "\t Training loss (single batch): 1.294437289237976\n",
      "\t Training loss (single batch): 1.514038324356079\n",
      "\t Training loss (single batch): 0.9820050001144409\n",
      "\t Training loss (single batch): 1.2208367586135864\n",
      "\t Training loss (single batch): 1.3469009399414062\n",
      "\t Training loss (single batch): 1.1949299573898315\n",
      "\t Training loss (single batch): 1.2181476354599\n",
      "\t Training loss (single batch): 1.3048735857009888\n",
      "\t Training loss (single batch): 1.4265145063400269\n",
      "\t Training loss (single batch): 0.8961716294288635\n",
      "\t Training loss (single batch): 1.4848517179489136\n",
      "\t Training loss (single batch): 0.9828685522079468\n",
      "\t Training loss (single batch): 1.5766589641571045\n",
      "\t Training loss (single batch): 0.913561224937439\n",
      "\t Training loss (single batch): 1.1533422470092773\n",
      "\t Training loss (single batch): 1.2732197046279907\n",
      "\t Training loss (single batch): 0.9970203638076782\n",
      "\t Training loss (single batch): 1.1671029329299927\n",
      "\t Training loss (single batch): 1.0036754608154297\n",
      "\t Training loss (single batch): 1.4441215991973877\n",
      "\t Training loss (single batch): 1.3292816877365112\n",
      "\t Training loss (single batch): 1.3236478567123413\n",
      "\t Training loss (single batch): 1.4814589023590088\n",
      "\t Training loss (single batch): 1.4566534757614136\n",
      "\t Training loss (single batch): 0.9504813551902771\n",
      "\t Training loss (single batch): 1.1565715074539185\n",
      "\t Training loss (single batch): 1.2057347297668457\n",
      "\t Training loss (single batch): 1.1251081228256226\n",
      "\t Training loss (single batch): 1.3515533208847046\n",
      "\t Training loss (single batch): 1.6648610830307007\n",
      "\t Training loss (single batch): 1.4323309659957886\n",
      "\t Training loss (single batch): 1.4813796281814575\n",
      "\t Training loss (single batch): 1.1306779384613037\n",
      "\t Training loss (single batch): 1.4568809270858765\n",
      "\t Training loss (single batch): 1.060125470161438\n",
      "\t Training loss (single batch): 1.3474723100662231\n",
      "\t Training loss (single batch): 1.2027723789215088\n",
      "\t Training loss (single batch): 1.3870038986206055\n",
      "\t Training loss (single batch): 1.3523409366607666\n",
      "\t Training loss (single batch): 1.1767306327819824\n",
      "\t Training loss (single batch): 1.3973958492279053\n",
      "\t Training loss (single batch): 1.7713643312454224\n",
      "\t Training loss (single batch): 1.4898604154586792\n",
      "\t Training loss (single batch): 1.5107731819152832\n",
      "\t Training loss (single batch): 1.4570176601409912\n",
      "\t Training loss (single batch): 1.5352615118026733\n",
      "\t Training loss (single batch): 0.5869147181510925\n",
      "\t Training loss (single batch): 1.149026870727539\n",
      "\t Training loss (single batch): 1.2613526582717896\n",
      "\t Training loss (single batch): 1.3079670667648315\n",
      "\t Training loss (single batch): 1.4846457242965698\n",
      "\t Training loss (single batch): 1.327474594116211\n",
      "\t Training loss (single batch): 1.4111603498458862\n",
      "\t Training loss (single batch): 1.9912161827087402\n",
      "\t Training loss (single batch): 1.2401175498962402\n",
      "##################################\n",
      "## EPOCH 31\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9872117042541504\n",
      "\t Training loss (single batch): 0.9823940396308899\n",
      "\t Training loss (single batch): 0.8618857860565186\n",
      "\t Training loss (single batch): 1.3353718519210815\n",
      "\t Training loss (single batch): 1.1327847242355347\n",
      "\t Training loss (single batch): 1.0645700693130493\n",
      "\t Training loss (single batch): 0.838448703289032\n",
      "\t Training loss (single batch): 1.3639636039733887\n",
      "\t Training loss (single batch): 1.1039808988571167\n",
      "\t Training loss (single batch): 1.056702971458435\n",
      "\t Training loss (single batch): 1.2543976306915283\n",
      "\t Training loss (single batch): 1.3440120220184326\n",
      "\t Training loss (single batch): 1.1884149312973022\n",
      "\t Training loss (single batch): 1.6508511304855347\n",
      "\t Training loss (single batch): 2.0095458030700684\n",
      "\t Training loss (single batch): 1.4198108911514282\n",
      "\t Training loss (single batch): 0.7796834707260132\n",
      "\t Training loss (single batch): 1.4961425065994263\n",
      "\t Training loss (single batch): 1.2202587127685547\n",
      "\t Training loss (single batch): 1.3665465116500854\n",
      "\t Training loss (single batch): 1.1246916055679321\n",
      "\t Training loss (single batch): 1.5635435581207275\n",
      "\t Training loss (single batch): 1.0929784774780273\n",
      "\t Training loss (single batch): 1.5439091920852661\n",
      "\t Training loss (single batch): 1.1557097434997559\n",
      "\t Training loss (single batch): 1.0611395835876465\n",
      "\t Training loss (single batch): 1.6022409200668335\n",
      "\t Training loss (single batch): 1.2464381456375122\n",
      "\t Training loss (single batch): 1.044871211051941\n",
      "\t Training loss (single batch): 1.056804895401001\n",
      "\t Training loss (single batch): 1.1064671277999878\n",
      "\t Training loss (single batch): 1.042001724243164\n",
      "\t Training loss (single batch): 0.9610002636909485\n",
      "\t Training loss (single batch): 1.3025301694869995\n",
      "\t Training loss (single batch): 1.7552884817123413\n",
      "\t Training loss (single batch): 1.106276273727417\n",
      "\t Training loss (single batch): 1.0901159048080444\n",
      "\t Training loss (single batch): 1.0088742971420288\n",
      "\t Training loss (single batch): 1.2463115453720093\n",
      "\t Training loss (single batch): 0.8220846056938171\n",
      "\t Training loss (single batch): 1.1723179817199707\n",
      "\t Training loss (single batch): 1.092944860458374\n",
      "\t Training loss (single batch): 1.0999068021774292\n",
      "\t Training loss (single batch): 1.2405034303665161\n",
      "\t Training loss (single batch): 0.9515606760978699\n",
      "\t Training loss (single batch): 1.1049244403839111\n",
      "\t Training loss (single batch): 0.9717937707901001\n",
      "\t Training loss (single batch): 1.3425735235214233\n",
      "\t Training loss (single batch): 0.9785937070846558\n",
      "\t Training loss (single batch): 1.6337430477142334\n",
      "\t Training loss (single batch): 1.3545734882354736\n",
      "\t Training loss (single batch): 1.3806462287902832\n",
      "\t Training loss (single batch): 1.0942654609680176\n",
      "\t Training loss (single batch): 1.5188795328140259\n",
      "\t Training loss (single batch): 1.480713129043579\n",
      "\t Training loss (single batch): 1.3792259693145752\n",
      "\t Training loss (single batch): 1.2964040040969849\n",
      "\t Training loss (single batch): 0.834275484085083\n",
      "\t Training loss (single batch): 1.3544455766677856\n",
      "\t Training loss (single batch): 0.7102876305580139\n",
      "\t Training loss (single batch): 1.1571729183197021\n",
      "\t Training loss (single batch): 0.8701167702674866\n",
      "\t Training loss (single batch): 0.8755226135253906\n",
      "\t Training loss (single batch): 2.118972063064575\n",
      "\t Training loss (single batch): 1.548564076423645\n",
      "\t Training loss (single batch): 1.5385509729385376\n",
      "\t Training loss (single batch): 1.3842566013336182\n",
      "\t Training loss (single batch): 0.8974301218986511\n",
      "\t Training loss (single batch): 0.8028455972671509\n",
      "\t Training loss (single batch): 1.1529884338378906\n",
      "\t Training loss (single batch): 1.6042531728744507\n",
      "\t Training loss (single batch): 1.0989187955856323\n",
      "\t Training loss (single batch): 0.8151872158050537\n",
      "\t Training loss (single batch): 0.9848892092704773\n",
      "\t Training loss (single batch): 1.3207790851593018\n",
      "\t Training loss (single batch): 0.7000734210014343\n",
      "\t Training loss (single batch): 1.1867318153381348\n",
      "\t Training loss (single batch): 1.6112500429153442\n",
      "\t Training loss (single batch): 1.7153925895690918\n",
      "\t Training loss (single batch): 1.3638360500335693\n",
      "\t Training loss (single batch): 1.8117460012435913\n",
      "\t Training loss (single batch): 1.1765803098678589\n",
      "\t Training loss (single batch): 1.2332829236984253\n",
      "\t Training loss (single batch): 1.3583379983901978\n",
      "\t Training loss (single batch): 1.4893804788589478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2066929340362549\n",
      "\t Training loss (single batch): 1.133245825767517\n",
      "\t Training loss (single batch): 1.1717382669448853\n",
      "\t Training loss (single batch): 1.2663743495941162\n",
      "\t Training loss (single batch): 1.5452344417572021\n",
      "\t Training loss (single batch): 1.0638467073440552\n",
      "\t Training loss (single batch): 0.9325516223907471\n",
      "\t Training loss (single batch): 1.3178389072418213\n",
      "\t Training loss (single batch): 0.8108089566230774\n",
      "\t Training loss (single batch): 1.7640159130096436\n",
      "\t Training loss (single batch): 0.8536686301231384\n",
      "\t Training loss (single batch): 0.7775309085845947\n",
      "\t Training loss (single batch): 1.5298434495925903\n",
      "\t Training loss (single batch): 0.935979962348938\n",
      "\t Training loss (single batch): 0.7730942964553833\n",
      "\t Training loss (single batch): 0.9712845087051392\n",
      "\t Training loss (single batch): 1.4499599933624268\n",
      "\t Training loss (single batch): 1.2824233770370483\n",
      "\t Training loss (single batch): 1.0629003047943115\n",
      "\t Training loss (single batch): 1.2370784282684326\n",
      "\t Training loss (single batch): 1.4261115789413452\n",
      "\t Training loss (single batch): 1.6314924955368042\n",
      "\t Training loss (single batch): 1.1681480407714844\n",
      "\t Training loss (single batch): 1.5678781270980835\n",
      "\t Training loss (single batch): 1.4277081489562988\n",
      "\t Training loss (single batch): 1.2582423686981201\n",
      "\t Training loss (single batch): 1.5124366283416748\n",
      "\t Training loss (single batch): 2.3463642597198486\n",
      "\t Training loss (single batch): 1.3752872943878174\n",
      "\t Training loss (single batch): 1.0289149284362793\n",
      "\t Training loss (single batch): 1.7815613746643066\n",
      "\t Training loss (single batch): 1.2268121242523193\n",
      "\t Training loss (single batch): 1.2718286514282227\n",
      "\t Training loss (single batch): 1.2027252912521362\n",
      "\t Training loss (single batch): 1.0507864952087402\n",
      "\t Training loss (single batch): 1.428301215171814\n",
      "\t Training loss (single batch): 1.0306587219238281\n",
      "\t Training loss (single batch): 1.5701816082000732\n",
      "\t Training loss (single batch): 1.3993587493896484\n",
      "\t Training loss (single batch): 1.157948613166809\n",
      "\t Training loss (single batch): 1.5843815803527832\n",
      "\t Training loss (single batch): 0.904219925403595\n",
      "\t Training loss (single batch): 1.5554163455963135\n",
      "\t Training loss (single batch): 0.9499965310096741\n",
      "\t Training loss (single batch): 1.0942260026931763\n",
      "\t Training loss (single batch): 1.2581393718719482\n",
      "\t Training loss (single batch): 1.305692434310913\n",
      "\t Training loss (single batch): 1.1927827596664429\n",
      "\t Training loss (single batch): 1.2989156246185303\n",
      "\t Training loss (single batch): 1.2311705350875854\n",
      "\t Training loss (single batch): 1.1878201961517334\n",
      "\t Training loss (single batch): 0.9057478308677673\n",
      "\t Training loss (single batch): 1.531770944595337\n",
      "\t Training loss (single batch): 1.4365571737289429\n",
      "\t Training loss (single batch): 0.7641634345054626\n",
      "\t Training loss (single batch): 0.8355203866958618\n",
      "\t Training loss (single batch): 1.3497973680496216\n",
      "\t Training loss (single batch): 1.110515832901001\n",
      "\t Training loss (single batch): 1.4517579078674316\n",
      "\t Training loss (single batch): 1.3404688835144043\n",
      "\t Training loss (single batch): 1.3915106058120728\n",
      "\t Training loss (single batch): 1.4514445066452026\n",
      "\t Training loss (single batch): 1.4511733055114746\n",
      "\t Training loss (single batch): 1.0024603605270386\n",
      "\t Training loss (single batch): 1.0827053785324097\n",
      "\t Training loss (single batch): 0.8953630924224854\n",
      "\t Training loss (single batch): 0.9760184288024902\n",
      "\t Training loss (single batch): 1.110480785369873\n",
      "\t Training loss (single batch): 1.3716483116149902\n",
      "\t Training loss (single batch): 1.035526156425476\n",
      "\t Training loss (single batch): 1.0467474460601807\n",
      "\t Training loss (single batch): 1.0869505405426025\n",
      "\t Training loss (single batch): 1.3966598510742188\n",
      "\t Training loss (single batch): 1.6774195432662964\n",
      "\t Training loss (single batch): 1.3047159910202026\n",
      "\t Training loss (single batch): 1.0869085788726807\n",
      "\t Training loss (single batch): 1.1652308702468872\n",
      "\t Training loss (single batch): 1.1179077625274658\n",
      "\t Training loss (single batch): 0.9529610276222229\n",
      "\t Training loss (single batch): 1.1152393817901611\n",
      "\t Training loss (single batch): 1.3242422342300415\n",
      "\t Training loss (single batch): 1.1443878412246704\n",
      "\t Training loss (single batch): 0.8748448491096497\n",
      "\t Training loss (single batch): 1.4717506170272827\n",
      "\t Training loss (single batch): 0.8502576947212219\n",
      "\t Training loss (single batch): 1.6298658847808838\n",
      "\t Training loss (single batch): 1.1727912425994873\n",
      "\t Training loss (single batch): 1.1615897417068481\n",
      "\t Training loss (single batch): 0.9963127374649048\n",
      "\t Training loss (single batch): 1.248276948928833\n",
      "\t Training loss (single batch): 1.0130032300949097\n",
      "\t Training loss (single batch): 1.0814393758773804\n",
      "\t Training loss (single batch): 1.4210937023162842\n",
      "\t Training loss (single batch): 1.896044135093689\n",
      "\t Training loss (single batch): 1.4143249988555908\n",
      "\t Training loss (single batch): 1.2991379499435425\n",
      "\t Training loss (single batch): 0.8105499744415283\n",
      "\t Training loss (single batch): 1.4228073358535767\n",
      "\t Training loss (single batch): 1.2891944646835327\n",
      "\t Training loss (single batch): 0.972656786441803\n",
      "\t Training loss (single batch): 1.1457059383392334\n",
      "\t Training loss (single batch): 1.2306455373764038\n",
      "\t Training loss (single batch): 1.025955319404602\n",
      "\t Training loss (single batch): 1.3003196716308594\n",
      "\t Training loss (single batch): 0.8842099905014038\n",
      "\t Training loss (single batch): 1.5562760829925537\n",
      "\t Training loss (single batch): 0.9232082366943359\n",
      "\t Training loss (single batch): 1.2639435529708862\n",
      "\t Training loss (single batch): 1.4286093711853027\n",
      "\t Training loss (single batch): 1.0718061923980713\n",
      "\t Training loss (single batch): 1.4559381008148193\n",
      "\t Training loss (single batch): 1.0866940021514893\n",
      "\t Training loss (single batch): 1.0930752754211426\n",
      "\t Training loss (single batch): 1.7049537897109985\n",
      "\t Training loss (single batch): 1.799971103668213\n",
      "\t Training loss (single batch): 0.7858244180679321\n",
      "\t Training loss (single batch): 0.9250870943069458\n",
      "\t Training loss (single batch): 1.0879014730453491\n",
      "\t Training loss (single batch): 1.2366595268249512\n",
      "\t Training loss (single batch): 1.5064197778701782\n",
      "\t Training loss (single batch): 1.0995413064956665\n",
      "\t Training loss (single batch): 0.948491096496582\n",
      "\t Training loss (single batch): 1.6397696733474731\n",
      "\t Training loss (single batch): 0.8938392400741577\n",
      "\t Training loss (single batch): 1.0510543584823608\n",
      "\t Training loss (single batch): 1.381730079650879\n",
      "\t Training loss (single batch): 1.2664610147476196\n",
      "\t Training loss (single batch): 0.8357557654380798\n",
      "\t Training loss (single batch): 1.0883017778396606\n",
      "\t Training loss (single batch): 1.1839702129364014\n",
      "\t Training loss (single batch): 1.6003715991973877\n",
      "\t Training loss (single batch): 1.1539584398269653\n",
      "\t Training loss (single batch): 1.2737181186676025\n",
      "\t Training loss (single batch): 1.5104105472564697\n",
      "\t Training loss (single batch): 1.0575977563858032\n",
      "\t Training loss (single batch): 1.2948503494262695\n",
      "\t Training loss (single batch): 1.277327299118042\n",
      "\t Training loss (single batch): 1.1752533912658691\n",
      "\t Training loss (single batch): 1.230617642402649\n",
      "\t Training loss (single batch): 1.9700324535369873\n",
      "\t Training loss (single batch): 1.3050341606140137\n",
      "\t Training loss (single batch): 1.3015789985656738\n",
      "\t Training loss (single batch): 1.0644426345825195\n",
      "\t Training loss (single batch): 1.2253639698028564\n",
      "\t Training loss (single batch): 0.6604517102241516\n",
      "\t Training loss (single batch): 1.0714281797409058\n",
      "\t Training loss (single batch): 1.742605209350586\n",
      "\t Training loss (single batch): 1.2095543146133423\n",
      "\t Training loss (single batch): 1.678934097290039\n",
      "\t Training loss (single batch): 1.1033283472061157\n",
      "\t Training loss (single batch): 1.534814476966858\n",
      "\t Training loss (single batch): 1.06630277633667\n",
      "\t Training loss (single batch): 1.5064306259155273\n",
      "\t Training loss (single batch): 1.138085961341858\n",
      "\t Training loss (single batch): 1.4872667789459229\n",
      "\t Training loss (single batch): 1.186126708984375\n",
      "\t Training loss (single batch): 1.0849307775497437\n",
      "\t Training loss (single batch): 1.433225393295288\n",
      "\t Training loss (single batch): 1.1493552923202515\n",
      "\t Training loss (single batch): 0.9499543905258179\n",
      "\t Training loss (single batch): 1.357884168624878\n",
      "\t Training loss (single batch): 1.2946685552597046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4002878665924072\n",
      "\t Training loss (single batch): 1.0172109603881836\n",
      "\t Training loss (single batch): 1.1529185771942139\n",
      "\t Training loss (single batch): 0.7644555568695068\n",
      "\t Training loss (single batch): 1.103046178817749\n",
      "\t Training loss (single batch): 1.0089625120162964\n",
      "\t Training loss (single batch): 1.560534954071045\n",
      "\t Training loss (single batch): 0.9474964737892151\n",
      "\t Training loss (single batch): 1.066887378692627\n",
      "\t Training loss (single batch): 1.4661990404129028\n",
      "\t Training loss (single batch): 1.1296465396881104\n",
      "\t Training loss (single batch): 1.022194266319275\n",
      "\t Training loss (single batch): 0.8447726964950562\n",
      "\t Training loss (single batch): 1.6274293661117554\n",
      "\t Training loss (single batch): 1.0287169218063354\n",
      "\t Training loss (single batch): 1.7317428588867188\n",
      "\t Training loss (single batch): 0.9861478805541992\n",
      "\t Training loss (single batch): 1.3178746700286865\n",
      "\t Training loss (single batch): 1.0156855583190918\n",
      "\t Training loss (single batch): 1.2605504989624023\n",
      "\t Training loss (single batch): 1.2641502618789673\n",
      "\t Training loss (single batch): 1.0373808145523071\n",
      "\t Training loss (single batch): 0.9886401891708374\n",
      "\t Training loss (single batch): 1.355675220489502\n",
      "\t Training loss (single batch): 1.7683323621749878\n",
      "\t Training loss (single batch): 1.159626841545105\n",
      "\t Training loss (single batch): 1.2733042240142822\n",
      "\t Training loss (single batch): 1.0804611444473267\n",
      "\t Training loss (single batch): 0.9739202260971069\n",
      "\t Training loss (single batch): 1.0805385112762451\n",
      "\t Training loss (single batch): 0.9615651369094849\n",
      "\t Training loss (single batch): 1.1481562852859497\n",
      "\t Training loss (single batch): 1.3100937604904175\n",
      "\t Training loss (single batch): 1.064957618713379\n",
      "\t Training loss (single batch): 1.1136868000030518\n",
      "\t Training loss (single batch): 1.2475944757461548\n",
      "\t Training loss (single batch): 0.8795967102050781\n",
      "\t Training loss (single batch): 1.0635281801223755\n",
      "\t Training loss (single batch): 1.2664194107055664\n",
      "\t Training loss (single batch): 0.8609353303909302\n",
      "\t Training loss (single batch): 1.3995164632797241\n",
      "\t Training loss (single batch): 1.8045605421066284\n",
      "\t Training loss (single batch): 2.0301568508148193\n",
      "\t Training loss (single batch): 1.5558110475540161\n",
      "\t Training loss (single batch): 0.886170506477356\n",
      "\t Training loss (single batch): 1.469214916229248\n",
      "\t Training loss (single batch): 1.2850639820098877\n",
      "\t Training loss (single batch): 1.2828030586242676\n",
      "\t Training loss (single batch): 1.0752753019332886\n",
      "\t Training loss (single batch): 1.779137372970581\n",
      "\t Training loss (single batch): 1.474836826324463\n",
      "\t Training loss (single batch): 1.6100108623504639\n",
      "\t Training loss (single batch): 1.532597303390503\n",
      "\t Training loss (single batch): 1.3218258619308472\n",
      "\t Training loss (single batch): 1.7403253316879272\n",
      "\t Training loss (single batch): 1.5311416387557983\n",
      "\t Training loss (single batch): 1.1188582181930542\n",
      "\t Training loss (single batch): 0.8767622709274292\n",
      "\t Training loss (single batch): 1.3133519887924194\n",
      "\t Training loss (single batch): 0.9864157438278198\n",
      "\t Training loss (single batch): 1.1858494281768799\n",
      "\t Training loss (single batch): 1.199609398841858\n",
      "\t Training loss (single batch): 1.454391360282898\n",
      "\t Training loss (single batch): 1.1397135257720947\n",
      "\t Training loss (single batch): 0.7956802845001221\n",
      "\t Training loss (single batch): 1.3450219631195068\n",
      "\t Training loss (single batch): 1.2589547634124756\n",
      "\t Training loss (single batch): 1.5478843450546265\n",
      "\t Training loss (single batch): 1.5109790563583374\n",
      "\t Training loss (single batch): 0.5961579084396362\n",
      "##################################\n",
      "## EPOCH 32\n",
      "##################################\n",
      "\t Training loss (single batch): 1.103232979774475\n",
      "\t Training loss (single batch): 1.3060139417648315\n",
      "\t Training loss (single batch): 1.5634948015213013\n",
      "\t Training loss (single batch): 1.282561182975769\n",
      "\t Training loss (single batch): 0.9652372598648071\n",
      "\t Training loss (single batch): 1.1020753383636475\n",
      "\t Training loss (single batch): 1.1035356521606445\n",
      "\t Training loss (single batch): 1.0375134944915771\n",
      "\t Training loss (single batch): 1.0474748611450195\n",
      "\t Training loss (single batch): 1.6568424701690674\n",
      "\t Training loss (single batch): 1.4300585985183716\n",
      "\t Training loss (single batch): 1.1611183881759644\n",
      "\t Training loss (single batch): 1.4817956686019897\n",
      "\t Training loss (single batch): 1.4965533018112183\n",
      "\t Training loss (single batch): 1.1471929550170898\n",
      "\t Training loss (single batch): 0.7735434770584106\n",
      "\t Training loss (single batch): 1.5816199779510498\n",
      "\t Training loss (single batch): 1.687174916267395\n",
      "\t Training loss (single batch): 1.3438682556152344\n",
      "\t Training loss (single batch): 1.1090561151504517\n",
      "\t Training loss (single batch): 0.9238647222518921\n",
      "\t Training loss (single batch): 1.163915991783142\n",
      "\t Training loss (single batch): 0.976876437664032\n",
      "\t Training loss (single batch): 1.379102349281311\n",
      "\t Training loss (single batch): 1.660218596458435\n",
      "\t Training loss (single batch): 1.097263216972351\n",
      "\t Training loss (single batch): 1.1085841655731201\n",
      "\t Training loss (single batch): 1.0463005304336548\n",
      "\t Training loss (single batch): 1.2939608097076416\n",
      "\t Training loss (single batch): 1.004409909248352\n",
      "\t Training loss (single batch): 0.884706974029541\n",
      "\t Training loss (single batch): 1.2450065612792969\n",
      "\t Training loss (single batch): 1.5158109664916992\n",
      "\t Training loss (single batch): 1.311803936958313\n",
      "\t Training loss (single batch): 1.5763940811157227\n",
      "\t Training loss (single batch): 1.545266032218933\n",
      "\t Training loss (single batch): 1.2714788913726807\n",
      "\t Training loss (single batch): 0.8216326236724854\n",
      "\t Training loss (single batch): 1.6035066843032837\n",
      "\t Training loss (single batch): 0.9501411318778992\n",
      "\t Training loss (single batch): 0.8901543021202087\n",
      "\t Training loss (single batch): 1.4053982496261597\n",
      "\t Training loss (single batch): 1.194830298423767\n",
      "\t Training loss (single batch): 0.9883328676223755\n",
      "\t Training loss (single batch): 1.5466066598892212\n",
      "\t Training loss (single batch): 1.125244140625\n",
      "\t Training loss (single batch): 1.4483146667480469\n",
      "\t Training loss (single batch): 1.0865751504898071\n",
      "\t Training loss (single batch): 0.7413812279701233\n",
      "\t Training loss (single batch): 1.5229262113571167\n",
      "\t Training loss (single batch): 1.2991615533828735\n",
      "\t Training loss (single batch): 1.4375147819519043\n",
      "\t Training loss (single batch): 1.6506226062774658\n",
      "\t Training loss (single batch): 1.3595538139343262\n",
      "\t Training loss (single batch): 1.350592851638794\n",
      "\t Training loss (single batch): 1.2185850143432617\n",
      "\t Training loss (single batch): 1.38714599609375\n",
      "\t Training loss (single batch): 0.7984169125556946\n",
      "\t Training loss (single batch): 1.1891978979110718\n",
      "\t Training loss (single batch): 1.909584641456604\n",
      "\t Training loss (single batch): 1.471374273300171\n",
      "\t Training loss (single batch): 1.364524483680725\n",
      "\t Training loss (single batch): 1.2067866325378418\n",
      "\t Training loss (single batch): 1.460322380065918\n",
      "\t Training loss (single batch): 1.4812514781951904\n",
      "\t Training loss (single batch): 0.8901732563972473\n",
      "\t Training loss (single batch): 1.2996113300323486\n",
      "\t Training loss (single batch): 1.2728818655014038\n",
      "\t Training loss (single batch): 1.1784523725509644\n",
      "\t Training loss (single batch): 1.176234245300293\n",
      "\t Training loss (single batch): 1.4645239114761353\n",
      "\t Training loss (single batch): 1.311833143234253\n",
      "\t Training loss (single batch): 1.4046632051467896\n",
      "\t Training loss (single batch): 1.1450512409210205\n",
      "\t Training loss (single batch): 1.2908098697662354\n",
      "\t Training loss (single batch): 1.2527025938034058\n",
      "\t Training loss (single batch): 1.1494452953338623\n",
      "\t Training loss (single batch): 1.15012788772583\n",
      "\t Training loss (single batch): 1.2089864015579224\n",
      "\t Training loss (single batch): 0.6898291707038879\n",
      "\t Training loss (single batch): 1.4580137729644775\n",
      "\t Training loss (single batch): 1.7202444076538086\n",
      "\t Training loss (single batch): 1.3370616436004639\n",
      "\t Training loss (single batch): 1.2434395551681519\n",
      "\t Training loss (single batch): 0.7913402318954468\n",
      "\t Training loss (single batch): 1.7205557823181152\n",
      "\t Training loss (single batch): 0.9603939056396484\n",
      "\t Training loss (single batch): 1.0689998865127563\n",
      "\t Training loss (single batch): 1.264483094215393\n",
      "\t Training loss (single batch): 1.079658031463623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4413079023361206\n",
      "\t Training loss (single batch): 1.3390177488327026\n",
      "\t Training loss (single batch): 1.4197356700897217\n",
      "\t Training loss (single batch): 1.1379449367523193\n",
      "\t Training loss (single batch): 1.4640495777130127\n",
      "\t Training loss (single batch): 1.5415232181549072\n",
      "\t Training loss (single batch): 1.0611426830291748\n",
      "\t Training loss (single batch): 1.2450157403945923\n",
      "\t Training loss (single batch): 1.1160085201263428\n",
      "\t Training loss (single batch): 1.4800537824630737\n",
      "\t Training loss (single batch): 1.2434988021850586\n",
      "\t Training loss (single batch): 1.0983842611312866\n",
      "\t Training loss (single batch): 1.1740837097167969\n",
      "\t Training loss (single batch): 1.2511687278747559\n",
      "\t Training loss (single batch): 1.5384018421173096\n",
      "\t Training loss (single batch): 1.2967500686645508\n",
      "\t Training loss (single batch): 1.1143134832382202\n",
      "\t Training loss (single batch): 1.2241489887237549\n",
      "\t Training loss (single batch): 0.8046532273292542\n",
      "\t Training loss (single batch): 1.2912707328796387\n",
      "\t Training loss (single batch): 1.3427128791809082\n",
      "\t Training loss (single batch): 1.4587793350219727\n",
      "\t Training loss (single batch): 0.9679125547409058\n",
      "\t Training loss (single batch): 1.1502923965454102\n",
      "\t Training loss (single batch): 0.952294647693634\n",
      "\t Training loss (single batch): 0.9659100770950317\n",
      "\t Training loss (single batch): 1.531878113746643\n",
      "\t Training loss (single batch): 1.5335884094238281\n",
      "\t Training loss (single batch): 2.0218756198883057\n",
      "\t Training loss (single batch): 1.7154358625411987\n",
      "\t Training loss (single batch): 1.149255394935608\n",
      "\t Training loss (single batch): 1.1649686098098755\n",
      "\t Training loss (single batch): 1.3201942443847656\n",
      "\t Training loss (single batch): 1.3170005083084106\n",
      "\t Training loss (single batch): 1.2145779132843018\n",
      "\t Training loss (single batch): 1.0473411083221436\n",
      "\t Training loss (single batch): 1.339840292930603\n",
      "\t Training loss (single batch): 1.367139220237732\n",
      "\t Training loss (single batch): 0.963934600353241\n",
      "\t Training loss (single batch): 1.2135018110275269\n",
      "\t Training loss (single batch): 0.913363516330719\n",
      "\t Training loss (single batch): 1.546587347984314\n",
      "\t Training loss (single batch): 1.535521388053894\n",
      "\t Training loss (single batch): 0.9899147152900696\n",
      "\t Training loss (single batch): 0.9890285134315491\n",
      "\t Training loss (single batch): 1.6798697710037231\n",
      "\t Training loss (single batch): 1.1165626049041748\n",
      "\t Training loss (single batch): 1.3455194234848022\n",
      "\t Training loss (single batch): 1.1518995761871338\n",
      "\t Training loss (single batch): 1.1370612382888794\n",
      "\t Training loss (single batch): 1.2224284410476685\n",
      "\t Training loss (single batch): 1.5888922214508057\n",
      "\t Training loss (single batch): 1.489640712738037\n",
      "\t Training loss (single batch): 1.6388884782791138\n",
      "\t Training loss (single batch): 1.5283100605010986\n",
      "\t Training loss (single batch): 0.9634455442428589\n",
      "\t Training loss (single batch): 1.0507862567901611\n",
      "\t Training loss (single batch): 1.3328880071640015\n",
      "\t Training loss (single batch): 1.1417702436447144\n",
      "\t Training loss (single batch): 0.8717664480209351\n",
      "\t Training loss (single batch): 1.047116994857788\n",
      "\t Training loss (single batch): 1.046393871307373\n",
      "\t Training loss (single batch): 1.5759581327438354\n",
      "\t Training loss (single batch): 0.9595409631729126\n",
      "\t Training loss (single batch): 1.1458890438079834\n",
      "\t Training loss (single batch): 1.4646615982055664\n",
      "\t Training loss (single batch): 1.558793067932129\n",
      "\t Training loss (single batch): 0.6504378318786621\n",
      "\t Training loss (single batch): 1.3970009088516235\n",
      "\t Training loss (single batch): 1.2136269807815552\n",
      "\t Training loss (single batch): 1.5701955556869507\n",
      "\t Training loss (single batch): 1.3085891008377075\n",
      "\t Training loss (single batch): 1.2740892171859741\n",
      "\t Training loss (single batch): 0.9522987604141235\n",
      "\t Training loss (single batch): 1.329734206199646\n",
      "\t Training loss (single batch): 0.9338217377662659\n",
      "\t Training loss (single batch): 1.3829457759857178\n",
      "\t Training loss (single batch): 1.2832965850830078\n",
      "\t Training loss (single batch): 0.786527693271637\n",
      "\t Training loss (single batch): 1.09589421749115\n",
      "\t Training loss (single batch): 1.4114445447921753\n",
      "\t Training loss (single batch): 1.339438796043396\n",
      "\t Training loss (single batch): 1.278961181640625\n",
      "\t Training loss (single batch): 1.1927952766418457\n",
      "\t Training loss (single batch): 1.434927225112915\n",
      "\t Training loss (single batch): 0.9660208821296692\n",
      "\t Training loss (single batch): 1.1282192468643188\n",
      "\t Training loss (single batch): 1.0669857263565063\n",
      "\t Training loss (single batch): 1.554807186126709\n",
      "\t Training loss (single batch): 0.9621111154556274\n",
      "\t Training loss (single batch): 1.3494123220443726\n",
      "\t Training loss (single batch): 1.3374093770980835\n",
      "\t Training loss (single batch): 1.6928157806396484\n",
      "\t Training loss (single batch): 1.4550129175186157\n",
      "\t Training loss (single batch): 1.4239723682403564\n",
      "\t Training loss (single batch): 1.0117143392562866\n",
      "\t Training loss (single batch): 1.5012292861938477\n",
      "\t Training loss (single batch): 0.9481881856918335\n",
      "\t Training loss (single batch): 1.6197535991668701\n",
      "\t Training loss (single batch): 1.552294373512268\n",
      "\t Training loss (single batch): 1.3735997676849365\n",
      "\t Training loss (single batch): 0.6894764304161072\n",
      "\t Training loss (single batch): 2.0234193801879883\n",
      "\t Training loss (single batch): 1.0703113079071045\n",
      "\t Training loss (single batch): 1.3708384037017822\n",
      "\t Training loss (single batch): 1.229421854019165\n",
      "\t Training loss (single batch): 1.3292008638381958\n",
      "\t Training loss (single batch): 1.1885812282562256\n",
      "\t Training loss (single batch): 1.3010005950927734\n",
      "\t Training loss (single batch): 1.2775781154632568\n",
      "\t Training loss (single batch): 1.4365580081939697\n",
      "\t Training loss (single batch): 1.159878134727478\n",
      "\t Training loss (single batch): 0.9864062666893005\n",
      "\t Training loss (single batch): 1.28804612159729\n",
      "\t Training loss (single batch): 1.0144944190979004\n",
      "\t Training loss (single batch): 1.3226354122161865\n",
      "\t Training loss (single batch): 1.097214698791504\n",
      "\t Training loss (single batch): 1.093485713005066\n",
      "\t Training loss (single batch): 1.242762565612793\n",
      "\t Training loss (single batch): 1.7685511112213135\n",
      "\t Training loss (single batch): 1.6718162298202515\n",
      "\t Training loss (single batch): 1.0491487979888916\n",
      "\t Training loss (single batch): 1.6889362335205078\n",
      "\t Training loss (single batch): 1.2358123064041138\n",
      "\t Training loss (single batch): 0.8712427020072937\n",
      "\t Training loss (single batch): 1.2322713136672974\n",
      "\t Training loss (single batch): 1.1444262266159058\n",
      "\t Training loss (single batch): 1.5510797500610352\n",
      "\t Training loss (single batch): 1.1048787832260132\n",
      "\t Training loss (single batch): 1.5307083129882812\n",
      "\t Training loss (single batch): 1.3135180473327637\n",
      "\t Training loss (single batch): 1.5413715839385986\n",
      "\t Training loss (single batch): 1.383422613143921\n",
      "\t Training loss (single batch): 1.3770933151245117\n",
      "\t Training loss (single batch): 1.0662767887115479\n",
      "\t Training loss (single batch): 1.1047216653823853\n",
      "\t Training loss (single batch): 1.0704864263534546\n",
      "\t Training loss (single batch): 1.2284082174301147\n",
      "\t Training loss (single batch): 1.288312554359436\n",
      "\t Training loss (single batch): 1.1895921230316162\n",
      "\t Training loss (single batch): 1.234959363937378\n",
      "\t Training loss (single batch): 1.3881770372390747\n",
      "\t Training loss (single batch): 0.8631786704063416\n",
      "\t Training loss (single batch): 1.6656173467636108\n",
      "\t Training loss (single batch): 1.1007225513458252\n",
      "\t Training loss (single batch): 1.1001850366592407\n",
      "\t Training loss (single batch): 1.3438156843185425\n",
      "\t Training loss (single batch): 1.405531883239746\n",
      "\t Training loss (single batch): 1.5084220170974731\n",
      "\t Training loss (single batch): 1.2782710790634155\n",
      "\t Training loss (single batch): 1.0021483898162842\n",
      "\t Training loss (single batch): 0.9518036842346191\n",
      "\t Training loss (single batch): 1.6216614246368408\n",
      "\t Training loss (single batch): 1.2525019645690918\n",
      "\t Training loss (single batch): 1.0650359392166138\n",
      "\t Training loss (single batch): 0.9209221005439758\n",
      "\t Training loss (single batch): 0.9717586040496826\n",
      "\t Training loss (single batch): 1.4075376987457275\n",
      "\t Training loss (single batch): 0.9581654071807861\n",
      "\t Training loss (single batch): 1.4379117488861084\n",
      "\t Training loss (single batch): 1.4679691791534424\n",
      "\t Training loss (single batch): 1.2917766571044922\n",
      "\t Training loss (single batch): 0.6824802756309509\n",
      "\t Training loss (single batch): 1.213504672050476\n",
      "\t Training loss (single batch): 1.1610965728759766\n",
      "\t Training loss (single batch): 1.1649903059005737\n",
      "\t Training loss (single batch): 1.197479248046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3810858726501465\n",
      "\t Training loss (single batch): 1.813427209854126\n",
      "\t Training loss (single batch): 1.2047137022018433\n",
      "\t Training loss (single batch): 1.0181920528411865\n",
      "\t Training loss (single batch): 1.0191898345947266\n",
      "\t Training loss (single batch): 1.4926677942276\n",
      "\t Training loss (single batch): 1.2070285081863403\n",
      "\t Training loss (single batch): 0.9506455063819885\n",
      "\t Training loss (single batch): 1.183174967765808\n",
      "\t Training loss (single batch): 1.1700851917266846\n",
      "\t Training loss (single batch): 1.267167329788208\n",
      "\t Training loss (single batch): 1.3757121562957764\n",
      "\t Training loss (single batch): 1.4821168184280396\n",
      "\t Training loss (single batch): 1.0745491981506348\n",
      "\t Training loss (single batch): 1.3264999389648438\n",
      "\t Training loss (single batch): 1.0114564895629883\n",
      "\t Training loss (single batch): 1.6015205383300781\n",
      "\t Training loss (single batch): 1.83477783203125\n",
      "\t Training loss (single batch): 1.2614160776138306\n",
      "\t Training loss (single batch): 1.1537351608276367\n",
      "\t Training loss (single batch): 1.103006362915039\n",
      "\t Training loss (single batch): 0.9089518785476685\n",
      "\t Training loss (single batch): 0.9276919364929199\n",
      "\t Training loss (single batch): 1.5220838785171509\n",
      "\t Training loss (single batch): 1.1010785102844238\n",
      "\t Training loss (single batch): 1.3058385848999023\n",
      "\t Training loss (single batch): 1.1579746007919312\n",
      "\t Training loss (single batch): 1.2303372621536255\n",
      "\t Training loss (single batch): 1.9482917785644531\n",
      "\t Training loss (single batch): 1.6068074703216553\n",
      "\t Training loss (single batch): 1.2692992687225342\n",
      "\t Training loss (single batch): 1.6184908151626587\n",
      "\t Training loss (single batch): 0.9732296466827393\n",
      "\t Training loss (single batch): 2.0938186645507812\n",
      "\t Training loss (single batch): 1.1732221841812134\n",
      "\t Training loss (single batch): 1.161566972732544\n",
      "\t Training loss (single batch): 1.1151877641677856\n",
      "\t Training loss (single batch): 1.2972288131713867\n",
      "\t Training loss (single batch): 1.4737898111343384\n",
      "\t Training loss (single batch): 1.4102100133895874\n",
      "\t Training loss (single batch): 1.3835266828536987\n",
      "\t Training loss (single batch): 1.4145514965057373\n",
      "\t Training loss (single batch): 0.8793590664863586\n",
      "\t Training loss (single batch): 1.2669249773025513\n",
      "\t Training loss (single batch): 1.5458165407180786\n",
      "\t Training loss (single batch): 1.3531100749969482\n",
      "\t Training loss (single batch): 1.2108880281448364\n",
      "\t Training loss (single batch): 1.200262188911438\n",
      "\t Training loss (single batch): 1.2659764289855957\n",
      "\t Training loss (single batch): 1.55863356590271\n",
      "\t Training loss (single batch): 1.0963650941848755\n",
      "\t Training loss (single batch): 1.05771803855896\n",
      "\t Training loss (single batch): 0.9773662686347961\n",
      "\t Training loss (single batch): 1.199143886566162\n",
      "\t Training loss (single batch): 1.1765817403793335\n",
      "\t Training loss (single batch): 1.348511815071106\n",
      "\t Training loss (single batch): 0.6857028007507324\n",
      "\t Training loss (single batch): 1.2019565105438232\n",
      "\t Training loss (single batch): 0.8781687617301941\n",
      "\t Training loss (single batch): 2.1796770095825195\n",
      "##################################\n",
      "## EPOCH 33\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1290645599365234\n",
      "\t Training loss (single batch): 1.3517462015151978\n",
      "\t Training loss (single batch): 2.027862310409546\n",
      "\t Training loss (single batch): 1.0946762561798096\n",
      "\t Training loss (single batch): 1.0480225086212158\n",
      "\t Training loss (single batch): 1.555230736732483\n",
      "\t Training loss (single batch): 1.2387819290161133\n",
      "\t Training loss (single batch): 1.7197630405426025\n",
      "\t Training loss (single batch): 1.183416724205017\n",
      "\t Training loss (single batch): 1.341252326965332\n",
      "\t Training loss (single batch): 1.798579216003418\n",
      "\t Training loss (single batch): 0.818652868270874\n",
      "\t Training loss (single batch): 1.5442100763320923\n",
      "\t Training loss (single batch): 1.1534991264343262\n",
      "\t Training loss (single batch): 0.994419515132904\n",
      "\t Training loss (single batch): 0.8621811270713806\n",
      "\t Training loss (single batch): 1.4052739143371582\n",
      "\t Training loss (single batch): 1.2717819213867188\n",
      "\t Training loss (single batch): 1.4087188243865967\n",
      "\t Training loss (single batch): 1.290004014968872\n",
      "\t Training loss (single batch): 1.1226083040237427\n",
      "\t Training loss (single batch): 0.9486137628555298\n",
      "\t Training loss (single batch): 0.9015469551086426\n",
      "\t Training loss (single batch): 1.1860672235488892\n",
      "\t Training loss (single batch): 1.1481233835220337\n",
      "\t Training loss (single batch): 1.5896077156066895\n",
      "\t Training loss (single batch): 0.960404098033905\n",
      "\t Training loss (single batch): 1.0235141515731812\n",
      "\t Training loss (single batch): 1.3989282846450806\n",
      "\t Training loss (single batch): 1.218065857887268\n",
      "\t Training loss (single batch): 1.0440317392349243\n",
      "\t Training loss (single batch): 1.1108579635620117\n",
      "\t Training loss (single batch): 1.1344478130340576\n",
      "\t Training loss (single batch): 1.8640596866607666\n",
      "\t Training loss (single batch): 1.7524384260177612\n",
      "\t Training loss (single batch): 1.3054488897323608\n",
      "\t Training loss (single batch): 1.2989476919174194\n",
      "\t Training loss (single batch): 1.2924809455871582\n",
      "\t Training loss (single batch): 1.2652244567871094\n",
      "\t Training loss (single batch): 1.4101097583770752\n",
      "\t Training loss (single batch): 1.1528352499008179\n",
      "\t Training loss (single batch): 1.1517367362976074\n",
      "\t Training loss (single batch): 1.8228771686553955\n",
      "\t Training loss (single batch): 1.2451496124267578\n",
      "\t Training loss (single batch): 1.2309259176254272\n",
      "\t Training loss (single batch): 1.1233294010162354\n",
      "\t Training loss (single batch): 1.2340816259384155\n",
      "\t Training loss (single batch): 1.3804388046264648\n",
      "\t Training loss (single batch): 1.5631438493728638\n",
      "\t Training loss (single batch): 0.8022412061691284\n",
      "\t Training loss (single batch): 0.9963394999504089\n",
      "\t Training loss (single batch): 0.9342745542526245\n",
      "\t Training loss (single batch): 0.7383370995521545\n",
      "\t Training loss (single batch): 1.2279226779937744\n",
      "\t Training loss (single batch): 1.4425088167190552\n",
      "\t Training loss (single batch): 1.1303801536560059\n",
      "\t Training loss (single batch): 1.3672512769699097\n",
      "\t Training loss (single batch): 1.3218374252319336\n",
      "\t Training loss (single batch): 0.879642128944397\n",
      "\t Training loss (single batch): 1.3413336277008057\n",
      "\t Training loss (single batch): 1.1304454803466797\n",
      "\t Training loss (single batch): 1.1045618057250977\n",
      "\t Training loss (single batch): 1.5542951822280884\n",
      "\t Training loss (single batch): 1.4099770784378052\n",
      "\t Training loss (single batch): 1.3855043649673462\n",
      "\t Training loss (single batch): 1.2499905824661255\n",
      "\t Training loss (single batch): 1.4640978574752808\n",
      "\t Training loss (single batch): 1.4917528629302979\n",
      "\t Training loss (single batch): 0.9511358737945557\n",
      "\t Training loss (single batch): 1.2025729417800903\n",
      "\t Training loss (single batch): 1.1829605102539062\n",
      "\t Training loss (single batch): 1.417614459991455\n",
      "\t Training loss (single batch): 1.4785469770431519\n",
      "\t Training loss (single batch): 1.07568359375\n",
      "\t Training loss (single batch): 1.2425541877746582\n",
      "\t Training loss (single batch): 1.0734554529190063\n",
      "\t Training loss (single batch): 1.105064868927002\n",
      "\t Training loss (single batch): 1.0443756580352783\n",
      "\t Training loss (single batch): 0.9898194670677185\n",
      "\t Training loss (single batch): 1.5198274850845337\n",
      "\t Training loss (single batch): 1.440828561782837\n",
      "\t Training loss (single batch): 1.157053828239441\n",
      "\t Training loss (single batch): 0.8908289670944214\n",
      "\t Training loss (single batch): 1.2856858968734741\n",
      "\t Training loss (single batch): 1.0707262754440308\n",
      "\t Training loss (single batch): 1.218959927558899\n",
      "\t Training loss (single batch): 1.6393600702285767\n",
      "\t Training loss (single batch): 0.9543457627296448\n",
      "\t Training loss (single batch): 1.3139852285385132\n",
      "\t Training loss (single batch): 1.1678565740585327\n",
      "\t Training loss (single batch): 0.7091777920722961\n",
      "\t Training loss (single batch): 1.2462866306304932\n",
      "\t Training loss (single batch): 0.7936769127845764\n",
      "\t Training loss (single batch): 1.2003964185714722\n",
      "\t Training loss (single batch): 1.2988483905792236\n",
      "\t Training loss (single batch): 1.4690998792648315\n",
      "\t Training loss (single batch): 1.502397894859314\n",
      "\t Training loss (single batch): 1.780841588973999\n",
      "\t Training loss (single batch): 0.9907833337783813\n",
      "\t Training loss (single batch): 0.9322602152824402\n",
      "\t Training loss (single batch): 1.3003697395324707\n",
      "\t Training loss (single batch): 1.0397497415542603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1647405624389648\n",
      "\t Training loss (single batch): 1.0275682210922241\n",
      "\t Training loss (single batch): 1.2799266576766968\n",
      "\t Training loss (single batch): 1.3200912475585938\n",
      "\t Training loss (single batch): 1.1318329572677612\n",
      "\t Training loss (single batch): 0.9559895992279053\n",
      "\t Training loss (single batch): 1.211216926574707\n",
      "\t Training loss (single batch): 1.0764185190200806\n",
      "\t Training loss (single batch): 1.093546986579895\n",
      "\t Training loss (single batch): 1.424286961555481\n",
      "\t Training loss (single batch): 1.097150206565857\n",
      "\t Training loss (single batch): 1.1066099405288696\n",
      "\t Training loss (single batch): 1.3726407289505005\n",
      "\t Training loss (single batch): 0.8911030888557434\n",
      "\t Training loss (single batch): 1.2403477430343628\n",
      "\t Training loss (single batch): 0.8410930037498474\n",
      "\t Training loss (single batch): 1.0841255187988281\n",
      "\t Training loss (single batch): 1.6026602983474731\n",
      "\t Training loss (single batch): 1.06609046459198\n",
      "\t Training loss (single batch): 2.0063679218292236\n",
      "\t Training loss (single batch): 1.792161464691162\n",
      "\t Training loss (single batch): 1.1313295364379883\n",
      "\t Training loss (single batch): 1.1777681112289429\n",
      "\t Training loss (single batch): 1.051965355873108\n",
      "\t Training loss (single batch): 1.0569379329681396\n",
      "\t Training loss (single batch): 1.2443125247955322\n",
      "\t Training loss (single batch): 1.3395671844482422\n",
      "\t Training loss (single batch): 0.879076361656189\n",
      "\t Training loss (single batch): 1.272215723991394\n",
      "\t Training loss (single batch): 1.4608941078186035\n",
      "\t Training loss (single batch): 1.4741618633270264\n",
      "\t Training loss (single batch): 1.6364686489105225\n",
      "\t Training loss (single batch): 1.253727674484253\n",
      "\t Training loss (single batch): 1.7822942733764648\n",
      "\t Training loss (single batch): 1.353009819984436\n",
      "\t Training loss (single batch): 1.6434379816055298\n",
      "\t Training loss (single batch): 1.4850152730941772\n",
      "\t Training loss (single batch): 1.2939770221710205\n",
      "\t Training loss (single batch): 1.4867600202560425\n",
      "\t Training loss (single batch): 1.2058018445968628\n",
      "\t Training loss (single batch): 1.0389755964279175\n",
      "\t Training loss (single batch): 1.1524142026901245\n",
      "\t Training loss (single batch): 0.9474213123321533\n",
      "\t Training loss (single batch): 1.2451199293136597\n",
      "\t Training loss (single batch): 1.7575985193252563\n",
      "\t Training loss (single batch): 0.9754908084869385\n",
      "\t Training loss (single batch): 1.2330001592636108\n",
      "\t Training loss (single batch): 0.7569906711578369\n",
      "\t Training loss (single batch): 0.7230179905891418\n",
      "\t Training loss (single batch): 0.9359647631645203\n",
      "\t Training loss (single batch): 0.9413829445838928\n",
      "\t Training loss (single batch): 1.0887086391448975\n",
      "\t Training loss (single batch): 1.3556727170944214\n",
      "\t Training loss (single batch): 1.0450226068496704\n",
      "\t Training loss (single batch): 0.9923550486564636\n",
      "\t Training loss (single batch): 0.8550240397453308\n",
      "\t Training loss (single batch): 1.5517507791519165\n",
      "\t Training loss (single batch): 0.821660578250885\n",
      "\t Training loss (single batch): 1.519707202911377\n",
      "\t Training loss (single batch): 0.9717345833778381\n",
      "\t Training loss (single batch): 1.3387950658798218\n",
      "\t Training loss (single batch): 0.8872379064559937\n",
      "\t Training loss (single batch): 1.0787962675094604\n",
      "\t Training loss (single batch): 1.1854950189590454\n",
      "\t Training loss (single batch): 0.91554856300354\n",
      "\t Training loss (single batch): 1.382561445236206\n",
      "\t Training loss (single batch): 1.5361591577529907\n",
      "\t Training loss (single batch): 1.464040756225586\n",
      "\t Training loss (single batch): 1.3160077333450317\n",
      "\t Training loss (single batch): 1.0043458938598633\n",
      "\t Training loss (single batch): 1.9354945421218872\n",
      "\t Training loss (single batch): 1.3467704057693481\n",
      "\t Training loss (single batch): 1.3512040376663208\n",
      "\t Training loss (single batch): 1.4118238687515259\n",
      "\t Training loss (single batch): 1.342321753501892\n",
      "\t Training loss (single batch): 0.9973581433296204\n",
      "\t Training loss (single batch): 0.806540846824646\n",
      "\t Training loss (single batch): 1.4909085035324097\n",
      "\t Training loss (single batch): 1.491410255432129\n",
      "\t Training loss (single batch): 0.7583214640617371\n",
      "\t Training loss (single batch): 1.2468314170837402\n",
      "\t Training loss (single batch): 0.9992043375968933\n",
      "\t Training loss (single batch): 0.8183995485305786\n",
      "\t Training loss (single batch): 0.9982715845108032\n",
      "\t Training loss (single batch): 1.3257688283920288\n",
      "\t Training loss (single batch): 1.33164644241333\n",
      "\t Training loss (single batch): 1.4304765462875366\n",
      "\t Training loss (single batch): 1.0029325485229492\n",
      "\t Training loss (single batch): 1.21920645236969\n",
      "\t Training loss (single batch): 1.0518648624420166\n",
      "\t Training loss (single batch): 1.124566912651062\n",
      "\t Training loss (single batch): 1.3599032163619995\n",
      "\t Training loss (single batch): 1.3023911714553833\n",
      "\t Training loss (single batch): 1.2897050380706787\n",
      "\t Training loss (single batch): 1.481445074081421\n",
      "\t Training loss (single batch): 1.5757195949554443\n",
      "\t Training loss (single batch): 1.696161150932312\n",
      "\t Training loss (single batch): 1.2091223001480103\n",
      "\t Training loss (single batch): 1.1582311391830444\n",
      "\t Training loss (single batch): 1.2068219184875488\n",
      "\t Training loss (single batch): 0.99075847864151\n",
      "\t Training loss (single batch): 0.9658951759338379\n",
      "\t Training loss (single batch): 1.209023118019104\n",
      "\t Training loss (single batch): 1.2133917808532715\n",
      "\t Training loss (single batch): 1.3971786499023438\n",
      "\t Training loss (single batch): 0.997408926486969\n",
      "\t Training loss (single batch): 1.1135635375976562\n",
      "\t Training loss (single batch): 1.51302969455719\n",
      "\t Training loss (single batch): 0.818159818649292\n",
      "\t Training loss (single batch): 1.3311575651168823\n",
      "\t Training loss (single batch): 1.3211320638656616\n",
      "\t Training loss (single batch): 1.2263703346252441\n",
      "\t Training loss (single batch): 1.3903998136520386\n",
      "\t Training loss (single batch): 1.0824755430221558\n",
      "\t Training loss (single batch): 1.8143491744995117\n",
      "\t Training loss (single batch): 1.4085807800292969\n",
      "\t Training loss (single batch): 1.4102002382278442\n",
      "\t Training loss (single batch): 1.1717948913574219\n",
      "\t Training loss (single batch): 1.45408296585083\n",
      "\t Training loss (single batch): 1.4849905967712402\n",
      "\t Training loss (single batch): 1.0723426342010498\n",
      "\t Training loss (single batch): 1.1326017379760742\n",
      "\t Training loss (single batch): 1.3922665119171143\n",
      "\t Training loss (single batch): 1.3256592750549316\n",
      "\t Training loss (single batch): 1.3054357767105103\n",
      "\t Training loss (single batch): 1.5743907690048218\n",
      "\t Training loss (single batch): 0.9575543403625488\n",
      "\t Training loss (single batch): 1.3377089500427246\n",
      "\t Training loss (single batch): 1.1914836168289185\n",
      "\t Training loss (single batch): 1.41197669506073\n",
      "\t Training loss (single batch): 1.2135542631149292\n",
      "\t Training loss (single batch): 1.7595363855361938\n",
      "\t Training loss (single batch): 0.7667257785797119\n",
      "\t Training loss (single batch): 1.1728770732879639\n",
      "\t Training loss (single batch): 1.1081148386001587\n",
      "\t Training loss (single batch): 1.4709237813949585\n",
      "\t Training loss (single batch): 0.7874639630317688\n",
      "\t Training loss (single batch): 1.4979815483093262\n",
      "\t Training loss (single batch): 1.6554102897644043\n",
      "\t Training loss (single batch): 1.086027979850769\n",
      "\t Training loss (single batch): 1.0844519138336182\n",
      "\t Training loss (single batch): 1.1989047527313232\n",
      "\t Training loss (single batch): 1.6599617004394531\n",
      "\t Training loss (single batch): 1.24415922164917\n",
      "\t Training loss (single batch): 1.0217547416687012\n",
      "\t Training loss (single batch): 1.2430627346038818\n",
      "\t Training loss (single batch): 0.7163426876068115\n",
      "\t Training loss (single batch): 1.258378505706787\n",
      "\t Training loss (single batch): 1.3560588359832764\n",
      "\t Training loss (single batch): 1.395944356918335\n",
      "\t Training loss (single batch): 1.232345700263977\n",
      "\t Training loss (single batch): 1.3433743715286255\n",
      "\t Training loss (single batch): 1.4719831943511963\n",
      "\t Training loss (single batch): 1.8248704671859741\n",
      "\t Training loss (single batch): 1.1889963150024414\n",
      "\t Training loss (single batch): 0.9470753073692322\n",
      "\t Training loss (single batch): 1.1884543895721436\n",
      "\t Training loss (single batch): 1.2422763109207153\n",
      "\t Training loss (single batch): 1.3729114532470703\n",
      "\t Training loss (single batch): 1.2670427560806274\n",
      "\t Training loss (single batch): 1.122241497039795\n",
      "\t Training loss (single batch): 1.6124205589294434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2665973901748657\n",
      "\t Training loss (single batch): 1.0250763893127441\n",
      "\t Training loss (single batch): 0.9492610096931458\n",
      "\t Training loss (single batch): 0.8849900364875793\n",
      "\t Training loss (single batch): 1.5293101072311401\n",
      "\t Training loss (single batch): 1.0091779232025146\n",
      "\t Training loss (single batch): 1.617228627204895\n",
      "\t Training loss (single batch): 0.9972533583641052\n",
      "\t Training loss (single batch): 0.7539868950843811\n",
      "\t Training loss (single batch): 1.5238840579986572\n",
      "\t Training loss (single batch): 1.122037649154663\n",
      "\t Training loss (single batch): 1.2017338275909424\n",
      "\t Training loss (single batch): 1.134739637374878\n",
      "\t Training loss (single batch): 1.062273383140564\n",
      "\t Training loss (single batch): 1.0973609685897827\n",
      "\t Training loss (single batch): 0.923862636089325\n",
      "\t Training loss (single batch): 1.0243983268737793\n",
      "\t Training loss (single batch): 0.8964083790779114\n",
      "\t Training loss (single batch): 1.0850213766098022\n",
      "\t Training loss (single batch): 1.3480159044265747\n",
      "\t Training loss (single batch): 0.7743490934371948\n",
      "\t Training loss (single batch): 1.743929147720337\n",
      "\t Training loss (single batch): 0.8239684104919434\n",
      "\t Training loss (single batch): 1.7088874578475952\n",
      "\t Training loss (single batch): 0.9809995889663696\n",
      "\t Training loss (single batch): 0.7301419973373413\n",
      "\t Training loss (single batch): 1.4938780069351196\n",
      "\t Training loss (single batch): 0.9504384398460388\n",
      "\t Training loss (single batch): 1.0332083702087402\n",
      "\t Training loss (single batch): 0.8241651058197021\n",
      "\t Training loss (single batch): 0.8568644523620605\n",
      "\t Training loss (single batch): 1.0241470336914062\n",
      "\t Training loss (single batch): 1.300606369972229\n",
      "\t Training loss (single batch): 0.845714271068573\n",
      "\t Training loss (single batch): 1.501281499862671\n",
      "\t Training loss (single batch): 1.2548041343688965\n",
      "\t Training loss (single batch): 0.8481493592262268\n",
      "\t Training loss (single batch): 0.9064579606056213\n",
      "\t Training loss (single batch): 1.3772886991500854\n",
      "\t Training loss (single batch): 1.7115275859832764\n",
      "\t Training loss (single batch): 1.1618096828460693\n",
      "\t Training loss (single batch): 1.3511275053024292\n",
      "\t Training loss (single batch): 1.5308154821395874\n",
      "\t Training loss (single batch): 1.2328678369522095\n",
      "\t Training loss (single batch): 1.3831225633621216\n",
      "\t Training loss (single batch): 1.5235692262649536\n",
      "\t Training loss (single batch): 1.352190613746643\n",
      "\t Training loss (single batch): 1.3350034952163696\n",
      "\t Training loss (single batch): 0.832743227481842\n",
      "\t Training loss (single batch): 1.1328386068344116\n",
      "\t Training loss (single batch): 1.7565093040466309\n",
      "\t Training loss (single batch): 1.1854745149612427\n",
      "\t Training loss (single batch): 2.3902816772460938\n",
      "##################################\n",
      "## EPOCH 34\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7076671123504639\n",
      "\t Training loss (single batch): 1.0563781261444092\n",
      "\t Training loss (single batch): 0.8793056607246399\n",
      "\t Training loss (single batch): 1.2139843702316284\n",
      "\t Training loss (single batch): 1.4416512250900269\n",
      "\t Training loss (single batch): 1.420155644416809\n",
      "\t Training loss (single batch): 1.5007628202438354\n",
      "\t Training loss (single batch): 1.682100772857666\n",
      "\t Training loss (single batch): 1.242891788482666\n",
      "\t Training loss (single batch): 0.9238179326057434\n",
      "\t Training loss (single batch): 0.7904636263847351\n",
      "\t Training loss (single batch): 1.4115540981292725\n",
      "\t Training loss (single batch): 1.3128846883773804\n",
      "\t Training loss (single batch): 1.4379478693008423\n",
      "\t Training loss (single batch): 1.4143013954162598\n",
      "\t Training loss (single batch): 1.0270322561264038\n",
      "\t Training loss (single batch): 1.40373957157135\n",
      "\t Training loss (single batch): 0.8647561073303223\n",
      "\t Training loss (single batch): 1.4335862398147583\n",
      "\t Training loss (single batch): 1.1860010623931885\n",
      "\t Training loss (single batch): 1.427634358406067\n",
      "\t Training loss (single batch): 1.44460129737854\n",
      "\t Training loss (single batch): 1.1817940473556519\n",
      "\t Training loss (single batch): 0.8657329082489014\n",
      "\t Training loss (single batch): 1.6739788055419922\n",
      "\t Training loss (single batch): 1.0612094402313232\n",
      "\t Training loss (single batch): 1.0827412605285645\n",
      "\t Training loss (single batch): 1.4227635860443115\n",
      "\t Training loss (single batch): 1.1850736141204834\n",
      "\t Training loss (single batch): 1.8360172510147095\n",
      "\t Training loss (single batch): 1.0230597257614136\n",
      "\t Training loss (single batch): 1.5286146402359009\n",
      "\t Training loss (single batch): 1.1701830625534058\n",
      "\t Training loss (single batch): 1.3204762935638428\n",
      "\t Training loss (single batch): 1.467613935470581\n",
      "\t Training loss (single batch): 0.963564395904541\n",
      "\t Training loss (single batch): 0.978238046169281\n",
      "\t Training loss (single batch): 1.3790792226791382\n",
      "\t Training loss (single batch): 1.3582671880722046\n",
      "\t Training loss (single batch): 1.5075117349624634\n",
      "\t Training loss (single batch): 0.9907270669937134\n",
      "\t Training loss (single batch): 0.8890162110328674\n",
      "\t Training loss (single batch): 1.1410267353057861\n",
      "\t Training loss (single batch): 1.0207008123397827\n",
      "\t Training loss (single batch): 1.4189985990524292\n",
      "\t Training loss (single batch): 1.4357388019561768\n",
      "\t Training loss (single batch): 1.4033920764923096\n",
      "\t Training loss (single batch): 1.2815053462982178\n",
      "\t Training loss (single batch): 1.2016764879226685\n",
      "\t Training loss (single batch): 1.2418568134307861\n",
      "\t Training loss (single batch): 1.7595553398132324\n",
      "\t Training loss (single batch): 1.2933160066604614\n",
      "\t Training loss (single batch): 0.9569280743598938\n",
      "\t Training loss (single batch): 1.0310304164886475\n",
      "\t Training loss (single batch): 1.197974681854248\n",
      "\t Training loss (single batch): 0.9038441181182861\n",
      "\t Training loss (single batch): 1.51995050907135\n",
      "\t Training loss (single batch): 0.973755419254303\n",
      "\t Training loss (single batch): 0.9184864163398743\n",
      "\t Training loss (single batch): 1.0874119997024536\n",
      "\t Training loss (single batch): 1.1083866357803345\n",
      "\t Training loss (single batch): 0.9915392994880676\n",
      "\t Training loss (single batch): 1.4719032049179077\n",
      "\t Training loss (single batch): 1.4431706666946411\n",
      "\t Training loss (single batch): 1.3727296590805054\n",
      "\t Training loss (single batch): 1.1604026556015015\n",
      "\t Training loss (single batch): 1.0085738897323608\n",
      "\t Training loss (single batch): 0.8255968689918518\n",
      "\t Training loss (single batch): 1.3244602680206299\n",
      "\t Training loss (single batch): 1.1212345361709595\n",
      "\t Training loss (single batch): 1.5052387714385986\n",
      "\t Training loss (single batch): 0.9423238635063171\n",
      "\t Training loss (single batch): 1.0844851732254028\n",
      "\t Training loss (single batch): 1.0472779273986816\n",
      "\t Training loss (single batch): 1.3715732097625732\n",
      "\t Training loss (single batch): 1.349195957183838\n",
      "\t Training loss (single batch): 1.3223906755447388\n",
      "\t Training loss (single batch): 1.409348487854004\n",
      "\t Training loss (single batch): 1.2906206846237183\n",
      "\t Training loss (single batch): 1.2691457271575928\n",
      "\t Training loss (single batch): 1.565750241279602\n",
      "\t Training loss (single batch): 1.551621913909912\n",
      "\t Training loss (single batch): 1.454952597618103\n",
      "\t Training loss (single batch): 1.3341004848480225\n",
      "\t Training loss (single batch): 1.652090072631836\n",
      "\t Training loss (single batch): 1.2862465381622314\n",
      "\t Training loss (single batch): 1.0680586099624634\n",
      "\t Training loss (single batch): 1.095181941986084\n",
      "\t Training loss (single batch): 1.1674808263778687\n",
      "\t Training loss (single batch): 1.1796122789382935\n",
      "\t Training loss (single batch): 1.4827194213867188\n",
      "\t Training loss (single batch): 1.4740034341812134\n",
      "\t Training loss (single batch): 1.1720128059387207\n",
      "\t Training loss (single batch): 1.404428243637085\n",
      "\t Training loss (single batch): 1.1418720483779907\n",
      "\t Training loss (single batch): 1.2135570049285889\n",
      "\t Training loss (single batch): 0.7967694997787476\n",
      "\t Training loss (single batch): 1.0051532983779907\n",
      "\t Training loss (single batch): 0.9747157096862793\n",
      "\t Training loss (single batch): 0.6862815618515015\n",
      "\t Training loss (single batch): 1.8069981336593628\n",
      "\t Training loss (single batch): 1.8317972421646118\n",
      "\t Training loss (single batch): 1.2484852075576782\n",
      "\t Training loss (single batch): 0.6702868938446045\n",
      "\t Training loss (single batch): 0.855265200138092\n",
      "\t Training loss (single batch): 0.8797668218612671\n",
      "\t Training loss (single batch): 1.4162604808807373\n",
      "\t Training loss (single batch): 0.9745069146156311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.374887228012085\n",
      "\t Training loss (single batch): 1.498122215270996\n",
      "\t Training loss (single batch): 0.9424909949302673\n",
      "\t Training loss (single batch): 1.3867558240890503\n",
      "\t Training loss (single batch): 1.1133242845535278\n",
      "\t Training loss (single batch): 1.3444384336471558\n",
      "\t Training loss (single batch): 1.1413929462432861\n",
      "\t Training loss (single batch): 1.6205209493637085\n",
      "\t Training loss (single batch): 1.0051343441009521\n",
      "\t Training loss (single batch): 1.489458680152893\n",
      "\t Training loss (single batch): 1.4095836877822876\n",
      "\t Training loss (single batch): 0.8568065762519836\n",
      "\t Training loss (single batch): 1.2543621063232422\n",
      "\t Training loss (single batch): 0.9882122874259949\n",
      "\t Training loss (single batch): 1.3203485012054443\n",
      "\t Training loss (single batch): 1.6794065237045288\n",
      "\t Training loss (single batch): 0.8274729251861572\n",
      "\t Training loss (single batch): 0.9608558416366577\n",
      "\t Training loss (single batch): 1.0406465530395508\n",
      "\t Training loss (single batch): 1.6233415603637695\n",
      "\t Training loss (single batch): 2.0356554985046387\n",
      "\t Training loss (single batch): 1.2598036527633667\n",
      "\t Training loss (single batch): 1.5240739583969116\n",
      "\t Training loss (single batch): 1.5468621253967285\n",
      "\t Training loss (single batch): 1.2288678884506226\n",
      "\t Training loss (single batch): 1.5102121829986572\n",
      "\t Training loss (single batch): 0.9245011210441589\n",
      "\t Training loss (single batch): 1.574498176574707\n",
      "\t Training loss (single batch): 1.4061052799224854\n",
      "\t Training loss (single batch): 1.313264012336731\n",
      "\t Training loss (single batch): 0.9993168115615845\n",
      "\t Training loss (single batch): 0.7507567405700684\n",
      "\t Training loss (single batch): 1.0303479433059692\n",
      "\t Training loss (single batch): 0.7064958810806274\n",
      "\t Training loss (single batch): 1.2728577852249146\n",
      "\t Training loss (single batch): 1.5968576669692993\n",
      "\t Training loss (single batch): 1.4328668117523193\n",
      "\t Training loss (single batch): 1.3945777416229248\n",
      "\t Training loss (single batch): 0.8790587186813354\n",
      "\t Training loss (single batch): 1.2299093008041382\n",
      "\t Training loss (single batch): 1.3648653030395508\n",
      "\t Training loss (single batch): 1.627204418182373\n",
      "\t Training loss (single batch): 1.2446346282958984\n",
      "\t Training loss (single batch): 1.3069969415664673\n",
      "\t Training loss (single batch): 1.2857863903045654\n",
      "\t Training loss (single batch): 1.154943585395813\n",
      "\t Training loss (single batch): 1.1425251960754395\n",
      "\t Training loss (single batch): 1.1815463304519653\n",
      "\t Training loss (single batch): 1.2400487661361694\n",
      "\t Training loss (single batch): 1.2018336057662964\n",
      "\t Training loss (single batch): 1.3651995658874512\n",
      "\t Training loss (single batch): 1.0755999088287354\n",
      "\t Training loss (single batch): 1.822826862335205\n",
      "\t Training loss (single batch): 1.0318961143493652\n",
      "\t Training loss (single batch): 0.8539959192276001\n",
      "\t Training loss (single batch): 1.1214176416397095\n",
      "\t Training loss (single batch): 1.3415095806121826\n",
      "\t Training loss (single batch): 1.085822343826294\n",
      "\t Training loss (single batch): 1.5415477752685547\n",
      "\t Training loss (single batch): 1.0658886432647705\n",
      "\t Training loss (single batch): 1.2153993844985962\n",
      "\t Training loss (single batch): 1.4403095245361328\n",
      "\t Training loss (single batch): 0.9928029179573059\n",
      "\t Training loss (single batch): 0.8265300393104553\n",
      "\t Training loss (single batch): 1.376348853111267\n",
      "\t Training loss (single batch): 1.6877068281173706\n",
      "\t Training loss (single batch): 1.528438687324524\n",
      "\t Training loss (single batch): 1.354429841041565\n",
      "\t Training loss (single batch): 0.8848940134048462\n",
      "\t Training loss (single batch): 1.9846234321594238\n",
      "\t Training loss (single batch): 0.8339069485664368\n",
      "\t Training loss (single batch): 1.254966139793396\n",
      "\t Training loss (single batch): 1.208619236946106\n",
      "\t Training loss (single batch): 1.2016180753707886\n",
      "\t Training loss (single batch): 1.1731534004211426\n",
      "\t Training loss (single batch): 1.5222498178482056\n",
      "\t Training loss (single batch): 1.1816811561584473\n",
      "\t Training loss (single batch): 0.808169424533844\n",
      "\t Training loss (single batch): 0.9785150289535522\n",
      "\t Training loss (single batch): 1.035050630569458\n",
      "\t Training loss (single batch): 1.2201652526855469\n",
      "\t Training loss (single batch): 1.044276237487793\n",
      "\t Training loss (single batch): 1.1908900737762451\n",
      "\t Training loss (single batch): 1.069878339767456\n",
      "\t Training loss (single batch): 1.1455986499786377\n",
      "\t Training loss (single batch): 0.9876161813735962\n",
      "\t Training loss (single batch): 0.7432183027267456\n",
      "\t Training loss (single batch): 0.7779991626739502\n",
      "\t Training loss (single batch): 1.8985755443572998\n",
      "\t Training loss (single batch): 1.2040643692016602\n",
      "\t Training loss (single batch): 1.1289957761764526\n",
      "\t Training loss (single batch): 1.615706443786621\n",
      "\t Training loss (single batch): 1.8658374547958374\n",
      "\t Training loss (single batch): 0.7921114563941956\n",
      "\t Training loss (single batch): 0.854481041431427\n",
      "\t Training loss (single batch): 1.315041184425354\n",
      "\t Training loss (single batch): 0.7413644194602966\n",
      "\t Training loss (single batch): 1.6348744630813599\n",
      "\t Training loss (single batch): 1.6643620729446411\n",
      "\t Training loss (single batch): 1.0529811382293701\n",
      "\t Training loss (single batch): 1.5507618188858032\n",
      "\t Training loss (single batch): 1.005660891532898\n",
      "\t Training loss (single batch): 1.084071397781372\n",
      "\t Training loss (single batch): 0.9187129735946655\n",
      "\t Training loss (single batch): 1.0517150163650513\n",
      "\t Training loss (single batch): 1.033796787261963\n",
      "\t Training loss (single batch): 1.1739182472229004\n",
      "\t Training loss (single batch): 1.3234130144119263\n",
      "\t Training loss (single batch): 1.178829312324524\n",
      "\t Training loss (single batch): 1.2811436653137207\n",
      "\t Training loss (single batch): 1.3574328422546387\n",
      "\t Training loss (single batch): 0.9394773244857788\n",
      "\t Training loss (single batch): 1.571778416633606\n",
      "\t Training loss (single batch): 1.3239821195602417\n",
      "\t Training loss (single batch): 1.1098730564117432\n",
      "\t Training loss (single batch): 0.9506574869155884\n",
      "\t Training loss (single batch): 1.1543694734573364\n",
      "\t Training loss (single batch): 1.5670464038848877\n",
      "\t Training loss (single batch): 0.9044210910797119\n",
      "\t Training loss (single batch): 1.1943867206573486\n",
      "\t Training loss (single batch): 1.0353397130966187\n",
      "\t Training loss (single batch): 1.6518709659576416\n",
      "\t Training loss (single batch): 1.233095407485962\n",
      "\t Training loss (single batch): 1.2306679487228394\n",
      "\t Training loss (single batch): 1.0481854677200317\n",
      "\t Training loss (single batch): 1.205901861190796\n",
      "\t Training loss (single batch): 1.166257381439209\n",
      "\t Training loss (single batch): 1.3794944286346436\n",
      "\t Training loss (single batch): 0.9545121788978577\n",
      "\t Training loss (single batch): 1.6030265092849731\n",
      "\t Training loss (single batch): 1.379284143447876\n",
      "\t Training loss (single batch): 1.489467740058899\n",
      "\t Training loss (single batch): 0.9166983366012573\n",
      "\t Training loss (single batch): 1.1944396495819092\n",
      "\t Training loss (single batch): 1.170568823814392\n",
      "\t Training loss (single batch): 1.2288661003112793\n",
      "\t Training loss (single batch): 1.2196886539459229\n",
      "\t Training loss (single batch): 1.0297073125839233\n",
      "\t Training loss (single batch): 1.054266333580017\n",
      "\t Training loss (single batch): 1.6848427057266235\n",
      "\t Training loss (single batch): 1.2795403003692627\n",
      "\t Training loss (single batch): 1.2394970655441284\n",
      "\t Training loss (single batch): 0.8997687101364136\n",
      "\t Training loss (single batch): 0.9720964431762695\n",
      "\t Training loss (single batch): 1.1742331981658936\n",
      "\t Training loss (single batch): 1.5397955179214478\n",
      "\t Training loss (single batch): 1.048475742340088\n",
      "\t Training loss (single batch): 0.986347496509552\n",
      "\t Training loss (single batch): 1.2107998132705688\n",
      "\t Training loss (single batch): 1.1435036659240723\n",
      "\t Training loss (single batch): 1.312815546989441\n",
      "\t Training loss (single batch): 1.2337206602096558\n",
      "\t Training loss (single batch): 1.2386759519577026\n",
      "\t Training loss (single batch): 0.6554115414619446\n",
      "\t Training loss (single batch): 0.7759079933166504\n",
      "\t Training loss (single batch): 0.769864559173584\n",
      "\t Training loss (single batch): 0.8906409740447998\n",
      "\t Training loss (single batch): 1.8722606897354126\n",
      "\t Training loss (single batch): 0.7741557955741882\n",
      "\t Training loss (single batch): 1.0358630418777466\n",
      "\t Training loss (single batch): 1.172707200050354\n",
      "\t Training loss (single batch): 1.3625048398971558\n",
      "\t Training loss (single batch): 0.8862386345863342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1794397830963135\n",
      "\t Training loss (single batch): 1.0226407051086426\n",
      "\t Training loss (single batch): 1.385673999786377\n",
      "\t Training loss (single batch): 0.6806225776672363\n",
      "\t Training loss (single batch): 1.2305022478103638\n",
      "\t Training loss (single batch): 1.0207494497299194\n",
      "\t Training loss (single batch): 1.3095133304595947\n",
      "\t Training loss (single batch): 1.4618308544158936\n",
      "\t Training loss (single batch): 0.9339478015899658\n",
      "\t Training loss (single batch): 1.482819676399231\n",
      "\t Training loss (single batch): 0.5728091597557068\n",
      "\t Training loss (single batch): 1.1655018329620361\n",
      "\t Training loss (single batch): 1.127403974533081\n",
      "\t Training loss (single batch): 1.6274149417877197\n",
      "\t Training loss (single batch): 1.1782984733581543\n",
      "\t Training loss (single batch): 1.5675512552261353\n",
      "\t Training loss (single batch): 1.5580350160598755\n",
      "\t Training loss (single batch): 1.3062045574188232\n",
      "\t Training loss (single batch): 1.4661444425582886\n",
      "\t Training loss (single batch): 1.4482747316360474\n",
      "\t Training loss (single batch): 1.0956050157546997\n",
      "\t Training loss (single batch): 0.8432937264442444\n",
      "\t Training loss (single batch): 1.5381077527999878\n",
      "\t Training loss (single batch): 1.1879675388336182\n",
      "\t Training loss (single batch): 1.2687337398529053\n",
      "\t Training loss (single batch): 1.240826964378357\n",
      "\t Training loss (single batch): 1.7499741315841675\n",
      "\t Training loss (single batch): 1.0829919576644897\n",
      "\t Training loss (single batch): 1.1344163417816162\n",
      "\t Training loss (single batch): 1.1502814292907715\n",
      "\t Training loss (single batch): 1.386214256286621\n",
      "\t Training loss (single batch): 1.1555328369140625\n",
      "\t Training loss (single batch): 1.460105538368225\n",
      "\t Training loss (single batch): 1.2746896743774414\n",
      "\t Training loss (single batch): 1.4094537496566772\n",
      "\t Training loss (single batch): 1.2961012125015259\n",
      "\t Training loss (single batch): 0.8981585502624512\n",
      "\t Training loss (single batch): 1.1151134967803955\n",
      "\t Training loss (single batch): 0.7245864868164062\n",
      "\t Training loss (single batch): 1.2798793315887451\n",
      "\t Training loss (single batch): 1.236371397972107\n",
      "\t Training loss (single batch): 1.5003482103347778\n",
      "\t Training loss (single batch): 1.2750033140182495\n",
      "\t Training loss (single batch): 1.4856523275375366\n",
      "\t Training loss (single batch): 0.693789005279541\n",
      "\t Training loss (single batch): 0.7487306594848633\n",
      "##################################\n",
      "## EPOCH 35\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0991631746292114\n",
      "\t Training loss (single batch): 0.9477673768997192\n",
      "\t Training loss (single batch): 1.4533792734146118\n",
      "\t Training loss (single batch): 1.4420852661132812\n",
      "\t Training loss (single batch): 1.1039994955062866\n",
      "\t Training loss (single batch): 1.4715983867645264\n",
      "\t Training loss (single batch): 1.3877127170562744\n",
      "\t Training loss (single batch): 1.1592572927474976\n",
      "\t Training loss (single batch): 1.173416256904602\n",
      "\t Training loss (single batch): 1.895872950553894\n",
      "\t Training loss (single batch): 1.8639575242996216\n",
      "\t Training loss (single batch): 1.539711833000183\n",
      "\t Training loss (single batch): 1.3912972211837769\n",
      "\t Training loss (single batch): 1.5080128908157349\n",
      "\t Training loss (single batch): 1.1998646259307861\n",
      "\t Training loss (single batch): 1.9034836292266846\n",
      "\t Training loss (single batch): 1.8207950592041016\n",
      "\t Training loss (single batch): 1.6356825828552246\n",
      "\t Training loss (single batch): 1.7707562446594238\n",
      "\t Training loss (single batch): 1.412415623664856\n",
      "\t Training loss (single batch): 1.7065893411636353\n",
      "\t Training loss (single batch): 1.0650403499603271\n",
      "\t Training loss (single batch): 1.6180016994476318\n",
      "\t Training loss (single batch): 1.1510599851608276\n",
      "\t Training loss (single batch): 1.2096432447433472\n",
      "\t Training loss (single batch): 1.3213664293289185\n",
      "\t Training loss (single batch): 1.142825722694397\n",
      "\t Training loss (single batch): 1.5457426309585571\n",
      "\t Training loss (single batch): 1.3312127590179443\n",
      "\t Training loss (single batch): 1.103765845298767\n",
      "\t Training loss (single batch): 1.0354844331741333\n",
      "\t Training loss (single batch): 0.9805425405502319\n",
      "\t Training loss (single batch): 1.6439731121063232\n",
      "\t Training loss (single batch): 1.4724758863449097\n",
      "\t Training loss (single batch): 1.314016342163086\n",
      "\t Training loss (single batch): 1.4630290269851685\n",
      "\t Training loss (single batch): 1.0963689088821411\n",
      "\t Training loss (single batch): 1.6573681831359863\n",
      "\t Training loss (single batch): 1.5610297918319702\n",
      "\t Training loss (single batch): 1.0492687225341797\n",
      "\t Training loss (single batch): 1.5705831050872803\n",
      "\t Training loss (single batch): 1.6434406042099\n",
      "\t Training loss (single batch): 1.3606617450714111\n",
      "\t Training loss (single batch): 0.9702598452568054\n",
      "\t Training loss (single batch): 1.508878231048584\n",
      "\t Training loss (single batch): 1.2940001487731934\n",
      "\t Training loss (single batch): 1.4354197978973389\n",
      "\t Training loss (single batch): 1.0961384773254395\n",
      "\t Training loss (single batch): 0.8971242308616638\n",
      "\t Training loss (single batch): 1.24687659740448\n",
      "\t Training loss (single batch): 1.077378511428833\n",
      "\t Training loss (single batch): 0.8535477519035339\n",
      "\t Training loss (single batch): 1.3292285203933716\n",
      "\t Training loss (single batch): 1.213303804397583\n",
      "\t Training loss (single batch): 0.8887364864349365\n",
      "\t Training loss (single batch): 1.16056489944458\n",
      "\t Training loss (single batch): 0.9455556869506836\n",
      "\t Training loss (single batch): 1.4115418195724487\n",
      "\t Training loss (single batch): 1.020962119102478\n",
      "\t Training loss (single batch): 1.122603178024292\n",
      "\t Training loss (single batch): 1.2163594961166382\n",
      "\t Training loss (single batch): 1.3638696670532227\n",
      "\t Training loss (single batch): 0.7413262724876404\n",
      "\t Training loss (single batch): 0.7724733948707581\n",
      "\t Training loss (single batch): 1.3474857807159424\n",
      "\t Training loss (single batch): 1.0640987157821655\n",
      "\t Training loss (single batch): 1.521543264389038\n",
      "\t Training loss (single batch): 1.000296711921692\n",
      "\t Training loss (single batch): 0.9545360207557678\n",
      "\t Training loss (single batch): 2.022775411605835\n",
      "\t Training loss (single batch): 1.4103807210922241\n",
      "\t Training loss (single batch): 1.74311101436615\n",
      "\t Training loss (single batch): 1.423442006111145\n",
      "\t Training loss (single batch): 1.4834471940994263\n",
      "\t Training loss (single batch): 0.9610394239425659\n",
      "\t Training loss (single batch): 1.4306137561798096\n",
      "\t Training loss (single batch): 1.3380417823791504\n",
      "\t Training loss (single batch): 1.3855512142181396\n",
      "\t Training loss (single batch): 1.187864065170288\n",
      "\t Training loss (single batch): 1.129600167274475\n",
      "\t Training loss (single batch): 1.4755061864852905\n",
      "\t Training loss (single batch): 1.3301820755004883\n",
      "\t Training loss (single batch): 1.3724089860916138\n",
      "\t Training loss (single batch): 0.8590700626373291\n",
      "\t Training loss (single batch): 0.8439175486564636\n",
      "\t Training loss (single batch): 1.442698359489441\n",
      "\t Training loss (single batch): 1.198467493057251\n",
      "\t Training loss (single batch): 1.7968953847885132\n",
      "\t Training loss (single batch): 1.107423186302185\n",
      "\t Training loss (single batch): 0.8398776650428772\n",
      "\t Training loss (single batch): 1.041306734085083\n",
      "\t Training loss (single batch): 0.9047607183456421\n",
      "\t Training loss (single batch): 1.1840764284133911\n",
      "\t Training loss (single batch): 1.3133764266967773\n",
      "\t Training loss (single batch): 2.0232133865356445\n",
      "\t Training loss (single batch): 1.1718721389770508\n",
      "\t Training loss (single batch): 1.2600750923156738\n",
      "\t Training loss (single batch): 1.5296757221221924\n",
      "\t Training loss (single batch): 1.2365316152572632\n",
      "\t Training loss (single batch): 1.0174583196640015\n",
      "\t Training loss (single batch): 1.3465064764022827\n",
      "\t Training loss (single batch): 1.0347265005111694\n",
      "\t Training loss (single batch): 0.8576775789260864\n",
      "\t Training loss (single batch): 1.2677911520004272\n",
      "\t Training loss (single batch): 0.9790554046630859\n",
      "\t Training loss (single batch): 1.0830540657043457\n",
      "\t Training loss (single batch): 1.764412760734558\n",
      "\t Training loss (single batch): 1.3898781538009644\n",
      "\t Training loss (single batch): 1.375342607498169\n",
      "\t Training loss (single batch): 1.1354926824569702\n",
      "\t Training loss (single batch): 0.9448152780532837\n",
      "\t Training loss (single batch): 1.256736159324646\n",
      "\t Training loss (single batch): 1.6467206478118896\n",
      "\t Training loss (single batch): 1.260538935661316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1723272800445557\n",
      "\t Training loss (single batch): 1.6410677433013916\n",
      "\t Training loss (single batch): 0.8479666113853455\n",
      "\t Training loss (single batch): 1.3402706384658813\n",
      "\t Training loss (single batch): 1.3236393928527832\n",
      "\t Training loss (single batch): 0.9378252625465393\n",
      "\t Training loss (single batch): 0.9390192031860352\n",
      "\t Training loss (single batch): 0.8172297477722168\n",
      "\t Training loss (single batch): 1.6351181268692017\n",
      "\t Training loss (single batch): 1.3224684000015259\n",
      "\t Training loss (single batch): 0.9298862814903259\n",
      "\t Training loss (single batch): 1.4263908863067627\n",
      "\t Training loss (single batch): 1.2679240703582764\n",
      "\t Training loss (single batch): 1.3840850591659546\n",
      "\t Training loss (single batch): 1.5191779136657715\n",
      "\t Training loss (single batch): 1.1822447776794434\n",
      "\t Training loss (single batch): 1.511252522468567\n",
      "\t Training loss (single batch): 1.2101552486419678\n",
      "\t Training loss (single batch): 1.1214035749435425\n",
      "\t Training loss (single batch): 1.4656023979187012\n",
      "\t Training loss (single batch): 1.5549119710922241\n",
      "\t Training loss (single batch): 1.2693016529083252\n",
      "\t Training loss (single batch): 1.1749516725540161\n",
      "\t Training loss (single batch): 1.6027601957321167\n",
      "\t Training loss (single batch): 1.14480459690094\n",
      "\t Training loss (single batch): 1.6390594244003296\n",
      "\t Training loss (single batch): 0.801223635673523\n",
      "\t Training loss (single batch): 1.8651078939437866\n",
      "\t Training loss (single batch): 1.2854018211364746\n",
      "\t Training loss (single batch): 1.596924066543579\n",
      "\t Training loss (single batch): 0.8602050542831421\n",
      "\t Training loss (single batch): 1.1979001760482788\n",
      "\t Training loss (single batch): 1.1342560052871704\n",
      "\t Training loss (single batch): 0.9536808133125305\n",
      "\t Training loss (single batch): 0.9555947780609131\n",
      "\t Training loss (single batch): 1.151929259300232\n",
      "\t Training loss (single batch): 1.0662142038345337\n",
      "\t Training loss (single batch): 1.1990351676940918\n",
      "\t Training loss (single batch): 1.0825411081314087\n",
      "\t Training loss (single batch): 0.9017465114593506\n",
      "\t Training loss (single batch): 1.2482056617736816\n",
      "\t Training loss (single batch): 1.2871121168136597\n",
      "\t Training loss (single batch): 0.9962117671966553\n",
      "\t Training loss (single batch): 1.6259945631027222\n",
      "\t Training loss (single batch): 1.0316166877746582\n",
      "\t Training loss (single batch): 1.4821686744689941\n",
      "\t Training loss (single batch): 1.007567286491394\n",
      "\t Training loss (single batch): 0.9599094390869141\n",
      "\t Training loss (single batch): 1.044814109802246\n",
      "\t Training loss (single batch): 1.6233749389648438\n",
      "\t Training loss (single batch): 1.219518780708313\n",
      "\t Training loss (single batch): 1.1916319131851196\n",
      "\t Training loss (single batch): 2.2858564853668213\n",
      "\t Training loss (single batch): 0.671028733253479\n",
      "\t Training loss (single batch): 1.448717474937439\n",
      "\t Training loss (single batch): 0.984566330909729\n",
      "\t Training loss (single batch): 1.1376385688781738\n",
      "\t Training loss (single batch): 0.9529845714569092\n",
      "\t Training loss (single batch): 1.1380811929702759\n",
      "\t Training loss (single batch): 0.8923768997192383\n",
      "\t Training loss (single batch): 1.1797226667404175\n",
      "\t Training loss (single batch): 1.2946491241455078\n",
      "\t Training loss (single batch): 0.858227014541626\n",
      "\t Training loss (single batch): 1.2855547666549683\n",
      "\t Training loss (single batch): 1.5965206623077393\n",
      "\t Training loss (single batch): 0.9459381103515625\n",
      "\t Training loss (single batch): 1.3303855657577515\n",
      "\t Training loss (single batch): 1.023419737815857\n",
      "\t Training loss (single batch): 1.2833307981491089\n",
      "\t Training loss (single batch): 1.1563106775283813\n",
      "\t Training loss (single batch): 0.9103332161903381\n",
      "\t Training loss (single batch): 1.0696861743927002\n",
      "\t Training loss (single batch): 1.0731924772262573\n",
      "\t Training loss (single batch): 1.3604631423950195\n",
      "\t Training loss (single batch): 1.1292561292648315\n",
      "\t Training loss (single batch): 1.6485304832458496\n",
      "\t Training loss (single batch): 1.4065124988555908\n",
      "\t Training loss (single batch): 1.4897303581237793\n",
      "\t Training loss (single batch): 1.879757285118103\n",
      "\t Training loss (single batch): 1.4013807773590088\n",
      "\t Training loss (single batch): 1.874071478843689\n",
      "\t Training loss (single batch): 1.616113543510437\n",
      "\t Training loss (single batch): 1.195052981376648\n",
      "\t Training loss (single batch): 1.745162010192871\n",
      "\t Training loss (single batch): 1.4179062843322754\n",
      "\t Training loss (single batch): 0.9707902073860168\n",
      "\t Training loss (single batch): 0.9348033666610718\n",
      "\t Training loss (single batch): 1.3420295715332031\n",
      "\t Training loss (single batch): 1.4686880111694336\n",
      "\t Training loss (single batch): 1.0889161825180054\n",
      "\t Training loss (single batch): 1.9414957761764526\n",
      "\t Training loss (single batch): 0.9399994015693665\n",
      "\t Training loss (single batch): 1.2260353565216064\n",
      "\t Training loss (single batch): 0.9589373469352722\n",
      "\t Training loss (single batch): 0.8035291433334351\n",
      "\t Training loss (single batch): 1.3715869188308716\n",
      "\t Training loss (single batch): 1.0629816055297852\n",
      "\t Training loss (single batch): 1.327605128288269\n",
      "\t Training loss (single batch): 1.6549986600875854\n",
      "\t Training loss (single batch): 1.1017674207687378\n",
      "\t Training loss (single batch): 1.2119982242584229\n",
      "\t Training loss (single batch): 1.3674973249435425\n",
      "\t Training loss (single batch): 1.4737634658813477\n",
      "\t Training loss (single batch): 1.1671032905578613\n",
      "\t Training loss (single batch): 1.2028316259384155\n",
      "\t Training loss (single batch): 1.105095624923706\n",
      "\t Training loss (single batch): 1.0109171867370605\n",
      "\t Training loss (single batch): 1.4935506582260132\n",
      "\t Training loss (single batch): 1.1419496536254883\n",
      "\t Training loss (single batch): 1.4347524642944336\n",
      "\t Training loss (single batch): 1.05235755443573\n",
      "\t Training loss (single batch): 1.2711176872253418\n",
      "\t Training loss (single batch): 0.6888629794120789\n",
      "\t Training loss (single batch): 0.8768904209136963\n",
      "\t Training loss (single batch): 1.011383295059204\n",
      "\t Training loss (single batch): 1.1071751117706299\n",
      "\t Training loss (single batch): 1.5313512086868286\n",
      "\t Training loss (single batch): 1.4950487613677979\n",
      "\t Training loss (single batch): 1.9843121767044067\n",
      "\t Training loss (single batch): 1.2667410373687744\n",
      "\t Training loss (single batch): 1.4791874885559082\n",
      "\t Training loss (single batch): 1.5657000541687012\n",
      "\t Training loss (single batch): 1.1314035654067993\n",
      "\t Training loss (single batch): 0.8356150388717651\n",
      "\t Training loss (single batch): 1.2758245468139648\n",
      "\t Training loss (single batch): 1.0644817352294922\n",
      "\t Training loss (single batch): 1.3705499172210693\n",
      "\t Training loss (single batch): 1.1961684226989746\n",
      "\t Training loss (single batch): 1.1059043407440186\n",
      "\t Training loss (single batch): 1.271139144897461\n",
      "\t Training loss (single batch): 1.0023490190505981\n",
      "\t Training loss (single batch): 0.9512215852737427\n",
      "\t Training loss (single batch): 1.1256550550460815\n",
      "\t Training loss (single batch): 1.4684280157089233\n",
      "\t Training loss (single batch): 1.2211285829544067\n",
      "\t Training loss (single batch): 1.2707080841064453\n",
      "\t Training loss (single batch): 1.1020588874816895\n",
      "\t Training loss (single batch): 1.1395686864852905\n",
      "\t Training loss (single batch): 1.521863341331482\n",
      "\t Training loss (single batch): 1.5184228420257568\n",
      "\t Training loss (single batch): 1.3759325742721558\n",
      "\t Training loss (single batch): 1.279733419418335\n",
      "\t Training loss (single batch): 1.3398388624191284\n",
      "\t Training loss (single batch): 1.5935932397842407\n",
      "\t Training loss (single batch): 1.376969337463379\n",
      "\t Training loss (single batch): 1.2158074378967285\n",
      "\t Training loss (single batch): 1.1860764026641846\n",
      "\t Training loss (single batch): 1.0207041501998901\n",
      "\t Training loss (single batch): 1.0884360074996948\n",
      "\t Training loss (single batch): 1.2972795963287354\n",
      "\t Training loss (single batch): 1.1372263431549072\n",
      "\t Training loss (single batch): 1.326142430305481\n",
      "\t Training loss (single batch): 1.280527949333191\n",
      "\t Training loss (single batch): 1.1372414827346802\n",
      "\t Training loss (single batch): 1.3644288778305054\n",
      "\t Training loss (single batch): 0.947971522808075\n",
      "\t Training loss (single batch): 1.1763437986373901\n",
      "\t Training loss (single batch): 0.8235809206962585\n",
      "\t Training loss (single batch): 1.4489197731018066\n",
      "\t Training loss (single batch): 1.093290090560913\n",
      "\t Training loss (single batch): 1.3979121446609497\n",
      "\t Training loss (single batch): 1.1488920450210571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5591919422149658\n",
      "\t Training loss (single batch): 1.383385419845581\n",
      "\t Training loss (single batch): 1.5543583631515503\n",
      "\t Training loss (single batch): 1.3115639686584473\n",
      "\t Training loss (single batch): 1.087992787361145\n",
      "\t Training loss (single batch): 1.7248167991638184\n",
      "\t Training loss (single batch): 0.9302226901054382\n",
      "\t Training loss (single batch): 0.8418614268302917\n",
      "\t Training loss (single batch): 1.0210678577423096\n",
      "\t Training loss (single batch): 1.2034921646118164\n",
      "\t Training loss (single batch): 1.4898269176483154\n",
      "\t Training loss (single batch): 1.2510474920272827\n",
      "\t Training loss (single batch): 1.557536244392395\n",
      "\t Training loss (single batch): 1.7292369604110718\n",
      "\t Training loss (single batch): 0.7972710728645325\n",
      "\t Training loss (single batch): 1.6501590013504028\n",
      "\t Training loss (single batch): 0.9533249139785767\n",
      "\t Training loss (single batch): 1.141813039779663\n",
      "\t Training loss (single batch): 0.9376239776611328\n",
      "\t Training loss (single batch): 1.1762025356292725\n",
      "\t Training loss (single batch): 0.9305405616760254\n",
      "\t Training loss (single batch): 1.2008131742477417\n",
      "\t Training loss (single batch): 1.0018302202224731\n",
      "\t Training loss (single batch): 1.1541301012039185\n",
      "\t Training loss (single batch): 1.3430577516555786\n",
      "\t Training loss (single batch): 1.0172784328460693\n",
      "\t Training loss (single batch): 0.8699492812156677\n",
      "\t Training loss (single batch): 0.8275775909423828\n",
      "\t Training loss (single batch): 1.177160382270813\n",
      "\t Training loss (single batch): 0.8396756052970886\n",
      "\t Training loss (single batch): 0.9109257459640503\n",
      "\t Training loss (single batch): 1.1781227588653564\n",
      "\t Training loss (single batch): 1.9289567470550537\n",
      "\t Training loss (single batch): 0.9976979494094849\n",
      "\t Training loss (single batch): 0.8921440839767456\n",
      "\t Training loss (single batch): 0.990386426448822\n",
      "\t Training loss (single batch): 0.8991734981536865\n",
      "\t Training loss (single batch): 1.264449954032898\n",
      "\t Training loss (single batch): 1.2306177616119385\n",
      "\t Training loss (single batch): 1.2975863218307495\n",
      "\t Training loss (single batch): 1.220171332359314\n",
      "##################################\n",
      "## EPOCH 36\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1965916156768799\n",
      "\t Training loss (single batch): 1.3696099519729614\n",
      "\t Training loss (single batch): 1.27217435836792\n",
      "\t Training loss (single batch): 1.1022472381591797\n",
      "\t Training loss (single batch): 1.1750006675720215\n",
      "\t Training loss (single batch): 0.9890710115432739\n",
      "\t Training loss (single batch): 0.8696251511573792\n",
      "\t Training loss (single batch): 0.8612059950828552\n",
      "\t Training loss (single batch): 1.1646087169647217\n",
      "\t Training loss (single batch): 1.2232604026794434\n",
      "\t Training loss (single batch): 0.9275875687599182\n",
      "\t Training loss (single batch): 0.8415135145187378\n",
      "\t Training loss (single batch): 1.0136665105819702\n",
      "\t Training loss (single batch): 1.355992078781128\n",
      "\t Training loss (single batch): 1.1622812747955322\n",
      "\t Training loss (single batch): 1.4841986894607544\n",
      "\t Training loss (single batch): 1.0693553686141968\n",
      "\t Training loss (single batch): 1.7838622331619263\n",
      "\t Training loss (single batch): 1.2651628255844116\n",
      "\t Training loss (single batch): 2.100304126739502\n",
      "\t Training loss (single batch): 1.2699826955795288\n",
      "\t Training loss (single batch): 0.8368848562240601\n",
      "\t Training loss (single batch): 1.2583134174346924\n",
      "\t Training loss (single batch): 1.8898273706436157\n",
      "\t Training loss (single batch): 0.8662027716636658\n",
      "\t Training loss (single batch): 1.1667226552963257\n",
      "\t Training loss (single batch): 1.2354748249053955\n",
      "\t Training loss (single batch): 1.3876949548721313\n",
      "\t Training loss (single batch): 1.4895501136779785\n",
      "\t Training loss (single batch): 1.1483728885650635\n",
      "\t Training loss (single batch): 1.290697693824768\n",
      "\t Training loss (single batch): 0.945541262626648\n",
      "\t Training loss (single batch): 1.3977099657058716\n",
      "\t Training loss (single batch): 1.078622579574585\n",
      "\t Training loss (single batch): 1.1753833293914795\n",
      "\t Training loss (single batch): 1.5027683973312378\n",
      "\t Training loss (single batch): 1.3740198612213135\n",
      "\t Training loss (single batch): 0.9067367911338806\n",
      "\t Training loss (single batch): 1.517713189125061\n",
      "\t Training loss (single batch): 1.6046099662780762\n",
      "\t Training loss (single batch): 1.4062488079071045\n",
      "\t Training loss (single batch): 0.8570055365562439\n",
      "\t Training loss (single batch): 1.9669654369354248\n",
      "\t Training loss (single batch): 1.6192599534988403\n",
      "\t Training loss (single batch): 1.2136952877044678\n",
      "\t Training loss (single batch): 0.8367134928703308\n",
      "\t Training loss (single batch): 0.9983437061309814\n",
      "\t Training loss (single batch): 1.4776523113250732\n",
      "\t Training loss (single batch): 1.5528584718704224\n",
      "\t Training loss (single batch): 1.2814345359802246\n",
      "\t Training loss (single batch): 1.4424738883972168\n",
      "\t Training loss (single batch): 1.358449935913086\n",
      "\t Training loss (single batch): 1.1918907165527344\n",
      "\t Training loss (single batch): 1.1523468494415283\n",
      "\t Training loss (single batch): 1.2913378477096558\n",
      "\t Training loss (single batch): 1.0448330640792847\n",
      "\t Training loss (single batch): 1.429924488067627\n",
      "\t Training loss (single batch): 1.1461478471755981\n",
      "\t Training loss (single batch): 0.977574348449707\n",
      "\t Training loss (single batch): 1.6626440286636353\n",
      "\t Training loss (single batch): 1.122372031211853\n",
      "\t Training loss (single batch): 0.846515953540802\n",
      "\t Training loss (single batch): 0.9667726159095764\n",
      "\t Training loss (single batch): 2.215437173843384\n",
      "\t Training loss (single batch): 1.0413485765457153\n",
      "\t Training loss (single batch): 0.7278775572776794\n",
      "\t Training loss (single batch): 1.4784955978393555\n",
      "\t Training loss (single batch): 1.1135001182556152\n",
      "\t Training loss (single batch): 1.6484334468841553\n",
      "\t Training loss (single batch): 1.0903342962265015\n",
      "\t Training loss (single batch): 1.1605719327926636\n",
      "\t Training loss (single batch): 1.0262539386749268\n",
      "\t Training loss (single batch): 1.0877349376678467\n",
      "\t Training loss (single batch): 1.4697307348251343\n",
      "\t Training loss (single batch): 1.2858328819274902\n",
      "\t Training loss (single batch): 0.872721791267395\n",
      "\t Training loss (single batch): 1.505588412284851\n",
      "\t Training loss (single batch): 1.4752471446990967\n",
      "\t Training loss (single batch): 1.1079033613204956\n",
      "\t Training loss (single batch): 1.1036362648010254\n",
      "\t Training loss (single batch): 1.387887716293335\n",
      "\t Training loss (single batch): 1.4343905448913574\n",
      "\t Training loss (single batch): 1.3398592472076416\n",
      "\t Training loss (single batch): 1.5619028806686401\n",
      "\t Training loss (single batch): 1.3983529806137085\n",
      "\t Training loss (single batch): 1.5088567733764648\n",
      "\t Training loss (single batch): 1.2031079530715942\n",
      "\t Training loss (single batch): 1.0731626749038696\n",
      "\t Training loss (single batch): 1.451310634613037\n",
      "\t Training loss (single batch): 1.3580154180526733\n",
      "\t Training loss (single batch): 1.1466683149337769\n",
      "\t Training loss (single batch): 1.04853355884552\n",
      "\t Training loss (single batch): 1.655312180519104\n",
      "\t Training loss (single batch): 1.1251863241195679\n",
      "\t Training loss (single batch): 1.0448859930038452\n",
      "\t Training loss (single batch): 1.1789695024490356\n",
      "\t Training loss (single batch): 1.5134358406066895\n",
      "\t Training loss (single batch): 1.0834463834762573\n",
      "\t Training loss (single batch): 1.3945657014846802\n",
      "\t Training loss (single batch): 0.9387223720550537\n",
      "\t Training loss (single batch): 0.9259213805198669\n",
      "\t Training loss (single batch): 1.3603742122650146\n",
      "\t Training loss (single batch): 1.2922089099884033\n",
      "\t Training loss (single batch): 1.4330949783325195\n",
      "\t Training loss (single batch): 1.4736456871032715\n",
      "\t Training loss (single batch): 1.1323702335357666\n",
      "\t Training loss (single batch): 1.7513606548309326\n",
      "\t Training loss (single batch): 1.2470459938049316\n",
      "\t Training loss (single batch): 1.140563726425171\n",
      "\t Training loss (single batch): 1.5556591749191284\n",
      "\t Training loss (single batch): 1.238649606704712\n",
      "\t Training loss (single batch): 1.3532218933105469\n",
      "\t Training loss (single batch): 1.2829618453979492\n",
      "\t Training loss (single batch): 1.1882511377334595\n",
      "\t Training loss (single batch): 1.099179744720459\n",
      "\t Training loss (single batch): 1.0772408246994019\n",
      "\t Training loss (single batch): 1.2832062244415283\n",
      "\t Training loss (single batch): 1.082944631576538\n",
      "\t Training loss (single batch): 1.0766185522079468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6482995748519897\n",
      "\t Training loss (single batch): 1.4594128131866455\n",
      "\t Training loss (single batch): 1.3428000211715698\n",
      "\t Training loss (single batch): 1.0046751499176025\n",
      "\t Training loss (single batch): 1.2066006660461426\n",
      "\t Training loss (single batch): 0.9715771675109863\n",
      "\t Training loss (single batch): 1.146936058998108\n",
      "\t Training loss (single batch): 0.9853766560554504\n",
      "\t Training loss (single batch): 1.2528647184371948\n",
      "\t Training loss (single batch): 1.4768050909042358\n",
      "\t Training loss (single batch): 0.9640471935272217\n",
      "\t Training loss (single batch): 1.0091848373413086\n",
      "\t Training loss (single batch): 1.488175868988037\n",
      "\t Training loss (single batch): 0.8442969918251038\n",
      "\t Training loss (single batch): 1.4219509363174438\n",
      "\t Training loss (single batch): 1.2783066034317017\n",
      "\t Training loss (single batch): 1.5649601221084595\n",
      "\t Training loss (single batch): 1.1906646490097046\n",
      "\t Training loss (single batch): 1.414833664894104\n",
      "\t Training loss (single batch): 1.377413272857666\n",
      "\t Training loss (single batch): 1.4490370750427246\n",
      "\t Training loss (single batch): 1.372319221496582\n",
      "\t Training loss (single batch): 0.8265727162361145\n",
      "\t Training loss (single batch): 1.058823823928833\n",
      "\t Training loss (single batch): 1.281307339668274\n",
      "\t Training loss (single batch): 0.6546221971511841\n",
      "\t Training loss (single batch): 1.410723328590393\n",
      "\t Training loss (single batch): 1.273973822593689\n",
      "\t Training loss (single batch): 1.248539924621582\n",
      "\t Training loss (single batch): 1.2826002836227417\n",
      "\t Training loss (single batch): 1.1226149797439575\n",
      "\t Training loss (single batch): 1.5929359197616577\n",
      "\t Training loss (single batch): 1.4690052270889282\n",
      "\t Training loss (single batch): 1.3633060455322266\n",
      "\t Training loss (single batch): 1.1187866926193237\n",
      "\t Training loss (single batch): 1.028107762336731\n",
      "\t Training loss (single batch): 1.160765528678894\n",
      "\t Training loss (single batch): 1.034185528755188\n",
      "\t Training loss (single batch): 1.2548978328704834\n",
      "\t Training loss (single batch): 1.4770734310150146\n",
      "\t Training loss (single batch): 0.9131808876991272\n",
      "\t Training loss (single batch): 1.2611072063446045\n",
      "\t Training loss (single batch): 1.3686954975128174\n",
      "\t Training loss (single batch): 1.4116157293319702\n",
      "\t Training loss (single batch): 2.033622980117798\n",
      "\t Training loss (single batch): 1.501423954963684\n",
      "\t Training loss (single batch): 1.31217622756958\n",
      "\t Training loss (single batch): 1.065977692604065\n",
      "\t Training loss (single batch): 1.013802170753479\n",
      "\t Training loss (single batch): 1.2559484243392944\n",
      "\t Training loss (single batch): 0.9912822842597961\n",
      "\t Training loss (single batch): 1.7979776859283447\n",
      "\t Training loss (single batch): 1.4368873834609985\n",
      "\t Training loss (single batch): 1.3181260824203491\n",
      "\t Training loss (single batch): 1.9252464771270752\n",
      "\t Training loss (single batch): 1.1612133979797363\n",
      "\t Training loss (single batch): 1.575827717781067\n",
      "\t Training loss (single batch): 1.3605831861495972\n",
      "\t Training loss (single batch): 1.1606690883636475\n",
      "\t Training loss (single batch): 1.2882643938064575\n",
      "\t Training loss (single batch): 1.171044111251831\n",
      "\t Training loss (single batch): 1.3281205892562866\n",
      "\t Training loss (single batch): 1.2617175579071045\n",
      "\t Training loss (single batch): 0.849983811378479\n",
      "\t Training loss (single batch): 1.3918954133987427\n",
      "\t Training loss (single batch): 1.0466687679290771\n",
      "\t Training loss (single batch): 1.2086750268936157\n",
      "\t Training loss (single batch): 0.977429211139679\n",
      "\t Training loss (single batch): 0.8761877417564392\n",
      "\t Training loss (single batch): 0.9421504139900208\n",
      "\t Training loss (single batch): 1.0509809255599976\n",
      "\t Training loss (single batch): 1.3064393997192383\n",
      "\t Training loss (single batch): 1.2113252878189087\n",
      "\t Training loss (single batch): 0.9043318033218384\n",
      "\t Training loss (single batch): 1.2710959911346436\n",
      "\t Training loss (single batch): 1.2309587001800537\n",
      "\t Training loss (single batch): 0.8906253576278687\n",
      "\t Training loss (single batch): 1.2029091119766235\n",
      "\t Training loss (single batch): 0.9361535906791687\n",
      "\t Training loss (single batch): 1.2191674709320068\n",
      "\t Training loss (single batch): 0.7215800285339355\n",
      "\t Training loss (single batch): 0.9993013143539429\n",
      "\t Training loss (single batch): 1.4666929244995117\n",
      "\t Training loss (single batch): 1.219567060470581\n",
      "\t Training loss (single batch): 1.0867722034454346\n",
      "\t Training loss (single batch): 1.6679480075836182\n",
      "\t Training loss (single batch): 0.9732872843742371\n",
      "\t Training loss (single batch): 1.029704213142395\n",
      "\t Training loss (single batch): 1.2590018510818481\n",
      "\t Training loss (single batch): 1.4305468797683716\n",
      "\t Training loss (single batch): 1.2223702669143677\n",
      "\t Training loss (single batch): 1.3245341777801514\n",
      "\t Training loss (single batch): 0.9164618253707886\n",
      "\t Training loss (single batch): 1.2904871702194214\n",
      "\t Training loss (single batch): 1.777626395225525\n",
      "\t Training loss (single batch): 1.0874518156051636\n",
      "\t Training loss (single batch): 1.273814082145691\n",
      "\t Training loss (single batch): 1.141145944595337\n",
      "\t Training loss (single batch): 1.0273871421813965\n",
      "\t Training loss (single batch): 0.9033327698707581\n",
      "\t Training loss (single batch): 1.5372475385665894\n",
      "\t Training loss (single batch): 0.8307362198829651\n",
      "\t Training loss (single batch): 1.2090400457382202\n",
      "\t Training loss (single batch): 1.3879047632217407\n",
      "\t Training loss (single batch): 1.3023205995559692\n",
      "\t Training loss (single batch): 1.1161925792694092\n",
      "\t Training loss (single batch): 1.0983948707580566\n",
      "\t Training loss (single batch): 1.7379429340362549\n",
      "\t Training loss (single batch): 0.9472548961639404\n",
      "\t Training loss (single batch): 0.9704428911209106\n",
      "\t Training loss (single batch): 1.1983833312988281\n",
      "\t Training loss (single batch): 1.1473852396011353\n",
      "\t Training loss (single batch): 1.0750869512557983\n",
      "\t Training loss (single batch): 1.3332176208496094\n",
      "\t Training loss (single batch): 1.236975908279419\n",
      "\t Training loss (single batch): 1.356754183769226\n",
      "\t Training loss (single batch): 1.0466779470443726\n",
      "\t Training loss (single batch): 1.20004403591156\n",
      "\t Training loss (single batch): 1.8365397453308105\n",
      "\t Training loss (single batch): 1.2660890817642212\n",
      "\t Training loss (single batch): 1.3802335262298584\n",
      "\t Training loss (single batch): 0.9628549814224243\n",
      "\t Training loss (single batch): 1.0957533121109009\n",
      "\t Training loss (single batch): 1.1706953048706055\n",
      "\t Training loss (single batch): 1.2129615545272827\n",
      "\t Training loss (single batch): 1.2447587251663208\n",
      "\t Training loss (single batch): 1.388245701789856\n",
      "\t Training loss (single batch): 1.3571958541870117\n",
      "\t Training loss (single batch): 1.146860957145691\n",
      "\t Training loss (single batch): 1.0261176824569702\n",
      "\t Training loss (single batch): 1.0942922830581665\n",
      "\t Training loss (single batch): 1.3328965902328491\n",
      "\t Training loss (single batch): 0.7068372368812561\n",
      "\t Training loss (single batch): 0.8125977516174316\n",
      "\t Training loss (single batch): 1.490236759185791\n",
      "\t Training loss (single batch): 1.5922446250915527\n",
      "\t Training loss (single batch): 1.5535945892333984\n",
      "\t Training loss (single batch): 1.734723448753357\n",
      "\t Training loss (single batch): 1.3960565328598022\n",
      "\t Training loss (single batch): 1.3669384717941284\n",
      "\t Training loss (single batch): 1.2302460670471191\n",
      "\t Training loss (single batch): 1.4805492162704468\n",
      "\t Training loss (single batch): 0.8166926503181458\n",
      "\t Training loss (single batch): 1.4543592929840088\n",
      "\t Training loss (single batch): 1.4565544128417969\n",
      "\t Training loss (single batch): 0.8657687306404114\n",
      "\t Training loss (single batch): 1.3747215270996094\n",
      "\t Training loss (single batch): 1.4782308340072632\n",
      "\t Training loss (single batch): 1.2341102361679077\n",
      "\t Training loss (single batch): 1.2749005556106567\n",
      "\t Training loss (single batch): 1.0420525074005127\n",
      "\t Training loss (single batch): 0.9775481224060059\n",
      "\t Training loss (single batch): 1.392093300819397\n",
      "\t Training loss (single batch): 1.4548147916793823\n",
      "\t Training loss (single batch): 1.3678098917007446\n",
      "\t Training loss (single batch): 1.328985333442688\n",
      "\t Training loss (single batch): 1.2166175842285156\n",
      "\t Training loss (single batch): 1.2051841020584106\n",
      "\t Training loss (single batch): 1.1183720827102661\n",
      "\t Training loss (single batch): 1.1564054489135742\n",
      "\t Training loss (single batch): 1.2769070863723755\n",
      "\t Training loss (single batch): 1.1125530004501343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6218301057815552\n",
      "\t Training loss (single batch): 1.4387145042419434\n",
      "\t Training loss (single batch): 0.7669041156768799\n",
      "\t Training loss (single batch): 0.8399791717529297\n",
      "\t Training loss (single batch): 0.9094640016555786\n",
      "\t Training loss (single batch): 1.1157656908035278\n",
      "\t Training loss (single batch): 1.7250161170959473\n",
      "\t Training loss (single batch): 1.1416590213775635\n",
      "\t Training loss (single batch): 1.4360690116882324\n",
      "\t Training loss (single batch): 1.3731954097747803\n",
      "\t Training loss (single batch): 1.0024179220199585\n",
      "\t Training loss (single batch): 0.9524754285812378\n",
      "\t Training loss (single batch): 1.1471235752105713\n",
      "\t Training loss (single batch): 2.0489234924316406\n",
      "\t Training loss (single batch): 0.9446130990982056\n",
      "\t Training loss (single batch): 1.0165177583694458\n",
      "\t Training loss (single batch): 1.4715495109558105\n",
      "\t Training loss (single batch): 1.8197277784347534\n",
      "\t Training loss (single batch): 1.2222375869750977\n",
      "\t Training loss (single batch): 1.0897386074066162\n",
      "\t Training loss (single batch): 0.8787216544151306\n",
      "\t Training loss (single batch): 0.9780135750770569\n",
      "\t Training loss (single batch): 0.9865245819091797\n",
      "\t Training loss (single batch): 1.027321696281433\n",
      "\t Training loss (single batch): 1.3592784404754639\n",
      "\t Training loss (single batch): 1.594835877418518\n",
      "\t Training loss (single batch): 1.391170620918274\n",
      "\t Training loss (single batch): 1.0975450277328491\n",
      "\t Training loss (single batch): 1.4717235565185547\n",
      "\t Training loss (single batch): 1.414181113243103\n",
      "\t Training loss (single batch): 1.008728265762329\n",
      "\t Training loss (single batch): 1.0333747863769531\n",
      "\t Training loss (single batch): 1.3934987783432007\n",
      "\t Training loss (single batch): 1.0417171716690063\n",
      "\t Training loss (single batch): 1.1060924530029297\n",
      "\t Training loss (single batch): 0.8201213479042053\n",
      "##################################\n",
      "## EPOCH 37\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1442571878433228\n",
      "\t Training loss (single batch): 0.8753249645233154\n",
      "\t Training loss (single batch): 1.1920596361160278\n",
      "\t Training loss (single batch): 1.2704899311065674\n",
      "\t Training loss (single batch): 1.2618863582611084\n",
      "\t Training loss (single batch): 1.1396641731262207\n",
      "\t Training loss (single batch): 1.4620497226715088\n",
      "\t Training loss (single batch): 1.0100176334381104\n",
      "\t Training loss (single batch): 1.1273643970489502\n",
      "\t Training loss (single batch): 1.225651741027832\n",
      "\t Training loss (single batch): 1.229964017868042\n",
      "\t Training loss (single batch): 1.1591379642486572\n",
      "\t Training loss (single batch): 1.1608318090438843\n",
      "\t Training loss (single batch): 1.1186602115631104\n",
      "\t Training loss (single batch): 1.270480990409851\n",
      "\t Training loss (single batch): 1.009187936782837\n",
      "\t Training loss (single batch): 0.8958992958068848\n",
      "\t Training loss (single batch): 1.4003334045410156\n",
      "\t Training loss (single batch): 0.80777907371521\n",
      "\t Training loss (single batch): 1.1069931983947754\n",
      "\t Training loss (single batch): 1.3856712579727173\n",
      "\t Training loss (single batch): 0.9914923906326294\n",
      "\t Training loss (single batch): 1.968849539756775\n",
      "\t Training loss (single batch): 0.834712564945221\n",
      "\t Training loss (single batch): 1.3498170375823975\n",
      "\t Training loss (single batch): 0.8513488173484802\n",
      "\t Training loss (single batch): 1.2470124959945679\n",
      "\t Training loss (single batch): 0.8633682131767273\n",
      "\t Training loss (single batch): 1.4117177724838257\n",
      "\t Training loss (single batch): 1.321698784828186\n",
      "\t Training loss (single batch): 1.559852123260498\n",
      "\t Training loss (single batch): 0.8828283548355103\n",
      "\t Training loss (single batch): 1.0561692714691162\n",
      "\t Training loss (single batch): 0.9879516363143921\n",
      "\t Training loss (single batch): 1.186684489250183\n",
      "\t Training loss (single batch): 1.367327094078064\n",
      "\t Training loss (single batch): 1.188232183456421\n",
      "\t Training loss (single batch): 1.2149184942245483\n",
      "\t Training loss (single batch): 0.875048041343689\n",
      "\t Training loss (single batch): 1.2621721029281616\n",
      "\t Training loss (single batch): 1.1962764263153076\n",
      "\t Training loss (single batch): 1.1635942459106445\n",
      "\t Training loss (single batch): 1.8159666061401367\n",
      "\t Training loss (single batch): 1.4055253267288208\n",
      "\t Training loss (single batch): 1.0579837560653687\n",
      "\t Training loss (single batch): 1.4274996519088745\n",
      "\t Training loss (single batch): 1.0414305925369263\n",
      "\t Training loss (single batch): 0.9131131172180176\n",
      "\t Training loss (single batch): 0.9936055541038513\n",
      "\t Training loss (single batch): 1.7272803783416748\n",
      "\t Training loss (single batch): 1.1826231479644775\n",
      "\t Training loss (single batch): 1.4575533866882324\n",
      "\t Training loss (single batch): 1.0722302198410034\n",
      "\t Training loss (single batch): 1.4831757545471191\n",
      "\t Training loss (single batch): 1.1921045780181885\n",
      "\t Training loss (single batch): 1.3508739471435547\n",
      "\t Training loss (single batch): 1.130576729774475\n",
      "\t Training loss (single batch): 1.793318748474121\n",
      "\t Training loss (single batch): 1.5603266954421997\n",
      "\t Training loss (single batch): 0.9986568689346313\n",
      "\t Training loss (single batch): 1.1991091966629028\n",
      "\t Training loss (single batch): 1.3199801445007324\n",
      "\t Training loss (single batch): 1.5352418422698975\n",
      "\t Training loss (single batch): 1.018433928489685\n",
      "\t Training loss (single batch): 0.9187670350074768\n",
      "\t Training loss (single batch): 1.4069559574127197\n",
      "\t Training loss (single batch): 1.1411285400390625\n",
      "\t Training loss (single batch): 0.8101785778999329\n",
      "\t Training loss (single batch): 1.4366118907928467\n",
      "\t Training loss (single batch): 1.45027756690979\n",
      "\t Training loss (single batch): 1.3301340341567993\n",
      "\t Training loss (single batch): 1.8064249753952026\n",
      "\t Training loss (single batch): 0.892533540725708\n",
      "\t Training loss (single batch): 0.7310723662376404\n",
      "\t Training loss (single batch): 0.9515774846076965\n",
      "\t Training loss (single batch): 1.2460697889328003\n",
      "\t Training loss (single batch): 1.19600510597229\n",
      "\t Training loss (single batch): 1.2151095867156982\n",
      "\t Training loss (single batch): 1.0632632970809937\n",
      "\t Training loss (single batch): 1.2592227458953857\n",
      "\t Training loss (single batch): 1.4905872344970703\n",
      "\t Training loss (single batch): 1.5304372310638428\n",
      "\t Training loss (single batch): 0.9510242342948914\n",
      "\t Training loss (single batch): 1.118224024772644\n",
      "\t Training loss (single batch): 1.2554174661636353\n",
      "\t Training loss (single batch): 0.908111572265625\n",
      "\t Training loss (single batch): 1.2723705768585205\n",
      "\t Training loss (single batch): 0.8811482787132263\n",
      "\t Training loss (single batch): 1.0027443170547485\n",
      "\t Training loss (single batch): 1.870114803314209\n",
      "\t Training loss (single batch): 1.1320604085922241\n",
      "\t Training loss (single batch): 1.787298321723938\n",
      "\t Training loss (single batch): 1.0973174571990967\n",
      "\t Training loss (single batch): 0.9345870614051819\n",
      "\t Training loss (single batch): 0.8445494771003723\n",
      "\t Training loss (single batch): 1.0956045389175415\n",
      "\t Training loss (single batch): 0.9650169014930725\n",
      "\t Training loss (single batch): 1.6658402681350708\n",
      "\t Training loss (single batch): 1.3534148931503296\n",
      "\t Training loss (single batch): 1.1005268096923828\n",
      "\t Training loss (single batch): 1.319840669631958\n",
      "\t Training loss (single batch): 1.1761062145233154\n",
      "\t Training loss (single batch): 1.1808909177780151\n",
      "\t Training loss (single batch): 1.2590572834014893\n",
      "\t Training loss (single batch): 1.4427978992462158\n",
      "\t Training loss (single batch): 1.3168178796768188\n",
      "\t Training loss (single batch): 1.4089179039001465\n",
      "\t Training loss (single batch): 1.5186176300048828\n",
      "\t Training loss (single batch): 1.3628655672073364\n",
      "\t Training loss (single batch): 1.1788982152938843\n",
      "\t Training loss (single batch): 0.9836909174919128\n",
      "\t Training loss (single batch): 0.995737612247467\n",
      "\t Training loss (single batch): 1.7539390325546265\n",
      "\t Training loss (single batch): 1.4061603546142578\n",
      "\t Training loss (single batch): 0.8702404499053955\n",
      "\t Training loss (single batch): 1.360636591911316\n",
      "\t Training loss (single batch): 1.5973378419876099\n",
      "\t Training loss (single batch): 1.1944093704223633\n",
      "\t Training loss (single batch): 1.4534388780593872\n",
      "\t Training loss (single batch): 0.9182825684547424\n",
      "\t Training loss (single batch): 0.7705678343772888\n",
      "\t Training loss (single batch): 0.9016328454017639\n",
      "\t Training loss (single batch): 1.070515513420105\n",
      "\t Training loss (single batch): 1.169708251953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.48192298412323\n",
      "\t Training loss (single batch): 1.0332807302474976\n",
      "\t Training loss (single batch): 0.8721963167190552\n",
      "\t Training loss (single batch): 1.422385573387146\n",
      "\t Training loss (single batch): 1.4601526260375977\n",
      "\t Training loss (single batch): 1.0733104944229126\n",
      "\t Training loss (single batch): 1.4793713092803955\n",
      "\t Training loss (single batch): 1.1202435493469238\n",
      "\t Training loss (single batch): 1.3069263696670532\n",
      "\t Training loss (single batch): 1.3605320453643799\n",
      "\t Training loss (single batch): 1.5913870334625244\n",
      "\t Training loss (single batch): 1.6351711750030518\n",
      "\t Training loss (single batch): 1.3806061744689941\n",
      "\t Training loss (single batch): 1.2989667654037476\n",
      "\t Training loss (single batch): 1.7357203960418701\n",
      "\t Training loss (single batch): 0.88538658618927\n",
      "\t Training loss (single batch): 1.2797956466674805\n",
      "\t Training loss (single batch): 1.6033002138137817\n",
      "\t Training loss (single batch): 1.4608644247055054\n",
      "\t Training loss (single batch): 1.4351274967193604\n",
      "\t Training loss (single batch): 1.3054136037826538\n",
      "\t Training loss (single batch): 1.230676293373108\n",
      "\t Training loss (single batch): 0.940650224685669\n",
      "\t Training loss (single batch): 1.2460359334945679\n",
      "\t Training loss (single batch): 1.8199317455291748\n",
      "\t Training loss (single batch): 1.1549134254455566\n",
      "\t Training loss (single batch): 1.3054605722427368\n",
      "\t Training loss (single batch): 1.2514090538024902\n",
      "\t Training loss (single batch): 1.0466864109039307\n",
      "\t Training loss (single batch): 1.1013580560684204\n",
      "\t Training loss (single batch): 1.3927321434020996\n",
      "\t Training loss (single batch): 1.0529078245162964\n",
      "\t Training loss (single batch): 0.7179112434387207\n",
      "\t Training loss (single batch): 1.2964093685150146\n",
      "\t Training loss (single batch): 0.9749723076820374\n",
      "\t Training loss (single batch): 1.5211279392242432\n",
      "\t Training loss (single batch): 1.015397071838379\n",
      "\t Training loss (single batch): 0.9588672518730164\n",
      "\t Training loss (single batch): 1.2054805755615234\n",
      "\t Training loss (single batch): 1.7128891944885254\n",
      "\t Training loss (single batch): 1.2316415309906006\n",
      "\t Training loss (single batch): 0.8763322830200195\n",
      "\t Training loss (single batch): 1.225592017173767\n",
      "\t Training loss (single batch): 1.8248984813690186\n",
      "\t Training loss (single batch): 1.048144817352295\n",
      "\t Training loss (single batch): 1.1043815612792969\n",
      "\t Training loss (single batch): 1.2356433868408203\n",
      "\t Training loss (single batch): 1.223615288734436\n",
      "\t Training loss (single batch): 1.6063967943191528\n",
      "\t Training loss (single batch): 1.4732202291488647\n",
      "\t Training loss (single batch): 1.2283822298049927\n",
      "\t Training loss (single batch): 0.8218948841094971\n",
      "\t Training loss (single batch): 1.4515756368637085\n",
      "\t Training loss (single batch): 1.3389852046966553\n",
      "\t Training loss (single batch): 1.1601368188858032\n",
      "\t Training loss (single batch): 1.5673645734786987\n",
      "\t Training loss (single batch): 0.9891070127487183\n",
      "\t Training loss (single batch): 1.12151300907135\n",
      "\t Training loss (single batch): 1.0265523195266724\n",
      "\t Training loss (single batch): 1.3993810415267944\n",
      "\t Training loss (single batch): 1.36271333694458\n",
      "\t Training loss (single batch): 1.1263538599014282\n",
      "\t Training loss (single batch): 1.7022916078567505\n",
      "\t Training loss (single batch): 1.110887050628662\n",
      "\t Training loss (single batch): 1.1954514980316162\n",
      "\t Training loss (single batch): 1.6567494869232178\n",
      "\t Training loss (single batch): 1.5430140495300293\n",
      "\t Training loss (single batch): 1.1453324556350708\n",
      "\t Training loss (single batch): 1.420132040977478\n",
      "\t Training loss (single batch): 1.5628613233566284\n",
      "\t Training loss (single batch): 1.4474804401397705\n",
      "\t Training loss (single batch): 1.7025694847106934\n",
      "\t Training loss (single batch): 1.4241214990615845\n",
      "\t Training loss (single batch): 0.893632709980011\n",
      "\t Training loss (single batch): 0.9923454523086548\n",
      "\t Training loss (single batch): 1.279966115951538\n",
      "\t Training loss (single batch): 1.2383806705474854\n",
      "\t Training loss (single batch): 1.200127363204956\n",
      "\t Training loss (single batch): 0.9966035485267639\n",
      "\t Training loss (single batch): 1.212144136428833\n",
      "\t Training loss (single batch): 1.3161736726760864\n",
      "\t Training loss (single batch): 1.2057416439056396\n",
      "\t Training loss (single batch): 1.1439889669418335\n",
      "\t Training loss (single batch): 1.247692346572876\n",
      "\t Training loss (single batch): 1.1671011447906494\n",
      "\t Training loss (single batch): 1.6993801593780518\n",
      "\t Training loss (single batch): 1.4495415687561035\n",
      "\t Training loss (single batch): 0.8184562921524048\n",
      "\t Training loss (single batch): 1.1631921529769897\n",
      "\t Training loss (single batch): 1.147874355316162\n",
      "\t Training loss (single batch): 1.2843389511108398\n",
      "\t Training loss (single batch): 1.2503917217254639\n",
      "\t Training loss (single batch): 1.1933624744415283\n",
      "\t Training loss (single batch): 1.3055205345153809\n",
      "\t Training loss (single batch): 1.040610432624817\n",
      "\t Training loss (single batch): 0.9751735329627991\n",
      "\t Training loss (single batch): 1.471206784248352\n",
      "\t Training loss (single batch): 1.0112905502319336\n",
      "\t Training loss (single batch): 1.1354118585586548\n",
      "\t Training loss (single batch): 1.3407375812530518\n",
      "\t Training loss (single batch): 1.450096845626831\n",
      "\t Training loss (single batch): 1.3993076086044312\n",
      "\t Training loss (single batch): 1.5585777759552002\n",
      "\t Training loss (single batch): 1.1263539791107178\n",
      "\t Training loss (single batch): 1.2524768114089966\n",
      "\t Training loss (single batch): 1.3601744174957275\n",
      "\t Training loss (single batch): 1.362009048461914\n",
      "\t Training loss (single batch): 0.883904755115509\n",
      "\t Training loss (single batch): 1.464192271232605\n",
      "\t Training loss (single batch): 1.1266478300094604\n",
      "\t Training loss (single batch): 1.3454155921936035\n",
      "\t Training loss (single batch): 0.9294164180755615\n",
      "\t Training loss (single batch): 0.8444539308547974\n",
      "\t Training loss (single batch): 1.238039493560791\n",
      "\t Training loss (single batch): 1.2723357677459717\n",
      "\t Training loss (single batch): 1.1013251543045044\n",
      "\t Training loss (single batch): 0.8834707736968994\n",
      "\t Training loss (single batch): 1.2339699268341064\n",
      "\t Training loss (single batch): 1.335523009300232\n",
      "\t Training loss (single batch): 1.0172195434570312\n",
      "\t Training loss (single batch): 1.0312352180480957\n",
      "\t Training loss (single batch): 1.4251664876937866\n",
      "\t Training loss (single batch): 0.8164742588996887\n",
      "\t Training loss (single batch): 1.5995770692825317\n",
      "\t Training loss (single batch): 1.2027761936187744\n",
      "\t Training loss (single batch): 1.5251356363296509\n",
      "\t Training loss (single batch): 1.8817936182022095\n",
      "\t Training loss (single batch): 0.7683486938476562\n",
      "\t Training loss (single batch): 0.8395633101463318\n",
      "\t Training loss (single batch): 1.233978033065796\n",
      "\t Training loss (single batch): 0.7777275443077087\n",
      "\t Training loss (single batch): 1.4956990480422974\n",
      "\t Training loss (single batch): 1.2007958889007568\n",
      "\t Training loss (single batch): 1.2673295736312866\n",
      "\t Training loss (single batch): 0.8062506914138794\n",
      "\t Training loss (single batch): 0.9401205778121948\n",
      "\t Training loss (single batch): 1.1545614004135132\n",
      "\t Training loss (single batch): 1.649741291999817\n",
      "\t Training loss (single batch): 1.626483678817749\n",
      "\t Training loss (single batch): 1.244858980178833\n",
      "\t Training loss (single batch): 0.824661374092102\n",
      "\t Training loss (single batch): 1.162382960319519\n",
      "\t Training loss (single batch): 1.1212379932403564\n",
      "\t Training loss (single batch): 0.842258095741272\n",
      "\t Training loss (single batch): 1.3343743085861206\n",
      "\t Training loss (single batch): 1.0516788959503174\n",
      "\t Training loss (single batch): 0.9589743614196777\n",
      "\t Training loss (single batch): 1.0297579765319824\n",
      "\t Training loss (single batch): 0.9865067601203918\n",
      "\t Training loss (single batch): 0.8773072361946106\n",
      "\t Training loss (single batch): 1.2646634578704834\n",
      "\t Training loss (single batch): 1.1206046342849731\n",
      "\t Training loss (single batch): 1.009567379951477\n",
      "\t Training loss (single batch): 1.213260531425476\n",
      "\t Training loss (single batch): 1.165215015411377\n",
      "\t Training loss (single batch): 1.2313421964645386\n",
      "\t Training loss (single batch): 1.2359216213226318\n",
      "\t Training loss (single batch): 1.0094131231307983\n",
      "\t Training loss (single batch): 1.5153896808624268\n",
      "\t Training loss (single batch): 1.3003835678100586\n",
      "\t Training loss (single batch): 1.2951040267944336\n",
      "\t Training loss (single batch): 1.2820031642913818\n",
      "\t Training loss (single batch): 1.0881154537200928\n",
      "\t Training loss (single batch): 1.0901738405227661\n",
      "\t Training loss (single batch): 1.133273720741272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1831942796707153\n",
      "\t Training loss (single batch): 1.3423234224319458\n",
      "\t Training loss (single batch): 0.9602048397064209\n",
      "\t Training loss (single batch): 1.3155544996261597\n",
      "\t Training loss (single batch): 1.5009995698928833\n",
      "\t Training loss (single batch): 1.2929869890213013\n",
      "\t Training loss (single batch): 0.8941788077354431\n",
      "\t Training loss (single batch): 1.5087076425552368\n",
      "\t Training loss (single batch): 1.158103585243225\n",
      "\t Training loss (single batch): 1.1631218194961548\n",
      "\t Training loss (single batch): 1.2413160800933838\n",
      "\t Training loss (single batch): 0.804409384727478\n",
      "\t Training loss (single batch): 0.8650378584861755\n",
      "\t Training loss (single batch): 1.352845311164856\n",
      "\t Training loss (single batch): 1.175201177597046\n",
      "\t Training loss (single batch): 1.4585884809494019\n",
      "\t Training loss (single batch): 1.029816746711731\n",
      "\t Training loss (single batch): 1.1881309747695923\n",
      "\t Training loss (single batch): 1.5062347650527954\n",
      "\t Training loss (single batch): 1.9509097337722778\n",
      "\t Training loss (single batch): 1.2823140621185303\n",
      "\t Training loss (single batch): 0.8083389401435852\n",
      "\t Training loss (single batch): 0.956875205039978\n",
      "\t Training loss (single batch): 0.8417186141014099\n",
      "\t Training loss (single batch): 1.3212703466415405\n",
      "\t Training loss (single batch): 1.179244875907898\n",
      "\t Training loss (single batch): 1.267768383026123\n",
      "\t Training loss (single batch): 0.47576943039894104\n",
      "##################################\n",
      "## EPOCH 38\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0504810810089111\n",
      "\t Training loss (single batch): 1.6382906436920166\n",
      "\t Training loss (single batch): 1.3334236145019531\n",
      "\t Training loss (single batch): 1.0015990734100342\n",
      "\t Training loss (single batch): 0.9197403788566589\n",
      "\t Training loss (single batch): 1.0485618114471436\n",
      "\t Training loss (single batch): 1.0436888933181763\n",
      "\t Training loss (single batch): 1.5646467208862305\n",
      "\t Training loss (single batch): 1.1747087240219116\n",
      "\t Training loss (single batch): 1.0534073114395142\n",
      "\t Training loss (single batch): 1.3769346475601196\n",
      "\t Training loss (single batch): 0.6411570906639099\n",
      "\t Training loss (single batch): 2.021841287612915\n",
      "\t Training loss (single batch): 1.1990898847579956\n",
      "\t Training loss (single batch): 1.6541187763214111\n",
      "\t Training loss (single batch): 1.2526359558105469\n",
      "\t Training loss (single batch): 1.1813308000564575\n",
      "\t Training loss (single batch): 1.3185232877731323\n",
      "\t Training loss (single batch): 1.1730934381484985\n",
      "\t Training loss (single batch): 0.6831910014152527\n",
      "\t Training loss (single batch): 1.7005990743637085\n",
      "\t Training loss (single batch): 0.8896834850311279\n",
      "\t Training loss (single batch): 0.9189383387565613\n",
      "\t Training loss (single batch): 0.7628129124641418\n",
      "\t Training loss (single batch): 0.7867439389228821\n",
      "\t Training loss (single batch): 0.7816668748855591\n",
      "\t Training loss (single batch): 1.4870198965072632\n",
      "\t Training loss (single batch): 1.9408038854599\n",
      "\t Training loss (single batch): 1.2465341091156006\n",
      "\t Training loss (single batch): 1.6888065338134766\n",
      "\t Training loss (single batch): 1.7072807550430298\n",
      "\t Training loss (single batch): 1.9291213750839233\n",
      "\t Training loss (single batch): 0.7829967141151428\n",
      "\t Training loss (single batch): 1.103714942932129\n",
      "\t Training loss (single batch): 1.541178584098816\n",
      "\t Training loss (single batch): 1.3317829370498657\n",
      "\t Training loss (single batch): 0.8740524649620056\n",
      "\t Training loss (single batch): 0.8450264930725098\n",
      "\t Training loss (single batch): 1.4204000234603882\n",
      "\t Training loss (single batch): 1.5803879499435425\n",
      "\t Training loss (single batch): 0.9283472895622253\n",
      "\t Training loss (single batch): 1.3444099426269531\n",
      "\t Training loss (single batch): 1.828099012374878\n",
      "\t Training loss (single batch): 1.2789111137390137\n",
      "\t Training loss (single batch): 1.0021013021469116\n",
      "\t Training loss (single batch): 1.700424313545227\n",
      "\t Training loss (single batch): 0.9397508502006531\n",
      "\t Training loss (single batch): 1.0791985988616943\n",
      "\t Training loss (single batch): 1.5287864208221436\n",
      "\t Training loss (single batch): 1.1887309551239014\n",
      "\t Training loss (single batch): 1.4098199605941772\n",
      "\t Training loss (single batch): 1.3337280750274658\n",
      "\t Training loss (single batch): 1.1473561525344849\n",
      "\t Training loss (single batch): 1.1388014554977417\n",
      "\t Training loss (single batch): 1.3363581895828247\n",
      "\t Training loss (single batch): 0.840803325176239\n",
      "\t Training loss (single batch): 1.4971368312835693\n",
      "\t Training loss (single batch): 1.000477910041809\n",
      "\t Training loss (single batch): 0.8722522258758545\n",
      "\t Training loss (single batch): 0.8487034440040588\n",
      "\t Training loss (single batch): 1.3016752004623413\n",
      "\t Training loss (single batch): 1.3864731788635254\n",
      "\t Training loss (single batch): 0.873078465461731\n",
      "\t Training loss (single batch): 1.3500150442123413\n",
      "\t Training loss (single batch): 1.646898865699768\n",
      "\t Training loss (single batch): 1.1164724826812744\n",
      "\t Training loss (single batch): 1.1265069246292114\n",
      "\t Training loss (single batch): 1.1532539129257202\n",
      "\t Training loss (single batch): 0.9839025139808655\n",
      "\t Training loss (single batch): 1.0900814533233643\n",
      "\t Training loss (single batch): 1.475289225578308\n",
      "\t Training loss (single batch): 1.2005064487457275\n",
      "\t Training loss (single batch): 1.2519282102584839\n",
      "\t Training loss (single batch): 1.2249153852462769\n",
      "\t Training loss (single batch): 1.139291763305664\n",
      "\t Training loss (single batch): 0.9684467315673828\n",
      "\t Training loss (single batch): 1.2691349983215332\n",
      "\t Training loss (single batch): 1.171080231666565\n",
      "\t Training loss (single batch): 2.021296262741089\n",
      "\t Training loss (single batch): 1.0205084085464478\n",
      "\t Training loss (single batch): 1.4257559776306152\n",
      "\t Training loss (single batch): 1.6057085990905762\n",
      "\t Training loss (single batch): 1.5069514513015747\n",
      "\t Training loss (single batch): 1.001672387123108\n",
      "\t Training loss (single batch): 0.98417729139328\n",
      "\t Training loss (single batch): 1.1320418119430542\n",
      "\t Training loss (single batch): 0.8609297275543213\n",
      "\t Training loss (single batch): 1.3846243619918823\n",
      "\t Training loss (single batch): 1.2920262813568115\n",
      "\t Training loss (single batch): 1.057739496231079\n",
      "\t Training loss (single batch): 1.1384650468826294\n",
      "\t Training loss (single batch): 1.718030571937561\n",
      "\t Training loss (single batch): 1.0574629306793213\n",
      "\t Training loss (single batch): 0.9395178556442261\n",
      "\t Training loss (single batch): 1.1615337133407593\n",
      "\t Training loss (single batch): 0.9327265024185181\n",
      "\t Training loss (single batch): 1.1755688190460205\n",
      "\t Training loss (single batch): 0.9506335854530334\n",
      "\t Training loss (single batch): 1.0330828428268433\n",
      "\t Training loss (single batch): 1.424674153327942\n",
      "\t Training loss (single batch): 1.7913224697113037\n",
      "\t Training loss (single batch): 1.2067023515701294\n",
      "\t Training loss (single batch): 1.0882182121276855\n",
      "\t Training loss (single batch): 1.3896374702453613\n",
      "\t Training loss (single batch): 1.3998721837997437\n",
      "\t Training loss (single batch): 1.647916316986084\n",
      "\t Training loss (single batch): 0.9064083099365234\n",
      "\t Training loss (single batch): 1.2367111444473267\n",
      "\t Training loss (single batch): 1.3280861377716064\n",
      "\t Training loss (single batch): 1.273315191268921\n",
      "\t Training loss (single batch): 1.2189545631408691\n",
      "\t Training loss (single batch): 1.333337426185608\n",
      "\t Training loss (single batch): 1.4485687017440796\n",
      "\t Training loss (single batch): 1.2072978019714355\n",
      "\t Training loss (single batch): 1.1586782932281494\n",
      "\t Training loss (single batch): 1.0009765625\n",
      "\t Training loss (single batch): 1.02252197265625\n",
      "\t Training loss (single batch): 1.2481496334075928\n",
      "\t Training loss (single batch): 1.3389499187469482\n",
      "\t Training loss (single batch): 1.1818348169326782\n",
      "\t Training loss (single batch): 0.9780610799789429\n",
      "\t Training loss (single batch): 1.1756154298782349\n",
      "\t Training loss (single batch): 1.6484947204589844\n",
      "\t Training loss (single batch): 1.2018872499465942\n",
      "\t Training loss (single batch): 1.0966991186141968\n",
      "\t Training loss (single batch): 1.2688028812408447\n",
      "\t Training loss (single batch): 1.6353390216827393\n",
      "\t Training loss (single batch): 1.6526100635528564\n",
      "\t Training loss (single batch): 1.5087735652923584\n",
      "\t Training loss (single batch): 1.0837398767471313\n",
      "\t Training loss (single batch): 1.0732150077819824\n",
      "\t Training loss (single batch): 1.0683149099349976\n",
      "\t Training loss (single batch): 1.2496312856674194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1685620546340942\n",
      "\t Training loss (single batch): 1.116222620010376\n",
      "\t Training loss (single batch): 0.8991879224777222\n",
      "\t Training loss (single batch): 0.8918836116790771\n",
      "\t Training loss (single batch): 1.0816680192947388\n",
      "\t Training loss (single batch): 1.3919999599456787\n",
      "\t Training loss (single batch): 1.2187855243682861\n",
      "\t Training loss (single batch): 0.8746522068977356\n",
      "\t Training loss (single batch): 1.293666958808899\n",
      "\t Training loss (single batch): 1.34485924243927\n",
      "\t Training loss (single batch): 0.9887516498565674\n",
      "\t Training loss (single batch): 1.3692198991775513\n",
      "\t Training loss (single batch): 1.3421601057052612\n",
      "\t Training loss (single batch): 1.2677274942398071\n",
      "\t Training loss (single batch): 1.3226772546768188\n",
      "\t Training loss (single batch): 1.349877953529358\n",
      "\t Training loss (single batch): 0.9778734445571899\n",
      "\t Training loss (single batch): 1.1428316831588745\n",
      "\t Training loss (single batch): 0.9516497254371643\n",
      "\t Training loss (single batch): 1.3219338655471802\n",
      "\t Training loss (single batch): 1.5914502143859863\n",
      "\t Training loss (single batch): 1.0157976150512695\n",
      "\t Training loss (single batch): 1.537496566772461\n",
      "\t Training loss (single batch): 0.988097071647644\n",
      "\t Training loss (single batch): 1.7772501707077026\n",
      "\t Training loss (single batch): 1.2603522539138794\n",
      "\t Training loss (single batch): 1.329586148262024\n",
      "\t Training loss (single batch): 1.0867189168930054\n",
      "\t Training loss (single batch): 1.4951118230819702\n",
      "\t Training loss (single batch): 1.1640971899032593\n",
      "\t Training loss (single batch): 1.632428526878357\n",
      "\t Training loss (single batch): 1.4243192672729492\n",
      "\t Training loss (single batch): 1.0364949703216553\n",
      "\t Training loss (single batch): 1.2109770774841309\n",
      "\t Training loss (single batch): 1.2716904878616333\n",
      "\t Training loss (single batch): 1.111793875694275\n",
      "\t Training loss (single batch): 1.3218388557434082\n",
      "\t Training loss (single batch): 1.452948808670044\n",
      "\t Training loss (single batch): 1.3898069858551025\n",
      "\t Training loss (single batch): 1.1171422004699707\n",
      "\t Training loss (single batch): 1.3061500787734985\n",
      "\t Training loss (single batch): 1.3682339191436768\n",
      "\t Training loss (single batch): 1.3728688955307007\n",
      "\t Training loss (single batch): 1.0764511823654175\n",
      "\t Training loss (single batch): 1.4461212158203125\n",
      "\t Training loss (single batch): 1.1263757944107056\n",
      "\t Training loss (single batch): 1.6311441659927368\n",
      "\t Training loss (single batch): 1.448400616645813\n",
      "\t Training loss (single batch): 1.1359542608261108\n",
      "\t Training loss (single batch): 1.00861394405365\n",
      "\t Training loss (single batch): 1.183704137802124\n",
      "\t Training loss (single batch): 1.2592133283615112\n",
      "\t Training loss (single batch): 1.0393340587615967\n",
      "\t Training loss (single batch): 0.9741390943527222\n",
      "\t Training loss (single batch): 0.853466272354126\n",
      "\t Training loss (single batch): 1.2128145694732666\n",
      "\t Training loss (single batch): 1.536246418952942\n",
      "\t Training loss (single batch): 0.8089346885681152\n",
      "\t Training loss (single batch): 1.4726231098175049\n",
      "\t Training loss (single batch): 1.5709195137023926\n",
      "\t Training loss (single batch): 0.9646857380867004\n",
      "\t Training loss (single batch): 1.285887360572815\n",
      "\t Training loss (single batch): 1.4701173305511475\n",
      "\t Training loss (single batch): 1.4106966257095337\n",
      "\t Training loss (single batch): 1.2606265544891357\n",
      "\t Training loss (single batch): 1.1523809432983398\n",
      "\t Training loss (single batch): 1.4880796670913696\n",
      "\t Training loss (single batch): 1.67681086063385\n",
      "\t Training loss (single batch): 1.1276520490646362\n",
      "\t Training loss (single batch): 1.2174193859100342\n",
      "\t Training loss (single batch): 0.9276920557022095\n",
      "\t Training loss (single batch): 0.6756190061569214\n",
      "\t Training loss (single batch): 1.431443691253662\n",
      "\t Training loss (single batch): 0.5786100625991821\n",
      "\t Training loss (single batch): 0.8416570425033569\n",
      "\t Training loss (single batch): 1.412124752998352\n",
      "\t Training loss (single batch): 1.6573048830032349\n",
      "\t Training loss (single batch): 1.005741000175476\n",
      "\t Training loss (single batch): 1.1219769716262817\n",
      "\t Training loss (single batch): 0.9344284534454346\n",
      "\t Training loss (single batch): 1.2627155780792236\n",
      "\t Training loss (single batch): 0.6898983120918274\n",
      "\t Training loss (single batch): 1.188375473022461\n",
      "\t Training loss (single batch): 1.533643364906311\n",
      "\t Training loss (single batch): 1.0920896530151367\n",
      "\t Training loss (single batch): 0.9173629283905029\n",
      "\t Training loss (single batch): 1.2230545282363892\n",
      "\t Training loss (single batch): 1.5686872005462646\n",
      "\t Training loss (single batch): 1.1509991884231567\n",
      "\t Training loss (single batch): 1.8177261352539062\n",
      "\t Training loss (single batch): 0.9980694651603699\n",
      "\t Training loss (single batch): 1.2449333667755127\n",
      "\t Training loss (single batch): 1.4084067344665527\n",
      "\t Training loss (single batch): 1.418991208076477\n",
      "\t Training loss (single batch): 1.142374038696289\n",
      "\t Training loss (single batch): 1.1643038988113403\n",
      "\t Training loss (single batch): 1.1445939540863037\n",
      "\t Training loss (single batch): 1.2933412790298462\n",
      "\t Training loss (single batch): 1.1691126823425293\n",
      "\t Training loss (single batch): 1.1884628534317017\n",
      "\t Training loss (single batch): 0.9421553611755371\n",
      "\t Training loss (single batch): 1.037642002105713\n",
      "\t Training loss (single batch): 1.2070344686508179\n",
      "\t Training loss (single batch): 1.391255497932434\n",
      "\t Training loss (single batch): 1.1388425827026367\n",
      "\t Training loss (single batch): 0.9768858551979065\n",
      "\t Training loss (single batch): 1.2310658693313599\n",
      "\t Training loss (single batch): 1.692513346672058\n",
      "\t Training loss (single batch): 1.1358040571212769\n",
      "\t Training loss (single batch): 1.3416229486465454\n",
      "\t Training loss (single batch): 1.082288384437561\n",
      "\t Training loss (single batch): 1.2627005577087402\n",
      "\t Training loss (single batch): 1.0833266973495483\n",
      "\t Training loss (single batch): 1.3914457559585571\n",
      "\t Training loss (single batch): 0.873729944229126\n",
      "\t Training loss (single batch): 1.4105501174926758\n",
      "\t Training loss (single batch): 1.1393375396728516\n",
      "\t Training loss (single batch): 1.0758196115493774\n",
      "\t Training loss (single batch): 1.5194408893585205\n",
      "\t Training loss (single batch): 1.2005982398986816\n",
      "\t Training loss (single batch): 1.6657048463821411\n",
      "\t Training loss (single batch): 1.168093204498291\n",
      "\t Training loss (single batch): 1.1506922245025635\n",
      "\t Training loss (single batch): 1.2229217290878296\n",
      "\t Training loss (single batch): 1.4649500846862793\n",
      "\t Training loss (single batch): 1.6466381549835205\n",
      "\t Training loss (single batch): 1.0617529153823853\n",
      "\t Training loss (single batch): 1.404914140701294\n",
      "\t Training loss (single batch): 1.1534969806671143\n",
      "\t Training loss (single batch): 0.9043724536895752\n",
      "\t Training loss (single batch): 1.56435227394104\n",
      "\t Training loss (single batch): 1.02989661693573\n",
      "\t Training loss (single batch): 1.225706934928894\n",
      "\t Training loss (single batch): 1.5716019868850708\n",
      "\t Training loss (single batch): 1.3235275745391846\n",
      "\t Training loss (single batch): 0.8545113801956177\n",
      "\t Training loss (single batch): 1.0486482381820679\n",
      "\t Training loss (single batch): 0.7825033664703369\n",
      "\t Training loss (single batch): 1.2571402788162231\n",
      "\t Training loss (single batch): 0.9507210850715637\n",
      "\t Training loss (single batch): 1.6066547632217407\n",
      "\t Training loss (single batch): 1.48451566696167\n",
      "\t Training loss (single batch): 1.088844656944275\n",
      "\t Training loss (single batch): 1.331534504890442\n",
      "\t Training loss (single batch): 1.8253265619277954\n",
      "\t Training loss (single batch): 1.1945327520370483\n",
      "\t Training loss (single batch): 1.1593632698059082\n",
      "\t Training loss (single batch): 1.080357551574707\n",
      "\t Training loss (single batch): 0.9917370676994324\n",
      "\t Training loss (single batch): 1.7210314273834229\n",
      "\t Training loss (single batch): 1.3344708681106567\n",
      "\t Training loss (single batch): 1.5439255237579346\n",
      "\t Training loss (single batch): 1.2460347414016724\n",
      "\t Training loss (single batch): 1.2823745012283325\n",
      "\t Training loss (single batch): 1.4722474813461304\n",
      "\t Training loss (single batch): 1.2652552127838135\n",
      "\t Training loss (single batch): 0.950384795665741\n",
      "\t Training loss (single batch): 1.478066325187683\n",
      "\t Training loss (single batch): 1.744551181793213\n",
      "\t Training loss (single batch): 1.314462423324585\n",
      "\t Training loss (single batch): 0.9253107905387878\n",
      "\t Training loss (single batch): 1.6491369009017944\n",
      "\t Training loss (single batch): 1.0568021535873413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2002044916152954\n",
      "\t Training loss (single batch): 0.9973870515823364\n",
      "\t Training loss (single batch): 1.1798979043960571\n",
      "\t Training loss (single batch): 1.122205376625061\n",
      "\t Training loss (single batch): 1.4810835123062134\n",
      "\t Training loss (single batch): 1.1615921258926392\n",
      "\t Training loss (single batch): 1.1904515027999878\n",
      "\t Training loss (single batch): 0.7072263956069946\n",
      "\t Training loss (single batch): 0.7345409393310547\n",
      "\t Training loss (single batch): 1.2703135013580322\n",
      "\t Training loss (single batch): 0.9596896767616272\n",
      "\t Training loss (single batch): 1.547481656074524\n",
      "\t Training loss (single batch): 1.009459137916565\n",
      "\t Training loss (single batch): 1.024404525756836\n",
      "\t Training loss (single batch): 1.6456576585769653\n",
      "\t Training loss (single batch): 1.3573548793792725\n",
      "\t Training loss (single batch): 0.8319149017333984\n",
      "\t Training loss (single batch): 1.0805809497833252\n",
      "\t Training loss (single batch): 1.028653621673584\n",
      "\t Training loss (single batch): 0.9877749681472778\n",
      "\t Training loss (single batch): 2.157707452774048\n",
      "##################################\n",
      "## EPOCH 39\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5562206506729126\n",
      "\t Training loss (single batch): 1.1031100749969482\n",
      "\t Training loss (single batch): 1.2187962532043457\n",
      "\t Training loss (single batch): 0.9992885589599609\n",
      "\t Training loss (single batch): 1.02694833278656\n",
      "\t Training loss (single batch): 1.1789137125015259\n",
      "\t Training loss (single batch): 1.4747419357299805\n",
      "\t Training loss (single batch): 1.5744078159332275\n",
      "\t Training loss (single batch): 1.2314153909683228\n",
      "\t Training loss (single batch): 1.4343217611312866\n",
      "\t Training loss (single batch): 1.5844563245773315\n",
      "\t Training loss (single batch): 1.226285457611084\n",
      "\t Training loss (single batch): 1.0060971975326538\n",
      "\t Training loss (single batch): 1.0958658456802368\n",
      "\t Training loss (single batch): 1.2481491565704346\n",
      "\t Training loss (single batch): 1.0508688688278198\n",
      "\t Training loss (single batch): 1.5712456703186035\n",
      "\t Training loss (single batch): 1.2312370538711548\n",
      "\t Training loss (single batch): 1.43684983253479\n",
      "\t Training loss (single batch): 1.0005134344100952\n",
      "\t Training loss (single batch): 1.309936761856079\n",
      "\t Training loss (single batch): 0.6629989743232727\n",
      "\t Training loss (single batch): 1.4596197605133057\n",
      "\t Training loss (single batch): 0.9837485551834106\n",
      "\t Training loss (single batch): 1.3524408340454102\n",
      "\t Training loss (single batch): 1.214728593826294\n",
      "\t Training loss (single batch): 1.1599822044372559\n",
      "\t Training loss (single batch): 1.0938345193862915\n",
      "\t Training loss (single batch): 1.147493839263916\n",
      "\t Training loss (single batch): 1.2481955289840698\n",
      "\t Training loss (single batch): 1.0021154880523682\n",
      "\t Training loss (single batch): 1.3329088687896729\n",
      "\t Training loss (single batch): 1.194427251815796\n",
      "\t Training loss (single batch): 1.018654704093933\n",
      "\t Training loss (single batch): 1.3076655864715576\n",
      "\t Training loss (single batch): 1.5193073749542236\n",
      "\t Training loss (single batch): 1.0771504640579224\n",
      "\t Training loss (single batch): 1.6034048795700073\n",
      "\t Training loss (single batch): 0.7214087247848511\n",
      "\t Training loss (single batch): 1.0635790824890137\n",
      "\t Training loss (single batch): 0.8344006538391113\n",
      "\t Training loss (single batch): 1.3646996021270752\n",
      "\t Training loss (single batch): 1.3367074728012085\n",
      "\t Training loss (single batch): 1.1214632987976074\n",
      "\t Training loss (single batch): 1.2718935012817383\n",
      "\t Training loss (single batch): 1.1083275079727173\n",
      "\t Training loss (single batch): 1.1841752529144287\n",
      "\t Training loss (single batch): 1.4058398008346558\n",
      "\t Training loss (single batch): 1.0036336183547974\n",
      "\t Training loss (single batch): 1.1603527069091797\n",
      "\t Training loss (single batch): 1.0969678163528442\n",
      "\t Training loss (single batch): 1.1041193008422852\n",
      "\t Training loss (single batch): 1.5002162456512451\n",
      "\t Training loss (single batch): 1.048190951347351\n",
      "\t Training loss (single batch): 1.2480958700180054\n",
      "\t Training loss (single batch): 1.8676002025604248\n",
      "\t Training loss (single batch): 1.0863492488861084\n",
      "\t Training loss (single batch): 1.1930439472198486\n",
      "\t Training loss (single batch): 0.6601894497871399\n",
      "\t Training loss (single batch): 1.171199917793274\n",
      "\t Training loss (single batch): 0.898423433303833\n",
      "\t Training loss (single batch): 1.5825014114379883\n",
      "\t Training loss (single batch): 0.8511711955070496\n",
      "\t Training loss (single batch): 0.9835652112960815\n",
      "\t Training loss (single batch): 1.10824453830719\n",
      "\t Training loss (single batch): 1.2093628644943237\n",
      "\t Training loss (single batch): 1.0240484476089478\n",
      "\t Training loss (single batch): 1.0959149599075317\n",
      "\t Training loss (single batch): 1.368024230003357\n",
      "\t Training loss (single batch): 0.9732968807220459\n",
      "\t Training loss (single batch): 1.5213871002197266\n",
      "\t Training loss (single batch): 1.1305365562438965\n",
      "\t Training loss (single batch): 0.8798933625221252\n",
      "\t Training loss (single batch): 1.3656506538391113\n",
      "\t Training loss (single batch): 1.507821798324585\n",
      "\t Training loss (single batch): 1.1934797763824463\n",
      "\t Training loss (single batch): 1.2545342445373535\n",
      "\t Training loss (single batch): 0.7631414532661438\n",
      "\t Training loss (single batch): 1.412174105644226\n",
      "\t Training loss (single batch): 0.9951048493385315\n",
      "\t Training loss (single batch): 1.8499352931976318\n",
      "\t Training loss (single batch): 0.993216872215271\n",
      "\t Training loss (single batch): 1.2345446348190308\n",
      "\t Training loss (single batch): 1.2243459224700928\n",
      "\t Training loss (single batch): 0.9994146823883057\n",
      "\t Training loss (single batch): 1.4472304582595825\n",
      "\t Training loss (single batch): 1.4772465229034424\n",
      "\t Training loss (single batch): 1.4792720079421997\n",
      "\t Training loss (single batch): 1.0998743772506714\n",
      "\t Training loss (single batch): 0.8397365212440491\n",
      "\t Training loss (single batch): 1.389109492301941\n",
      "\t Training loss (single batch): 1.6230796575546265\n",
      "\t Training loss (single batch): 1.295570731163025\n",
      "\t Training loss (single batch): 1.0203109979629517\n",
      "\t Training loss (single batch): 1.110309362411499\n",
      "\t Training loss (single batch): 1.1980429887771606\n",
      "\t Training loss (single batch): 1.3577293157577515\n",
      "\t Training loss (single batch): 0.9979315400123596\n",
      "\t Training loss (single batch): 1.471221685409546\n",
      "\t Training loss (single batch): 1.003847360610962\n",
      "\t Training loss (single batch): 0.9313483834266663\n",
      "\t Training loss (single batch): 1.1978161334991455\n",
      "\t Training loss (single batch): 1.3592917919158936\n",
      "\t Training loss (single batch): 1.4361960887908936\n",
      "\t Training loss (single batch): 1.4430514574050903\n",
      "\t Training loss (single batch): 1.4648438692092896\n",
      "\t Training loss (single batch): 1.0523369312286377\n",
      "\t Training loss (single batch): 1.249776840209961\n",
      "\t Training loss (single batch): 1.0801700353622437\n",
      "\t Training loss (single batch): 1.2297455072402954\n",
      "\t Training loss (single batch): 1.3219122886657715\n",
      "\t Training loss (single batch): 1.0418081283569336\n",
      "\t Training loss (single batch): 1.4433648586273193\n",
      "\t Training loss (single batch): 1.1511954069137573\n",
      "\t Training loss (single batch): 1.5342562198638916\n",
      "\t Training loss (single batch): 0.9970272183418274\n",
      "\t Training loss (single batch): 0.9963451027870178\n",
      "\t Training loss (single batch): 1.5509440898895264\n",
      "\t Training loss (single batch): 1.051639199256897\n",
      "\t Training loss (single batch): 0.8475072383880615\n",
      "\t Training loss (single batch): 1.2187414169311523\n",
      "\t Training loss (single batch): 1.4559626579284668\n",
      "\t Training loss (single batch): 1.2770646810531616\n",
      "\t Training loss (single batch): 1.3530502319335938\n",
      "\t Training loss (single batch): 1.2613064050674438\n",
      "\t Training loss (single batch): 1.5308412313461304\n",
      "\t Training loss (single batch): 1.0425403118133545\n",
      "\t Training loss (single batch): 1.0504883527755737\n",
      "\t Training loss (single batch): 0.9650508761405945\n",
      "\t Training loss (single batch): 1.1618118286132812\n",
      "\t Training loss (single batch): 0.5630927681922913\n",
      "\t Training loss (single batch): 1.086328387260437\n",
      "\t Training loss (single batch): 1.5319024324417114\n",
      "\t Training loss (single batch): 1.2341525554656982\n",
      "\t Training loss (single batch): 1.520111083984375\n",
      "\t Training loss (single batch): 0.9604954719543457\n",
      "\t Training loss (single batch): 1.1293095350265503\n",
      "\t Training loss (single batch): 0.9298307299613953\n",
      "\t Training loss (single batch): 1.316667079925537\n",
      "\t Training loss (single batch): 0.822259783744812\n",
      "\t Training loss (single batch): 1.2868038415908813\n",
      "\t Training loss (single batch): 1.592531442642212\n",
      "\t Training loss (single batch): 1.4381178617477417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0948299169540405\n",
      "\t Training loss (single batch): 1.0153003931045532\n",
      "\t Training loss (single batch): 0.9166851043701172\n",
      "\t Training loss (single batch): 1.0499188899993896\n",
      "\t Training loss (single batch): 1.228541612625122\n",
      "\t Training loss (single batch): 1.525620937347412\n",
      "\t Training loss (single batch): 1.456624150276184\n",
      "\t Training loss (single batch): 1.0338696241378784\n",
      "\t Training loss (single batch): 0.9384515881538391\n",
      "\t Training loss (single batch): 1.535453200340271\n",
      "\t Training loss (single batch): 1.3413417339324951\n",
      "\t Training loss (single batch): 1.5572272539138794\n",
      "\t Training loss (single batch): 1.0519448518753052\n",
      "\t Training loss (single batch): 1.0103895664215088\n",
      "\t Training loss (single batch): 1.7143168449401855\n",
      "\t Training loss (single batch): 0.889957845211029\n",
      "\t Training loss (single batch): 1.2518231868743896\n",
      "\t Training loss (single batch): 0.6963070034980774\n",
      "\t Training loss (single batch): 1.4119058847427368\n",
      "\t Training loss (single batch): 1.3387974500656128\n",
      "\t Training loss (single batch): 1.121469497680664\n",
      "\t Training loss (single batch): 1.6390515565872192\n",
      "\t Training loss (single batch): 0.835706889629364\n",
      "\t Training loss (single batch): 1.0747638940811157\n",
      "\t Training loss (single batch): 1.147900104522705\n",
      "\t Training loss (single batch): 1.1652830839157104\n",
      "\t Training loss (single batch): 1.0179412364959717\n",
      "\t Training loss (single batch): 0.8050553202629089\n",
      "\t Training loss (single batch): 1.0578972101211548\n",
      "\t Training loss (single batch): 0.9294405579566956\n",
      "\t Training loss (single batch): 0.8784741163253784\n",
      "\t Training loss (single batch): 1.0900278091430664\n",
      "\t Training loss (single batch): 1.0469518899917603\n",
      "\t Training loss (single batch): 1.3322992324829102\n",
      "\t Training loss (single batch): 1.517570972442627\n",
      "\t Training loss (single batch): 1.3444535732269287\n",
      "\t Training loss (single batch): 1.5620402097702026\n",
      "\t Training loss (single batch): 1.2098979949951172\n",
      "\t Training loss (single batch): 1.405237078666687\n",
      "\t Training loss (single batch): 1.5502402782440186\n",
      "\t Training loss (single batch): 1.1726032495498657\n",
      "\t Training loss (single batch): 1.235231876373291\n",
      "\t Training loss (single batch): 1.08085298538208\n",
      "\t Training loss (single batch): 1.8351117372512817\n",
      "\t Training loss (single batch): 1.0214892625808716\n",
      "\t Training loss (single batch): 1.1197946071624756\n",
      "\t Training loss (single batch): 1.3219406604766846\n",
      "\t Training loss (single batch): 1.0796481370925903\n",
      "\t Training loss (single batch): 0.9279972314834595\n",
      "\t Training loss (single batch): 0.9623234272003174\n",
      "\t Training loss (single batch): 1.1246681213378906\n",
      "\t Training loss (single batch): 0.8517444729804993\n",
      "\t Training loss (single batch): 1.570152759552002\n",
      "\t Training loss (single batch): 1.6308785676956177\n",
      "\t Training loss (single batch): 1.2581591606140137\n",
      "\t Training loss (single batch): 1.4330062866210938\n",
      "\t Training loss (single batch): 1.2445704936981201\n",
      "\t Training loss (single batch): 1.021458387374878\n",
      "\t Training loss (single batch): 1.4120664596557617\n",
      "\t Training loss (single batch): 0.9204183220863342\n",
      "\t Training loss (single batch): 1.2255820035934448\n",
      "\t Training loss (single batch): 1.1351780891418457\n",
      "\t Training loss (single batch): 1.1424823999404907\n",
      "\t Training loss (single batch): 1.2432645559310913\n",
      "\t Training loss (single batch): 1.0838980674743652\n",
      "\t Training loss (single batch): 0.8451156616210938\n",
      "\t Training loss (single batch): 1.1572579145431519\n",
      "\t Training loss (single batch): 1.219832420349121\n",
      "\t Training loss (single batch): 1.3016164302825928\n",
      "\t Training loss (single batch): 1.3256652355194092\n",
      "\t Training loss (single batch): 1.5207444429397583\n",
      "\t Training loss (single batch): 0.8727644681930542\n",
      "\t Training loss (single batch): 1.1151347160339355\n",
      "\t Training loss (single batch): 1.0192192792892456\n",
      "\t Training loss (single batch): 0.8158237934112549\n",
      "\t Training loss (single batch): 1.5081068277359009\n",
      "\t Training loss (single batch): 1.2084248065948486\n",
      "\t Training loss (single batch): 0.7411233186721802\n",
      "\t Training loss (single batch): 1.0930982828140259\n",
      "\t Training loss (single batch): 1.0431569814682007\n",
      "\t Training loss (single batch): 1.5903812646865845\n",
      "\t Training loss (single batch): 1.4813156127929688\n",
      "\t Training loss (single batch): 1.1118749380111694\n",
      "\t Training loss (single batch): 1.413989782333374\n",
      "\t Training loss (single batch): 1.0475521087646484\n",
      "\t Training loss (single batch): 1.0145677328109741\n",
      "\t Training loss (single batch): 0.8781827688217163\n",
      "\t Training loss (single batch): 1.0288572311401367\n",
      "\t Training loss (single batch): 1.3101861476898193\n",
      "\t Training loss (single batch): 1.2727113962173462\n",
      "\t Training loss (single batch): 1.040576457977295\n",
      "\t Training loss (single batch): 1.1099990606307983\n",
      "\t Training loss (single batch): 1.3231934309005737\n",
      "\t Training loss (single batch): 1.00119149684906\n",
      "\t Training loss (single batch): 0.9332709908485413\n",
      "\t Training loss (single batch): 1.354982852935791\n",
      "\t Training loss (single batch): 1.0891988277435303\n",
      "\t Training loss (single batch): 1.1072564125061035\n",
      "\t Training loss (single batch): 1.4095394611358643\n",
      "\t Training loss (single batch): 1.2522521018981934\n",
      "\t Training loss (single batch): 1.375357747077942\n",
      "\t Training loss (single batch): 0.9845407009124756\n",
      "\t Training loss (single batch): 1.2478175163269043\n",
      "\t Training loss (single batch): 1.1559855937957764\n",
      "\t Training loss (single batch): 1.0659008026123047\n",
      "\t Training loss (single batch): 1.2522112131118774\n",
      "\t Training loss (single batch): 1.4412567615509033\n",
      "\t Training loss (single batch): 1.012150526046753\n",
      "\t Training loss (single batch): 1.4038102626800537\n",
      "\t Training loss (single batch): 1.0645891427993774\n",
      "\t Training loss (single batch): 1.342789888381958\n",
      "\t Training loss (single batch): 0.9010460376739502\n",
      "\t Training loss (single batch): 1.658141016960144\n",
      "\t Training loss (single batch): 1.1767187118530273\n",
      "\t Training loss (single batch): 0.646654486656189\n",
      "\t Training loss (single batch): 1.0660896301269531\n",
      "\t Training loss (single batch): 1.070694923400879\n",
      "\t Training loss (single batch): 1.3052600622177124\n",
      "\t Training loss (single batch): 1.2926864624023438\n",
      "\t Training loss (single batch): 1.3143781423568726\n",
      "\t Training loss (single batch): 1.653733730316162\n",
      "\t Training loss (single batch): 1.303421974182129\n",
      "\t Training loss (single batch): 1.4440486431121826\n",
      "\t Training loss (single batch): 0.7988994717597961\n",
      "\t Training loss (single batch): 1.5995997190475464\n",
      "\t Training loss (single batch): 0.949800431728363\n",
      "\t Training loss (single batch): 1.08103609085083\n",
      "\t Training loss (single batch): 1.1808401346206665\n",
      "\t Training loss (single batch): 1.0318793058395386\n",
      "\t Training loss (single batch): 1.028256893157959\n",
      "\t Training loss (single batch): 1.1338262557983398\n",
      "\t Training loss (single batch): 1.4747318029403687\n",
      "\t Training loss (single batch): 1.0259112119674683\n",
      "\t Training loss (single batch): 1.3398741483688354\n",
      "\t Training loss (single batch): 1.4287141561508179\n",
      "\t Training loss (single batch): 0.910919189453125\n",
      "\t Training loss (single batch): 1.382994532585144\n",
      "\t Training loss (single batch): 0.9409443140029907\n",
      "\t Training loss (single batch): 1.3649287223815918\n",
      "\t Training loss (single batch): 1.2584985494613647\n",
      "\t Training loss (single batch): 1.3262325525283813\n",
      "\t Training loss (single batch): 1.7801792621612549\n",
      "\t Training loss (single batch): 1.3399585485458374\n",
      "\t Training loss (single batch): 1.0704163312911987\n",
      "\t Training loss (single batch): 1.262027382850647\n",
      "\t Training loss (single batch): 1.3194435834884644\n",
      "\t Training loss (single batch): 1.2187434434890747\n",
      "\t Training loss (single batch): 1.3923496007919312\n",
      "\t Training loss (single batch): 1.3468352556228638\n",
      "\t Training loss (single batch): 1.0830190181732178\n",
      "\t Training loss (single batch): 1.423208236694336\n",
      "\t Training loss (single batch): 1.079148769378662\n",
      "\t Training loss (single batch): 1.2050046920776367\n",
      "\t Training loss (single batch): 1.2132595777511597\n",
      "\t Training loss (single batch): 0.8839589953422546\n",
      "\t Training loss (single batch): 1.361617088317871\n",
      "\t Training loss (single batch): 0.8208339810371399\n",
      "\t Training loss (single batch): 1.009556770324707\n",
      "\t Training loss (single batch): 1.4180859327316284\n",
      "\t Training loss (single batch): 0.8627965450286865\n",
      "\t Training loss (single batch): 1.650976538658142\n",
      "\t Training loss (single batch): 1.10491943359375\n",
      "\t Training loss (single batch): 1.1877416372299194\n",
      "\t Training loss (single batch): 1.2271000146865845\n",
      "\t Training loss (single batch): 0.7664430737495422\n",
      "\t Training loss (single batch): 1.1198920011520386\n",
      "\t Training loss (single batch): 0.6089682579040527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1972103118896484\n",
      "\t Training loss (single batch): 1.1627483367919922\n",
      "\t Training loss (single batch): 1.543684482574463\n",
      "\t Training loss (single batch): 0.9490031003952026\n",
      "\t Training loss (single batch): 1.0282983779907227\n",
      "\t Training loss (single batch): 1.063118577003479\n",
      "\t Training loss (single batch): 1.3106921911239624\n",
      "##################################\n",
      "## EPOCH 40\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1781649589538574\n",
      "\t Training loss (single batch): 1.3239763975143433\n",
      "\t Training loss (single batch): 1.3312890529632568\n",
      "\t Training loss (single batch): 1.2264724969863892\n",
      "\t Training loss (single batch): 0.9338144063949585\n",
      "\t Training loss (single batch): 1.2191370725631714\n",
      "\t Training loss (single batch): 1.4749953746795654\n",
      "\t Training loss (single batch): 1.3613741397857666\n",
      "\t Training loss (single batch): 1.0951875448226929\n",
      "\t Training loss (single batch): 0.7914680242538452\n",
      "\t Training loss (single batch): 1.0600600242614746\n",
      "\t Training loss (single batch): 1.6550240516662598\n",
      "\t Training loss (single batch): 0.9053035378456116\n",
      "\t Training loss (single batch): 1.4529908895492554\n",
      "\t Training loss (single batch): 1.3233414888381958\n",
      "\t Training loss (single batch): 1.1650289297103882\n",
      "\t Training loss (single batch): 1.2573885917663574\n",
      "\t Training loss (single batch): 1.0702458620071411\n",
      "\t Training loss (single batch): 0.9650354981422424\n",
      "\t Training loss (single batch): 1.2891287803649902\n",
      "\t Training loss (single batch): 1.3022807836532593\n",
      "\t Training loss (single batch): 0.7404987812042236\n",
      "\t Training loss (single batch): 0.9599149227142334\n",
      "\t Training loss (single batch): 0.8362345099449158\n",
      "\t Training loss (single batch): 1.060150384902954\n",
      "\t Training loss (single batch): 1.4405789375305176\n",
      "\t Training loss (single batch): 1.09373140335083\n",
      "\t Training loss (single batch): 0.8698161840438843\n",
      "\t Training loss (single batch): 0.748928427696228\n",
      "\t Training loss (single batch): 1.3671035766601562\n",
      "\t Training loss (single batch): 0.7705779075622559\n",
      "\t Training loss (single batch): 1.2554759979248047\n",
      "\t Training loss (single batch): 1.1275928020477295\n",
      "\t Training loss (single batch): 1.325488805770874\n",
      "\t Training loss (single batch): 0.9686132669448853\n",
      "\t Training loss (single batch): 0.8218134641647339\n",
      "\t Training loss (single batch): 0.8965610861778259\n",
      "\t Training loss (single batch): 1.3683117628097534\n",
      "\t Training loss (single batch): 1.0205771923065186\n",
      "\t Training loss (single batch): 1.5158765316009521\n",
      "\t Training loss (single batch): 1.730594277381897\n",
      "\t Training loss (single batch): 1.2880691289901733\n",
      "\t Training loss (single batch): 1.16530179977417\n",
      "\t Training loss (single batch): 1.1465667486190796\n",
      "\t Training loss (single batch): 1.5720747709274292\n",
      "\t Training loss (single batch): 1.1273751258850098\n",
      "\t Training loss (single batch): 1.3755725622177124\n",
      "\t Training loss (single batch): 1.3878833055496216\n",
      "\t Training loss (single batch): 1.5144340991973877\n",
      "\t Training loss (single batch): 0.8803194761276245\n",
      "\t Training loss (single batch): 1.083878517150879\n",
      "\t Training loss (single batch): 0.980121910572052\n",
      "\t Training loss (single batch): 1.2440465688705444\n",
      "\t Training loss (single batch): 1.1636837720870972\n",
      "\t Training loss (single batch): 1.2748886346817017\n",
      "\t Training loss (single batch): 1.6174286603927612\n",
      "\t Training loss (single batch): 1.0819766521453857\n",
      "\t Training loss (single batch): 1.5470271110534668\n",
      "\t Training loss (single batch): 1.7379177808761597\n",
      "\t Training loss (single batch): 1.1512901782989502\n",
      "\t Training loss (single batch): 1.0831208229064941\n",
      "\t Training loss (single batch): 1.2306429147720337\n",
      "\t Training loss (single batch): 1.212241530418396\n",
      "\t Training loss (single batch): 1.1803611516952515\n",
      "\t Training loss (single batch): 1.8384333848953247\n",
      "\t Training loss (single batch): 1.6463582515716553\n",
      "\t Training loss (single batch): 1.1417256593704224\n",
      "\t Training loss (single batch): 1.663187861442566\n",
      "\t Training loss (single batch): 1.7584227323532104\n",
      "\t Training loss (single batch): 1.1473934650421143\n",
      "\t Training loss (single batch): 1.385440468788147\n",
      "\t Training loss (single batch): 1.3136553764343262\n",
      "\t Training loss (single batch): 1.154320240020752\n",
      "\t Training loss (single batch): 1.2486586570739746\n",
      "\t Training loss (single batch): 1.1798975467681885\n",
      "\t Training loss (single batch): 1.5102530717849731\n",
      "\t Training loss (single batch): 1.3232437372207642\n",
      "\t Training loss (single batch): 1.0245921611785889\n",
      "\t Training loss (single batch): 1.464264154434204\n",
      "\t Training loss (single batch): 0.862592339515686\n",
      "\t Training loss (single batch): 0.9724099636077881\n",
      "\t Training loss (single batch): 1.200015902519226\n",
      "\t Training loss (single batch): 1.2184113264083862\n",
      "\t Training loss (single batch): 1.0734046697616577\n",
      "\t Training loss (single batch): 1.9260426759719849\n",
      "\t Training loss (single batch): 1.0021944046020508\n",
      "\t Training loss (single batch): 1.0696444511413574\n",
      "\t Training loss (single batch): 1.0887502431869507\n",
      "\t Training loss (single batch): 1.994025707244873\n",
      "\t Training loss (single batch): 1.3774832487106323\n",
      "\t Training loss (single batch): 0.8423213958740234\n",
      "\t Training loss (single batch): 1.29050612449646\n",
      "\t Training loss (single batch): 1.1691155433654785\n",
      "\t Training loss (single batch): 1.323128342628479\n",
      "\t Training loss (single batch): 1.0366188287734985\n",
      "\t Training loss (single batch): 0.9452667236328125\n",
      "\t Training loss (single batch): 1.8474785089492798\n",
      "\t Training loss (single batch): 1.1066981554031372\n",
      "\t Training loss (single batch): 1.3788373470306396\n",
      "\t Training loss (single batch): 0.7919682264328003\n",
      "\t Training loss (single batch): 1.4518955945968628\n",
      "\t Training loss (single batch): 1.3742716312408447\n",
      "\t Training loss (single batch): 1.4066604375839233\n",
      "\t Training loss (single batch): 1.278123140335083\n",
      "\t Training loss (single batch): 0.7483627796173096\n",
      "\t Training loss (single batch): 1.5952342748641968\n",
      "\t Training loss (single batch): 1.2566171884536743\n",
      "\t Training loss (single batch): 1.056837797164917\n",
      "\t Training loss (single batch): 1.3689533472061157\n",
      "\t Training loss (single batch): 1.2666288614273071\n",
      "\t Training loss (single batch): 1.2662423849105835\n",
      "\t Training loss (single batch): 1.3170980215072632\n",
      "\t Training loss (single batch): 0.9295265078544617\n",
      "\t Training loss (single batch): 0.9678727984428406\n",
      "\t Training loss (single batch): 0.9228550791740417\n",
      "\t Training loss (single batch): 1.3194646835327148\n",
      "\t Training loss (single batch): 1.555826187133789\n",
      "\t Training loss (single batch): 1.0828423500061035\n",
      "\t Training loss (single batch): 1.5242893695831299\n",
      "\t Training loss (single batch): 1.3972779512405396\n",
      "\t Training loss (single batch): 1.0688204765319824\n",
      "\t Training loss (single batch): 1.2165409326553345\n",
      "\t Training loss (single batch): 1.2700825929641724\n",
      "\t Training loss (single batch): 1.4722864627838135\n",
      "\t Training loss (single batch): 0.9620736837387085\n",
      "\t Training loss (single batch): 1.162170648574829\n",
      "\t Training loss (single batch): 1.6569052934646606\n",
      "\t Training loss (single batch): 1.6861865520477295\n",
      "\t Training loss (single batch): 1.0704424381256104\n",
      "\t Training loss (single batch): 1.0885189771652222\n",
      "\t Training loss (single batch): 0.9258907437324524\n",
      "\t Training loss (single batch): 1.617989182472229\n",
      "\t Training loss (single batch): 1.4683425426483154\n",
      "\t Training loss (single batch): 1.119815468788147\n",
      "\t Training loss (single batch): 0.9281809329986572\n",
      "\t Training loss (single batch): 1.3742271661758423\n",
      "\t Training loss (single batch): 0.9055678844451904\n",
      "\t Training loss (single batch): 1.0903170108795166\n",
      "\t Training loss (single batch): 1.193765640258789\n",
      "\t Training loss (single batch): 1.1553845405578613\n",
      "\t Training loss (single batch): 1.0879557132720947\n",
      "\t Training loss (single batch): 0.9377387762069702\n",
      "\t Training loss (single batch): 1.5891780853271484\n",
      "\t Training loss (single batch): 1.5525535345077515\n",
      "\t Training loss (single batch): 1.7663012742996216\n",
      "\t Training loss (single batch): 0.8516395092010498\n",
      "\t Training loss (single batch): 1.421250820159912\n",
      "\t Training loss (single batch): 1.0460692644119263\n",
      "\t Training loss (single batch): 1.1297364234924316\n",
      "\t Training loss (single batch): 1.5680956840515137\n",
      "\t Training loss (single batch): 0.7726519107818604\n",
      "\t Training loss (single batch): 1.3727126121520996\n",
      "\t Training loss (single batch): 1.3777801990509033\n",
      "\t Training loss (single batch): 1.5567705631256104\n",
      "\t Training loss (single batch): 1.1549978256225586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0719952583312988\n",
      "\t Training loss (single batch): 1.6495879888534546\n",
      "\t Training loss (single batch): 1.5228357315063477\n",
      "\t Training loss (single batch): 1.1220734119415283\n",
      "\t Training loss (single batch): 0.9392684102058411\n",
      "\t Training loss (single batch): 1.18565833568573\n",
      "\t Training loss (single batch): 1.0600101947784424\n",
      "\t Training loss (single batch): 1.4290776252746582\n",
      "\t Training loss (single batch): 0.7944152355194092\n",
      "\t Training loss (single batch): 0.9107608199119568\n",
      "\t Training loss (single batch): 1.0419676303863525\n",
      "\t Training loss (single batch): 1.3934975862503052\n",
      "\t Training loss (single batch): 1.7027281522750854\n",
      "\t Training loss (single batch): 1.178882122039795\n",
      "\t Training loss (single batch): 1.4143760204315186\n",
      "\t Training loss (single batch): 1.1570268869400024\n",
      "\t Training loss (single batch): 1.1650102138519287\n",
      "\t Training loss (single batch): 1.0564179420471191\n",
      "\t Training loss (single batch): 0.8923543691635132\n",
      "\t Training loss (single batch): 1.3481134176254272\n",
      "\t Training loss (single batch): 1.0599461793899536\n",
      "\t Training loss (single batch): 1.0525791645050049\n",
      "\t Training loss (single batch): 1.8180516958236694\n",
      "\t Training loss (single batch): 1.421015739440918\n",
      "\t Training loss (single batch): 1.6569957733154297\n",
      "\t Training loss (single batch): 1.5117566585540771\n",
      "\t Training loss (single batch): 1.3622483015060425\n",
      "\t Training loss (single batch): 0.7168020606040955\n",
      "\t Training loss (single batch): 0.898722231388092\n",
      "\t Training loss (single batch): 1.5493682622909546\n",
      "\t Training loss (single batch): 1.564556360244751\n",
      "\t Training loss (single batch): 1.4749690294265747\n",
      "\t Training loss (single batch): 1.1209132671356201\n",
      "\t Training loss (single batch): 1.401499629020691\n",
      "\t Training loss (single batch): 0.7685691714286804\n",
      "\t Training loss (single batch): 0.7947016358375549\n",
      "\t Training loss (single batch): 1.1111184358596802\n",
      "\t Training loss (single batch): 1.029760718345642\n",
      "\t Training loss (single batch): 1.249887228012085\n",
      "\t Training loss (single batch): 1.2215031385421753\n",
      "\t Training loss (single batch): 1.4290908575057983\n",
      "\t Training loss (single batch): 1.442752718925476\n",
      "\t Training loss (single batch): 1.491229772567749\n",
      "\t Training loss (single batch): 1.20939040184021\n",
      "\t Training loss (single batch): 1.41034734249115\n",
      "\t Training loss (single batch): 1.6632729768753052\n",
      "\t Training loss (single batch): 1.0833287239074707\n",
      "\t Training loss (single batch): 0.9651784896850586\n",
      "\t Training loss (single batch): 1.1044930219650269\n",
      "\t Training loss (single batch): 1.7400822639465332\n",
      "\t Training loss (single batch): 1.4155770540237427\n",
      "\t Training loss (single batch): 0.8820257782936096\n",
      "\t Training loss (single batch): 1.0188778638839722\n",
      "\t Training loss (single batch): 1.2558972835540771\n",
      "\t Training loss (single batch): 1.4901586771011353\n",
      "\t Training loss (single batch): 1.2464946508407593\n",
      "\t Training loss (single batch): 1.3922643661499023\n",
      "\t Training loss (single batch): 1.5685925483703613\n",
      "\t Training loss (single batch): 1.5178711414337158\n",
      "\t Training loss (single batch): 0.938801646232605\n",
      "\t Training loss (single batch): 1.1023180484771729\n",
      "\t Training loss (single batch): 1.059941053390503\n",
      "\t Training loss (single batch): 1.363061785697937\n",
      "\t Training loss (single batch): 1.2901304960250854\n",
      "\t Training loss (single batch): 1.0733698606491089\n",
      "\t Training loss (single batch): 1.1677980422973633\n",
      "\t Training loss (single batch): 1.2425508499145508\n",
      "\t Training loss (single batch): 1.3555492162704468\n",
      "\t Training loss (single batch): 1.199695348739624\n",
      "\t Training loss (single batch): 1.047616958618164\n",
      "\t Training loss (single batch): 0.9535045027732849\n",
      "\t Training loss (single batch): 1.2599194049835205\n",
      "\t Training loss (single batch): 1.2155089378356934\n",
      "\t Training loss (single batch): 1.0369155406951904\n",
      "\t Training loss (single batch): 1.1911332607269287\n",
      "\t Training loss (single batch): 1.21812903881073\n",
      "\t Training loss (single batch): 1.1836270093917847\n",
      "\t Training loss (single batch): 1.4074227809906006\n",
      "\t Training loss (single batch): 1.0755621194839478\n",
      "\t Training loss (single batch): 0.7179694771766663\n",
      "\t Training loss (single batch): 1.0470575094223022\n",
      "\t Training loss (single batch): 1.2223503589630127\n",
      "\t Training loss (single batch): 0.9267820119857788\n",
      "\t Training loss (single batch): 1.0983115434646606\n",
      "\t Training loss (single batch): 1.461426854133606\n",
      "\t Training loss (single batch): 1.5208561420440674\n",
      "\t Training loss (single batch): 1.4156314134597778\n",
      "\t Training loss (single batch): 1.3145232200622559\n",
      "\t Training loss (single batch): 1.1799746751785278\n",
      "\t Training loss (single batch): 0.9696249961853027\n",
      "\t Training loss (single batch): 1.314028263092041\n",
      "\t Training loss (single batch): 1.4503754377365112\n",
      "\t Training loss (single batch): 1.3436017036437988\n",
      "\t Training loss (single batch): 1.0817538499832153\n",
      "\t Training loss (single batch): 1.3003196716308594\n",
      "\t Training loss (single batch): 1.632004976272583\n",
      "\t Training loss (single batch): 1.3020013570785522\n",
      "\t Training loss (single batch): 1.0278513431549072\n",
      "\t Training loss (single batch): 1.0446089506149292\n",
      "\t Training loss (single batch): 1.1735379695892334\n",
      "\t Training loss (single batch): 1.6725044250488281\n",
      "\t Training loss (single batch): 1.597745656967163\n",
      "\t Training loss (single batch): 1.115101933479309\n",
      "\t Training loss (single batch): 0.8614867925643921\n",
      "\t Training loss (single batch): 1.2263551950454712\n",
      "\t Training loss (single batch): 0.9429771900177002\n",
      "\t Training loss (single batch): 1.4203208684921265\n",
      "\t Training loss (single batch): 1.4892445802688599\n",
      "\t Training loss (single batch): 1.1681506633758545\n",
      "\t Training loss (single batch): 1.4497746229171753\n",
      "\t Training loss (single batch): 0.8925400972366333\n",
      "\t Training loss (single batch): 1.0318734645843506\n",
      "\t Training loss (single batch): 1.0644252300262451\n",
      "\t Training loss (single batch): 1.2912812232971191\n",
      "\t Training loss (single batch): 1.6929504871368408\n",
      "\t Training loss (single batch): 1.6793553829193115\n",
      "\t Training loss (single batch): 1.1732743978500366\n",
      "\t Training loss (single batch): 1.5782462358474731\n",
      "\t Training loss (single batch): 1.0871574878692627\n",
      "\t Training loss (single batch): 1.3499958515167236\n",
      "\t Training loss (single batch): 1.3974387645721436\n",
      "\t Training loss (single batch): 1.6942816972732544\n",
      "\t Training loss (single batch): 1.231915831565857\n",
      "\t Training loss (single batch): 0.8449705243110657\n",
      "\t Training loss (single batch): 1.1805944442749023\n",
      "\t Training loss (single batch): 1.2825641632080078\n",
      "\t Training loss (single batch): 1.6992762088775635\n",
      "\t Training loss (single batch): 1.2526578903198242\n",
      "\t Training loss (single batch): 1.8398810625076294\n",
      "\t Training loss (single batch): 1.799886703491211\n",
      "\t Training loss (single batch): 1.696299433708191\n",
      "\t Training loss (single batch): 1.2335448265075684\n",
      "\t Training loss (single batch): 1.1961064338684082\n",
      "\t Training loss (single batch): 1.3747707605361938\n",
      "\t Training loss (single batch): 1.115325927734375\n",
      "\t Training loss (single batch): 0.9524418115615845\n",
      "\t Training loss (single batch): 1.4093883037567139\n",
      "\t Training loss (single batch): 1.0966126918792725\n",
      "\t Training loss (single batch): 1.4541960954666138\n",
      "\t Training loss (single batch): 0.6244316697120667\n",
      "\t Training loss (single batch): 1.3324531316757202\n",
      "\t Training loss (single batch): 1.2968379259109497\n",
      "\t Training loss (single batch): 1.8723217248916626\n",
      "\t Training loss (single batch): 1.4423567056655884\n",
      "\t Training loss (single batch): 0.8706111907958984\n",
      "\t Training loss (single batch): 1.0088729858398438\n",
      "\t Training loss (single batch): 1.0656371116638184\n",
      "\t Training loss (single batch): 1.0963706970214844\n",
      "\t Training loss (single batch): 1.1259515285491943\n",
      "\t Training loss (single batch): 1.4792760610580444\n",
      "\t Training loss (single batch): 0.6392260789871216\n",
      "\t Training loss (single batch): 0.9869970679283142\n",
      "\t Training loss (single batch): 0.9522384405136108\n",
      "\t Training loss (single batch): 1.2674651145935059\n",
      "\t Training loss (single batch): 1.480108618736267\n",
      "\t Training loss (single batch): 1.1837512254714966\n",
      "\t Training loss (single batch): 0.8840792775154114\n",
      "\t Training loss (single batch): 1.0625743865966797\n",
      "\t Training loss (single batch): 0.9556758999824524\n",
      "\t Training loss (single batch): 1.071480631828308\n",
      "\t Training loss (single batch): 0.9120199680328369\n",
      "\t Training loss (single batch): 3.525144338607788\n",
      "##################################\n",
      "## EPOCH 41\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1723761558532715\n",
      "\t Training loss (single batch): 0.8916875720024109\n",
      "\t Training loss (single batch): 1.3969775438308716\n",
      "\t Training loss (single batch): 1.1653283834457397\n",
      "\t Training loss (single batch): 1.27276611328125\n",
      "\t Training loss (single batch): 0.8989076018333435\n",
      "\t Training loss (single batch): 1.2875603437423706\n",
      "\t Training loss (single batch): 1.2744554281234741\n",
      "\t Training loss (single batch): 1.5958926677703857\n",
      "\t Training loss (single batch): 0.9373286962509155\n",
      "\t Training loss (single batch): 1.6411364078521729\n",
      "\t Training loss (single batch): 1.4806878566741943\n",
      "\t Training loss (single batch): 1.4124486446380615\n",
      "\t Training loss (single batch): 1.1570199728012085\n",
      "\t Training loss (single batch): 1.5531787872314453\n",
      "\t Training loss (single batch): 1.579693078994751\n",
      "\t Training loss (single batch): 1.5321637392044067\n",
      "\t Training loss (single batch): 0.8977489471435547\n",
      "\t Training loss (single batch): 1.2911006212234497\n",
      "\t Training loss (single batch): 1.5767894983291626\n",
      "\t Training loss (single batch): 1.242370367050171\n",
      "\t Training loss (single batch): 1.3398929834365845\n",
      "\t Training loss (single batch): 1.672167181968689\n",
      "\t Training loss (single batch): 0.8155606389045715\n",
      "\t Training loss (single batch): 1.1378778219223022\n",
      "\t Training loss (single batch): 1.059688925743103\n",
      "\t Training loss (single batch): 0.8989019989967346\n",
      "\t Training loss (single batch): 0.8003756999969482\n",
      "\t Training loss (single batch): 0.962255597114563\n",
      "\t Training loss (single batch): 1.0759412050247192\n",
      "\t Training loss (single batch): 1.3418692350387573\n",
      "\t Training loss (single batch): 0.8862397074699402\n",
      "\t Training loss (single batch): 1.3818743228912354\n",
      "\t Training loss (single batch): 1.2221498489379883\n",
      "\t Training loss (single batch): 1.422105073928833\n",
      "\t Training loss (single batch): 1.258173942565918\n",
      "\t Training loss (single batch): 0.9799440503120422\n",
      "\t Training loss (single batch): 1.4264166355133057\n",
      "\t Training loss (single batch): 0.9362843632698059\n",
      "\t Training loss (single batch): 1.6004546880722046\n",
      "\t Training loss (single batch): 1.10110604763031\n",
      "\t Training loss (single batch): 1.0732877254486084\n",
      "\t Training loss (single batch): 0.7545982599258423\n",
      "\t Training loss (single batch): 1.1383304595947266\n",
      "\t Training loss (single batch): 1.3585706949234009\n",
      "\t Training loss (single batch): 1.1974308490753174\n",
      "\t Training loss (single batch): 1.6612343788146973\n",
      "\t Training loss (single batch): 1.4000245332717896\n",
      "\t Training loss (single batch): 1.3158957958221436\n",
      "\t Training loss (single batch): 0.7701616287231445\n",
      "\t Training loss (single batch): 1.4936885833740234\n",
      "\t Training loss (single batch): 0.9512507915496826\n",
      "\t Training loss (single batch): 0.9318157434463501\n",
      "\t Training loss (single batch): 1.192931890487671\n",
      "\t Training loss (single batch): 0.8028813600540161\n",
      "\t Training loss (single batch): 1.2456234693527222\n",
      "\t Training loss (single batch): 1.5503181219100952\n",
      "\t Training loss (single batch): 1.1149413585662842\n",
      "\t Training loss (single batch): 1.3629651069641113\n",
      "\t Training loss (single batch): 0.8641607165336609\n",
      "\t Training loss (single batch): 1.146860957145691\n",
      "\t Training loss (single batch): 1.4813791513442993\n",
      "\t Training loss (single batch): 0.8006561398506165\n",
      "\t Training loss (single batch): 1.1315668821334839\n",
      "\t Training loss (single batch): 1.4732625484466553\n",
      "\t Training loss (single batch): 1.330224633216858\n",
      "\t Training loss (single batch): 1.1674003601074219\n",
      "\t Training loss (single batch): 1.6874034404754639\n",
      "\t Training loss (single batch): 1.1482311487197876\n",
      "\t Training loss (single batch): 1.661535620689392\n",
      "\t Training loss (single batch): 1.3559075593948364\n",
      "\t Training loss (single batch): 0.8393922448158264\n",
      "\t Training loss (single batch): 0.7765730619430542\n",
      "\t Training loss (single batch): 1.5156283378601074\n",
      "\t Training loss (single batch): 1.2192413806915283\n",
      "\t Training loss (single batch): 1.1703981161117554\n",
      "\t Training loss (single batch): 1.460573434829712\n",
      "\t Training loss (single batch): 1.1777112483978271\n",
      "\t Training loss (single batch): 1.044480800628662\n",
      "\t Training loss (single batch): 1.0951848030090332\n",
      "\t Training loss (single batch): 1.2344404458999634\n",
      "\t Training loss (single batch): 1.3839706182479858\n",
      "\t Training loss (single batch): 1.114284873008728\n",
      "\t Training loss (single batch): 1.2501938343048096\n",
      "\t Training loss (single batch): 1.1523633003234863\n",
      "\t Training loss (single batch): 1.0740156173706055\n",
      "\t Training loss (single batch): 1.1308295726776123\n",
      "\t Training loss (single batch): 1.120151400566101\n",
      "\t Training loss (single batch): 1.3089334964752197\n",
      "\t Training loss (single batch): 1.2280819416046143\n",
      "\t Training loss (single batch): 1.4367094039916992\n",
      "\t Training loss (single batch): 1.6496657133102417\n",
      "\t Training loss (single batch): 1.4489277601242065\n",
      "\t Training loss (single batch): 1.3415096998214722\n",
      "\t Training loss (single batch): 1.0317233800888062\n",
      "\t Training loss (single batch): 1.106844186782837\n",
      "\t Training loss (single batch): 0.9700367450714111\n",
      "\t Training loss (single batch): 1.0671755075454712\n",
      "\t Training loss (single batch): 1.281034231185913\n",
      "\t Training loss (single batch): 1.4085054397583008\n",
      "\t Training loss (single batch): 1.2576130628585815\n",
      "\t Training loss (single batch): 1.6986030340194702\n",
      "\t Training loss (single batch): 1.3199800252914429\n",
      "\t Training loss (single batch): 0.8219431042671204\n",
      "\t Training loss (single batch): 1.1120778322219849\n",
      "\t Training loss (single batch): 1.2964202165603638\n",
      "\t Training loss (single batch): 1.0015918016433716\n",
      "\t Training loss (single batch): 0.5574612021446228\n",
      "\t Training loss (single batch): 0.9526260495185852\n",
      "\t Training loss (single batch): 1.4531854391098022\n",
      "\t Training loss (single batch): 1.213202714920044\n",
      "\t Training loss (single batch): 1.199546456336975\n",
      "\t Training loss (single batch): 0.7859316468238831\n",
      "\t Training loss (single batch): 1.6597373485565186\n",
      "\t Training loss (single batch): 1.0895395278930664\n",
      "\t Training loss (single batch): 1.1997385025024414\n",
      "\t Training loss (single batch): 1.2481433153152466\n",
      "\t Training loss (single batch): 1.547908902168274\n",
      "\t Training loss (single batch): 0.8720342516899109\n",
      "\t Training loss (single batch): 1.1793506145477295\n",
      "\t Training loss (single batch): 1.3771754503250122\n",
      "\t Training loss (single batch): 1.6122161149978638\n",
      "\t Training loss (single batch): 1.284724235534668\n",
      "\t Training loss (single batch): 1.3965115547180176\n",
      "\t Training loss (single batch): 1.4147707223892212\n",
      "\t Training loss (single batch): 1.3676234483718872\n",
      "\t Training loss (single batch): 0.9272198677062988\n",
      "\t Training loss (single batch): 1.3044072389602661\n",
      "\t Training loss (single batch): 1.2322595119476318\n",
      "\t Training loss (single batch): 1.5905072689056396\n",
      "\t Training loss (single batch): 1.3745644092559814\n",
      "\t Training loss (single batch): 0.9565705060958862\n",
      "\t Training loss (single batch): 1.1575552225112915\n",
      "\t Training loss (single batch): 1.2447105646133423\n",
      "\t Training loss (single batch): 1.436875820159912\n",
      "\t Training loss (single batch): 0.972223699092865\n",
      "\t Training loss (single batch): 1.1119927167892456\n",
      "\t Training loss (single batch): 1.6936585903167725\n",
      "\t Training loss (single batch): 1.5315756797790527\n",
      "\t Training loss (single batch): 0.8237001895904541\n",
      "\t Training loss (single batch): 0.8326003551483154\n",
      "\t Training loss (single batch): 1.664642333984375\n",
      "\t Training loss (single batch): 1.1383917331695557\n",
      "\t Training loss (single batch): 1.3936399221420288\n",
      "\t Training loss (single batch): 1.75300133228302\n",
      "\t Training loss (single batch): 1.0489723682403564\n",
      "\t Training loss (single batch): 1.2298572063446045\n",
      "\t Training loss (single batch): 1.465569019317627\n",
      "\t Training loss (single batch): 1.0681668519973755\n",
      "\t Training loss (single batch): 1.6154838800430298\n",
      "\t Training loss (single batch): 1.1982295513153076\n",
      "\t Training loss (single batch): 1.5083400011062622\n",
      "\t Training loss (single batch): 1.652180552482605\n",
      "\t Training loss (single batch): 1.2888381481170654\n",
      "\t Training loss (single batch): 0.8965237736701965\n",
      "\t Training loss (single batch): 1.4743844270706177\n",
      "\t Training loss (single batch): 0.7050102949142456\n",
      "\t Training loss (single batch): 0.8151023983955383\n",
      "\t Training loss (single batch): 1.402525782585144\n",
      "\t Training loss (single batch): 1.2399393320083618\n",
      "\t Training loss (single batch): 1.0842152833938599\n",
      "\t Training loss (single batch): 1.5741666555404663\n",
      "\t Training loss (single batch): 1.2283475399017334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4704779386520386\n",
      "\t Training loss (single batch): 1.4302300214767456\n",
      "\t Training loss (single batch): 1.320954442024231\n",
      "\t Training loss (single batch): 1.3822433948516846\n",
      "\t Training loss (single batch): 1.1317758560180664\n",
      "\t Training loss (single batch): 1.4870139360427856\n",
      "\t Training loss (single batch): 1.1954102516174316\n",
      "\t Training loss (single batch): 1.1438329219818115\n",
      "\t Training loss (single batch): 0.8499405384063721\n",
      "\t Training loss (single batch): 1.1173213720321655\n",
      "\t Training loss (single batch): 0.845537543296814\n",
      "\t Training loss (single batch): 1.150040626525879\n",
      "\t Training loss (single batch): 1.3877471685409546\n",
      "\t Training loss (single batch): 1.2882567644119263\n",
      "\t Training loss (single batch): 1.3905832767486572\n",
      "\t Training loss (single batch): 1.2338268756866455\n",
      "\t Training loss (single batch): 0.9942342042922974\n",
      "\t Training loss (single batch): 1.1773731708526611\n",
      "\t Training loss (single batch): 1.2667957544326782\n",
      "\t Training loss (single batch): 1.6122056245803833\n",
      "\t Training loss (single batch): 1.2165838479995728\n",
      "\t Training loss (single batch): 1.003453016281128\n",
      "\t Training loss (single batch): 1.5756009817123413\n",
      "\t Training loss (single batch): 1.3060567378997803\n",
      "\t Training loss (single batch): 1.1167759895324707\n",
      "\t Training loss (single batch): 0.645738959312439\n",
      "\t Training loss (single batch): 1.02759850025177\n",
      "\t Training loss (single batch): 1.0523582696914673\n",
      "\t Training loss (single batch): 1.3737937211990356\n",
      "\t Training loss (single batch): 1.4004937410354614\n",
      "\t Training loss (single batch): 0.9234068989753723\n",
      "\t Training loss (single batch): 0.8418089151382446\n",
      "\t Training loss (single batch): 1.301279902458191\n",
      "\t Training loss (single batch): 1.6695631742477417\n",
      "\t Training loss (single batch): 1.2103902101516724\n",
      "\t Training loss (single batch): 1.1676901578903198\n",
      "\t Training loss (single batch): 1.3671607971191406\n",
      "\t Training loss (single batch): 1.25244140625\n",
      "\t Training loss (single batch): 1.2156482934951782\n",
      "\t Training loss (single batch): 1.2486108541488647\n",
      "\t Training loss (single batch): 0.9237503409385681\n",
      "\t Training loss (single batch): 1.529139518737793\n",
      "\t Training loss (single batch): 1.7897580862045288\n",
      "\t Training loss (single batch): 1.2275575399398804\n",
      "\t Training loss (single batch): 1.1848983764648438\n",
      "\t Training loss (single batch): 1.3748693466186523\n",
      "\t Training loss (single batch): 1.4373313188552856\n",
      "\t Training loss (single batch): 1.0250334739685059\n",
      "\t Training loss (single batch): 1.21963632106781\n",
      "\t Training loss (single batch): 2.233798027038574\n",
      "\t Training loss (single batch): 0.703312873840332\n",
      "\t Training loss (single batch): 1.074349284172058\n",
      "\t Training loss (single batch): 1.8651390075683594\n",
      "\t Training loss (single batch): 1.6331862211227417\n",
      "\t Training loss (single batch): 1.1023445129394531\n",
      "\t Training loss (single batch): 1.6314976215362549\n",
      "\t Training loss (single batch): 1.310529351234436\n",
      "\t Training loss (single batch): 1.53016197681427\n",
      "\t Training loss (single batch): 1.0966858863830566\n",
      "\t Training loss (single batch): 0.9373798966407776\n",
      "\t Training loss (single batch): 1.5078288316726685\n",
      "\t Training loss (single batch): 1.227971076965332\n",
      "\t Training loss (single batch): 0.8631388545036316\n",
      "\t Training loss (single batch): 1.271217942237854\n",
      "\t Training loss (single batch): 0.9500630497932434\n",
      "\t Training loss (single batch): 1.6220955848693848\n",
      "\t Training loss (single batch): 1.3860163688659668\n",
      "\t Training loss (single batch): 1.0544151067733765\n",
      "\t Training loss (single batch): 1.0029563903808594\n",
      "\t Training loss (single batch): 1.062968134880066\n",
      "\t Training loss (single batch): 1.1054400205612183\n",
      "\t Training loss (single batch): 0.9214230179786682\n",
      "\t Training loss (single batch): 1.1345632076263428\n",
      "\t Training loss (single batch): 1.6789531707763672\n",
      "\t Training loss (single batch): 1.5468090772628784\n",
      "\t Training loss (single batch): 1.5296469926834106\n",
      "\t Training loss (single batch): 1.0478224754333496\n",
      "\t Training loss (single batch): 1.2057181596755981\n",
      "\t Training loss (single batch): 1.1383522748947144\n",
      "\t Training loss (single batch): 0.9154259562492371\n",
      "\t Training loss (single batch): 1.0209369659423828\n",
      "\t Training loss (single batch): 1.5147944688796997\n",
      "\t Training loss (single batch): 1.3228334188461304\n",
      "\t Training loss (single batch): 1.221416711807251\n",
      "\t Training loss (single batch): 1.021249771118164\n",
      "\t Training loss (single batch): 1.545334815979004\n",
      "\t Training loss (single batch): 1.0061873197555542\n",
      "\t Training loss (single batch): 1.2604782581329346\n",
      "\t Training loss (single batch): 1.283130407333374\n",
      "\t Training loss (single batch): 1.3277148008346558\n",
      "\t Training loss (single batch): 0.8765684962272644\n",
      "\t Training loss (single batch): 1.1663602590560913\n",
      "\t Training loss (single batch): 1.0487420558929443\n",
      "\t Training loss (single batch): 0.8487319946289062\n",
      "\t Training loss (single batch): 1.6091517210006714\n",
      "\t Training loss (single batch): 1.4384535551071167\n",
      "\t Training loss (single batch): 0.8014093637466431\n",
      "\t Training loss (single batch): 1.5470081567764282\n",
      "\t Training loss (single batch): 0.6337360143661499\n",
      "\t Training loss (single batch): 1.2628809213638306\n",
      "\t Training loss (single batch): 1.5018569231033325\n",
      "\t Training loss (single batch): 0.9401663541793823\n",
      "\t Training loss (single batch): 1.2068711519241333\n",
      "\t Training loss (single batch): 1.6829309463500977\n",
      "\t Training loss (single batch): 1.0800089836120605\n",
      "\t Training loss (single batch): 0.9836233854293823\n",
      "\t Training loss (single batch): 1.184190273284912\n",
      "\t Training loss (single batch): 1.3333574533462524\n",
      "\t Training loss (single batch): 1.5504556894302368\n",
      "\t Training loss (single batch): 1.4059873819351196\n",
      "\t Training loss (single batch): 1.2267253398895264\n",
      "\t Training loss (single batch): 1.1131181716918945\n",
      "\t Training loss (single batch): 1.0530816316604614\n",
      "\t Training loss (single batch): 0.9778301119804382\n",
      "\t Training loss (single batch): 1.3410035371780396\n",
      "\t Training loss (single batch): 1.320888876914978\n",
      "\t Training loss (single batch): 1.5090404748916626\n",
      "\t Training loss (single batch): 1.3688653707504272\n",
      "\t Training loss (single batch): 1.3095958232879639\n",
      "\t Training loss (single batch): 1.3362693786621094\n",
      "\t Training loss (single batch): 1.2919774055480957\n",
      "\t Training loss (single batch): 1.2932775020599365\n",
      "\t Training loss (single batch): 1.4734845161437988\n",
      "\t Training loss (single batch): 1.7896583080291748\n",
      "\t Training loss (single batch): 1.1825023889541626\n",
      "\t Training loss (single batch): 1.3803596496582031\n",
      "\t Training loss (single batch): 0.8061224222183228\n",
      "\t Training loss (single batch): 0.8715577125549316\n",
      "\t Training loss (single batch): 0.7720204591751099\n",
      "\t Training loss (single batch): 1.1600086688995361\n",
      "\t Training loss (single batch): 1.3453394174575806\n",
      "\t Training loss (single batch): 1.4415862560272217\n",
      "\t Training loss (single batch): 0.7110621333122253\n",
      "\t Training loss (single batch): 0.7720207571983337\n",
      "\t Training loss (single batch): 1.0322028398513794\n",
      "\t Training loss (single batch): 1.383488655090332\n",
      "\t Training loss (single batch): 1.2955632209777832\n",
      "\t Training loss (single batch): 0.8788479566574097\n",
      "\t Training loss (single batch): 2.129016399383545\n",
      "\t Training loss (single batch): 1.3501228094100952\n",
      "\t Training loss (single batch): 1.3920031785964966\n",
      "\t Training loss (single batch): 1.2321617603302002\n",
      "\t Training loss (single batch): 1.1798985004425049\n",
      "\t Training loss (single batch): 1.4838945865631104\n",
      "\t Training loss (single batch): 1.4795749187469482\n",
      "\t Training loss (single batch): 1.1849372386932373\n",
      "\t Training loss (single batch): 1.3228600025177002\n",
      "\t Training loss (single batch): 1.2438812255859375\n",
      "\t Training loss (single batch): 0.9038061499595642\n",
      "\t Training loss (single batch): 0.9412133693695068\n",
      "\t Training loss (single batch): 1.1511956453323364\n",
      "\t Training loss (single batch): 1.8022880554199219\n",
      "\t Training loss (single batch): 1.2877730131149292\n",
      "\t Training loss (single batch): 0.6115502119064331\n",
      "##################################\n",
      "## EPOCH 42\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2875819206237793\n",
      "\t Training loss (single batch): 1.3357151746749878\n",
      "\t Training loss (single batch): 1.4168089628219604\n",
      "\t Training loss (single batch): 1.1796900033950806\n",
      "\t Training loss (single batch): 1.2009820938110352\n",
      "\t Training loss (single batch): 0.8510115742683411\n",
      "\t Training loss (single batch): 0.7433547973632812\n",
      "\t Training loss (single batch): 0.9429240226745605\n",
      "\t Training loss (single batch): 1.6191840171813965\n",
      "\t Training loss (single batch): 0.9153918623924255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.197381615638733\n",
      "\t Training loss (single batch): 1.2544736862182617\n",
      "\t Training loss (single batch): 0.7960900068283081\n",
      "\t Training loss (single batch): 1.014103651046753\n",
      "\t Training loss (single batch): 1.5160754919052124\n",
      "\t Training loss (single batch): 1.014693260192871\n",
      "\t Training loss (single batch): 1.4739892482757568\n",
      "\t Training loss (single batch): 1.4915308952331543\n",
      "\t Training loss (single batch): 0.9767732620239258\n",
      "\t Training loss (single batch): 1.526848316192627\n",
      "\t Training loss (single batch): 1.0484806299209595\n",
      "\t Training loss (single batch): 1.0257792472839355\n",
      "\t Training loss (single batch): 1.1121044158935547\n",
      "\t Training loss (single batch): 1.2236838340759277\n",
      "\t Training loss (single batch): 1.5017999410629272\n",
      "\t Training loss (single batch): 1.2644469738006592\n",
      "\t Training loss (single batch): 0.9457707405090332\n",
      "\t Training loss (single batch): 1.4488855600357056\n",
      "\t Training loss (single batch): 1.0403372049331665\n",
      "\t Training loss (single batch): 1.0799323320388794\n",
      "\t Training loss (single batch): 1.4646413326263428\n",
      "\t Training loss (single batch): 1.0905001163482666\n",
      "\t Training loss (single batch): 1.8799813985824585\n",
      "\t Training loss (single batch): 0.6248461008071899\n",
      "\t Training loss (single batch): 0.8691076040267944\n",
      "\t Training loss (single batch): 0.7042548060417175\n",
      "\t Training loss (single batch): 1.2580499649047852\n",
      "\t Training loss (single batch): 0.8549258708953857\n",
      "\t Training loss (single batch): 0.8857314586639404\n",
      "\t Training loss (single batch): 1.3726990222930908\n",
      "\t Training loss (single batch): 1.0794862508773804\n",
      "\t Training loss (single batch): 1.7662863731384277\n",
      "\t Training loss (single batch): 0.8785414099693298\n",
      "\t Training loss (single batch): 1.3053468465805054\n",
      "\t Training loss (single batch): 0.9067633748054504\n",
      "\t Training loss (single batch): 1.5152873992919922\n",
      "\t Training loss (single batch): 1.5399571657180786\n",
      "\t Training loss (single batch): 1.0242058038711548\n",
      "\t Training loss (single batch): 0.926562488079071\n",
      "\t Training loss (single batch): 1.4807066917419434\n",
      "\t Training loss (single batch): 1.4774686098098755\n",
      "\t Training loss (single batch): 1.3564702272415161\n",
      "\t Training loss (single batch): 1.2535945177078247\n",
      "\t Training loss (single batch): 1.161332130432129\n",
      "\t Training loss (single batch): 1.1025890111923218\n",
      "\t Training loss (single batch): 1.253394365310669\n",
      "\t Training loss (single batch): 0.9254913926124573\n",
      "\t Training loss (single batch): 0.9971833825111389\n",
      "\t Training loss (single batch): 0.8973674178123474\n",
      "\t Training loss (single batch): 1.3452064990997314\n",
      "\t Training loss (single batch): 1.0997734069824219\n",
      "\t Training loss (single batch): 1.1756585836410522\n",
      "\t Training loss (single batch): 1.5344455242156982\n",
      "\t Training loss (single batch): 0.8923749327659607\n",
      "\t Training loss (single batch): 1.3700438737869263\n",
      "\t Training loss (single batch): 1.3750795125961304\n",
      "\t Training loss (single batch): 1.8192720413208008\n",
      "\t Training loss (single batch): 1.1004539728164673\n",
      "\t Training loss (single batch): 1.2860437631607056\n",
      "\t Training loss (single batch): 1.2917019128799438\n",
      "\t Training loss (single batch): 1.1834046840667725\n",
      "\t Training loss (single batch): 1.0952409505844116\n",
      "\t Training loss (single batch): 1.4222816228866577\n",
      "\t Training loss (single batch): 1.5381371974945068\n",
      "\t Training loss (single batch): 1.1882963180541992\n",
      "\t Training loss (single batch): 1.5281412601470947\n",
      "\t Training loss (single batch): 1.4802515506744385\n",
      "\t Training loss (single batch): 1.7581464052200317\n",
      "\t Training loss (single batch): 0.965813159942627\n",
      "\t Training loss (single batch): 1.2879008054733276\n",
      "\t Training loss (single batch): 0.9413124918937683\n",
      "\t Training loss (single batch): 0.9602100253105164\n",
      "\t Training loss (single batch): 1.4109238386154175\n",
      "\t Training loss (single batch): 1.0596240758895874\n",
      "\t Training loss (single batch): 1.2930489778518677\n",
      "\t Training loss (single batch): 1.1014375686645508\n",
      "\t Training loss (single batch): 1.3333598375320435\n",
      "\t Training loss (single batch): 1.496180772781372\n",
      "\t Training loss (single batch): 1.3198022842407227\n",
      "\t Training loss (single batch): 2.2236037254333496\n",
      "\t Training loss (single batch): 1.2072863578796387\n",
      "\t Training loss (single batch): 1.2136518955230713\n",
      "\t Training loss (single batch): 1.399480938911438\n",
      "\t Training loss (single batch): 1.0321768522262573\n",
      "\t Training loss (single batch): 1.2567957639694214\n",
      "\t Training loss (single batch): 1.0649200677871704\n",
      "\t Training loss (single batch): 1.462773323059082\n",
      "\t Training loss (single batch): 1.5387160778045654\n",
      "\t Training loss (single batch): 1.3388817310333252\n",
      "\t Training loss (single batch): 1.5207573175430298\n",
      "\t Training loss (single batch): 1.7840754985809326\n",
      "\t Training loss (single batch): 1.3077219724655151\n",
      "\t Training loss (single batch): 1.1693856716156006\n",
      "\t Training loss (single batch): 1.5474480390548706\n",
      "\t Training loss (single batch): 1.5712735652923584\n",
      "\t Training loss (single batch): 1.7375184297561646\n",
      "\t Training loss (single batch): 1.4136664867401123\n",
      "\t Training loss (single batch): 1.3872531652450562\n",
      "\t Training loss (single batch): 1.3151601552963257\n",
      "\t Training loss (single batch): 1.0950933694839478\n",
      "\t Training loss (single batch): 0.8224388360977173\n",
      "\t Training loss (single batch): 1.0395275354385376\n",
      "\t Training loss (single batch): 1.026474118232727\n",
      "\t Training loss (single batch): 1.0418241024017334\n",
      "\t Training loss (single batch): 1.3982164859771729\n",
      "\t Training loss (single batch): 1.2664554119110107\n",
      "\t Training loss (single batch): 1.215242862701416\n",
      "\t Training loss (single batch): 1.3245770931243896\n",
      "\t Training loss (single batch): 0.9725476503372192\n",
      "\t Training loss (single batch): 1.2049124240875244\n",
      "\t Training loss (single batch): 1.215051293373108\n",
      "\t Training loss (single batch): 0.7484468817710876\n",
      "\t Training loss (single batch): 1.735280156135559\n",
      "\t Training loss (single batch): 1.350238561630249\n",
      "\t Training loss (single batch): 1.4851140975952148\n",
      "\t Training loss (single batch): 0.6980125904083252\n",
      "\t Training loss (single batch): 0.7159609794616699\n",
      "\t Training loss (single batch): 1.1448838710784912\n",
      "\t Training loss (single batch): 1.413527488708496\n",
      "\t Training loss (single batch): 1.2942395210266113\n",
      "\t Training loss (single batch): 1.0012948513031006\n",
      "\t Training loss (single batch): 1.3258167505264282\n",
      "\t Training loss (single batch): 0.9492703080177307\n",
      "\t Training loss (single batch): 0.8422666788101196\n",
      "\t Training loss (single batch): 0.9320030808448792\n",
      "\t Training loss (single batch): 1.1293160915374756\n",
      "\t Training loss (single batch): 1.517754316329956\n",
      "\t Training loss (single batch): 1.2244102954864502\n",
      "\t Training loss (single batch): 1.3333488702774048\n",
      "\t Training loss (single batch): 1.6624126434326172\n",
      "\t Training loss (single batch): 1.0960593223571777\n",
      "\t Training loss (single batch): 0.8546479940414429\n",
      "\t Training loss (single batch): 1.3044705390930176\n",
      "\t Training loss (single batch): 0.7809517979621887\n",
      "\t Training loss (single batch): 1.2385025024414062\n",
      "\t Training loss (single batch): 1.207484483718872\n",
      "\t Training loss (single batch): 1.2199574708938599\n",
      "\t Training loss (single batch): 1.3372647762298584\n",
      "\t Training loss (single batch): 1.0649598836898804\n",
      "\t Training loss (single batch): 1.249449372291565\n",
      "\t Training loss (single batch): 0.9997329115867615\n",
      "\t Training loss (single batch): 1.3892042636871338\n",
      "\t Training loss (single batch): 1.2153887748718262\n",
      "\t Training loss (single batch): 1.2838183641433716\n",
      "\t Training loss (single batch): 1.1934279203414917\n",
      "\t Training loss (single batch): 1.5358870029449463\n",
      "\t Training loss (single batch): 1.4720790386199951\n",
      "\t Training loss (single batch): 1.7861747741699219\n",
      "\t Training loss (single batch): 1.1564759016036987\n",
      "\t Training loss (single batch): 1.0268428325653076\n",
      "\t Training loss (single batch): 0.9491114020347595\n",
      "\t Training loss (single batch): 1.0860600471496582\n",
      "\t Training loss (single batch): 0.9948583841323853\n",
      "\t Training loss (single batch): 0.9371894598007202\n",
      "\t Training loss (single batch): 1.071851134300232\n",
      "\t Training loss (single batch): 1.1844311952590942\n",
      "\t Training loss (single batch): 1.404261589050293\n",
      "\t Training loss (single batch): 1.0572659969329834\n",
      "\t Training loss (single batch): 1.1216695308685303\n",
      "\t Training loss (single batch): 1.1522328853607178\n",
      "\t Training loss (single batch): 1.7339451313018799\n",
      "\t Training loss (single batch): 0.8371850252151489\n",
      "\t Training loss (single batch): 1.5433502197265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9110777378082275\n",
      "\t Training loss (single batch): 1.294684648513794\n",
      "\t Training loss (single batch): 1.266575574874878\n",
      "\t Training loss (single batch): 1.4230843782424927\n",
      "\t Training loss (single batch): 1.3781137466430664\n",
      "\t Training loss (single batch): 1.3504185676574707\n",
      "\t Training loss (single batch): 1.4711973667144775\n",
      "\t Training loss (single batch): 1.1482605934143066\n",
      "\t Training loss (single batch): 1.1050571203231812\n",
      "\t Training loss (single batch): 1.4790409803390503\n",
      "\t Training loss (single batch): 1.066232681274414\n",
      "\t Training loss (single batch): 1.0520002841949463\n",
      "\t Training loss (single batch): 1.4304405450820923\n",
      "\t Training loss (single batch): 1.4286938905715942\n",
      "\t Training loss (single batch): 1.767438530921936\n",
      "\t Training loss (single batch): 0.9710559844970703\n",
      "\t Training loss (single batch): 1.053732991218567\n",
      "\t Training loss (single batch): 1.1912426948547363\n",
      "\t Training loss (single batch): 1.5809890031814575\n",
      "\t Training loss (single batch): 0.9529041647911072\n",
      "\t Training loss (single batch): 1.1070935726165771\n",
      "\t Training loss (single batch): 1.0143358707427979\n",
      "\t Training loss (single batch): 0.7690553069114685\n",
      "\t Training loss (single batch): 1.7125792503356934\n",
      "\t Training loss (single batch): 1.1964668035507202\n",
      "\t Training loss (single batch): 1.0489091873168945\n",
      "\t Training loss (single batch): 1.5206575393676758\n",
      "\t Training loss (single batch): 1.2109789848327637\n",
      "\t Training loss (single batch): 1.3161288499832153\n",
      "\t Training loss (single batch): 1.443530797958374\n",
      "\t Training loss (single batch): 0.9936371445655823\n",
      "\t Training loss (single batch): 1.2190099954605103\n",
      "\t Training loss (single batch): 1.516004204750061\n",
      "\t Training loss (single batch): 0.9005890488624573\n",
      "\t Training loss (single batch): 0.5928442478179932\n",
      "\t Training loss (single batch): 1.3240243196487427\n",
      "\t Training loss (single batch): 1.4718101024627686\n",
      "\t Training loss (single batch): 1.3125017881393433\n",
      "\t Training loss (single batch): 0.777593195438385\n",
      "\t Training loss (single batch): 0.8700978755950928\n",
      "\t Training loss (single batch): 1.6412978172302246\n",
      "\t Training loss (single batch): 0.7653130292892456\n",
      "\t Training loss (single batch): 1.2655383348464966\n",
      "\t Training loss (single batch): 1.0078763961791992\n",
      "\t Training loss (single batch): 0.6755386590957642\n",
      "\t Training loss (single batch): 1.1139849424362183\n",
      "\t Training loss (single batch): 1.0231032371520996\n",
      "\t Training loss (single batch): 1.2935829162597656\n",
      "\t Training loss (single batch): 1.1324610710144043\n",
      "\t Training loss (single batch): 1.2370887994766235\n",
      "\t Training loss (single batch): 0.9067988395690918\n",
      "\t Training loss (single batch): 0.8895034790039062\n",
      "\t Training loss (single batch): 0.9703888893127441\n",
      "\t Training loss (single batch): 1.2684483528137207\n",
      "\t Training loss (single batch): 1.1937181949615479\n",
      "\t Training loss (single batch): 1.643500566482544\n",
      "\t Training loss (single batch): 1.2456611394882202\n",
      "\t Training loss (single batch): 1.0757849216461182\n",
      "\t Training loss (single batch): 1.2743897438049316\n",
      "\t Training loss (single batch): 1.3527113199234009\n",
      "\t Training loss (single batch): 1.5353307723999023\n",
      "\t Training loss (single batch): 1.0560481548309326\n",
      "\t Training loss (single batch): 1.7601070404052734\n",
      "\t Training loss (single batch): 1.4222700595855713\n",
      "\t Training loss (single batch): 0.8564236164093018\n",
      "\t Training loss (single batch): 1.5758945941925049\n",
      "\t Training loss (single batch): 1.1016731262207031\n",
      "\t Training loss (single batch): 1.0804049968719482\n",
      "\t Training loss (single batch): 0.957930326461792\n",
      "\t Training loss (single batch): 1.321541666984558\n",
      "\t Training loss (single batch): 0.9443202018737793\n",
      "\t Training loss (single batch): 1.6566195487976074\n",
      "\t Training loss (single batch): 1.3841290473937988\n",
      "\t Training loss (single batch): 1.5667213201522827\n",
      "\t Training loss (single batch): 1.1646149158477783\n",
      "\t Training loss (single batch): 1.50497305393219\n",
      "\t Training loss (single batch): 0.9584038853645325\n",
      "\t Training loss (single batch): 1.3051600456237793\n",
      "\t Training loss (single batch): 1.2068084478378296\n",
      "\t Training loss (single batch): 1.4110565185546875\n",
      "\t Training loss (single batch): 1.5286285877227783\n",
      "\t Training loss (single batch): 1.1830588579177856\n",
      "\t Training loss (single batch): 1.3707138299942017\n",
      "\t Training loss (single batch): 1.427302360534668\n",
      "\t Training loss (single batch): 1.3341058492660522\n",
      "\t Training loss (single batch): 1.0114260911941528\n",
      "\t Training loss (single batch): 1.2823517322540283\n",
      "\t Training loss (single batch): 1.0689483880996704\n",
      "\t Training loss (single batch): 1.4002858400344849\n",
      "\t Training loss (single batch): 1.399843454360962\n",
      "\t Training loss (single batch): 0.9784228205680847\n",
      "\t Training loss (single batch): 1.3831652402877808\n",
      "\t Training loss (single batch): 0.9265937805175781\n",
      "\t Training loss (single batch): 1.1663572788238525\n",
      "\t Training loss (single batch): 1.2888070344924927\n",
      "\t Training loss (single batch): 1.3712953329086304\n",
      "\t Training loss (single batch): 1.4836581945419312\n",
      "\t Training loss (single batch): 1.4295423030853271\n",
      "\t Training loss (single batch): 0.8691514134407043\n",
      "\t Training loss (single batch): 1.6401557922363281\n",
      "\t Training loss (single batch): 0.9456831216812134\n",
      "\t Training loss (single batch): 1.2809251546859741\n",
      "\t Training loss (single batch): 1.0220979452133179\n",
      "\t Training loss (single batch): 1.5205092430114746\n",
      "\t Training loss (single batch): 1.3601585626602173\n",
      "\t Training loss (single batch): 0.7676501870155334\n",
      "\t Training loss (single batch): 0.9541174173355103\n",
      "\t Training loss (single batch): 1.1614127159118652\n",
      "\t Training loss (single batch): 1.070123553276062\n",
      "\t Training loss (single batch): 1.2807551622390747\n",
      "\t Training loss (single batch): 1.2070109844207764\n",
      "\t Training loss (single batch): 1.679149866104126\n",
      "\t Training loss (single batch): 1.3759738206863403\n",
      "\t Training loss (single batch): 1.2664997577667236\n",
      "\t Training loss (single batch): 0.9119799733161926\n",
      "\t Training loss (single batch): 1.4689607620239258\n",
      "\t Training loss (single batch): 1.4877240657806396\n",
      "\t Training loss (single batch): 1.1123566627502441\n",
      "\t Training loss (single batch): 1.7185900211334229\n",
      "\t Training loss (single batch): 1.3297841548919678\n",
      "\t Training loss (single batch): 1.1539019346237183\n",
      "\t Training loss (single batch): 1.0172802209854126\n",
      "\t Training loss (single batch): 1.3144248723983765\n",
      "\t Training loss (single batch): 1.12371027469635\n",
      "\t Training loss (single batch): 0.9776167869567871\n",
      "\t Training loss (single batch): 1.7984209060668945\n",
      "\t Training loss (single batch): 1.3108556270599365\n",
      "\t Training loss (single batch): 1.1069334745407104\n",
      "\t Training loss (single batch): 1.1399956941604614\n",
      "\t Training loss (single batch): 1.102500319480896\n",
      "\t Training loss (single batch): 1.2605997323989868\n",
      "\t Training loss (single batch): 0.7935473918914795\n",
      "\t Training loss (single batch): 1.252031683921814\n",
      "\t Training loss (single batch): 1.5534131526947021\n",
      "\t Training loss (single batch): 1.3028823137283325\n",
      "\t Training loss (single batch): 1.295391321182251\n",
      "\t Training loss (single batch): 1.487656593322754\n",
      "\t Training loss (single batch): 1.0788683891296387\n",
      "\t Training loss (single batch): 0.9113214612007141\n",
      "\t Training loss (single batch): 1.117987871170044\n",
      "\t Training loss (single batch): 0.8669768571853638\n",
      "\t Training loss (single batch): 1.0625399351119995\n",
      "\t Training loss (single batch): 0.9990749359130859\n",
      "\t Training loss (single batch): 1.5738853216171265\n",
      "##################################\n",
      "## EPOCH 43\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2835991382598877\n",
      "\t Training loss (single batch): 0.8050724267959595\n",
      "\t Training loss (single batch): 0.7899691462516785\n",
      "\t Training loss (single batch): 0.8278166651725769\n",
      "\t Training loss (single batch): 1.2392187118530273\n",
      "\t Training loss (single batch): 0.8949243426322937\n",
      "\t Training loss (single batch): 1.403419852256775\n",
      "\t Training loss (single batch): 1.0879175662994385\n",
      "\t Training loss (single batch): 1.3297855854034424\n",
      "\t Training loss (single batch): 1.0405956506729126\n",
      "\t Training loss (single batch): 1.581078052520752\n",
      "\t Training loss (single batch): 1.2404954433441162\n",
      "\t Training loss (single batch): 1.2716832160949707\n",
      "\t Training loss (single batch): 0.8745137453079224\n",
      "\t Training loss (single batch): 1.0134799480438232\n",
      "\t Training loss (single batch): 0.8248149752616882\n",
      "\t Training loss (single batch): 1.1931315660476685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2316343784332275\n",
      "\t Training loss (single batch): 0.8035656809806824\n",
      "\t Training loss (single batch): 1.5646721124649048\n",
      "\t Training loss (single batch): 1.2736132144927979\n",
      "\t Training loss (single batch): 1.698362946510315\n",
      "\t Training loss (single batch): 1.3456162214279175\n",
      "\t Training loss (single batch): 1.0279737710952759\n",
      "\t Training loss (single batch): 1.2604899406433105\n",
      "\t Training loss (single batch): 0.967701256275177\n",
      "\t Training loss (single batch): 0.8553485870361328\n",
      "\t Training loss (single batch): 1.346793293952942\n",
      "\t Training loss (single batch): 1.6070835590362549\n",
      "\t Training loss (single batch): 1.2387676239013672\n",
      "\t Training loss (single batch): 1.1093354225158691\n",
      "\t Training loss (single batch): 1.1118242740631104\n",
      "\t Training loss (single batch): 0.8891077637672424\n",
      "\t Training loss (single batch): 1.285260558128357\n",
      "\t Training loss (single batch): 1.2877837419509888\n",
      "\t Training loss (single batch): 0.9740232229232788\n",
      "\t Training loss (single batch): 1.7174469232559204\n",
      "\t Training loss (single batch): 1.3903056383132935\n",
      "\t Training loss (single batch): 0.9382866024971008\n",
      "\t Training loss (single batch): 1.1976646184921265\n",
      "\t Training loss (single batch): 1.5436197519302368\n",
      "\t Training loss (single batch): 1.5543923377990723\n",
      "\t Training loss (single batch): 1.3124933242797852\n",
      "\t Training loss (single batch): 1.0288010835647583\n",
      "\t Training loss (single batch): 0.9390033483505249\n",
      "\t Training loss (single batch): 1.674635648727417\n",
      "\t Training loss (single batch): 0.9766165614128113\n",
      "\t Training loss (single batch): 0.7277060747146606\n",
      "\t Training loss (single batch): 1.4281830787658691\n",
      "\t Training loss (single batch): 1.186480164527893\n",
      "\t Training loss (single batch): 1.8095016479492188\n",
      "\t Training loss (single batch): 1.2570980787277222\n",
      "\t Training loss (single batch): 0.6453333497047424\n",
      "\t Training loss (single batch): 0.8583952784538269\n",
      "\t Training loss (single batch): 0.7796899080276489\n",
      "\t Training loss (single batch): 1.3996723890304565\n",
      "\t Training loss (single batch): 1.6928424835205078\n",
      "\t Training loss (single batch): 1.6306886672973633\n",
      "\t Training loss (single batch): 0.9845137000083923\n",
      "\t Training loss (single batch): 1.8084279298782349\n",
      "\t Training loss (single batch): 1.0743616819381714\n",
      "\t Training loss (single batch): 1.1515175104141235\n",
      "\t Training loss (single batch): 1.0056034326553345\n",
      "\t Training loss (single batch): 0.8382901549339294\n",
      "\t Training loss (single batch): 1.377184510231018\n",
      "\t Training loss (single batch): 1.124657154083252\n",
      "\t Training loss (single batch): 1.3126295804977417\n",
      "\t Training loss (single batch): 1.3815407752990723\n",
      "\t Training loss (single batch): 0.8111812472343445\n",
      "\t Training loss (single batch): 1.0260627269744873\n",
      "\t Training loss (single batch): 1.278385877609253\n",
      "\t Training loss (single batch): 1.0405926704406738\n",
      "\t Training loss (single batch): 0.9875725507736206\n",
      "\t Training loss (single batch): 1.3493142127990723\n",
      "\t Training loss (single batch): 0.9868258237838745\n",
      "\t Training loss (single batch): 1.8693825006484985\n",
      "\t Training loss (single batch): 1.1107248067855835\n",
      "\t Training loss (single batch): 0.6801251769065857\n",
      "\t Training loss (single batch): 1.251630187034607\n",
      "\t Training loss (single batch): 0.9117531776428223\n",
      "\t Training loss (single batch): 1.1880868673324585\n",
      "\t Training loss (single batch): 1.336412787437439\n",
      "\t Training loss (single batch): 1.427276849746704\n",
      "\t Training loss (single batch): 0.9603936076164246\n",
      "\t Training loss (single batch): 1.0167126655578613\n",
      "\t Training loss (single batch): 1.0625675916671753\n",
      "\t Training loss (single batch): 1.2121645212173462\n",
      "\t Training loss (single batch): 1.6687346696853638\n",
      "\t Training loss (single batch): 1.0337026119232178\n",
      "\t Training loss (single batch): 1.3928179740905762\n",
      "\t Training loss (single batch): 1.4803005456924438\n",
      "\t Training loss (single batch): 1.2701735496520996\n",
      "\t Training loss (single batch): 1.0064544677734375\n",
      "\t Training loss (single batch): 0.9882133603096008\n",
      "\t Training loss (single batch): 0.7854534387588501\n",
      "\t Training loss (single batch): 1.19511079788208\n",
      "\t Training loss (single batch): 1.0118087530136108\n",
      "\t Training loss (single batch): 1.404788851737976\n",
      "\t Training loss (single batch): 1.3819717168807983\n",
      "\t Training loss (single batch): 1.3809531927108765\n",
      "\t Training loss (single batch): 1.2468804121017456\n",
      "\t Training loss (single batch): 0.7098699808120728\n",
      "\t Training loss (single batch): 0.9748398661613464\n",
      "\t Training loss (single batch): 1.3959094285964966\n",
      "\t Training loss (single batch): 1.5233814716339111\n",
      "\t Training loss (single batch): 0.8972309231758118\n",
      "\t Training loss (single batch): 1.1744897365570068\n",
      "\t Training loss (single batch): 1.0131018161773682\n",
      "\t Training loss (single batch): 1.230636715888977\n",
      "\t Training loss (single batch): 0.9681795239448547\n",
      "\t Training loss (single batch): 0.8347665667533875\n",
      "\t Training loss (single batch): 1.246470332145691\n",
      "\t Training loss (single batch): 1.1223224401474\n",
      "\t Training loss (single batch): 1.3449206352233887\n",
      "\t Training loss (single batch): 0.7770406603813171\n",
      "\t Training loss (single batch): 1.0865724086761475\n",
      "\t Training loss (single batch): 1.102531909942627\n",
      "\t Training loss (single batch): 1.5331804752349854\n",
      "\t Training loss (single batch): 1.7499430179595947\n",
      "\t Training loss (single batch): 1.399931788444519\n",
      "\t Training loss (single batch): 1.7408969402313232\n",
      "\t Training loss (single batch): 0.9608367681503296\n",
      "\t Training loss (single batch): 1.3106906414031982\n",
      "\t Training loss (single batch): 0.7449264526367188\n",
      "\t Training loss (single batch): 1.3628320693969727\n",
      "\t Training loss (single batch): 1.0433682203292847\n",
      "\t Training loss (single batch): 1.2188465595245361\n",
      "\t Training loss (single batch): 1.291957974433899\n",
      "\t Training loss (single batch): 1.1140453815460205\n",
      "\t Training loss (single batch): 1.1507107019424438\n",
      "\t Training loss (single batch): 1.3474007844924927\n",
      "\t Training loss (single batch): 1.0809298753738403\n",
      "\t Training loss (single batch): 1.5607396364212036\n",
      "\t Training loss (single batch): 1.2583063840866089\n",
      "\t Training loss (single batch): 0.9450429677963257\n",
      "\t Training loss (single batch): 1.0655674934387207\n",
      "\t Training loss (single batch): 1.0308609008789062\n",
      "\t Training loss (single batch): 1.5100830793380737\n",
      "\t Training loss (single batch): 1.4232890605926514\n",
      "\t Training loss (single batch): 1.145367980003357\n",
      "\t Training loss (single batch): 1.1626638174057007\n",
      "\t Training loss (single batch): 1.567976474761963\n",
      "\t Training loss (single batch): 0.8388728499412537\n",
      "\t Training loss (single batch): 0.7470281720161438\n",
      "\t Training loss (single batch): 1.6647664308547974\n",
      "\t Training loss (single batch): 0.9956152439117432\n",
      "\t Training loss (single batch): 1.1500039100646973\n",
      "\t Training loss (single batch): 1.4859654903411865\n",
      "\t Training loss (single batch): 1.1658295392990112\n",
      "\t Training loss (single batch): 0.8060809373855591\n",
      "\t Training loss (single batch): 1.1858844757080078\n",
      "\t Training loss (single batch): 1.2810453176498413\n",
      "\t Training loss (single batch): 1.640489935874939\n",
      "\t Training loss (single batch): 0.9655690789222717\n",
      "\t Training loss (single batch): 0.9813140630722046\n",
      "\t Training loss (single batch): 1.0711016654968262\n",
      "\t Training loss (single batch): 1.4705153703689575\n",
      "\t Training loss (single batch): 0.9698314666748047\n",
      "\t Training loss (single batch): 0.9649888873100281\n",
      "\t Training loss (single batch): 0.9614652991294861\n",
      "\t Training loss (single batch): 1.9573272466659546\n",
      "\t Training loss (single batch): 1.101684331893921\n",
      "\t Training loss (single batch): 1.1138108968734741\n",
      "\t Training loss (single batch): 1.147841453552246\n",
      "\t Training loss (single batch): 1.0761642456054688\n",
      "\t Training loss (single batch): 1.1620312929153442\n",
      "\t Training loss (single batch): 1.2325193881988525\n",
      "\t Training loss (single batch): 1.516905426979065\n",
      "\t Training loss (single batch): 1.3707923889160156\n",
      "\t Training loss (single batch): 1.717542052268982\n",
      "\t Training loss (single batch): 0.8206506967544556\n",
      "\t Training loss (single batch): 1.2299460172653198\n",
      "\t Training loss (single batch): 1.3354687690734863\n",
      "\t Training loss (single batch): 0.9384744167327881\n",
      "\t Training loss (single batch): 1.1409775018692017\n",
      "\t Training loss (single batch): 1.4297189712524414\n",
      "\t Training loss (single batch): 1.1118388175964355\n",
      "\t Training loss (single batch): 1.2071126699447632\n",
      "\t Training loss (single batch): 1.1475473642349243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0116251707077026\n",
      "\t Training loss (single batch): 1.7105345726013184\n",
      "\t Training loss (single batch): 0.8011555671691895\n",
      "\t Training loss (single batch): 1.1270159482955933\n",
      "\t Training loss (single batch): 1.404011607170105\n",
      "\t Training loss (single batch): 1.1844542026519775\n",
      "\t Training loss (single batch): 1.0127842426300049\n",
      "\t Training loss (single batch): 1.3638439178466797\n",
      "\t Training loss (single batch): 1.159705400466919\n",
      "\t Training loss (single batch): 1.2264372110366821\n",
      "\t Training loss (single batch): 1.1766116619110107\n",
      "\t Training loss (single batch): 1.3130499124526978\n",
      "\t Training loss (single batch): 1.2607433795928955\n",
      "\t Training loss (single batch): 1.2459832429885864\n",
      "\t Training loss (single batch): 0.7500566840171814\n",
      "\t Training loss (single batch): 1.0149848461151123\n",
      "\t Training loss (single batch): 1.181471824645996\n",
      "\t Training loss (single batch): 0.743528425693512\n",
      "\t Training loss (single batch): 1.9726746082305908\n",
      "\t Training loss (single batch): 1.0293864011764526\n",
      "\t Training loss (single batch): 1.0017107725143433\n",
      "\t Training loss (single batch): 0.9896140694618225\n",
      "\t Training loss (single batch): 1.0697838068008423\n",
      "\t Training loss (single batch): 1.3207887411117554\n",
      "\t Training loss (single batch): 1.2341609001159668\n",
      "\t Training loss (single batch): 1.0354927778244019\n",
      "\t Training loss (single batch): 1.1450029611587524\n",
      "\t Training loss (single batch): 1.1122914552688599\n",
      "\t Training loss (single batch): 1.0821815729141235\n",
      "\t Training loss (single batch): 1.7135281562805176\n",
      "\t Training loss (single batch): 1.5279994010925293\n",
      "\t Training loss (single batch): 1.4897385835647583\n",
      "\t Training loss (single batch): 1.102062702178955\n",
      "\t Training loss (single batch): 1.2697572708129883\n",
      "\t Training loss (single batch): 0.8472347259521484\n",
      "\t Training loss (single batch): 0.7519330382347107\n",
      "\t Training loss (single batch): 1.0668302774429321\n",
      "\t Training loss (single batch): 1.4256645441055298\n",
      "\t Training loss (single batch): 1.5765349864959717\n",
      "\t Training loss (single batch): 1.1783913373947144\n",
      "\t Training loss (single batch): 1.0402408838272095\n",
      "\t Training loss (single batch): 1.630944848060608\n",
      "\t Training loss (single batch): 2.2354838848114014\n",
      "\t Training loss (single batch): 1.3278818130493164\n",
      "\t Training loss (single batch): 1.389405608177185\n",
      "\t Training loss (single batch): 1.2442792654037476\n",
      "\t Training loss (single batch): 1.0436633825302124\n",
      "\t Training loss (single batch): 1.0082015991210938\n",
      "\t Training loss (single batch): 1.0742415189743042\n",
      "\t Training loss (single batch): 0.893049418926239\n",
      "\t Training loss (single batch): 1.0058666467666626\n",
      "\t Training loss (single batch): 0.8305206298828125\n",
      "\t Training loss (single batch): 1.546965479850769\n",
      "\t Training loss (single batch): 1.3446182012557983\n",
      "\t Training loss (single batch): 1.6755270957946777\n",
      "\t Training loss (single batch): 1.1243648529052734\n",
      "\t Training loss (single batch): 0.8224342465400696\n",
      "\t Training loss (single batch): 1.266424536705017\n",
      "\t Training loss (single batch): 1.4180431365966797\n",
      "\t Training loss (single batch): 1.648465871810913\n",
      "\t Training loss (single batch): 1.9162650108337402\n",
      "\t Training loss (single batch): 1.2053993940353394\n",
      "\t Training loss (single batch): 0.8348755836486816\n",
      "\t Training loss (single batch): 1.26080322265625\n",
      "\t Training loss (single batch): 1.5006341934204102\n",
      "\t Training loss (single batch): 1.3464233875274658\n",
      "\t Training loss (single batch): 1.3092687129974365\n",
      "\t Training loss (single batch): 1.0210824012756348\n",
      "\t Training loss (single batch): 0.9258200526237488\n",
      "\t Training loss (single batch): 1.0828402042388916\n",
      "\t Training loss (single batch): 1.3314628601074219\n",
      "\t Training loss (single batch): 0.7202870845794678\n",
      "\t Training loss (single batch): 2.1929736137390137\n",
      "\t Training loss (single batch): 1.3034058809280396\n",
      "\t Training loss (single batch): 1.0674762725830078\n",
      "\t Training loss (single batch): 0.760248064994812\n",
      "\t Training loss (single batch): 1.072469711303711\n",
      "\t Training loss (single batch): 0.937234103679657\n",
      "\t Training loss (single batch): 1.240276575088501\n",
      "\t Training loss (single batch): 1.404059648513794\n",
      "\t Training loss (single batch): 1.3290683031082153\n",
      "\t Training loss (single batch): 1.2918918132781982\n",
      "\t Training loss (single batch): 1.0711374282836914\n",
      "\t Training loss (single batch): 1.9033925533294678\n",
      "\t Training loss (single batch): 1.4761561155319214\n",
      "\t Training loss (single batch): 1.3493901491165161\n",
      "\t Training loss (single batch): 0.9321762919425964\n",
      "\t Training loss (single batch): 1.4037290811538696\n",
      "\t Training loss (single batch): 1.1948580741882324\n",
      "\t Training loss (single batch): 1.3228671550750732\n",
      "\t Training loss (single batch): 0.9060682058334351\n",
      "\t Training loss (single batch): 0.8532594442367554\n",
      "\t Training loss (single batch): 1.1140791177749634\n",
      "\t Training loss (single batch): 1.4488297700881958\n",
      "\t Training loss (single batch): 1.3099067211151123\n",
      "\t Training loss (single batch): 1.3051096200942993\n",
      "\t Training loss (single batch): 1.2256296873092651\n",
      "\t Training loss (single batch): 1.029181718826294\n",
      "\t Training loss (single batch): 0.794386625289917\n",
      "\t Training loss (single batch): 1.121279001235962\n",
      "\t Training loss (single batch): 1.5809876918792725\n",
      "\t Training loss (single batch): 0.7541880011558533\n",
      "\t Training loss (single batch): 0.8841580748558044\n",
      "\t Training loss (single batch): 1.069031000137329\n",
      "\t Training loss (single batch): 1.1212058067321777\n",
      "\t Training loss (single batch): 1.424537181854248\n",
      "\t Training loss (single batch): 1.2075793743133545\n",
      "\t Training loss (single batch): 1.098010540008545\n",
      "\t Training loss (single batch): 0.9064772725105286\n",
      "\t Training loss (single batch): 1.3838332891464233\n",
      "\t Training loss (single batch): 1.6269899606704712\n",
      "\t Training loss (single batch): 1.3401340246200562\n",
      "\t Training loss (single batch): 1.215449571609497\n",
      "\t Training loss (single batch): 1.2042759656906128\n",
      "\t Training loss (single batch): 1.2542258501052856\n",
      "\t Training loss (single batch): 1.2638026475906372\n",
      "\t Training loss (single batch): 1.1819363832473755\n",
      "\t Training loss (single batch): 0.979447066783905\n",
      "\t Training loss (single batch): 2.0204148292541504\n",
      "\t Training loss (single batch): 0.6396827101707458\n",
      "\t Training loss (single batch): 0.9733995795249939\n",
      "\t Training loss (single batch): 1.2202037572860718\n",
      "\t Training loss (single batch): 1.0149438381195068\n",
      "\t Training loss (single batch): 1.4621299505233765\n",
      "\t Training loss (single batch): 0.976460874080658\n",
      "\t Training loss (single batch): 1.1310144662857056\n",
      "\t Training loss (single batch): 1.635929822921753\n",
      "\t Training loss (single batch): 1.1628657579421997\n",
      "\t Training loss (single batch): 1.4041937589645386\n",
      "\t Training loss (single batch): 1.1171951293945312\n",
      "\t Training loss (single batch): 1.6201822757720947\n",
      "\t Training loss (single batch): 1.5855127573013306\n",
      "\t Training loss (single batch): 1.070814847946167\n",
      "\t Training loss (single batch): 1.0613019466400146\n",
      "\t Training loss (single batch): 0.884292483329773\n",
      "\t Training loss (single batch): 1.3938381671905518\n",
      "\t Training loss (single batch): 1.7085893154144287\n",
      "\t Training loss (single batch): 1.4244645833969116\n",
      "##################################\n",
      "## EPOCH 44\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2074998617172241\n",
      "\t Training loss (single batch): 1.5359567403793335\n",
      "\t Training loss (single batch): 1.1197975873947144\n",
      "\t Training loss (single batch): 0.886965274810791\n",
      "\t Training loss (single batch): 1.5405296087265015\n",
      "\t Training loss (single batch): 1.0624138116836548\n",
      "\t Training loss (single batch): 1.4130562543869019\n",
      "\t Training loss (single batch): 1.1166720390319824\n",
      "\t Training loss (single batch): 0.978366494178772\n",
      "\t Training loss (single batch): 0.8810001015663147\n",
      "\t Training loss (single batch): 1.0899018049240112\n",
      "\t Training loss (single batch): 1.339805006980896\n",
      "\t Training loss (single batch): 1.3940300941467285\n",
      "\t Training loss (single batch): 1.0407131910324097\n",
      "\t Training loss (single batch): 1.5142956972122192\n",
      "\t Training loss (single batch): 1.3487763404846191\n",
      "\t Training loss (single batch): 1.596765160560608\n",
      "\t Training loss (single batch): 1.644490122795105\n",
      "\t Training loss (single batch): 1.0437346696853638\n",
      "\t Training loss (single batch): 1.324291467666626\n",
      "\t Training loss (single batch): 1.1963022947311401\n",
      "\t Training loss (single batch): 1.161566138267517\n",
      "\t Training loss (single batch): 0.9209249019622803\n",
      "\t Training loss (single batch): 1.0641045570373535\n",
      "\t Training loss (single batch): 1.1201106309890747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2051451206207275\n",
      "\t Training loss (single batch): 1.4688668251037598\n",
      "\t Training loss (single batch): 1.1767252683639526\n",
      "\t Training loss (single batch): 1.429681420326233\n",
      "\t Training loss (single batch): 1.3707504272460938\n",
      "\t Training loss (single batch): 0.9511326551437378\n",
      "\t Training loss (single batch): 1.0676316022872925\n",
      "\t Training loss (single batch): 0.7689980268478394\n",
      "\t Training loss (single batch): 1.2868025302886963\n",
      "\t Training loss (single batch): 1.2775901556015015\n",
      "\t Training loss (single batch): 1.3300895690917969\n",
      "\t Training loss (single batch): 1.1526024341583252\n",
      "\t Training loss (single batch): 1.235945224761963\n",
      "\t Training loss (single batch): 0.8396829962730408\n",
      "\t Training loss (single batch): 1.5115431547164917\n",
      "\t Training loss (single batch): 1.0099985599517822\n",
      "\t Training loss (single batch): 1.6095257997512817\n",
      "\t Training loss (single batch): 1.3134781122207642\n",
      "\t Training loss (single batch): 1.3811984062194824\n",
      "\t Training loss (single batch): 0.9413682222366333\n",
      "\t Training loss (single batch): 1.9689174890518188\n",
      "\t Training loss (single batch): 1.245571494102478\n",
      "\t Training loss (single batch): 1.0882272720336914\n",
      "\t Training loss (single batch): 1.147425651550293\n",
      "\t Training loss (single batch): 1.3701272010803223\n",
      "\t Training loss (single batch): 1.6167675256729126\n",
      "\t Training loss (single batch): 1.2221834659576416\n",
      "\t Training loss (single batch): 1.160681128501892\n",
      "\t Training loss (single batch): 1.6258354187011719\n",
      "\t Training loss (single batch): 1.3615094423294067\n",
      "\t Training loss (single batch): 1.097131371498108\n",
      "\t Training loss (single batch): 1.1025443077087402\n",
      "\t Training loss (single batch): 1.1176942586898804\n",
      "\t Training loss (single batch): 1.2705692052841187\n",
      "\t Training loss (single batch): 1.1420111656188965\n",
      "\t Training loss (single batch): 1.3365776538848877\n",
      "\t Training loss (single batch): 1.4454119205474854\n",
      "\t Training loss (single batch): 1.1282230615615845\n",
      "\t Training loss (single batch): 1.0439198017120361\n",
      "\t Training loss (single batch): 1.089717149734497\n",
      "\t Training loss (single batch): 1.123337984085083\n",
      "\t Training loss (single batch): 1.0096312761306763\n",
      "\t Training loss (single batch): 0.8657292723655701\n",
      "\t Training loss (single batch): 1.4677674770355225\n",
      "\t Training loss (single batch): 0.9928647875785828\n",
      "\t Training loss (single batch): 1.086087703704834\n",
      "\t Training loss (single batch): 1.1192082166671753\n",
      "\t Training loss (single batch): 1.4209171533584595\n",
      "\t Training loss (single batch): 0.8285865187644958\n",
      "\t Training loss (single batch): 1.447424292564392\n",
      "\t Training loss (single batch): 1.1654510498046875\n",
      "\t Training loss (single batch): 1.4194588661193848\n",
      "\t Training loss (single batch): 1.3203092813491821\n",
      "\t Training loss (single batch): 0.9735701680183411\n",
      "\t Training loss (single batch): 1.3851548433303833\n",
      "\t Training loss (single batch): 1.0523735284805298\n",
      "\t Training loss (single batch): 1.2356253862380981\n",
      "\t Training loss (single batch): 0.9794700145721436\n",
      "\t Training loss (single batch): 1.0401426553726196\n",
      "\t Training loss (single batch): 1.276785135269165\n",
      "\t Training loss (single batch): 1.0717697143554688\n",
      "\t Training loss (single batch): 1.1741316318511963\n",
      "\t Training loss (single batch): 1.170827865600586\n",
      "\t Training loss (single batch): 1.5846452713012695\n",
      "\t Training loss (single batch): 0.8631381988525391\n",
      "\t Training loss (single batch): 1.1926910877227783\n",
      "\t Training loss (single batch): 1.1251648664474487\n",
      "\t Training loss (single batch): 1.2201533317565918\n",
      "\t Training loss (single batch): 1.3953832387924194\n",
      "\t Training loss (single batch): 0.979422926902771\n",
      "\t Training loss (single batch): 1.7085328102111816\n",
      "\t Training loss (single batch): 1.110126256942749\n",
      "\t Training loss (single batch): 1.2022991180419922\n",
      "\t Training loss (single batch): 0.9983629584312439\n",
      "\t Training loss (single batch): 1.0730856657028198\n",
      "\t Training loss (single batch): 1.0484498739242554\n",
      "\t Training loss (single batch): 1.6805535554885864\n",
      "\t Training loss (single batch): 1.5992239713668823\n",
      "\t Training loss (single batch): 1.2639718055725098\n",
      "\t Training loss (single batch): 0.8296961188316345\n",
      "\t Training loss (single batch): 1.1091681718826294\n",
      "\t Training loss (single batch): 1.3816946744918823\n",
      "\t Training loss (single batch): 1.5343478918075562\n",
      "\t Training loss (single batch): 1.1360613107681274\n",
      "\t Training loss (single batch): 1.4293673038482666\n",
      "\t Training loss (single batch): 1.3404062986373901\n",
      "\t Training loss (single batch): 1.3061805963516235\n",
      "\t Training loss (single batch): 1.2230377197265625\n",
      "\t Training loss (single batch): 0.8602672815322876\n",
      "\t Training loss (single batch): 1.041571855545044\n",
      "\t Training loss (single batch): 1.3187799453735352\n",
      "\t Training loss (single batch): 1.1206443309783936\n",
      "\t Training loss (single batch): 0.9839869737625122\n",
      "\t Training loss (single batch): 1.6023850440979004\n",
      "\t Training loss (single batch): 1.1796125173568726\n",
      "\t Training loss (single batch): 0.8845404386520386\n",
      "\t Training loss (single batch): 1.0868819952011108\n",
      "\t Training loss (single batch): 1.0625395774841309\n",
      "\t Training loss (single batch): 1.395860195159912\n",
      "\t Training loss (single batch): 1.137865662574768\n",
      "\t Training loss (single batch): 1.3706411123275757\n",
      "\t Training loss (single batch): 0.8791472911834717\n",
      "\t Training loss (single batch): 1.127544641494751\n",
      "\t Training loss (single batch): 1.3705644607543945\n",
      "\t Training loss (single batch): 1.1447352170944214\n",
      "\t Training loss (single batch): 1.666124701499939\n",
      "\t Training loss (single batch): 1.0743706226348877\n",
      "\t Training loss (single batch): 1.3338212966918945\n",
      "\t Training loss (single batch): 1.4416509866714478\n",
      "\t Training loss (single batch): 1.6900041103363037\n",
      "\t Training loss (single batch): 1.5820410251617432\n",
      "\t Training loss (single batch): 0.7054797410964966\n",
      "\t Training loss (single batch): 1.3253154754638672\n",
      "\t Training loss (single batch): 1.3459237813949585\n",
      "\t Training loss (single batch): 0.7190846800804138\n",
      "\t Training loss (single batch): 0.875504195690155\n",
      "\t Training loss (single batch): 0.8866040110588074\n",
      "\t Training loss (single batch): 1.1755897998809814\n",
      "\t Training loss (single batch): 1.503483772277832\n",
      "\t Training loss (single batch): 1.1422175168991089\n",
      "\t Training loss (single batch): 0.8628438711166382\n",
      "\t Training loss (single batch): 1.236412525177002\n",
      "\t Training loss (single batch): 1.4225786924362183\n",
      "\t Training loss (single batch): 1.3492002487182617\n",
      "\t Training loss (single batch): 1.7476366758346558\n",
      "\t Training loss (single batch): 1.0489224195480347\n",
      "\t Training loss (single batch): 1.1325136423110962\n",
      "\t Training loss (single batch): 1.3127433061599731\n",
      "\t Training loss (single batch): 1.1813263893127441\n",
      "\t Training loss (single batch): 1.1623539924621582\n",
      "\t Training loss (single batch): 1.1104650497436523\n",
      "\t Training loss (single batch): 0.9479543566703796\n",
      "\t Training loss (single batch): 1.2972341775894165\n",
      "\t Training loss (single batch): 1.0439677238464355\n",
      "\t Training loss (single batch): 0.8527665734291077\n",
      "\t Training loss (single batch): 1.8106297254562378\n",
      "\t Training loss (single batch): 0.9511699676513672\n",
      "\t Training loss (single batch): 1.2340295314788818\n",
      "\t Training loss (single batch): 1.405419945716858\n",
      "\t Training loss (single batch): 1.3573896884918213\n",
      "\t Training loss (single batch): 1.396524429321289\n",
      "\t Training loss (single batch): 1.0404216051101685\n",
      "\t Training loss (single batch): 1.5114240646362305\n",
      "\t Training loss (single batch): 1.0437216758728027\n",
      "\t Training loss (single batch): 1.4477459192276\n",
      "\t Training loss (single batch): 1.1284363269805908\n",
      "\t Training loss (single batch): 1.5363060235977173\n",
      "\t Training loss (single batch): 1.2767921686172485\n",
      "\t Training loss (single batch): 1.7134333848953247\n",
      "\t Training loss (single batch): 1.5093740224838257\n",
      "\t Training loss (single batch): 1.416154384613037\n",
      "\t Training loss (single batch): 0.9359349012374878\n",
      "\t Training loss (single batch): 1.1383938789367676\n",
      "\t Training loss (single batch): 1.4749971628189087\n",
      "\t Training loss (single batch): 1.195539116859436\n",
      "\t Training loss (single batch): 1.1693421602249146\n",
      "\t Training loss (single batch): 1.2747963666915894\n",
      "\t Training loss (single batch): 1.2923533916473389\n",
      "\t Training loss (single batch): 1.079986810684204\n",
      "\t Training loss (single batch): 1.3121328353881836\n",
      "\t Training loss (single batch): 1.5789660215377808\n",
      "\t Training loss (single batch): 1.7281686067581177\n",
      "\t Training loss (single batch): 1.168396234512329\n",
      "\t Training loss (single batch): 1.0475221872329712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.508090853691101\n",
      "\t Training loss (single batch): 1.3862204551696777\n",
      "\t Training loss (single batch): 1.233901023864746\n",
      "\t Training loss (single batch): 1.2846348285675049\n",
      "\t Training loss (single batch): 1.4169355630874634\n",
      "\t Training loss (single batch): 1.1705611944198608\n",
      "\t Training loss (single batch): 1.6315852403640747\n",
      "\t Training loss (single batch): 0.9298904538154602\n",
      "\t Training loss (single batch): 1.402014136314392\n",
      "\t Training loss (single batch): 0.973063051700592\n",
      "\t Training loss (single batch): 1.1534069776535034\n",
      "\t Training loss (single batch): 0.8565287590026855\n",
      "\t Training loss (single batch): 1.40473473072052\n",
      "\t Training loss (single batch): 1.2956775426864624\n",
      "\t Training loss (single batch): 1.4135117530822754\n",
      "\t Training loss (single batch): 1.4544333219528198\n",
      "\t Training loss (single batch): 1.2887414693832397\n",
      "\t Training loss (single batch): 2.188251256942749\n",
      "\t Training loss (single batch): 1.5203784704208374\n",
      "\t Training loss (single batch): 1.400078296661377\n",
      "\t Training loss (single batch): 0.9067318439483643\n",
      "\t Training loss (single batch): 1.5319229364395142\n",
      "\t Training loss (single batch): 1.0751663446426392\n",
      "\t Training loss (single batch): 0.9142454266548157\n",
      "\t Training loss (single batch): 1.2197262048721313\n",
      "\t Training loss (single batch): 1.1126667261123657\n",
      "\t Training loss (single batch): 1.3283600807189941\n",
      "\t Training loss (single batch): 0.8849033117294312\n",
      "\t Training loss (single batch): 1.3170831203460693\n",
      "\t Training loss (single batch): 1.2692866325378418\n",
      "\t Training loss (single batch): 1.3952456712722778\n",
      "\t Training loss (single batch): 1.167878270149231\n",
      "\t Training loss (single batch): 0.9171890616416931\n",
      "\t Training loss (single batch): 1.1682324409484863\n",
      "\t Training loss (single batch): 1.7718535661697388\n",
      "\t Training loss (single batch): 1.0549594163894653\n",
      "\t Training loss (single batch): 1.1951643228530884\n",
      "\t Training loss (single batch): 1.8075917959213257\n",
      "\t Training loss (single batch): 0.9568084478378296\n",
      "\t Training loss (single batch): 1.5496227741241455\n",
      "\t Training loss (single batch): 0.9350169897079468\n",
      "\t Training loss (single batch): 2.1369268894195557\n",
      "\t Training loss (single batch): 0.937606155872345\n",
      "\t Training loss (single batch): 1.2722411155700684\n",
      "\t Training loss (single batch): 1.0860822200775146\n",
      "\t Training loss (single batch): 0.9407057762145996\n",
      "\t Training loss (single batch): 1.0212810039520264\n",
      "\t Training loss (single batch): 1.1124876737594604\n",
      "\t Training loss (single batch): 1.371937870979309\n",
      "\t Training loss (single batch): 1.1284679174423218\n",
      "\t Training loss (single batch): 0.6973854899406433\n",
      "\t Training loss (single batch): 0.9257254004478455\n",
      "\t Training loss (single batch): 1.405680775642395\n",
      "\t Training loss (single batch): 1.262769341468811\n",
      "\t Training loss (single batch): 0.8912892937660217\n",
      "\t Training loss (single batch): 0.8812757134437561\n",
      "\t Training loss (single batch): 1.2648102045059204\n",
      "\t Training loss (single batch): 1.2007131576538086\n",
      "\t Training loss (single batch): 1.3565629720687866\n",
      "\t Training loss (single batch): 0.9976349472999573\n",
      "\t Training loss (single batch): 1.2735320329666138\n",
      "\t Training loss (single batch): 1.1555691957473755\n",
      "\t Training loss (single batch): 1.4354716539382935\n",
      "\t Training loss (single batch): 1.2747780084609985\n",
      "\t Training loss (single batch): 1.227311611175537\n",
      "\t Training loss (single batch): 1.1110838651657104\n",
      "\t Training loss (single batch): 1.3213990926742554\n",
      "\t Training loss (single batch): 1.0769554376602173\n",
      "\t Training loss (single batch): 0.9465949535369873\n",
      "\t Training loss (single batch): 1.3187403678894043\n",
      "\t Training loss (single batch): 0.7878382802009583\n",
      "\t Training loss (single batch): 0.912980854511261\n",
      "\t Training loss (single batch): 1.3849197626113892\n",
      "\t Training loss (single batch): 1.4964632987976074\n",
      "\t Training loss (single batch): 0.973983883857727\n",
      "\t Training loss (single batch): 1.3245028257369995\n",
      "\t Training loss (single batch): 1.1789504289627075\n",
      "\t Training loss (single batch): 1.0734748840332031\n",
      "\t Training loss (single batch): 1.4393043518066406\n",
      "\t Training loss (single batch): 0.9674961566925049\n",
      "\t Training loss (single batch): 0.8289815783500671\n",
      "\t Training loss (single batch): 1.7414441108703613\n",
      "\t Training loss (single batch): 0.9634267687797546\n",
      "\t Training loss (single batch): 1.0815762281417847\n",
      "\t Training loss (single batch): 1.183512568473816\n",
      "\t Training loss (single batch): 1.686458706855774\n",
      "\t Training loss (single batch): 1.7325530052185059\n",
      "\t Training loss (single batch): 0.9083782434463501\n",
      "\t Training loss (single batch): 1.8413594961166382\n",
      "\t Training loss (single batch): 1.4013322591781616\n",
      "\t Training loss (single batch): 1.0753788948059082\n",
      "\t Training loss (single batch): 1.110612154006958\n",
      "\t Training loss (single batch): 0.8531670570373535\n",
      "\t Training loss (single batch): 1.6018714904785156\n",
      "\t Training loss (single batch): 1.740198016166687\n",
      "\t Training loss (single batch): 1.7725093364715576\n",
      "\t Training loss (single batch): 1.884263038635254\n",
      "\t Training loss (single batch): 0.8158091306686401\n",
      "\t Training loss (single batch): 0.9499083161354065\n",
      "\t Training loss (single batch): 0.9397648572921753\n",
      "\t Training loss (single batch): 0.6958813071250916\n",
      "\t Training loss (single batch): 1.2794954776763916\n",
      "\t Training loss (single batch): 1.0001693964004517\n",
      "\t Training loss (single batch): 1.0778422355651855\n",
      "\t Training loss (single batch): 1.3999468088150024\n",
      "\t Training loss (single batch): 1.4059927463531494\n",
      "\t Training loss (single batch): 0.819834291934967\n",
      "\t Training loss (single batch): 1.2552186250686646\n",
      "\t Training loss (single batch): 1.6375818252563477\n",
      "\t Training loss (single batch): 0.9345158338546753\n",
      "\t Training loss (single batch): 1.0951781272888184\n",
      "\t Training loss (single batch): 1.1078884601593018\n",
      "\t Training loss (single batch): 1.4600543975830078\n",
      "\t Training loss (single batch): 0.9328410029411316\n",
      "\t Training loss (single batch): 1.0132883787155151\n",
      "\t Training loss (single batch): 1.6036410331726074\n",
      "\t Training loss (single batch): 1.44770348072052\n",
      "\t Training loss (single batch): 1.2604647874832153\n",
      "\t Training loss (single batch): 1.3076109886169434\n",
      "\t Training loss (single batch): 0.8022190928459167\n",
      "\t Training loss (single batch): 1.4675947427749634\n",
      "\t Training loss (single batch): 1.1453802585601807\n",
      "\t Training loss (single batch): 1.501128911972046\n",
      "\t Training loss (single batch): 1.3075189590454102\n",
      "\t Training loss (single batch): 1.0925832986831665\n",
      "\t Training loss (single batch): 1.328122854232788\n",
      "\t Training loss (single batch): 1.0914555788040161\n",
      "\t Training loss (single batch): 2.109229803085327\n",
      "##################################\n",
      "## EPOCH 45\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5402393341064453\n",
      "\t Training loss (single batch): 1.1259809732437134\n",
      "\t Training loss (single batch): 0.9433607459068298\n",
      "\t Training loss (single batch): 1.6644296646118164\n",
      "\t Training loss (single batch): 1.2968838214874268\n",
      "\t Training loss (single batch): 1.38313627243042\n",
      "\t Training loss (single batch): 1.468915581703186\n",
      "\t Training loss (single batch): 0.8711315989494324\n",
      "\t Training loss (single batch): 1.8258260488510132\n",
      "\t Training loss (single batch): 0.8952645659446716\n",
      "\t Training loss (single batch): 1.6619634628295898\n",
      "\t Training loss (single batch): 1.2116252183914185\n",
      "\t Training loss (single batch): 1.319877028465271\n",
      "\t Training loss (single batch): 1.4839321374893188\n",
      "\t Training loss (single batch): 1.1808407306671143\n",
      "\t Training loss (single batch): 1.2738360166549683\n",
      "\t Training loss (single batch): 1.0985928773880005\n",
      "\t Training loss (single batch): 0.7571231126785278\n",
      "\t Training loss (single batch): 1.284077763557434\n",
      "\t Training loss (single batch): 1.0606095790863037\n",
      "\t Training loss (single batch): 1.2529124021530151\n",
      "\t Training loss (single batch): 0.9072743654251099\n",
      "\t Training loss (single batch): 1.2223902940750122\n",
      "\t Training loss (single batch): 0.8986592292785645\n",
      "\t Training loss (single batch): 1.308648705482483\n",
      "\t Training loss (single batch): 0.9510934948921204\n",
      "\t Training loss (single batch): 1.2668654918670654\n",
      "\t Training loss (single batch): 1.3225051164627075\n",
      "\t Training loss (single batch): 1.353297233581543\n",
      "\t Training loss (single batch): 1.0333322286605835\n",
      "\t Training loss (single batch): 1.3097419738769531\n",
      "\t Training loss (single batch): 1.3121273517608643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1686463356018066\n",
      "\t Training loss (single batch): 1.5870921611785889\n",
      "\t Training loss (single batch): 1.378728985786438\n",
      "\t Training loss (single batch): 0.76658695936203\n",
      "\t Training loss (single batch): 1.0601450204849243\n",
      "\t Training loss (single batch): 2.059830665588379\n",
      "\t Training loss (single batch): 1.7346049547195435\n",
      "\t Training loss (single batch): 1.7236216068267822\n",
      "\t Training loss (single batch): 0.9344507455825806\n",
      "\t Training loss (single batch): 0.8557824492454529\n",
      "\t Training loss (single batch): 1.1993461847305298\n",
      "\t Training loss (single batch): 1.4559087753295898\n",
      "\t Training loss (single batch): 0.9423958659172058\n",
      "\t Training loss (single batch): 0.7843888401985168\n",
      "\t Training loss (single batch): 1.3480441570281982\n",
      "\t Training loss (single batch): 0.788857638835907\n",
      "\t Training loss (single batch): 1.0431255102157593\n",
      "\t Training loss (single batch): 1.044376254081726\n",
      "\t Training loss (single batch): 1.141485571861267\n",
      "\t Training loss (single batch): 1.3976463079452515\n",
      "\t Training loss (single batch): 0.9498026371002197\n",
      "\t Training loss (single batch): 1.7418168783187866\n",
      "\t Training loss (single batch): 1.1829885244369507\n",
      "\t Training loss (single batch): 1.4592686891555786\n",
      "\t Training loss (single batch): 1.5387707948684692\n",
      "\t Training loss (single batch): 0.9092221856117249\n",
      "\t Training loss (single batch): 1.2490813732147217\n",
      "\t Training loss (single batch): 0.9651134014129639\n",
      "\t Training loss (single batch): 1.7397511005401611\n",
      "\t Training loss (single batch): 1.1034857034683228\n",
      "\t Training loss (single batch): 0.7570883631706238\n",
      "\t Training loss (single batch): 1.4445245265960693\n",
      "\t Training loss (single batch): 1.4052003622055054\n",
      "\t Training loss (single batch): 1.2980127334594727\n",
      "\t Training loss (single batch): 1.356668472290039\n",
      "\t Training loss (single batch): 1.0245251655578613\n",
      "\t Training loss (single batch): 1.8117456436157227\n",
      "\t Training loss (single batch): 1.126596212387085\n",
      "\t Training loss (single batch): 1.3774827718734741\n",
      "\t Training loss (single batch): 0.9318135976791382\n",
      "\t Training loss (single batch): 1.352160930633545\n",
      "\t Training loss (single batch): 1.1136208772659302\n",
      "\t Training loss (single batch): 0.9751307368278503\n",
      "\t Training loss (single batch): 1.1994006633758545\n",
      "\t Training loss (single batch): 1.4519703388214111\n",
      "\t Training loss (single batch): 1.276501178741455\n",
      "\t Training loss (single batch): 1.4005119800567627\n",
      "\t Training loss (single batch): 1.3554331064224243\n",
      "\t Training loss (single batch): 0.8548086285591125\n",
      "\t Training loss (single batch): 1.3025918006896973\n",
      "\t Training loss (single batch): 1.4712636470794678\n",
      "\t Training loss (single batch): 1.0940470695495605\n",
      "\t Training loss (single batch): 1.3732740879058838\n",
      "\t Training loss (single batch): 1.6306177377700806\n",
      "\t Training loss (single batch): 1.0149353742599487\n",
      "\t Training loss (single batch): 0.9261993169784546\n",
      "\t Training loss (single batch): 1.6788681745529175\n",
      "\t Training loss (single batch): 1.0481113195419312\n",
      "\t Training loss (single batch): 1.5229748487472534\n",
      "\t Training loss (single batch): 0.9963223934173584\n",
      "\t Training loss (single batch): 1.3365733623504639\n",
      "\t Training loss (single batch): 1.3399513959884644\n",
      "\t Training loss (single batch): 1.659759283065796\n",
      "\t Training loss (single batch): 1.2483210563659668\n",
      "\t Training loss (single batch): 1.0231785774230957\n",
      "\t Training loss (single batch): 1.4886802434921265\n",
      "\t Training loss (single batch): 1.0306512117385864\n",
      "\t Training loss (single batch): 1.4734413623809814\n",
      "\t Training loss (single batch): 1.1567519903182983\n",
      "\t Training loss (single batch): 0.9645466804504395\n",
      "\t Training loss (single batch): 1.379714012145996\n",
      "\t Training loss (single batch): 1.1562176942825317\n",
      "\t Training loss (single batch): 1.3742015361785889\n",
      "\t Training loss (single batch): 2.0748190879821777\n",
      "\t Training loss (single batch): 1.3240861892700195\n",
      "\t Training loss (single batch): 1.1655490398406982\n",
      "\t Training loss (single batch): 1.252445936203003\n",
      "\t Training loss (single batch): 0.8761046528816223\n",
      "\t Training loss (single batch): 1.184651494026184\n",
      "\t Training loss (single batch): 1.6365572214126587\n",
      "\t Training loss (single batch): 1.3014475107192993\n",
      "\t Training loss (single batch): 0.9858792424201965\n",
      "\t Training loss (single batch): 1.1808910369873047\n",
      "\t Training loss (single batch): 1.5852636098861694\n",
      "\t Training loss (single batch): 1.163346290588379\n",
      "\t Training loss (single batch): 0.9211819171905518\n",
      "\t Training loss (single batch): 1.3645366430282593\n",
      "\t Training loss (single batch): 1.5745491981506348\n",
      "\t Training loss (single batch): 1.2652918100357056\n",
      "\t Training loss (single batch): 1.4605191946029663\n",
      "\t Training loss (single batch): 1.3870052099227905\n",
      "\t Training loss (single batch): 1.2660222053527832\n",
      "\t Training loss (single batch): 1.2809090614318848\n",
      "\t Training loss (single batch): 1.3354179859161377\n",
      "\t Training loss (single batch): 1.7926371097564697\n",
      "\t Training loss (single batch): 1.2090933322906494\n",
      "\t Training loss (single batch): 1.4905080795288086\n",
      "\t Training loss (single batch): 1.4704172611236572\n",
      "\t Training loss (single batch): 0.5914530754089355\n",
      "\t Training loss (single batch): 1.2332104444503784\n",
      "\t Training loss (single batch): 1.048003911972046\n",
      "\t Training loss (single batch): 0.9013627767562866\n",
      "\t Training loss (single batch): 1.4767990112304688\n",
      "\t Training loss (single batch): 1.2484771013259888\n",
      "\t Training loss (single batch): 1.3349803686141968\n",
      "\t Training loss (single batch): 1.0871721506118774\n",
      "\t Training loss (single batch): 1.305023193359375\n",
      "\t Training loss (single batch): 1.4004994630813599\n",
      "\t Training loss (single batch): 1.4906959533691406\n",
      "\t Training loss (single batch): 1.217061996459961\n",
      "\t Training loss (single batch): 1.2659590244293213\n",
      "\t Training loss (single batch): 1.0313422679901123\n",
      "\t Training loss (single batch): 1.1866838932037354\n",
      "\t Training loss (single batch): 0.8592208623886108\n",
      "\t Training loss (single batch): 0.9313284754753113\n",
      "\t Training loss (single batch): 1.18979012966156\n",
      "\t Training loss (single batch): 1.255638837814331\n",
      "\t Training loss (single batch): 1.329107642173767\n",
      "\t Training loss (single batch): 1.378204107284546\n",
      "\t Training loss (single batch): 1.0174391269683838\n",
      "\t Training loss (single batch): 0.9308830499649048\n",
      "\t Training loss (single batch): 1.233993411064148\n",
      "\t Training loss (single batch): 1.461280345916748\n",
      "\t Training loss (single batch): 2.2365572452545166\n",
      "\t Training loss (single batch): 1.3829481601715088\n",
      "\t Training loss (single batch): 0.9863055348396301\n",
      "\t Training loss (single batch): 1.8494065999984741\n",
      "\t Training loss (single batch): 1.4371232986450195\n",
      "\t Training loss (single batch): 1.2644922733306885\n",
      "\t Training loss (single batch): 1.0397883653640747\n",
      "\t Training loss (single batch): 1.2009037733078003\n",
      "\t Training loss (single batch): 1.3392374515533447\n",
      "\t Training loss (single batch): 1.3127614259719849\n",
      "\t Training loss (single batch): 1.4336436986923218\n",
      "\t Training loss (single batch): 1.2513811588287354\n",
      "\t Training loss (single batch): 1.0508735179901123\n",
      "\t Training loss (single batch): 1.2096500396728516\n",
      "\t Training loss (single batch): 1.1666966676712036\n",
      "\t Training loss (single batch): 1.373486876487732\n",
      "\t Training loss (single batch): 1.2498629093170166\n",
      "\t Training loss (single batch): 1.157824158668518\n",
      "\t Training loss (single batch): 0.8911975026130676\n",
      "\t Training loss (single batch): 1.1460210084915161\n",
      "\t Training loss (single batch): 0.9972825646400452\n",
      "\t Training loss (single batch): 1.3312323093414307\n",
      "\t Training loss (single batch): 1.2290931940078735\n",
      "\t Training loss (single batch): 1.5515892505645752\n",
      "\t Training loss (single batch): 1.0828537940979004\n",
      "\t Training loss (single batch): 1.0573002099990845\n",
      "\t Training loss (single batch): 1.1969354152679443\n",
      "\t Training loss (single batch): 1.2178409099578857\n",
      "\t Training loss (single batch): 1.189377784729004\n",
      "\t Training loss (single batch): 1.0303970575332642\n",
      "\t Training loss (single batch): 1.2341468334197998\n",
      "\t Training loss (single batch): 1.370116114616394\n",
      "\t Training loss (single batch): 1.2682443857192993\n",
      "\t Training loss (single batch): 1.6937130689620972\n",
      "\t Training loss (single batch): 1.1838546991348267\n",
      "\t Training loss (single batch): 0.9572606086730957\n",
      "\t Training loss (single batch): 1.2543493509292603\n",
      "\t Training loss (single batch): 1.2848033905029297\n",
      "\t Training loss (single batch): 1.2236016988754272\n",
      "\t Training loss (single batch): 0.9660959243774414\n",
      "\t Training loss (single batch): 0.6954865455627441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.265848994255066\n",
      "\t Training loss (single batch): 1.3515480756759644\n",
      "\t Training loss (single batch): 1.3735555410385132\n",
      "\t Training loss (single batch): 1.2101080417633057\n",
      "\t Training loss (single batch): 1.4368623495101929\n",
      "\t Training loss (single batch): 0.8506459593772888\n",
      "\t Training loss (single batch): 1.0268034934997559\n",
      "\t Training loss (single batch): 1.2864925861358643\n",
      "\t Training loss (single batch): 1.5256322622299194\n",
      "\t Training loss (single batch): 1.0633388757705688\n",
      "\t Training loss (single batch): 1.3502867221832275\n",
      "\t Training loss (single batch): 1.3274414539337158\n",
      "\t Training loss (single batch): 1.568365216255188\n",
      "\t Training loss (single batch): 0.9421156048774719\n",
      "\t Training loss (single batch): 1.3198424577713013\n",
      "\t Training loss (single batch): 1.7184404134750366\n",
      "\t Training loss (single batch): 1.4603008031845093\n",
      "\t Training loss (single batch): 1.1064867973327637\n",
      "\t Training loss (single batch): 1.1768354177474976\n",
      "\t Training loss (single batch): 1.4790455102920532\n",
      "\t Training loss (single batch): 1.361113429069519\n",
      "\t Training loss (single batch): 1.378435730934143\n",
      "\t Training loss (single batch): 1.1493792533874512\n",
      "\t Training loss (single batch): 1.2833373546600342\n",
      "\t Training loss (single batch): 1.1166778802871704\n",
      "\t Training loss (single batch): 1.1155825853347778\n",
      "\t Training loss (single batch): 1.2385510206222534\n",
      "\t Training loss (single batch): 1.4173141717910767\n",
      "\t Training loss (single batch): 1.4601612091064453\n",
      "\t Training loss (single batch): 0.9346457719802856\n",
      "\t Training loss (single batch): 1.6392909288406372\n",
      "\t Training loss (single batch): 1.5069546699523926\n",
      "\t Training loss (single batch): 1.0850718021392822\n",
      "\t Training loss (single batch): 1.059078574180603\n",
      "\t Training loss (single batch): 1.3279366493225098\n",
      "\t Training loss (single batch): 1.2698044776916504\n",
      "\t Training loss (single batch): 0.8937013149261475\n",
      "\t Training loss (single batch): 1.3322633504867554\n",
      "\t Training loss (single batch): 0.9947041869163513\n",
      "\t Training loss (single batch): 1.254435420036316\n",
      "\t Training loss (single batch): 1.2454662322998047\n",
      "\t Training loss (single batch): 1.331987977027893\n",
      "\t Training loss (single batch): 0.9946619272232056\n",
      "\t Training loss (single batch): 0.9600155353546143\n",
      "\t Training loss (single batch): 1.504837155342102\n",
      "\t Training loss (single batch): 1.2701157331466675\n",
      "\t Training loss (single batch): 1.3291378021240234\n",
      "\t Training loss (single batch): 0.7918970584869385\n",
      "\t Training loss (single batch): 1.4669116735458374\n",
      "\t Training loss (single batch): 1.1742326021194458\n",
      "\t Training loss (single batch): 1.285464882850647\n",
      "\t Training loss (single batch): 1.1264598369598389\n",
      "\t Training loss (single batch): 1.0395898818969727\n",
      "\t Training loss (single batch): 1.5752618312835693\n",
      "\t Training loss (single batch): 1.1753898859024048\n",
      "\t Training loss (single batch): 1.1278611421585083\n",
      "\t Training loss (single batch): 1.049239158630371\n",
      "\t Training loss (single batch): 0.9159954190254211\n",
      "\t Training loss (single batch): 1.4462523460388184\n",
      "\t Training loss (single batch): 1.1387676000595093\n",
      "\t Training loss (single batch): 1.2182071208953857\n",
      "\t Training loss (single batch): 1.2960174083709717\n",
      "\t Training loss (single batch): 1.3014318943023682\n",
      "\t Training loss (single batch): 1.4640593528747559\n",
      "\t Training loss (single batch): 1.4830033779144287\n",
      "\t Training loss (single batch): 1.24392831325531\n",
      "\t Training loss (single batch): 1.208539605140686\n",
      "\t Training loss (single batch): 1.3117051124572754\n",
      "\t Training loss (single batch): 1.1179417371749878\n",
      "\t Training loss (single batch): 1.0436019897460938\n",
      "\t Training loss (single batch): 1.2854018211364746\n",
      "\t Training loss (single batch): 1.469291090965271\n",
      "\t Training loss (single batch): 0.9589453935623169\n",
      "\t Training loss (single batch): 1.3893932104110718\n",
      "\t Training loss (single batch): 1.3384748697280884\n",
      "\t Training loss (single batch): 1.0708341598510742\n",
      "\t Training loss (single batch): 0.819963276386261\n",
      "\t Training loss (single batch): 1.2449253797531128\n",
      "\t Training loss (single batch): 1.3636384010314941\n",
      "\t Training loss (single batch): 1.0355534553527832\n",
      "\t Training loss (single batch): 1.4113935232162476\n",
      "\t Training loss (single batch): 1.2765713930130005\n",
      "\t Training loss (single batch): 1.5630428791046143\n",
      "\t Training loss (single batch): 1.272514820098877\n",
      "\t Training loss (single batch): 0.9121214151382446\n",
      "\t Training loss (single batch): 1.1194602251052856\n",
      "\t Training loss (single batch): 1.1496644020080566\n",
      "\t Training loss (single batch): 1.6658636331558228\n",
      "\t Training loss (single batch): 1.3468477725982666\n",
      "\t Training loss (single batch): 0.8753536939620972\n",
      "\t Training loss (single batch): 0.9231894612312317\n",
      "\t Training loss (single batch): 1.218582034111023\n",
      "\t Training loss (single batch): 1.4654288291931152\n",
      "\t Training loss (single batch): 0.8826682567596436\n",
      "\t Training loss (single batch): 1.7829420566558838\n",
      "\t Training loss (single batch): 1.1237045526504517\n",
      "\t Training loss (single batch): 0.7297333478927612\n",
      "\t Training loss (single batch): 0.8677921295166016\n",
      "\t Training loss (single batch): 0.9794847369194031\n",
      "\t Training loss (single batch): 1.017810583114624\n",
      "\t Training loss (single batch): 1.0460182428359985\n",
      "\t Training loss (single batch): 1.391756534576416\n",
      "\t Training loss (single batch): 1.30776846408844\n",
      "\t Training loss (single batch): 1.029208779335022\n",
      "\t Training loss (single batch): 1.5016237497329712\n",
      "\t Training loss (single batch): 1.1776396036148071\n",
      "\t Training loss (single batch): 0.9560025930404663\n",
      "\t Training loss (single batch): 1.289966106414795\n",
      "\t Training loss (single batch): 0.9817086458206177\n",
      "\t Training loss (single batch): 1.4173414707183838\n",
      "\t Training loss (single batch): 1.355512022972107\n",
      "\t Training loss (single batch): 1.9749141931533813\n",
      "\t Training loss (single batch): 1.2283916473388672\n",
      "\t Training loss (single batch): 1.1620755195617676\n",
      "\t Training loss (single batch): 1.1137845516204834\n",
      "\t Training loss (single batch): 1.4650132656097412\n",
      "\t Training loss (single batch): 1.5255517959594727\n",
      "\t Training loss (single batch): 0.9994632601737976\n",
      "\t Training loss (single batch): 1.5401712656021118\n",
      "\t Training loss (single batch): 0.9634143114089966\n",
      "\t Training loss (single batch): 0.6520515084266663\n",
      "##################################\n",
      "## EPOCH 46\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4086073637008667\n",
      "\t Training loss (single batch): 1.1952773332595825\n",
      "\t Training loss (single batch): 1.02413809299469\n",
      "\t Training loss (single batch): 1.798333764076233\n",
      "\t Training loss (single batch): 1.0927327871322632\n",
      "\t Training loss (single batch): 1.166486382484436\n",
      "\t Training loss (single batch): 0.814039409160614\n",
      "\t Training loss (single batch): 1.5038598775863647\n",
      "\t Training loss (single batch): 1.2514991760253906\n",
      "\t Training loss (single batch): 1.005977749824524\n",
      "\t Training loss (single batch): 1.3958685398101807\n",
      "\t Training loss (single batch): 1.0029186010360718\n",
      "\t Training loss (single batch): 0.9151136875152588\n",
      "\t Training loss (single batch): 0.893447995185852\n",
      "\t Training loss (single batch): 0.7105823159217834\n",
      "\t Training loss (single batch): 0.7324375510215759\n",
      "\t Training loss (single batch): 1.297576904296875\n",
      "\t Training loss (single batch): 1.1097756624221802\n",
      "\t Training loss (single batch): 1.0458611249923706\n",
      "\t Training loss (single batch): 1.2910634279251099\n",
      "\t Training loss (single batch): 1.1446727514266968\n",
      "\t Training loss (single batch): 0.8034423589706421\n",
      "\t Training loss (single batch): 1.4822938442230225\n",
      "\t Training loss (single batch): 1.344862937927246\n",
      "\t Training loss (single batch): 1.1486977338790894\n",
      "\t Training loss (single batch): 0.8537850975990295\n",
      "\t Training loss (single batch): 0.9287227392196655\n",
      "\t Training loss (single batch): 1.1038683652877808\n",
      "\t Training loss (single batch): 1.4102319478988647\n",
      "\t Training loss (single batch): 1.1329410076141357\n",
      "\t Training loss (single batch): 0.8325194120407104\n",
      "\t Training loss (single batch): 1.2291433811187744\n",
      "\t Training loss (single batch): 1.3337923288345337\n",
      "\t Training loss (single batch): 1.2889195680618286\n",
      "\t Training loss (single batch): 1.0656849145889282\n",
      "\t Training loss (single batch): 0.8654963970184326\n",
      "\t Training loss (single batch): 1.1342802047729492\n",
      "\t Training loss (single batch): 0.8041298985481262\n",
      "\t Training loss (single batch): 1.4595818519592285\n",
      "\t Training loss (single batch): 1.2527236938476562\n",
      "\t Training loss (single batch): 1.008859395980835\n",
      "\t Training loss (single batch): 1.0363926887512207\n",
      "\t Training loss (single batch): 1.0797532796859741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3341282606124878\n",
      "\t Training loss (single batch): 1.407151699066162\n",
      "\t Training loss (single batch): 1.178115725517273\n",
      "\t Training loss (single batch): 1.6970981359481812\n",
      "\t Training loss (single batch): 0.9873447418212891\n",
      "\t Training loss (single batch): 1.1979517936706543\n",
      "\t Training loss (single batch): 0.8496050238609314\n",
      "\t Training loss (single batch): 1.0394469499588013\n",
      "\t Training loss (single batch): 1.4662193059921265\n",
      "\t Training loss (single batch): 1.4594662189483643\n",
      "\t Training loss (single batch): 1.542560338973999\n",
      "\t Training loss (single batch): 0.9815413951873779\n",
      "\t Training loss (single batch): 0.9968412518501282\n",
      "\t Training loss (single batch): 1.5017123222351074\n",
      "\t Training loss (single batch): 1.4413889646530151\n",
      "\t Training loss (single batch): 1.1088331937789917\n",
      "\t Training loss (single batch): 1.6120038032531738\n",
      "\t Training loss (single batch): 1.5657058954238892\n",
      "\t Training loss (single batch): 1.8008813858032227\n",
      "\t Training loss (single batch): 1.6769707202911377\n",
      "\t Training loss (single batch): 0.9921636581420898\n",
      "\t Training loss (single batch): 1.3323438167572021\n",
      "\t Training loss (single batch): 1.2470142841339111\n",
      "\t Training loss (single batch): 0.8955681324005127\n",
      "\t Training loss (single batch): 1.3341951370239258\n",
      "\t Training loss (single batch): 1.5832533836364746\n",
      "\t Training loss (single batch): 1.3011151552200317\n",
      "\t Training loss (single batch): 1.091881513595581\n",
      "\t Training loss (single batch): 1.6324512958526611\n",
      "\t Training loss (single batch): 1.1630915403366089\n",
      "\t Training loss (single batch): 1.1764880418777466\n",
      "\t Training loss (single batch): 1.3385380506515503\n",
      "\t Training loss (single batch): 0.9982122182846069\n",
      "\t Training loss (single batch): 1.2410269975662231\n",
      "\t Training loss (single batch): 1.143604040145874\n",
      "\t Training loss (single batch): 1.1884682178497314\n",
      "\t Training loss (single batch): 1.0704710483551025\n",
      "\t Training loss (single batch): 0.7268673181533813\n",
      "\t Training loss (single batch): 0.4656020700931549\n",
      "\t Training loss (single batch): 0.8645036220550537\n",
      "\t Training loss (single batch): 1.2361112833023071\n",
      "\t Training loss (single batch): 1.702953815460205\n",
      "\t Training loss (single batch): 1.4599542617797852\n",
      "\t Training loss (single batch): 1.4701008796691895\n",
      "\t Training loss (single batch): 1.4065954685211182\n",
      "\t Training loss (single batch): 1.5622934103012085\n",
      "\t Training loss (single batch): 1.099427580833435\n",
      "\t Training loss (single batch): 1.4767321348190308\n",
      "\t Training loss (single batch): 1.5552271604537964\n",
      "\t Training loss (single batch): 1.2477641105651855\n",
      "\t Training loss (single batch): 1.223349690437317\n",
      "\t Training loss (single batch): 0.9186480641365051\n",
      "\t Training loss (single batch): 1.0720868110656738\n",
      "\t Training loss (single batch): 1.3801515102386475\n",
      "\t Training loss (single batch): 1.0510048866271973\n",
      "\t Training loss (single batch): 0.8171067833900452\n",
      "\t Training loss (single batch): 1.5840922594070435\n",
      "\t Training loss (single batch): 1.0114279985427856\n",
      "\t Training loss (single batch): 1.733170509338379\n",
      "\t Training loss (single batch): 1.4220459461212158\n",
      "\t Training loss (single batch): 1.002736210823059\n",
      "\t Training loss (single batch): 1.3528220653533936\n",
      "\t Training loss (single batch): 1.2167646884918213\n",
      "\t Training loss (single batch): 1.0939041376113892\n",
      "\t Training loss (single batch): 0.8647629022598267\n",
      "\t Training loss (single batch): 1.0691229104995728\n",
      "\t Training loss (single batch): 1.0565706491470337\n",
      "\t Training loss (single batch): 0.9419469237327576\n",
      "\t Training loss (single batch): 1.3461846113204956\n",
      "\t Training loss (single batch): 1.4895466566085815\n",
      "\t Training loss (single batch): 1.4359074831008911\n",
      "\t Training loss (single batch): 1.0389375686645508\n",
      "\t Training loss (single batch): 1.2253745794296265\n",
      "\t Training loss (single batch): 1.2316932678222656\n",
      "\t Training loss (single batch): 1.0417557954788208\n",
      "\t Training loss (single batch): 1.9215525388717651\n",
      "\t Training loss (single batch): 1.1134955883026123\n",
      "\t Training loss (single batch): 0.6988623142242432\n",
      "\t Training loss (single batch): 1.0537188053131104\n",
      "\t Training loss (single batch): 1.042823076248169\n",
      "\t Training loss (single batch): 0.9944325685501099\n",
      "\t Training loss (single batch): 1.1702935695648193\n",
      "\t Training loss (single batch): 1.105453610420227\n",
      "\t Training loss (single batch): 1.6730811595916748\n",
      "\t Training loss (single batch): 1.4407639503479004\n",
      "\t Training loss (single batch): 1.1837488412857056\n",
      "\t Training loss (single batch): 1.3761224746704102\n",
      "\t Training loss (single batch): 1.1506531238555908\n",
      "\t Training loss (single batch): 1.1148380041122437\n",
      "\t Training loss (single batch): 1.7321537733078003\n",
      "\t Training loss (single batch): 1.5642961263656616\n",
      "\t Training loss (single batch): 1.6393402814865112\n",
      "\t Training loss (single batch): 1.7994744777679443\n",
      "\t Training loss (single batch): 1.1481413841247559\n",
      "\t Training loss (single batch): 1.252311110496521\n",
      "\t Training loss (single batch): 1.1763242483139038\n",
      "\t Training loss (single batch): 1.2390457391738892\n",
      "\t Training loss (single batch): 1.7678031921386719\n",
      "\t Training loss (single batch): 0.9466062188148499\n",
      "\t Training loss (single batch): 1.2178282737731934\n",
      "\t Training loss (single batch): 0.6330884099006653\n",
      "\t Training loss (single batch): 1.022904396057129\n",
      "\t Training loss (single batch): 1.4542776346206665\n",
      "\t Training loss (single batch): 1.6179946660995483\n",
      "\t Training loss (single batch): 1.3394521474838257\n",
      "\t Training loss (single batch): 1.414423942565918\n",
      "\t Training loss (single batch): 1.4077823162078857\n",
      "\t Training loss (single batch): 1.1596091985702515\n",
      "\t Training loss (single batch): 1.1120978593826294\n",
      "\t Training loss (single batch): 1.2029904127120972\n",
      "\t Training loss (single batch): 1.2960845232009888\n",
      "\t Training loss (single batch): 1.2230325937271118\n",
      "\t Training loss (single batch): 1.0578291416168213\n",
      "\t Training loss (single batch): 0.9920418858528137\n",
      "\t Training loss (single batch): 1.1678221225738525\n",
      "\t Training loss (single batch): 1.3392993211746216\n",
      "\t Training loss (single batch): 0.8506226539611816\n",
      "\t Training loss (single batch): 1.185243844985962\n",
      "\t Training loss (single batch): 0.9837022423744202\n",
      "\t Training loss (single batch): 1.0377670526504517\n",
      "\t Training loss (single batch): 1.1265076398849487\n",
      "\t Training loss (single batch): 1.3475233316421509\n",
      "\t Training loss (single batch): 1.5301685333251953\n",
      "\t Training loss (single batch): 1.3394840955734253\n",
      "\t Training loss (single batch): 1.2806497812271118\n",
      "\t Training loss (single batch): 1.1820462942123413\n",
      "\t Training loss (single batch): 0.9270955324172974\n",
      "\t Training loss (single batch): 1.1104135513305664\n",
      "\t Training loss (single batch): 1.012523889541626\n",
      "\t Training loss (single batch): 0.941289484500885\n",
      "\t Training loss (single batch): 1.237350583076477\n",
      "\t Training loss (single batch): 1.0861949920654297\n",
      "\t Training loss (single batch): 1.1375657320022583\n",
      "\t Training loss (single batch): 0.9942454695701599\n",
      "\t Training loss (single batch): 1.3230434656143188\n",
      "\t Training loss (single batch): 1.1265140771865845\n",
      "\t Training loss (single batch): 1.3482208251953125\n",
      "\t Training loss (single batch): 1.3176687955856323\n",
      "\t Training loss (single batch): 1.4645923376083374\n",
      "\t Training loss (single batch): 1.3431674242019653\n",
      "\t Training loss (single batch): 1.5248600244522095\n",
      "\t Training loss (single batch): 1.9927953481674194\n",
      "\t Training loss (single batch): 1.3068348169326782\n",
      "\t Training loss (single batch): 1.3288551568984985\n",
      "\t Training loss (single batch): 1.301999568939209\n",
      "\t Training loss (single batch): 1.518924593925476\n",
      "\t Training loss (single batch): 1.1356117725372314\n",
      "\t Training loss (single batch): 1.0992628335952759\n",
      "\t Training loss (single batch): 0.9873989820480347\n",
      "\t Training loss (single batch): 0.9514575004577637\n",
      "\t Training loss (single batch): 1.6236226558685303\n",
      "\t Training loss (single batch): 1.3391318321228027\n",
      "\t Training loss (single batch): 1.2776647806167603\n",
      "\t Training loss (single batch): 1.1174654960632324\n",
      "\t Training loss (single batch): 1.3632923364639282\n",
      "\t Training loss (single batch): 0.9389559030532837\n",
      "\t Training loss (single batch): 1.0465835332870483\n",
      "\t Training loss (single batch): 1.5008866786956787\n",
      "\t Training loss (single batch): 1.3362510204315186\n",
      "\t Training loss (single batch): 1.2318116426467896\n",
      "\t Training loss (single batch): 1.458857774734497\n",
      "\t Training loss (single batch): 0.9606563448905945\n",
      "\t Training loss (single batch): 1.2951323986053467\n",
      "\t Training loss (single batch): 0.759953498840332\n",
      "\t Training loss (single batch): 1.4760268926620483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2180230617523193\n",
      "\t Training loss (single batch): 1.9035789966583252\n",
      "\t Training loss (single batch): 1.1796469688415527\n",
      "\t Training loss (single batch): 1.4034415483474731\n",
      "\t Training loss (single batch): 0.8840590715408325\n",
      "\t Training loss (single batch): 1.188693642616272\n",
      "\t Training loss (single batch): 1.4738036394119263\n",
      "\t Training loss (single batch): 1.5126597881317139\n",
      "\t Training loss (single batch): 1.178365707397461\n",
      "\t Training loss (single batch): 1.236441731452942\n",
      "\t Training loss (single batch): 1.1604065895080566\n",
      "\t Training loss (single batch): 1.229284405708313\n",
      "\t Training loss (single batch): 1.0652800798416138\n",
      "\t Training loss (single batch): 1.2725692987442017\n",
      "\t Training loss (single batch): 1.029649019241333\n",
      "\t Training loss (single batch): 1.703727126121521\n",
      "\t Training loss (single batch): 1.6859428882598877\n",
      "\t Training loss (single batch): 1.3982144594192505\n",
      "\t Training loss (single batch): 1.130703330039978\n",
      "\t Training loss (single batch): 1.1625280380249023\n",
      "\t Training loss (single batch): 1.8194854259490967\n",
      "\t Training loss (single batch): 1.188347339630127\n",
      "\t Training loss (single batch): 1.2616437673568726\n",
      "\t Training loss (single batch): 0.9952648282051086\n",
      "\t Training loss (single batch): 0.984599232673645\n",
      "\t Training loss (single batch): 1.3203322887420654\n",
      "\t Training loss (single batch): 1.637787938117981\n",
      "\t Training loss (single batch): 1.224263310432434\n",
      "\t Training loss (single batch): 1.443724274635315\n",
      "\t Training loss (single batch): 0.9386402368545532\n",
      "\t Training loss (single batch): 0.9557347297668457\n",
      "\t Training loss (single batch): 1.356123924255371\n",
      "\t Training loss (single batch): 1.3814727067947388\n",
      "\t Training loss (single batch): 0.9556395411491394\n",
      "\t Training loss (single batch): 1.4525914192199707\n",
      "\t Training loss (single batch): 1.2409920692443848\n",
      "\t Training loss (single batch): 1.3763818740844727\n",
      "\t Training loss (single batch): 1.4144206047058105\n",
      "\t Training loss (single batch): 1.0514633655548096\n",
      "\t Training loss (single batch): 0.7860962152481079\n",
      "\t Training loss (single batch): 1.1887624263763428\n",
      "\t Training loss (single batch): 1.7272921800613403\n",
      "\t Training loss (single batch): 1.5799142122268677\n",
      "\t Training loss (single batch): 1.4725288152694702\n",
      "\t Training loss (single batch): 1.4499727487564087\n",
      "\t Training loss (single batch): 1.5143632888793945\n",
      "\t Training loss (single batch): 1.0629935264587402\n",
      "\t Training loss (single batch): 1.0529274940490723\n",
      "\t Training loss (single batch): 1.9532248973846436\n",
      "\t Training loss (single batch): 0.9625034332275391\n",
      "\t Training loss (single batch): 0.9764018654823303\n",
      "\t Training loss (single batch): 1.2719541788101196\n",
      "\t Training loss (single batch): 1.3873594999313354\n",
      "\t Training loss (single batch): 1.5774184465408325\n",
      "\t Training loss (single batch): 0.9431694149971008\n",
      "\t Training loss (single batch): 1.165121078491211\n",
      "\t Training loss (single batch): 1.3643890619277954\n",
      "\t Training loss (single batch): 1.0065698623657227\n",
      "\t Training loss (single batch): 0.9682196378707886\n",
      "\t Training loss (single batch): 1.1274430751800537\n",
      "\t Training loss (single batch): 1.3030235767364502\n",
      "\t Training loss (single batch): 0.8735682368278503\n",
      "\t Training loss (single batch): 1.437599778175354\n",
      "\t Training loss (single batch): 1.5121862888336182\n",
      "\t Training loss (single batch): 1.0552603006362915\n",
      "\t Training loss (single batch): 1.212430477142334\n",
      "\t Training loss (single batch): 1.038746953010559\n",
      "\t Training loss (single batch): 1.267841100692749\n",
      "\t Training loss (single batch): 1.7425169944763184\n",
      "\t Training loss (single batch): 1.333972692489624\n",
      "\t Training loss (single batch): 0.9791484475135803\n",
      "\t Training loss (single batch): 1.6558910608291626\n",
      "\t Training loss (single batch): 0.8628910779953003\n",
      "\t Training loss (single batch): 1.2198160886764526\n",
      "\t Training loss (single batch): 1.251481533050537\n",
      "\t Training loss (single batch): 1.2138664722442627\n",
      "\t Training loss (single batch): 1.3681628704071045\n",
      "\t Training loss (single batch): 1.1961891651153564\n",
      "\t Training loss (single batch): 1.2309304475784302\n",
      "\t Training loss (single batch): 1.1408079862594604\n",
      "\t Training loss (single batch): 1.5775041580200195\n",
      "\t Training loss (single batch): 1.1959702968597412\n",
      "\t Training loss (single batch): 1.2868233919143677\n",
      "\t Training loss (single batch): 1.3672735691070557\n",
      "\t Training loss (single batch): 1.5736054182052612\n",
      "\t Training loss (single batch): 1.4636870622634888\n",
      "\t Training loss (single batch): 1.0175538063049316\n",
      "\t Training loss (single batch): 1.4188989400863647\n",
      "\t Training loss (single batch): 1.2940559387207031\n",
      "\t Training loss (single batch): 1.0648788213729858\n",
      "\t Training loss (single batch): 1.3666870594024658\n",
      "\t Training loss (single batch): 1.132256269454956\n",
      "\t Training loss (single batch): 1.485392689704895\n",
      "\t Training loss (single batch): 0.8719534873962402\n",
      "\t Training loss (single batch): 1.2845792770385742\n",
      "\t Training loss (single batch): 1.3026434183120728\n",
      "\t Training loss (single batch): 1.0249462127685547\n",
      "\t Training loss (single batch): 2.146156072616577\n",
      "\t Training loss (single batch): 1.3726625442504883\n",
      "\t Training loss (single batch): 0.9869989156723022\n",
      "\t Training loss (single batch): 0.9048081636428833\n",
      "\t Training loss (single batch): 1.2391529083251953\n",
      "\t Training loss (single batch): 1.2138572931289673\n",
      "\t Training loss (single batch): 1.0612211227416992\n",
      "\t Training loss (single batch): 1.0050389766693115\n",
      "\t Training loss (single batch): 1.3753705024719238\n",
      "\t Training loss (single batch): 1.0035513639450073\n",
      "\t Training loss (single batch): 1.0983858108520508\n",
      "\t Training loss (single batch): 0.8388509154319763\n",
      "##################################\n",
      "## EPOCH 47\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3615059852600098\n",
      "\t Training loss (single batch): 1.2087533473968506\n",
      "\t Training loss (single batch): 1.3457154035568237\n",
      "\t Training loss (single batch): 1.1072980165481567\n",
      "\t Training loss (single batch): 1.9111555814743042\n",
      "\t Training loss (single batch): 0.9167683124542236\n",
      "\t Training loss (single batch): 1.1094008684158325\n",
      "\t Training loss (single batch): 1.35957670211792\n",
      "\t Training loss (single batch): 1.4137110710144043\n",
      "\t Training loss (single batch): 1.0340639352798462\n",
      "\t Training loss (single batch): 1.3257973194122314\n",
      "\t Training loss (single batch): 1.6172107458114624\n",
      "\t Training loss (single batch): 1.3929444551467896\n",
      "\t Training loss (single batch): 1.1457624435424805\n",
      "\t Training loss (single batch): 1.19414484500885\n",
      "\t Training loss (single batch): 0.8627833127975464\n",
      "\t Training loss (single batch): 1.170888066291809\n",
      "\t Training loss (single batch): 1.2419754266738892\n",
      "\t Training loss (single batch): 0.8359480500221252\n",
      "\t Training loss (single batch): 0.8087995052337646\n",
      "\t Training loss (single batch): 1.250707983970642\n",
      "\t Training loss (single batch): 1.1375713348388672\n",
      "\t Training loss (single batch): 1.1590367555618286\n",
      "\t Training loss (single batch): 0.7739949822425842\n",
      "\t Training loss (single batch): 0.9112361073493958\n",
      "\t Training loss (single batch): 0.863639771938324\n",
      "\t Training loss (single batch): 0.7979657053947449\n",
      "\t Training loss (single batch): 1.578632116317749\n",
      "\t Training loss (single batch): 0.9414119124412537\n",
      "\t Training loss (single batch): 1.1912933588027954\n",
      "\t Training loss (single batch): 0.5475834608078003\n",
      "\t Training loss (single batch): 1.3012707233428955\n",
      "\t Training loss (single batch): 1.179837703704834\n",
      "\t Training loss (single batch): 1.2518032789230347\n",
      "\t Training loss (single batch): 1.3589134216308594\n",
      "\t Training loss (single batch): 1.830336570739746\n",
      "\t Training loss (single batch): 0.9592885375022888\n",
      "\t Training loss (single batch): 1.195326805114746\n",
      "\t Training loss (single batch): 1.4261616468429565\n",
      "\t Training loss (single batch): 1.020839810371399\n",
      "\t Training loss (single batch): 1.0501788854599\n",
      "\t Training loss (single batch): 1.4339104890823364\n",
      "\t Training loss (single batch): 1.0879161357879639\n",
      "\t Training loss (single batch): 1.194934368133545\n",
      "\t Training loss (single batch): 0.8467108607292175\n",
      "\t Training loss (single batch): 1.342203140258789\n",
      "\t Training loss (single batch): 1.579119324684143\n",
      "\t Training loss (single batch): 1.0979914665222168\n",
      "\t Training loss (single batch): 0.9366310238838196\n",
      "\t Training loss (single batch): 1.5446990728378296\n",
      "\t Training loss (single batch): 1.5717891454696655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1118072271347046\n",
      "\t Training loss (single batch): 1.466822862625122\n",
      "\t Training loss (single batch): 1.2890586853027344\n",
      "\t Training loss (single batch): 1.2248862981796265\n",
      "\t Training loss (single batch): 1.4038593769073486\n",
      "\t Training loss (single batch): 1.574326515197754\n",
      "\t Training loss (single batch): 1.2461175918579102\n",
      "\t Training loss (single batch): 1.434991717338562\n",
      "\t Training loss (single batch): 1.2016433477401733\n",
      "\t Training loss (single batch): 0.8355199694633484\n",
      "\t Training loss (single batch): 1.1501387357711792\n",
      "\t Training loss (single batch): 1.529720664024353\n",
      "\t Training loss (single batch): 1.461704969406128\n",
      "\t Training loss (single batch): 0.884463906288147\n",
      "\t Training loss (single batch): 1.833343267440796\n",
      "\t Training loss (single batch): 1.5648987293243408\n",
      "\t Training loss (single batch): 1.461645483970642\n",
      "\t Training loss (single batch): 1.010359287261963\n",
      "\t Training loss (single batch): 1.0915220975875854\n",
      "\t Training loss (single batch): 1.1132254600524902\n",
      "\t Training loss (single batch): 1.789398193359375\n",
      "\t Training loss (single batch): 1.4140597581863403\n",
      "\t Training loss (single batch): 0.8251675367355347\n",
      "\t Training loss (single batch): 1.0184944868087769\n",
      "\t Training loss (single batch): 1.1295567750930786\n",
      "\t Training loss (single batch): 0.9644497036933899\n",
      "\t Training loss (single batch): 1.258187174797058\n",
      "\t Training loss (single batch): 1.6035451889038086\n",
      "\t Training loss (single batch): 1.2133629322052002\n",
      "\t Training loss (single batch): 0.8789685368537903\n",
      "\t Training loss (single batch): 1.0979958772659302\n",
      "\t Training loss (single batch): 0.9683402180671692\n",
      "\t Training loss (single batch): 1.559483289718628\n",
      "\t Training loss (single batch): 1.3720731735229492\n",
      "\t Training loss (single batch): 1.0216505527496338\n",
      "\t Training loss (single batch): 1.4689143896102905\n",
      "\t Training loss (single batch): 1.7137978076934814\n",
      "\t Training loss (single batch): 1.213647723197937\n",
      "\t Training loss (single batch): 0.807515561580658\n",
      "\t Training loss (single batch): 1.1431901454925537\n",
      "\t Training loss (single batch): 1.076094627380371\n",
      "\t Training loss (single batch): 1.1934038400650024\n",
      "\t Training loss (single batch): 1.1235520839691162\n",
      "\t Training loss (single batch): 1.1640111207962036\n",
      "\t Training loss (single batch): 1.4455715417861938\n",
      "\t Training loss (single batch): 1.014351725578308\n",
      "\t Training loss (single batch): 1.4360132217407227\n",
      "\t Training loss (single batch): 1.1774566173553467\n",
      "\t Training loss (single batch): 1.313466191291809\n",
      "\t Training loss (single batch): 1.4758745431900024\n",
      "\t Training loss (single batch): 1.089623212814331\n",
      "\t Training loss (single batch): 1.2621748447418213\n",
      "\t Training loss (single batch): 1.355679988861084\n",
      "\t Training loss (single batch): 0.9477459192276001\n",
      "\t Training loss (single batch): 1.2954704761505127\n",
      "\t Training loss (single batch): 1.3827069997787476\n",
      "\t Training loss (single batch): 1.2573513984680176\n",
      "\t Training loss (single batch): 1.4932485818862915\n",
      "\t Training loss (single batch): 1.5615074634552002\n",
      "\t Training loss (single batch): 1.3883273601531982\n",
      "\t Training loss (single batch): 0.9819415807723999\n",
      "\t Training loss (single batch): 1.5857164859771729\n",
      "\t Training loss (single batch): 1.2464025020599365\n",
      "\t Training loss (single batch): 1.56229567527771\n",
      "\t Training loss (single batch): 1.271167516708374\n",
      "\t Training loss (single batch): 0.6907346248626709\n",
      "\t Training loss (single batch): 1.750719428062439\n",
      "\t Training loss (single batch): 1.6486194133758545\n",
      "\t Training loss (single batch): 1.3468445539474487\n",
      "\t Training loss (single batch): 1.1666250228881836\n",
      "\t Training loss (single batch): 1.0007380247116089\n",
      "\t Training loss (single batch): 1.261528491973877\n",
      "\t Training loss (single batch): 1.0489387512207031\n",
      "\t Training loss (single batch): 1.0330520868301392\n",
      "\t Training loss (single batch): 0.8904411196708679\n",
      "\t Training loss (single batch): 1.3527125120162964\n",
      "\t Training loss (single batch): 1.6060905456542969\n",
      "\t Training loss (single batch): 1.6603673696517944\n",
      "\t Training loss (single batch): 1.1158709526062012\n",
      "\t Training loss (single batch): 1.297771692276001\n",
      "\t Training loss (single batch): 0.6792130470275879\n",
      "\t Training loss (single batch): 0.9051149487495422\n",
      "\t Training loss (single batch): 0.6970148086547852\n",
      "\t Training loss (single batch): 1.6997604370117188\n",
      "\t Training loss (single batch): 0.8522082567214966\n",
      "\t Training loss (single batch): 1.1439024209976196\n",
      "\t Training loss (single batch): 1.3464902639389038\n",
      "\t Training loss (single batch): 0.7740271687507629\n",
      "\t Training loss (single batch): 0.8457435965538025\n",
      "\t Training loss (single batch): 1.0835298299789429\n",
      "\t Training loss (single batch): 1.0449374914169312\n",
      "\t Training loss (single batch): 0.822745144367218\n",
      "\t Training loss (single batch): 1.7325537204742432\n",
      "\t Training loss (single batch): 1.3879939317703247\n",
      "\t Training loss (single batch): 1.5885210037231445\n",
      "\t Training loss (single batch): 0.9637297987937927\n",
      "\t Training loss (single batch): 1.3144651651382446\n",
      "\t Training loss (single batch): 1.2286642789840698\n",
      "\t Training loss (single batch): 0.9471439719200134\n",
      "\t Training loss (single batch): 1.0528244972229004\n",
      "\t Training loss (single batch): 1.251128911972046\n",
      "\t Training loss (single batch): 1.1432604789733887\n",
      "\t Training loss (single batch): 1.166081190109253\n",
      "\t Training loss (single batch): 1.603987455368042\n",
      "\t Training loss (single batch): 1.0585726499557495\n",
      "\t Training loss (single batch): 1.5861668586730957\n",
      "\t Training loss (single batch): 1.4089410305023193\n",
      "\t Training loss (single batch): 1.2880079746246338\n",
      "\t Training loss (single batch): 1.4026930332183838\n",
      "\t Training loss (single batch): 1.2039471864700317\n",
      "\t Training loss (single batch): 1.398484468460083\n",
      "\t Training loss (single batch): 1.8472589254379272\n",
      "\t Training loss (single batch): 1.6286994218826294\n",
      "\t Training loss (single batch): 1.5436372756958008\n",
      "\t Training loss (single batch): 1.138347864151001\n",
      "\t Training loss (single batch): 1.1116929054260254\n",
      "\t Training loss (single batch): 0.8698287010192871\n",
      "\t Training loss (single batch): 1.1990827322006226\n",
      "\t Training loss (single batch): 0.9261372089385986\n",
      "\t Training loss (single batch): 1.1741116046905518\n",
      "\t Training loss (single batch): 1.2368098497390747\n",
      "\t Training loss (single batch): 1.3398371934890747\n",
      "\t Training loss (single batch): 0.9218109250068665\n",
      "\t Training loss (single batch): 1.4619216918945312\n",
      "\t Training loss (single batch): 1.793278455734253\n",
      "\t Training loss (single batch): 1.4957090616226196\n",
      "\t Training loss (single batch): 1.0922801494598389\n",
      "\t Training loss (single batch): 1.377007246017456\n",
      "\t Training loss (single batch): 1.0380922555923462\n",
      "\t Training loss (single batch): 1.2661151885986328\n",
      "\t Training loss (single batch): 1.4325041770935059\n",
      "\t Training loss (single batch): 1.0559841394424438\n",
      "\t Training loss (single batch): 1.8135563135147095\n",
      "\t Training loss (single batch): 1.1322689056396484\n",
      "\t Training loss (single batch): 1.5201517343521118\n",
      "\t Training loss (single batch): 1.4640915393829346\n",
      "\t Training loss (single batch): 1.057151436805725\n",
      "\t Training loss (single batch): 1.027016043663025\n",
      "\t Training loss (single batch): 1.2588422298431396\n",
      "\t Training loss (single batch): 1.1641285419464111\n",
      "\t Training loss (single batch): 1.1462010145187378\n",
      "\t Training loss (single batch): 1.319153070449829\n",
      "\t Training loss (single batch): 1.4510043859481812\n",
      "\t Training loss (single batch): 1.1651986837387085\n",
      "\t Training loss (single batch): 1.0915395021438599\n",
      "\t Training loss (single batch): 1.575800895690918\n",
      "\t Training loss (single batch): 1.1492960453033447\n",
      "\t Training loss (single batch): 0.9302054047584534\n",
      "\t Training loss (single batch): 1.2794502973556519\n",
      "\t Training loss (single batch): 1.2081303596496582\n",
      "\t Training loss (single batch): 1.112170696258545\n",
      "\t Training loss (single batch): 1.3105096817016602\n",
      "\t Training loss (single batch): 0.9480500221252441\n",
      "\t Training loss (single batch): 0.7873695492744446\n",
      "\t Training loss (single batch): 1.6978235244750977\n",
      "\t Training loss (single batch): 1.0230517387390137\n",
      "\t Training loss (single batch): 0.8820030689239502\n",
      "\t Training loss (single batch): 1.366226077079773\n",
      "\t Training loss (single batch): 1.2643038034439087\n",
      "\t Training loss (single batch): 1.0649641752243042\n",
      "\t Training loss (single batch): 1.2535122632980347\n",
      "\t Training loss (single batch): 1.029854416847229\n",
      "\t Training loss (single batch): 0.9305627942085266\n",
      "\t Training loss (single batch): 1.384881854057312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9442552328109741\n",
      "\t Training loss (single batch): 1.3986754417419434\n",
      "\t Training loss (single batch): 0.9441096186637878\n",
      "\t Training loss (single batch): 0.9973952770233154\n",
      "\t Training loss (single batch): 0.8445217609405518\n",
      "\t Training loss (single batch): 1.3532345294952393\n",
      "\t Training loss (single batch): 0.8878169059753418\n",
      "\t Training loss (single batch): 1.6187347173690796\n",
      "\t Training loss (single batch): 1.6334125995635986\n",
      "\t Training loss (single batch): 1.2800270318984985\n",
      "\t Training loss (single batch): 1.468550443649292\n",
      "\t Training loss (single batch): 1.3847503662109375\n",
      "\t Training loss (single batch): 1.014984369277954\n",
      "\t Training loss (single batch): 1.1999815702438354\n",
      "\t Training loss (single batch): 1.3418909311294556\n",
      "\t Training loss (single batch): 1.230391502380371\n",
      "\t Training loss (single batch): 1.4762170314788818\n",
      "\t Training loss (single batch): 1.3629798889160156\n",
      "\t Training loss (single batch): 1.8710319995880127\n",
      "\t Training loss (single batch): 1.3642243146896362\n",
      "\t Training loss (single batch): 1.1617499589920044\n",
      "\t Training loss (single batch): 1.0702730417251587\n",
      "\t Training loss (single batch): 1.1254159212112427\n",
      "\t Training loss (single batch): 1.1613764762878418\n",
      "\t Training loss (single batch): 1.3254873752593994\n",
      "\t Training loss (single batch): 1.3081392049789429\n",
      "\t Training loss (single batch): 1.2128909826278687\n",
      "\t Training loss (single batch): 0.8972277045249939\n",
      "\t Training loss (single batch): 0.9504523873329163\n",
      "\t Training loss (single batch): 0.9662878513336182\n",
      "\t Training loss (single batch): 1.1592204570770264\n",
      "\t Training loss (single batch): 1.3135675191879272\n",
      "\t Training loss (single batch): 1.3924881219863892\n",
      "\t Training loss (single batch): 1.2732069492340088\n",
      "\t Training loss (single batch): 0.8879140019416809\n",
      "\t Training loss (single batch): 1.2792081832885742\n",
      "\t Training loss (single batch): 1.0387647151947021\n",
      "\t Training loss (single batch): 1.1843022108078003\n",
      "\t Training loss (single batch): 1.3917268514633179\n",
      "\t Training loss (single batch): 1.2240785360336304\n",
      "\t Training loss (single batch): 1.239426851272583\n",
      "\t Training loss (single batch): 1.0473682880401611\n",
      "\t Training loss (single batch): 1.5947424173355103\n",
      "\t Training loss (single batch): 1.4444527626037598\n",
      "\t Training loss (single batch): 1.4827960729599\n",
      "\t Training loss (single batch): 1.0115547180175781\n",
      "\t Training loss (single batch): 1.2802026271820068\n",
      "\t Training loss (single batch): 1.3205498456954956\n",
      "\t Training loss (single batch): 1.062231183052063\n",
      "\t Training loss (single batch): 0.8906255960464478\n",
      "\t Training loss (single batch): 1.183924913406372\n",
      "\t Training loss (single batch): 1.3694701194763184\n",
      "\t Training loss (single batch): 1.1669213771820068\n",
      "\t Training loss (single batch): 1.3011804819107056\n",
      "\t Training loss (single batch): 1.5851958990097046\n",
      "\t Training loss (single batch): 1.1741594076156616\n",
      "\t Training loss (single batch): 1.0845544338226318\n",
      "\t Training loss (single batch): 0.9250670075416565\n",
      "\t Training loss (single batch): 1.739772915840149\n",
      "\t Training loss (single batch): 1.7288193702697754\n",
      "\t Training loss (single batch): 1.1898952722549438\n",
      "\t Training loss (single batch): 1.0696895122528076\n",
      "\t Training loss (single batch): 1.1869206428527832\n",
      "\t Training loss (single batch): 1.0532640218734741\n",
      "\t Training loss (single batch): 1.5049917697906494\n",
      "\t Training loss (single batch): 0.9666359424591064\n",
      "\t Training loss (single batch): 1.3334038257598877\n",
      "\t Training loss (single batch): 1.6250513792037964\n",
      "\t Training loss (single batch): 1.0208473205566406\n",
      "\t Training loss (single batch): 1.132364273071289\n",
      "\t Training loss (single batch): 0.7608036398887634\n",
      "\t Training loss (single batch): 1.5657005310058594\n",
      "\t Training loss (single batch): 1.411866307258606\n",
      "\t Training loss (single batch): 1.0887430906295776\n",
      "\t Training loss (single batch): 1.6847296953201294\n",
      "\t Training loss (single batch): 1.2102150917053223\n",
      "\t Training loss (single batch): 0.7923797369003296\n",
      "\t Training loss (single batch): 1.1256197690963745\n",
      "\t Training loss (single batch): 1.3476836681365967\n",
      "\t Training loss (single batch): 1.2129803895950317\n",
      "\t Training loss (single batch): 1.2685856819152832\n",
      "\t Training loss (single batch): 1.0024974346160889\n",
      "\t Training loss (single batch): 1.7950026988983154\n",
      "\t Training loss (single batch): 1.095967173576355\n",
      "\t Training loss (single batch): 1.3510899543762207\n",
      "\t Training loss (single batch): 1.6796661615371704\n",
      "\t Training loss (single batch): 1.5822919607162476\n",
      "\t Training loss (single batch): 1.15114164352417\n",
      "\t Training loss (single batch): 1.5315121412277222\n",
      "\t Training loss (single batch): 1.126193642616272\n",
      "\t Training loss (single batch): 0.8804895877838135\n",
      "\t Training loss (single batch): 1.6629959344863892\n",
      "\t Training loss (single batch): 1.374057650566101\n",
      "\t Training loss (single batch): 0.8702720999717712\n",
      "\t Training loss (single batch): 1.039207935333252\n",
      "\t Training loss (single batch): 0.9148995876312256\n",
      "\t Training loss (single batch): 1.5440376996994019\n",
      "\t Training loss (single batch): 1.1355702877044678\n",
      "\t Training loss (single batch): 1.0647130012512207\n",
      "\t Training loss (single batch): 0.9541725516319275\n",
      "\t Training loss (single batch): 1.627480149269104\n",
      "\t Training loss (single batch): 0.8826987743377686\n",
      "##################################\n",
      "## EPOCH 48\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9806272983551025\n",
      "\t Training loss (single batch): 1.3291951417922974\n",
      "\t Training loss (single batch): 1.120889663696289\n",
      "\t Training loss (single batch): 1.612561821937561\n",
      "\t Training loss (single batch): 1.3549895286560059\n",
      "\t Training loss (single batch): 1.2993866205215454\n",
      "\t Training loss (single batch): 1.477166771888733\n",
      "\t Training loss (single batch): 1.3506460189819336\n",
      "\t Training loss (single batch): 1.3050159215927124\n",
      "\t Training loss (single batch): 1.1168315410614014\n",
      "\t Training loss (single batch): 1.0705080032348633\n",
      "\t Training loss (single batch): 1.6207524538040161\n",
      "\t Training loss (single batch): 0.8874635100364685\n",
      "\t Training loss (single batch): 0.896959125995636\n",
      "\t Training loss (single batch): 0.9073470830917358\n",
      "\t Training loss (single batch): 1.2044190168380737\n",
      "\t Training loss (single batch): 0.8727166056632996\n",
      "\t Training loss (single batch): 1.3064992427825928\n",
      "\t Training loss (single batch): 1.2751457691192627\n",
      "\t Training loss (single batch): 1.10171639919281\n",
      "\t Training loss (single batch): 1.3868640661239624\n",
      "\t Training loss (single batch): 1.7604947090148926\n",
      "\t Training loss (single batch): 0.6604593396186829\n",
      "\t Training loss (single batch): 1.337571620941162\n",
      "\t Training loss (single batch): 1.649425745010376\n",
      "\t Training loss (single batch): 1.4327901601791382\n",
      "\t Training loss (single batch): 1.4260563850402832\n",
      "\t Training loss (single batch): 1.632419228553772\n",
      "\t Training loss (single batch): 1.236104130744934\n",
      "\t Training loss (single batch): 1.1616625785827637\n",
      "\t Training loss (single batch): 0.8699929118156433\n",
      "\t Training loss (single batch): 1.0913130044937134\n",
      "\t Training loss (single batch): 1.4259384870529175\n",
      "\t Training loss (single batch): 1.1249233484268188\n",
      "\t Training loss (single batch): 1.3782974481582642\n",
      "\t Training loss (single batch): 1.209272027015686\n",
      "\t Training loss (single batch): 1.252318024635315\n",
      "\t Training loss (single batch): 1.267159342765808\n",
      "\t Training loss (single batch): 1.3220146894454956\n",
      "\t Training loss (single batch): 1.0631792545318604\n",
      "\t Training loss (single batch): 1.1251330375671387\n",
      "\t Training loss (single batch): 1.2309694290161133\n",
      "\t Training loss (single batch): 1.0654902458190918\n",
      "\t Training loss (single batch): 1.2366604804992676\n",
      "\t Training loss (single batch): 1.350908637046814\n",
      "\t Training loss (single batch): 1.78440260887146\n",
      "\t Training loss (single batch): 1.1282788515090942\n",
      "\t Training loss (single batch): 1.1469608545303345\n",
      "\t Training loss (single batch): 1.770521879196167\n",
      "\t Training loss (single batch): 0.8932824730873108\n",
      "\t Training loss (single batch): 0.802946150302887\n",
      "\t Training loss (single batch): 1.0702694654464722\n",
      "\t Training loss (single batch): 0.6987925171852112\n",
      "\t Training loss (single batch): 1.1550862789154053\n",
      "\t Training loss (single batch): 1.175142526626587\n",
      "\t Training loss (single batch): 1.0153453350067139\n",
      "\t Training loss (single batch): 1.2174824476242065\n",
      "\t Training loss (single batch): 1.2424628734588623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8217041492462158\n",
      "\t Training loss (single batch): 1.6829317808151245\n",
      "\t Training loss (single batch): 1.9109818935394287\n",
      "\t Training loss (single batch): 1.3501766920089722\n",
      "\t Training loss (single batch): 1.062878131866455\n",
      "\t Training loss (single batch): 0.9585672616958618\n",
      "\t Training loss (single batch): 1.0858250856399536\n",
      "\t Training loss (single batch): 1.3126140832901\n",
      "\t Training loss (single batch): 1.230426549911499\n",
      "\t Training loss (single batch): 1.231231451034546\n",
      "\t Training loss (single batch): 1.7740840911865234\n",
      "\t Training loss (single batch): 0.963782012462616\n",
      "\t Training loss (single batch): 1.5319749116897583\n",
      "\t Training loss (single batch): 0.9645295739173889\n",
      "\t Training loss (single batch): 0.9678277969360352\n",
      "\t Training loss (single batch): 0.9358253479003906\n",
      "\t Training loss (single batch): 1.3331903219223022\n",
      "\t Training loss (single batch): 1.47637140750885\n",
      "\t Training loss (single batch): 0.8440946340560913\n",
      "\t Training loss (single batch): 1.1999378204345703\n",
      "\t Training loss (single batch): 1.3818747997283936\n",
      "\t Training loss (single batch): 0.9404364824295044\n",
      "\t Training loss (single batch): 1.7403618097305298\n",
      "\t Training loss (single batch): 1.1470698118209839\n",
      "\t Training loss (single batch): 1.7164031267166138\n",
      "\t Training loss (single batch): 1.3808640241622925\n",
      "\t Training loss (single batch): 1.2354121208190918\n",
      "\t Training loss (single batch): 1.0688079595565796\n",
      "\t Training loss (single batch): 1.4919466972351074\n",
      "\t Training loss (single batch): 1.07517409324646\n",
      "\t Training loss (single batch): 0.9051046967506409\n",
      "\t Training loss (single batch): 0.9462838768959045\n",
      "\t Training loss (single batch): 1.2522737979888916\n",
      "\t Training loss (single batch): 1.1980133056640625\n",
      "\t Training loss (single batch): 1.3432495594024658\n",
      "\t Training loss (single batch): 1.175673484802246\n",
      "\t Training loss (single batch): 1.2665356397628784\n",
      "\t Training loss (single batch): 1.7074337005615234\n",
      "\t Training loss (single batch): 1.2263857126235962\n",
      "\t Training loss (single batch): 1.0031027793884277\n",
      "\t Training loss (single batch): 1.288425087928772\n",
      "\t Training loss (single batch): 1.2015163898468018\n",
      "\t Training loss (single batch): 1.3239777088165283\n",
      "\t Training loss (single batch): 1.4984081983566284\n",
      "\t Training loss (single batch): 1.3673878908157349\n",
      "\t Training loss (single batch): 1.0076420307159424\n",
      "\t Training loss (single batch): 1.435673475265503\n",
      "\t Training loss (single batch): 1.6552090644836426\n",
      "\t Training loss (single batch): 0.7864682078361511\n",
      "\t Training loss (single batch): 1.2141324281692505\n",
      "\t Training loss (single batch): 1.3650643825531006\n",
      "\t Training loss (single batch): 1.1992508172988892\n",
      "\t Training loss (single batch): 1.1677483320236206\n",
      "\t Training loss (single batch): 0.967790424823761\n",
      "\t Training loss (single batch): 1.0037016868591309\n",
      "\t Training loss (single batch): 1.3086638450622559\n",
      "\t Training loss (single batch): 1.4242970943450928\n",
      "\t Training loss (single batch): 0.8730859160423279\n",
      "\t Training loss (single batch): 1.0970754623413086\n",
      "\t Training loss (single batch): 1.6928131580352783\n",
      "\t Training loss (single batch): 0.9644397497177124\n",
      "\t Training loss (single batch): 1.216572642326355\n",
      "\t Training loss (single batch): 1.2826178073883057\n",
      "\t Training loss (single batch): 1.3938764333724976\n",
      "\t Training loss (single batch): 1.7567358016967773\n",
      "\t Training loss (single batch): 1.5245988368988037\n",
      "\t Training loss (single batch): 0.9536983370780945\n",
      "\t Training loss (single batch): 1.1071780920028687\n",
      "\t Training loss (single batch): 1.0258184671401978\n",
      "\t Training loss (single batch): 0.8691725134849548\n",
      "\t Training loss (single batch): 0.7120652794837952\n",
      "\t Training loss (single batch): 1.0379983186721802\n",
      "\t Training loss (single batch): 1.1424094438552856\n",
      "\t Training loss (single batch): 1.1887481212615967\n",
      "\t Training loss (single batch): 0.9035377502441406\n",
      "\t Training loss (single batch): 1.1391427516937256\n",
      "\t Training loss (single batch): 1.0924094915390015\n",
      "\t Training loss (single batch): 0.8240241408348083\n",
      "\t Training loss (single batch): 1.4442148208618164\n",
      "\t Training loss (single batch): 1.2968071699142456\n",
      "\t Training loss (single batch): 1.0863001346588135\n",
      "\t Training loss (single batch): 1.4445794820785522\n",
      "\t Training loss (single batch): 1.4351283311843872\n",
      "\t Training loss (single batch): 1.2549376487731934\n",
      "\t Training loss (single batch): 0.9410325288772583\n",
      "\t Training loss (single batch): 1.3818738460540771\n",
      "\t Training loss (single batch): 1.0696604251861572\n",
      "\t Training loss (single batch): 1.3985843658447266\n",
      "\t Training loss (single batch): 0.8447450399398804\n",
      "\t Training loss (single batch): 1.1354997158050537\n",
      "\t Training loss (single batch): 0.9483523368835449\n",
      "\t Training loss (single batch): 1.1974778175354004\n",
      "\t Training loss (single batch): 0.984158992767334\n",
      "\t Training loss (single batch): 1.1611069440841675\n",
      "\t Training loss (single batch): 1.5016651153564453\n",
      "\t Training loss (single batch): 1.634572982788086\n",
      "\t Training loss (single batch): 1.2703204154968262\n",
      "\t Training loss (single batch): 1.6295415163040161\n",
      "\t Training loss (single batch): 1.187419056892395\n",
      "\t Training loss (single batch): 1.7815276384353638\n",
      "\t Training loss (single batch): 1.2083443403244019\n",
      "\t Training loss (single batch): 1.1766867637634277\n",
      "\t Training loss (single batch): 1.7171982526779175\n",
      "\t Training loss (single batch): 1.140246868133545\n",
      "\t Training loss (single batch): 0.8357822299003601\n",
      "\t Training loss (single batch): 1.4209057092666626\n",
      "\t Training loss (single batch): 1.2862536907196045\n",
      "\t Training loss (single batch): 1.509731650352478\n",
      "\t Training loss (single batch): 1.141646385192871\n",
      "\t Training loss (single batch): 1.3815056085586548\n",
      "\t Training loss (single batch): 1.2225030660629272\n",
      "\t Training loss (single batch): 1.5794917345046997\n",
      "\t Training loss (single batch): 1.3398640155792236\n",
      "\t Training loss (single batch): 1.277994990348816\n",
      "\t Training loss (single batch): 1.3938612937927246\n",
      "\t Training loss (single batch): 0.9545090794563293\n",
      "\t Training loss (single batch): 0.8612061738967896\n",
      "\t Training loss (single batch): 1.1643680334091187\n",
      "\t Training loss (single batch): 0.7728174328804016\n",
      "\t Training loss (single batch): 0.8067119717597961\n",
      "\t Training loss (single batch): 0.9288399815559387\n",
      "\t Training loss (single batch): 1.0172418355941772\n",
      "\t Training loss (single batch): 1.1585570573806763\n",
      "\t Training loss (single batch): 1.2987562417984009\n",
      "\t Training loss (single batch): 1.1208539009094238\n",
      "\t Training loss (single batch): 1.0756076574325562\n",
      "\t Training loss (single batch): 1.5295673608779907\n",
      "\t Training loss (single batch): 1.237919569015503\n",
      "\t Training loss (single batch): 1.0780329704284668\n",
      "\t Training loss (single batch): 1.048862338066101\n",
      "\t Training loss (single batch): 1.0294634103775024\n",
      "\t Training loss (single batch): 1.167042851448059\n",
      "\t Training loss (single batch): 1.2277039289474487\n",
      "\t Training loss (single batch): 1.1357624530792236\n",
      "\t Training loss (single batch): 1.3497984409332275\n",
      "\t Training loss (single batch): 1.1871366500854492\n",
      "\t Training loss (single batch): 1.5262911319732666\n",
      "\t Training loss (single batch): 0.7859572172164917\n",
      "\t Training loss (single batch): 1.0180726051330566\n",
      "\t Training loss (single batch): 0.9776957035064697\n",
      "\t Training loss (single batch): 1.207204818725586\n",
      "\t Training loss (single batch): 0.8911055326461792\n",
      "\t Training loss (single batch): 1.3821632862091064\n",
      "\t Training loss (single batch): 1.1565353870391846\n",
      "\t Training loss (single batch): 1.0637792348861694\n",
      "\t Training loss (single batch): 1.0605311393737793\n",
      "\t Training loss (single batch): 1.0002857446670532\n",
      "\t Training loss (single batch): 1.3532352447509766\n",
      "\t Training loss (single batch): 0.8030804395675659\n",
      "\t Training loss (single batch): 0.9471128582954407\n",
      "\t Training loss (single batch): 2.018113136291504\n",
      "\t Training loss (single batch): 1.0272115468978882\n",
      "\t Training loss (single batch): 1.4988347291946411\n",
      "\t Training loss (single batch): 0.9160120487213135\n",
      "\t Training loss (single batch): 1.0960007905960083\n",
      "\t Training loss (single batch): 0.8317345380783081\n",
      "\t Training loss (single batch): 1.5144304037094116\n",
      "\t Training loss (single batch): 0.991177499294281\n",
      "\t Training loss (single batch): 1.1734739542007446\n",
      "\t Training loss (single batch): 0.8862262964248657\n",
      "\t Training loss (single batch): 1.1511496305465698\n",
      "\t Training loss (single batch): 1.907086968421936\n",
      "\t Training loss (single batch): 1.1697728633880615\n",
      "\t Training loss (single batch): 1.068551778793335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0528846979141235\n",
      "\t Training loss (single batch): 0.909771203994751\n",
      "\t Training loss (single batch): 0.8311429023742676\n",
      "\t Training loss (single batch): 1.5474287271499634\n",
      "\t Training loss (single batch): 1.487565517425537\n",
      "\t Training loss (single batch): 1.4294123649597168\n",
      "\t Training loss (single batch): 1.3796839714050293\n",
      "\t Training loss (single batch): 1.3516626358032227\n",
      "\t Training loss (single batch): 1.4725362062454224\n",
      "\t Training loss (single batch): 0.8972348570823669\n",
      "\t Training loss (single batch): 0.9655391573905945\n",
      "\t Training loss (single batch): 0.9474199414253235\n",
      "\t Training loss (single batch): 1.2258654832839966\n",
      "\t Training loss (single batch): 1.187259316444397\n",
      "\t Training loss (single batch): 1.0133212804794312\n",
      "\t Training loss (single batch): 1.4173113107681274\n",
      "\t Training loss (single batch): 0.8312194347381592\n",
      "\t Training loss (single batch): 1.0788317918777466\n",
      "\t Training loss (single batch): 1.2241013050079346\n",
      "\t Training loss (single batch): 1.6531386375427246\n",
      "\t Training loss (single batch): 0.7567203044891357\n",
      "\t Training loss (single batch): 0.9806943535804749\n",
      "\t Training loss (single batch): 1.220515251159668\n",
      "\t Training loss (single batch): 1.3246381282806396\n",
      "\t Training loss (single batch): 1.1490530967712402\n",
      "\t Training loss (single batch): 1.0103986263275146\n",
      "\t Training loss (single batch): 1.2956842184066772\n",
      "\t Training loss (single batch): 1.2762457132339478\n",
      "\t Training loss (single batch): 1.3168073892593384\n",
      "\t Training loss (single batch): 0.694089412689209\n",
      "\t Training loss (single batch): 0.9374869465827942\n",
      "\t Training loss (single batch): 1.2120143175125122\n",
      "\t Training loss (single batch): 1.0942950248718262\n",
      "\t Training loss (single batch): 1.602721095085144\n",
      "\t Training loss (single batch): 1.4384922981262207\n",
      "\t Training loss (single batch): 0.835938036441803\n",
      "\t Training loss (single batch): 1.2547379732131958\n",
      "\t Training loss (single batch): 1.3104716539382935\n",
      "\t Training loss (single batch): 1.5432378053665161\n",
      "\t Training loss (single batch): 1.0189807415008545\n",
      "\t Training loss (single batch): 1.737381935119629\n",
      "\t Training loss (single batch): 1.437645435333252\n",
      "\t Training loss (single batch): 1.2078062295913696\n",
      "\t Training loss (single batch): 1.171940803527832\n",
      "\t Training loss (single batch): 1.1107988357543945\n",
      "\t Training loss (single batch): 0.893254816532135\n",
      "\t Training loss (single batch): 1.8273365497589111\n",
      "\t Training loss (single batch): 1.3933402299880981\n",
      "\t Training loss (single batch): 1.30021071434021\n",
      "\t Training loss (single batch): 0.6643906831741333\n",
      "\t Training loss (single batch): 1.1737860441207886\n",
      "\t Training loss (single batch): 1.5304852724075317\n",
      "\t Training loss (single batch): 0.8177472949028015\n",
      "\t Training loss (single batch): 1.2115349769592285\n",
      "\t Training loss (single batch): 1.4956376552581787\n",
      "\t Training loss (single batch): 1.2122199535369873\n",
      "\t Training loss (single batch): 0.9164507985115051\n",
      "\t Training loss (single batch): 0.5616716742515564\n",
      "\t Training loss (single batch): 1.6132584810256958\n",
      "\t Training loss (single batch): 0.888696014881134\n",
      "\t Training loss (single batch): 1.4683469533920288\n",
      "\t Training loss (single batch): 1.2743918895721436\n",
      "\t Training loss (single batch): 1.3254828453063965\n",
      "\t Training loss (single batch): 1.5703036785125732\n",
      "\t Training loss (single batch): 1.5448877811431885\n",
      "\t Training loss (single batch): 1.086031198501587\n",
      "\t Training loss (single batch): 1.2089204788208008\n",
      "\t Training loss (single batch): 1.2196470499038696\n",
      "\t Training loss (single batch): 0.9425511956214905\n",
      "\t Training loss (single batch): 1.6328386068344116\n",
      "\t Training loss (single batch): 1.2196269035339355\n",
      "\t Training loss (single batch): 1.1954030990600586\n",
      "\t Training loss (single batch): 0.8936309814453125\n",
      "\t Training loss (single batch): 1.3184757232666016\n",
      "\t Training loss (single batch): 1.4952054023742676\n",
      "\t Training loss (single batch): 1.5728788375854492\n",
      "\t Training loss (single batch): 1.4806504249572754\n",
      "\t Training loss (single batch): 0.8019229173660278\n",
      "\t Training loss (single batch): 1.0779927968978882\n",
      "\t Training loss (single batch): 1.7174707651138306\n",
      "\t Training loss (single batch): 1.474680781364441\n",
      "\t Training loss (single batch): 1.467621922492981\n",
      "\t Training loss (single batch): 1.172116756439209\n",
      "\t Training loss (single batch): 1.1629923582077026\n",
      "\t Training loss (single batch): 1.1304140090942383\n",
      "\t Training loss (single batch): 1.0328742265701294\n",
      "\t Training loss (single batch): 1.1017266511917114\n",
      "\t Training loss (single batch): 1.1469171047210693\n",
      "\t Training loss (single batch): 1.100319743156433\n",
      "\t Training loss (single batch): 1.6230665445327759\n",
      "\t Training loss (single batch): 1.4086838960647583\n",
      "\t Training loss (single batch): 0.6919890642166138\n",
      "\t Training loss (single batch): 1.1571344137191772\n",
      "\t Training loss (single batch): 1.8021737337112427\n",
      "\t Training loss (single batch): 0.7021098732948303\n",
      "##################################\n",
      "## EPOCH 49\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3213527202606201\n",
      "\t Training loss (single batch): 0.871247410774231\n",
      "\t Training loss (single batch): 1.382529377937317\n",
      "\t Training loss (single batch): 1.4552351236343384\n",
      "\t Training loss (single batch): 1.4798896312713623\n",
      "\t Training loss (single batch): 1.1889898777008057\n",
      "\t Training loss (single batch): 1.573169231414795\n",
      "\t Training loss (single batch): 1.172521710395813\n",
      "\t Training loss (single batch): 0.9810630679130554\n",
      "\t Training loss (single batch): 1.1894755363464355\n",
      "\t Training loss (single batch): 1.4213730096817017\n",
      "\t Training loss (single batch): 1.1417820453643799\n",
      "\t Training loss (single batch): 1.4977141618728638\n",
      "\t Training loss (single batch): 1.27389657497406\n",
      "\t Training loss (single batch): 0.8903017640113831\n",
      "\t Training loss (single batch): 1.0372686386108398\n",
      "\t Training loss (single batch): 1.6610029935836792\n",
      "\t Training loss (single batch): 1.0578546524047852\n",
      "\t Training loss (single batch): 0.9043765664100647\n",
      "\t Training loss (single batch): 1.3758087158203125\n",
      "\t Training loss (single batch): 1.8381078243255615\n",
      "\t Training loss (single batch): 1.3234082460403442\n",
      "\t Training loss (single batch): 1.6948089599609375\n",
      "\t Training loss (single batch): 0.9481860399246216\n",
      "\t Training loss (single batch): 1.1781238317489624\n",
      "\t Training loss (single batch): 1.5613778829574585\n",
      "\t Training loss (single batch): 1.5320497751235962\n",
      "\t Training loss (single batch): 1.845927119255066\n",
      "\t Training loss (single batch): 1.0238476991653442\n",
      "\t Training loss (single batch): 1.3212989568710327\n",
      "\t Training loss (single batch): 1.5598700046539307\n",
      "\t Training loss (single batch): 1.4090975522994995\n",
      "\t Training loss (single batch): 1.084903359413147\n",
      "\t Training loss (single batch): 1.1104769706726074\n",
      "\t Training loss (single batch): 1.4035228490829468\n",
      "\t Training loss (single batch): 0.9978488087654114\n",
      "\t Training loss (single batch): 1.0417619943618774\n",
      "\t Training loss (single batch): 1.3264187574386597\n",
      "\t Training loss (single batch): 1.0109333992004395\n",
      "\t Training loss (single batch): 1.0984385013580322\n",
      "\t Training loss (single batch): 1.366783857345581\n",
      "\t Training loss (single batch): 1.2009696960449219\n",
      "\t Training loss (single batch): 0.8483725786209106\n",
      "\t Training loss (single batch): 1.3788628578186035\n",
      "\t Training loss (single batch): 0.7890397906303406\n",
      "\t Training loss (single batch): 1.5295038223266602\n",
      "\t Training loss (single batch): 1.4299120903015137\n",
      "\t Training loss (single batch): 1.2177814245224\n",
      "\t Training loss (single batch): 1.163831353187561\n",
      "\t Training loss (single batch): 1.1942301988601685\n",
      "\t Training loss (single batch): 1.0263477563858032\n",
      "\t Training loss (single batch): 1.5621591806411743\n",
      "\t Training loss (single batch): 1.195270299911499\n",
      "\t Training loss (single batch): 1.146705985069275\n",
      "\t Training loss (single batch): 1.319421648979187\n",
      "\t Training loss (single batch): 1.628361701965332\n",
      "\t Training loss (single batch): 1.3481123447418213\n",
      "\t Training loss (single batch): 1.304188847541809\n",
      "\t Training loss (single batch): 1.6119765043258667\n",
      "\t Training loss (single batch): 1.3734021186828613\n",
      "\t Training loss (single batch): 1.1121214628219604\n",
      "\t Training loss (single batch): 1.1481982469558716\n",
      "\t Training loss (single batch): 1.704091191291809\n",
      "\t Training loss (single batch): 1.5158048868179321\n",
      "\t Training loss (single batch): 1.2154533863067627\n",
      "\t Training loss (single batch): 1.4805669784545898\n",
      "\t Training loss (single batch): 1.174828290939331\n",
      "\t Training loss (single batch): 0.8773736953735352\n",
      "\t Training loss (single batch): 1.5008070468902588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8809729218482971\n",
      "\t Training loss (single batch): 0.9254125952720642\n",
      "\t Training loss (single batch): 1.119456171989441\n",
      "\t Training loss (single batch): 0.9632254838943481\n",
      "\t Training loss (single batch): 1.313042402267456\n",
      "\t Training loss (single batch): 0.9695499539375305\n",
      "\t Training loss (single batch): 1.1632057428359985\n",
      "\t Training loss (single batch): 0.9298243522644043\n",
      "\t Training loss (single batch): 1.3046879768371582\n",
      "\t Training loss (single batch): 0.6464336514472961\n",
      "\t Training loss (single batch): 1.043060541152954\n",
      "\t Training loss (single batch): 1.2990316152572632\n",
      "\t Training loss (single batch): 1.014518141746521\n",
      "\t Training loss (single batch): 1.0944041013717651\n",
      "\t Training loss (single batch): 1.4329004287719727\n",
      "\t Training loss (single batch): 1.0129567384719849\n",
      "\t Training loss (single batch): 1.0863581895828247\n",
      "\t Training loss (single batch): 1.3465075492858887\n",
      "\t Training loss (single batch): 1.5528215169906616\n",
      "\t Training loss (single batch): 0.6366139054298401\n",
      "\t Training loss (single batch): 1.390250325202942\n",
      "\t Training loss (single batch): 1.3530064821243286\n",
      "\t Training loss (single batch): 1.0087298154830933\n",
      "\t Training loss (single batch): 1.4365062713623047\n",
      "\t Training loss (single batch): 1.0183966159820557\n",
      "\t Training loss (single batch): 0.9397000074386597\n",
      "\t Training loss (single batch): 1.0347448587417603\n",
      "\t Training loss (single batch): 1.1018112897872925\n",
      "\t Training loss (single batch): 1.046991229057312\n",
      "\t Training loss (single batch): 1.3770561218261719\n",
      "\t Training loss (single batch): 1.626067876815796\n",
      "\t Training loss (single batch): 1.2468317747116089\n",
      "\t Training loss (single batch): 0.9546251893043518\n",
      "\t Training loss (single batch): 1.5651905536651611\n",
      "\t Training loss (single batch): 1.183885931968689\n",
      "\t Training loss (single batch): 1.3668475151062012\n",
      "\t Training loss (single batch): 0.9906417727470398\n",
      "\t Training loss (single batch): 1.081068515777588\n",
      "\t Training loss (single batch): 1.4326289892196655\n",
      "\t Training loss (single batch): 1.1437753438949585\n",
      "\t Training loss (single batch): 1.0167016983032227\n",
      "\t Training loss (single batch): 1.3160353899002075\n",
      "\t Training loss (single batch): 1.1182597875595093\n",
      "\t Training loss (single batch): 1.4480189085006714\n",
      "\t Training loss (single batch): 1.2513456344604492\n",
      "\t Training loss (single batch): 1.0057297945022583\n",
      "\t Training loss (single batch): 1.3415831327438354\n",
      "\t Training loss (single batch): 1.2665302753448486\n",
      "\t Training loss (single batch): 0.815467357635498\n",
      "\t Training loss (single batch): 1.3720473051071167\n",
      "\t Training loss (single batch): 1.316717505455017\n",
      "\t Training loss (single batch): 1.3189082145690918\n",
      "\t Training loss (single batch): 1.1630353927612305\n",
      "\t Training loss (single batch): 1.1435819864273071\n",
      "\t Training loss (single batch): 1.1476898193359375\n",
      "\t Training loss (single batch): 1.203023076057434\n",
      "\t Training loss (single batch): 1.0474748611450195\n",
      "\t Training loss (single batch): 1.2063475847244263\n",
      "\t Training loss (single batch): 1.474607229232788\n",
      "\t Training loss (single batch): 1.19148588180542\n",
      "\t Training loss (single batch): 1.0949714183807373\n",
      "\t Training loss (single batch): 1.3707488775253296\n",
      "\t Training loss (single batch): 1.1525590419769287\n",
      "\t Training loss (single batch): 1.0071611404418945\n",
      "\t Training loss (single batch): 0.7800023555755615\n",
      "\t Training loss (single batch): 1.4606400728225708\n",
      "\t Training loss (single batch): 0.9696358442306519\n",
      "\t Training loss (single batch): 1.0507586002349854\n",
      "\t Training loss (single batch): 0.9824492335319519\n",
      "\t Training loss (single batch): 1.1769472360610962\n",
      "\t Training loss (single batch): 1.5770758390426636\n",
      "\t Training loss (single batch): 1.2755646705627441\n",
      "\t Training loss (single batch): 0.7911551594734192\n",
      "\t Training loss (single batch): 1.175831913948059\n",
      "\t Training loss (single batch): 1.256068229675293\n",
      "\t Training loss (single batch): 1.0014649629592896\n",
      "\t Training loss (single batch): 1.2410229444503784\n",
      "\t Training loss (single batch): 1.5004562139511108\n",
      "\t Training loss (single batch): 1.4834702014923096\n",
      "\t Training loss (single batch): 1.3267794847488403\n",
      "\t Training loss (single batch): 1.2958307266235352\n",
      "\t Training loss (single batch): 0.8216382265090942\n",
      "\t Training loss (single batch): 1.1234945058822632\n",
      "\t Training loss (single batch): 1.083858847618103\n",
      "\t Training loss (single batch): 1.4825000762939453\n",
      "\t Training loss (single batch): 0.8470661044120789\n",
      "\t Training loss (single batch): 1.1810957193374634\n",
      "\t Training loss (single batch): 1.4204199314117432\n",
      "\t Training loss (single batch): 0.9664593935012817\n",
      "\t Training loss (single batch): 1.5529985427856445\n",
      "\t Training loss (single batch): 1.175679087638855\n",
      "\t Training loss (single batch): 1.6566157341003418\n",
      "\t Training loss (single batch): 1.4943028688430786\n",
      "\t Training loss (single batch): 1.201736330986023\n",
      "\t Training loss (single batch): 1.3267347812652588\n",
      "\t Training loss (single batch): 1.2323576211929321\n",
      "\t Training loss (single batch): 1.1387128829956055\n",
      "\t Training loss (single batch): 1.6590702533721924\n",
      "\t Training loss (single batch): 1.3773374557495117\n",
      "\t Training loss (single batch): 1.1710301637649536\n",
      "\t Training loss (single batch): 0.9233402013778687\n",
      "\t Training loss (single batch): 1.3201080560684204\n",
      "\t Training loss (single batch): 1.262514591217041\n",
      "\t Training loss (single batch): 1.0931819677352905\n",
      "\t Training loss (single batch): 0.7520661950111389\n",
      "\t Training loss (single batch): 1.1938925981521606\n",
      "\t Training loss (single batch): 1.0119194984436035\n",
      "\t Training loss (single batch): 1.064293622970581\n",
      "\t Training loss (single batch): 1.1951251029968262\n",
      "\t Training loss (single batch): 1.1959465742111206\n",
      "\t Training loss (single batch): 1.204687237739563\n",
      "\t Training loss (single batch): 1.018338680267334\n",
      "\t Training loss (single batch): 1.0245484113693237\n",
      "\t Training loss (single batch): 1.2914670705795288\n",
      "\t Training loss (single batch): 0.9552109837532043\n",
      "\t Training loss (single batch): 1.171777606010437\n",
      "\t Training loss (single batch): 1.1151123046875\n",
      "\t Training loss (single batch): 1.1192779541015625\n",
      "\t Training loss (single batch): 1.0227174758911133\n",
      "\t Training loss (single batch): 1.0856916904449463\n",
      "\t Training loss (single batch): 1.230499505996704\n",
      "\t Training loss (single batch): 0.9230378866195679\n",
      "\t Training loss (single batch): 1.2916265726089478\n",
      "\t Training loss (single batch): 1.4478527307510376\n",
      "\t Training loss (single batch): 1.1900804042816162\n",
      "\t Training loss (single batch): 1.6270934343338013\n",
      "\t Training loss (single batch): 1.1080743074417114\n",
      "\t Training loss (single batch): 0.8235902190208435\n",
      "\t Training loss (single batch): 1.4248888492584229\n",
      "\t Training loss (single batch): 1.695000171661377\n",
      "\t Training loss (single batch): 1.4216257333755493\n",
      "\t Training loss (single batch): 1.2648077011108398\n",
      "\t Training loss (single batch): 1.0532557964324951\n",
      "\t Training loss (single batch): 1.3717918395996094\n",
      "\t Training loss (single batch): 1.0537607669830322\n",
      "\t Training loss (single batch): 1.474496603012085\n",
      "\t Training loss (single batch): 1.009109616279602\n",
      "\t Training loss (single batch): 1.6185048818588257\n",
      "\t Training loss (single batch): 1.506604552268982\n",
      "\t Training loss (single batch): 0.8012993335723877\n",
      "\t Training loss (single batch): 1.4961159229278564\n",
      "\t Training loss (single batch): 1.3577629327774048\n",
      "\t Training loss (single batch): 1.3914388418197632\n",
      "\t Training loss (single batch): 1.3172938823699951\n",
      "\t Training loss (single batch): 1.6757978200912476\n",
      "\t Training loss (single batch): 1.2257053852081299\n",
      "\t Training loss (single batch): 1.1103874444961548\n",
      "\t Training loss (single batch): 0.9056277871131897\n",
      "\t Training loss (single batch): 1.2860735654830933\n",
      "\t Training loss (single batch): 1.132815957069397\n",
      "\t Training loss (single batch): 1.218517780303955\n",
      "\t Training loss (single batch): 1.446561574935913\n",
      "\t Training loss (single batch): 0.7732785940170288\n",
      "\t Training loss (single batch): 0.978691041469574\n",
      "\t Training loss (single batch): 0.9378138184547424\n",
      "\t Training loss (single batch): 1.22853422164917\n",
      "\t Training loss (single batch): 1.4378352165222168\n",
      "\t Training loss (single batch): 1.4414048194885254\n",
      "\t Training loss (single batch): 1.3361997604370117\n",
      "\t Training loss (single batch): 1.1825474500656128\n",
      "\t Training loss (single batch): 1.0101993083953857\n",
      "\t Training loss (single batch): 0.9498181343078613\n",
      "\t Training loss (single batch): 1.314919352531433\n",
      "\t Training loss (single batch): 0.8936217427253723\n",
      "\t Training loss (single batch): 1.3137595653533936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.27603280544281\n",
      "\t Training loss (single batch): 1.265056848526001\n",
      "\t Training loss (single batch): 0.9857997894287109\n",
      "\t Training loss (single batch): 1.772904872894287\n",
      "\t Training loss (single batch): 0.9197771549224854\n",
      "\t Training loss (single batch): 0.5786967277526855\n",
      "\t Training loss (single batch): 1.9094334840774536\n",
      "\t Training loss (single batch): 1.2731109857559204\n",
      "\t Training loss (single batch): 0.9334478974342346\n",
      "\t Training loss (single batch): 1.0197595357894897\n",
      "\t Training loss (single batch): 1.1768301725387573\n",
      "\t Training loss (single batch): 0.9428192973136902\n",
      "\t Training loss (single batch): 1.2155194282531738\n",
      "\t Training loss (single batch): 1.022384762763977\n",
      "\t Training loss (single batch): 1.7043575048446655\n",
      "\t Training loss (single batch): 0.9184005260467529\n",
      "\t Training loss (single batch): 0.8152503371238708\n",
      "\t Training loss (single batch): 1.1040291786193848\n",
      "\t Training loss (single batch): 1.354614496231079\n",
      "\t Training loss (single batch): 1.439971923828125\n",
      "\t Training loss (single batch): 1.35819411277771\n",
      "\t Training loss (single batch): 1.104143738746643\n",
      "\t Training loss (single batch): 1.3426213264465332\n",
      "\t Training loss (single batch): 1.2421493530273438\n",
      "\t Training loss (single batch): 1.0286198854446411\n",
      "\t Training loss (single batch): 1.296791434288025\n",
      "\t Training loss (single batch): 1.1247681379318237\n",
      "\t Training loss (single batch): 1.0984352827072144\n",
      "\t Training loss (single batch): 1.025180459022522\n",
      "\t Training loss (single batch): 1.5428962707519531\n",
      "\t Training loss (single batch): 1.2268694639205933\n",
      "\t Training loss (single batch): 1.2056690454483032\n",
      "\t Training loss (single batch): 1.291327714920044\n",
      "\t Training loss (single batch): 1.2937688827514648\n",
      "\t Training loss (single batch): 1.2082465887069702\n",
      "\t Training loss (single batch): 1.025858759880066\n",
      "\t Training loss (single batch): 1.0055584907531738\n",
      "\t Training loss (single batch): 1.1821311712265015\n",
      "\t Training loss (single batch): 0.9019854664802551\n",
      "\t Training loss (single batch): 1.2878910303115845\n",
      "\t Training loss (single batch): 1.5077675580978394\n",
      "\t Training loss (single batch): 1.1980257034301758\n",
      "\t Training loss (single batch): 1.2753641605377197\n",
      "\t Training loss (single batch): 1.1009736061096191\n",
      "\t Training loss (single batch): 1.8112281560897827\n",
      "\t Training loss (single batch): 1.2552742958068848\n",
      "\t Training loss (single batch): 1.2404074668884277\n",
      "\t Training loss (single batch): 1.3577818870544434\n",
      "\t Training loss (single batch): 1.5009493827819824\n",
      "\t Training loss (single batch): 1.5129529237747192\n",
      "\t Training loss (single batch): 1.1646029949188232\n",
      "\t Training loss (single batch): 1.3032726049423218\n",
      "\t Training loss (single batch): 1.316422700881958\n",
      "\t Training loss (single batch): 1.3084816932678223\n",
      "\t Training loss (single batch): 1.1725738048553467\n",
      "\t Training loss (single batch): 1.448144793510437\n",
      "\t Training loss (single batch): 1.2736525535583496\n",
      "\t Training loss (single batch): 1.0871113538742065\n",
      "\t Training loss (single batch): 1.3789331912994385\n",
      "\t Training loss (single batch): 0.9241101145744324\n",
      "\t Training loss (single batch): 1.3813865184783936\n",
      "\t Training loss (single batch): 1.5512747764587402\n",
      "\t Training loss (single batch): 1.1387536525726318\n",
      "\t Training loss (single batch): 1.0017169713974\n",
      "\t Training loss (single batch): 1.5134828090667725\n",
      "\t Training loss (single batch): 1.3689253330230713\n",
      "\t Training loss (single batch): 1.0120753049850464\n",
      "\t Training loss (single batch): 1.5843043327331543\n",
      "\t Training loss (single batch): 1.3223985433578491\n",
      "\t Training loss (single batch): 1.259666919708252\n",
      "\t Training loss (single batch): 1.3814632892608643\n",
      "\t Training loss (single batch): 1.148054599761963\n",
      "\t Training loss (single batch): 0.8408433198928833\n",
      "\t Training loss (single batch): 0.9903137683868408\n",
      "\t Training loss (single batch): 1.6858882904052734\n",
      "\t Training loss (single batch): 1.0588659048080444\n",
      "\t Training loss (single batch): 1.0173195600509644\n",
      "\t Training loss (single batch): 1.2684613466262817\n",
      "\t Training loss (single batch): 1.541021466255188\n",
      "\t Training loss (single batch): 1.1956576108932495\n",
      "\t Training loss (single batch): 1.0745635032653809\n",
      "\t Training loss (single batch): 1.053780436515808\n",
      "\t Training loss (single batch): 1.160339593887329\n",
      "##################################\n",
      "## EPOCH 50\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9567584991455078\n",
      "\t Training loss (single batch): 0.7803218364715576\n",
      "\t Training loss (single batch): 1.1691793203353882\n",
      "\t Training loss (single batch): 0.6385689973831177\n",
      "\t Training loss (single batch): 1.0199494361877441\n",
      "\t Training loss (single batch): 1.11139976978302\n",
      "\t Training loss (single batch): 1.2174147367477417\n",
      "\t Training loss (single batch): 1.7535901069641113\n",
      "\t Training loss (single batch): 1.1969786882400513\n",
      "\t Training loss (single batch): 1.5405529737472534\n",
      "\t Training loss (single batch): 2.1243932247161865\n",
      "\t Training loss (single batch): 1.1717302799224854\n",
      "\t Training loss (single batch): 1.2325319051742554\n",
      "\t Training loss (single batch): 1.2584266662597656\n",
      "\t Training loss (single batch): 1.5629920959472656\n",
      "\t Training loss (single batch): 1.2781851291656494\n",
      "\t Training loss (single batch): 1.3559731245040894\n",
      "\t Training loss (single batch): 1.565127968788147\n",
      "\t Training loss (single batch): 1.4845050573349\n",
      "\t Training loss (single batch): 0.9253380298614502\n",
      "\t Training loss (single batch): 1.5801054239273071\n",
      "\t Training loss (single batch): 1.4228758811950684\n",
      "\t Training loss (single batch): 1.1946699619293213\n",
      "\t Training loss (single batch): 0.8700001835823059\n",
      "\t Training loss (single batch): 1.3172166347503662\n",
      "\t Training loss (single batch): 1.1709198951721191\n",
      "\t Training loss (single batch): 1.4147471189498901\n",
      "\t Training loss (single batch): 1.6829230785369873\n",
      "\t Training loss (single batch): 1.3040274381637573\n",
      "\t Training loss (single batch): 1.1349400281906128\n",
      "\t Training loss (single batch): 0.8580418229103088\n",
      "\t Training loss (single batch): 0.9871070981025696\n",
      "\t Training loss (single batch): 0.8793724179267883\n",
      "\t Training loss (single batch): 1.5828304290771484\n",
      "\t Training loss (single batch): 1.1190803050994873\n",
      "\t Training loss (single batch): 0.905813455581665\n",
      "\t Training loss (single batch): 1.240196943283081\n",
      "\t Training loss (single batch): 1.0207116603851318\n",
      "\t Training loss (single batch): 1.5671905279159546\n",
      "\t Training loss (single batch): 0.8993825316429138\n",
      "\t Training loss (single batch): 1.232354998588562\n",
      "\t Training loss (single batch): 1.2126960754394531\n",
      "\t Training loss (single batch): 1.3574289083480835\n",
      "\t Training loss (single batch): 1.1154714822769165\n",
      "\t Training loss (single batch): 1.0020811557769775\n",
      "\t Training loss (single batch): 0.9319035410881042\n",
      "\t Training loss (single batch): 1.2257580757141113\n",
      "\t Training loss (single batch): 1.1516202688217163\n",
      "\t Training loss (single batch): 1.40303373336792\n",
      "\t Training loss (single batch): 1.4110753536224365\n",
      "\t Training loss (single batch): 0.9669762849807739\n",
      "\t Training loss (single batch): 1.2483919858932495\n",
      "\t Training loss (single batch): 1.311309576034546\n",
      "\t Training loss (single batch): 1.1719865798950195\n",
      "\t Training loss (single batch): 1.1784311532974243\n",
      "\t Training loss (single batch): 1.2076019048690796\n",
      "\t Training loss (single batch): 0.9063299298286438\n",
      "\t Training loss (single batch): 1.0258938074111938\n",
      "\t Training loss (single batch): 1.1255769729614258\n",
      "\t Training loss (single batch): 1.2771086692810059\n",
      "\t Training loss (single batch): 1.1086478233337402\n",
      "\t Training loss (single batch): 1.115014672279358\n",
      "\t Training loss (single batch): 0.9881826639175415\n",
      "\t Training loss (single batch): 1.0767625570297241\n",
      "\t Training loss (single batch): 1.1620087623596191\n",
      "\t Training loss (single batch): 0.782391369342804\n",
      "\t Training loss (single batch): 1.184607744216919\n",
      "\t Training loss (single batch): 0.6831768155097961\n",
      "\t Training loss (single batch): 1.837912917137146\n",
      "\t Training loss (single batch): 1.6394014358520508\n",
      "\t Training loss (single batch): 1.2041679620742798\n",
      "\t Training loss (single batch): 1.7050353288650513\n",
      "\t Training loss (single batch): 0.8459214568138123\n",
      "\t Training loss (single batch): 1.5734375715255737\n",
      "\t Training loss (single batch): 0.9944756627082825\n",
      "\t Training loss (single batch): 0.9711762070655823\n",
      "\t Training loss (single batch): 1.3451281785964966\n",
      "\t Training loss (single batch): 0.9139094352722168\n",
      "\t Training loss (single batch): 1.3738409280776978\n",
      "\t Training loss (single batch): 1.0393834114074707\n",
      "\t Training loss (single batch): 1.4884365797042847\n",
      "\t Training loss (single batch): 0.8994757533073425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8186900019645691\n",
      "\t Training loss (single batch): 1.1676101684570312\n",
      "\t Training loss (single batch): 1.4339604377746582\n",
      "\t Training loss (single batch): 1.423677682876587\n",
      "\t Training loss (single batch): 1.0612982511520386\n",
      "\t Training loss (single batch): 1.0810505151748657\n",
      "\t Training loss (single batch): 1.4457423686981201\n",
      "\t Training loss (single batch): 0.9086862206459045\n",
      "\t Training loss (single batch): 1.379881501197815\n",
      "\t Training loss (single batch): 1.3947848081588745\n",
      "\t Training loss (single batch): 1.2002664804458618\n",
      "\t Training loss (single batch): 1.098637580871582\n",
      "\t Training loss (single batch): 1.3339265584945679\n",
      "\t Training loss (single batch): 1.2182353734970093\n",
      "\t Training loss (single batch): 1.1646971702575684\n",
      "\t Training loss (single batch): 1.1682968139648438\n",
      "\t Training loss (single batch): 1.280298113822937\n",
      "\t Training loss (single batch): 1.3280971050262451\n",
      "\t Training loss (single batch): 1.567308783531189\n",
      "\t Training loss (single batch): 0.9404608011245728\n",
      "\t Training loss (single batch): 0.8835026025772095\n",
      "\t Training loss (single batch): 1.1328401565551758\n",
      "\t Training loss (single batch): 1.4399272203445435\n",
      "\t Training loss (single batch): 1.0828258991241455\n",
      "\t Training loss (single batch): 1.6021373271942139\n",
      "\t Training loss (single batch): 0.8688092827796936\n",
      "\t Training loss (single batch): 1.0021836757659912\n",
      "\t Training loss (single batch): 1.0602222681045532\n",
      "\t Training loss (single batch): 0.8838101625442505\n",
      "\t Training loss (single batch): 1.2425577640533447\n",
      "\t Training loss (single batch): 1.090003252029419\n",
      "\t Training loss (single batch): 0.9634604454040527\n",
      "\t Training loss (single batch): 1.1159517765045166\n",
      "\t Training loss (single batch): 1.5607457160949707\n",
      "\t Training loss (single batch): 1.5291802883148193\n",
      "\t Training loss (single batch): 0.793245792388916\n",
      "\t Training loss (single batch): 1.7467845678329468\n",
      "\t Training loss (single batch): 1.0285288095474243\n",
      "\t Training loss (single batch): 1.2320982217788696\n",
      "\t Training loss (single batch): 0.9707443714141846\n",
      "\t Training loss (single batch): 1.4322988986968994\n",
      "\t Training loss (single batch): 0.6622457504272461\n",
      "\t Training loss (single batch): 1.153774380683899\n",
      "\t Training loss (single batch): 1.7312506437301636\n",
      "\t Training loss (single batch): 1.3170908689498901\n",
      "\t Training loss (single batch): 0.8516395092010498\n",
      "\t Training loss (single batch): 1.2141997814178467\n",
      "\t Training loss (single batch): 1.0440750122070312\n",
      "\t Training loss (single batch): 1.4334642887115479\n",
      "\t Training loss (single batch): 1.4959818124771118\n",
      "\t Training loss (single batch): 1.254241704940796\n",
      "\t Training loss (single batch): 0.9541874527931213\n",
      "\t Training loss (single batch): 1.2115814685821533\n",
      "\t Training loss (single batch): 0.9844301342964172\n",
      "\t Training loss (single batch): 1.2500495910644531\n",
      "\t Training loss (single batch): 1.3152250051498413\n",
      "\t Training loss (single batch): 1.2787718772888184\n",
      "\t Training loss (single batch): 1.3190640211105347\n",
      "\t Training loss (single batch): 1.3136779069900513\n",
      "\t Training loss (single batch): 1.3651010990142822\n",
      "\t Training loss (single batch): 1.2092069387435913\n",
      "\t Training loss (single batch): 1.2064380645751953\n",
      "\t Training loss (single batch): 1.0686941146850586\n",
      "\t Training loss (single batch): 1.1575525999069214\n",
      "\t Training loss (single batch): 1.0094352960586548\n",
      "\t Training loss (single batch): 1.305585503578186\n",
      "\t Training loss (single batch): 1.279336929321289\n",
      "\t Training loss (single batch): 1.3237316608428955\n",
      "\t Training loss (single batch): 0.9727575778961182\n",
      "\t Training loss (single batch): 1.267334222793579\n",
      "\t Training loss (single batch): 1.1347304582595825\n",
      "\t Training loss (single batch): 1.50761079788208\n",
      "\t Training loss (single batch): 1.3103898763656616\n",
      "\t Training loss (single batch): 1.1312302350997925\n",
      "\t Training loss (single batch): 1.2894785404205322\n",
      "\t Training loss (single batch): 1.042147159576416\n",
      "\t Training loss (single batch): 2.003019094467163\n",
      "\t Training loss (single batch): 1.4050335884094238\n",
      "\t Training loss (single batch): 1.2203642129898071\n",
      "\t Training loss (single batch): 1.237139344215393\n",
      "\t Training loss (single batch): 1.0551977157592773\n",
      "\t Training loss (single batch): 1.1975725889205933\n",
      "\t Training loss (single batch): 1.5506277084350586\n",
      "\t Training loss (single batch): 1.137620449066162\n",
      "\t Training loss (single batch): 1.102294921875\n",
      "\t Training loss (single batch): 0.9803826212882996\n",
      "\t Training loss (single batch): 1.3360081911087036\n",
      "\t Training loss (single batch): 1.490007996559143\n",
      "\t Training loss (single batch): 1.378694772720337\n",
      "\t Training loss (single batch): 1.560280680656433\n",
      "\t Training loss (single batch): 1.0283342599868774\n",
      "\t Training loss (single batch): 1.2291098833084106\n",
      "\t Training loss (single batch): 1.2834736108779907\n",
      "\t Training loss (single batch): 1.2633428573608398\n",
      "\t Training loss (single batch): 1.2098599672317505\n",
      "\t Training loss (single batch): 1.3350634574890137\n",
      "\t Training loss (single batch): 1.030360460281372\n",
      "\t Training loss (single batch): 0.9830886721611023\n",
      "\t Training loss (single batch): 1.1802726984024048\n",
      "\t Training loss (single batch): 1.5374836921691895\n",
      "\t Training loss (single batch): 1.4650108814239502\n",
      "\t Training loss (single batch): 1.277617335319519\n",
      "\t Training loss (single batch): 0.8927851319313049\n",
      "\t Training loss (single batch): 1.4901533126831055\n",
      "\t Training loss (single batch): 1.498630404472351\n",
      "\t Training loss (single batch): 1.175510048866272\n",
      "\t Training loss (single batch): 1.2948757410049438\n",
      "\t Training loss (single batch): 1.2745511531829834\n",
      "\t Training loss (single batch): 0.8060916662216187\n",
      "\t Training loss (single batch): 1.1293267011642456\n",
      "\t Training loss (single batch): 1.3191407918930054\n",
      "\t Training loss (single batch): 1.360317587852478\n",
      "\t Training loss (single batch): 0.8556551337242126\n",
      "\t Training loss (single batch): 1.063209056854248\n",
      "\t Training loss (single batch): 1.4539510011672974\n",
      "\t Training loss (single batch): 1.2245985269546509\n",
      "\t Training loss (single batch): 1.258408784866333\n",
      "\t Training loss (single batch): 1.1754056215286255\n",
      "\t Training loss (single batch): 1.0770742893218994\n",
      "\t Training loss (single batch): 1.0556801557540894\n",
      "\t Training loss (single batch): 1.564865231513977\n",
      "\t Training loss (single batch): 1.149689793586731\n",
      "\t Training loss (single batch): 1.0384392738342285\n",
      "\t Training loss (single batch): 1.2907146215438843\n",
      "\t Training loss (single batch): 0.9851320385932922\n",
      "\t Training loss (single batch): 1.2237056493759155\n",
      "\t Training loss (single batch): 1.475786566734314\n",
      "\t Training loss (single batch): 1.1702836751937866\n",
      "\t Training loss (single batch): 1.5843408107757568\n",
      "\t Training loss (single batch): 1.188262701034546\n",
      "\t Training loss (single batch): 0.746906042098999\n",
      "\t Training loss (single batch): 0.6829273104667664\n",
      "\t Training loss (single batch): 1.0787657499313354\n",
      "\t Training loss (single batch): 1.3321346044540405\n",
      "\t Training loss (single batch): 1.1307474374771118\n",
      "\t Training loss (single batch): 1.1207650899887085\n",
      "\t Training loss (single batch): 1.3686656951904297\n",
      "\t Training loss (single batch): 1.087817907333374\n",
      "\t Training loss (single batch): 0.9794532656669617\n",
      "\t Training loss (single batch): 0.9755927324295044\n",
      "\t Training loss (single batch): 0.8291816115379333\n",
      "\t Training loss (single batch): 1.2768810987472534\n",
      "\t Training loss (single batch): 1.4392105340957642\n",
      "\t Training loss (single batch): 1.0948494672775269\n",
      "\t Training loss (single batch): 1.0161010026931763\n",
      "\t Training loss (single batch): 1.1025649309158325\n",
      "\t Training loss (single batch): 0.6307138800621033\n",
      "\t Training loss (single batch): 1.0006396770477295\n",
      "\t Training loss (single batch): 1.0853890180587769\n",
      "\t Training loss (single batch): 0.9372626543045044\n",
      "\t Training loss (single batch): 2.0108280181884766\n",
      "\t Training loss (single batch): 1.1234387159347534\n",
      "\t Training loss (single batch): 0.9783035516738892\n",
      "\t Training loss (single batch): 1.1583298444747925\n",
      "\t Training loss (single batch): 0.9165871739387512\n",
      "\t Training loss (single batch): 1.2914067506790161\n",
      "\t Training loss (single batch): 1.4861024618148804\n",
      "\t Training loss (single batch): 1.2309104204177856\n",
      "\t Training loss (single batch): 1.2969207763671875\n",
      "\t Training loss (single batch): 1.2230417728424072\n",
      "\t Training loss (single batch): 0.8819428086280823\n",
      "\t Training loss (single batch): 1.7075601816177368\n",
      "\t Training loss (single batch): 0.7683162093162537\n",
      "\t Training loss (single batch): 1.4862580299377441\n",
      "\t Training loss (single batch): 1.2439621686935425\n",
      "\t Training loss (single batch): 1.6336627006530762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1985063552856445\n",
      "\t Training loss (single batch): 1.2064638137817383\n",
      "\t Training loss (single batch): 1.5777226686477661\n",
      "\t Training loss (single batch): 1.7464605569839478\n",
      "\t Training loss (single batch): 1.6295703649520874\n",
      "\t Training loss (single batch): 1.3561609983444214\n",
      "\t Training loss (single batch): 1.4124234914779663\n",
      "\t Training loss (single batch): 0.904828667640686\n",
      "\t Training loss (single batch): 1.1040735244750977\n",
      "\t Training loss (single batch): 1.2645338773727417\n",
      "\t Training loss (single batch): 1.5925559997558594\n",
      "\t Training loss (single batch): 1.5136059522628784\n",
      "\t Training loss (single batch): 1.2655707597732544\n",
      "\t Training loss (single batch): 0.9100019335746765\n",
      "\t Training loss (single batch): 1.0892606973648071\n",
      "\t Training loss (single batch): 1.4211910963058472\n",
      "\t Training loss (single batch): 0.7891457080841064\n",
      "\t Training loss (single batch): 1.4303685426712036\n",
      "\t Training loss (single batch): 1.1723135709762573\n",
      "\t Training loss (single batch): 1.343559980392456\n",
      "\t Training loss (single batch): 1.410836100578308\n",
      "\t Training loss (single batch): 0.7562367916107178\n",
      "\t Training loss (single batch): 1.563703179359436\n",
      "\t Training loss (single batch): 1.1387919187545776\n",
      "\t Training loss (single batch): 1.049022912979126\n",
      "\t Training loss (single batch): 1.0530455112457275\n",
      "\t Training loss (single batch): 1.1450802087783813\n",
      "\t Training loss (single batch): 1.228319525718689\n",
      "\t Training loss (single batch): 1.2663252353668213\n",
      "\t Training loss (single batch): 0.9893584251403809\n",
      "\t Training loss (single batch): 0.9104210734367371\n",
      "\t Training loss (single batch): 1.247997760772705\n",
      "\t Training loss (single batch): 1.669893741607666\n",
      "\t Training loss (single batch): 0.8662233352661133\n",
      "\t Training loss (single batch): 0.9379561543464661\n",
      "\t Training loss (single batch): 1.4765225648880005\n",
      "\t Training loss (single batch): 1.684133768081665\n",
      "\t Training loss (single batch): 1.387006163597107\n",
      "\t Training loss (single batch): 1.5139800310134888\n",
      "\t Training loss (single batch): 1.1385642290115356\n",
      "\t Training loss (single batch): 1.293872356414795\n",
      "\t Training loss (single batch): 1.511392593383789\n",
      "\t Training loss (single batch): 1.1011277437210083\n",
      "\t Training loss (single batch): 1.1535967588424683\n",
      "\t Training loss (single batch): 0.9439721703529358\n",
      "\t Training loss (single batch): 1.5763068199157715\n",
      "\t Training loss (single batch): 0.9465427994728088\n",
      "\t Training loss (single batch): 1.111654281616211\n",
      "\t Training loss (single batch): 1.5179438591003418\n",
      "\t Training loss (single batch): 1.1866871118545532\n",
      "\t Training loss (single batch): 0.9908157587051392\n",
      "\t Training loss (single batch): 0.8094754219055176\n",
      "\t Training loss (single batch): 1.233156681060791\n",
      "\t Training loss (single batch): 1.4273717403411865\n",
      "\t Training loss (single batch): 1.2918548583984375\n",
      "\t Training loss (single batch): 1.6070444583892822\n",
      "\t Training loss (single batch): 1.5832250118255615\n",
      "\t Training loss (single batch): 1.4772385358810425\n",
      "\t Training loss (single batch): 1.3485058546066284\n",
      "\t Training loss (single batch): 0.8990380764007568\n",
      "\t Training loss (single batch): 1.571482539176941\n",
      "\t Training loss (single batch): 1.17528235912323\n",
      "\t Training loss (single batch): 1.1739000082015991\n",
      "\t Training loss (single batch): 0.8402869701385498\n",
      "\t Training loss (single batch): 1.2173001766204834\n",
      "\t Training loss (single batch): 0.9763023853302002\n",
      "\t Training loss (single batch): 0.8042142391204834\n",
      "\t Training loss (single batch): 0.9397892355918884\n",
      "\t Training loss (single batch): 1.0758954286575317\n",
      "##################################\n",
      "## EPOCH 51\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2221566438674927\n",
      "\t Training loss (single batch): 0.9880619645118713\n",
      "\t Training loss (single batch): 1.1308478116989136\n",
      "\t Training loss (single batch): 1.6561325788497925\n",
      "\t Training loss (single batch): 1.57230544090271\n",
      "\t Training loss (single batch): 1.5929993391036987\n",
      "\t Training loss (single batch): 1.1283931732177734\n",
      "\t Training loss (single batch): 1.2973345518112183\n",
      "\t Training loss (single batch): 1.3689264059066772\n",
      "\t Training loss (single batch): 1.2100977897644043\n",
      "\t Training loss (single batch): 1.3177212476730347\n",
      "\t Training loss (single batch): 1.2831014394760132\n",
      "\t Training loss (single batch): 1.113263726234436\n",
      "\t Training loss (single batch): 1.084387183189392\n",
      "\t Training loss (single batch): 1.073154330253601\n",
      "\t Training loss (single batch): 1.406050682067871\n",
      "\t Training loss (single batch): 1.5020315647125244\n",
      "\t Training loss (single batch): 1.3336454629898071\n",
      "\t Training loss (single batch): 1.05784010887146\n",
      "\t Training loss (single batch): 1.033235788345337\n",
      "\t Training loss (single batch): 1.0302051305770874\n",
      "\t Training loss (single batch): 0.9469567537307739\n",
      "\t Training loss (single batch): 1.5824131965637207\n",
      "\t Training loss (single batch): 1.5591814517974854\n",
      "\t Training loss (single batch): 0.7612942457199097\n",
      "\t Training loss (single batch): 1.4149419069290161\n",
      "\t Training loss (single batch): 1.4535812139511108\n",
      "\t Training loss (single batch): 1.4275282621383667\n",
      "\t Training loss (single batch): 1.2167483568191528\n",
      "\t Training loss (single batch): 1.1571433544158936\n",
      "\t Training loss (single batch): 1.356923222541809\n",
      "\t Training loss (single batch): 0.9578731060028076\n",
      "\t Training loss (single batch): 1.018335223197937\n",
      "\t Training loss (single batch): 1.1802186965942383\n",
      "\t Training loss (single batch): 0.7515318989753723\n",
      "\t Training loss (single batch): 1.1721282005310059\n",
      "\t Training loss (single batch): 1.2823145389556885\n",
      "\t Training loss (single batch): 2.0434329509735107\n",
      "\t Training loss (single batch): 1.0124093294143677\n",
      "\t Training loss (single batch): 1.1887269020080566\n",
      "\t Training loss (single batch): 1.755988597869873\n",
      "\t Training loss (single batch): 1.8260785341262817\n",
      "\t Training loss (single batch): 0.8306658864021301\n",
      "\t Training loss (single batch): 0.8857006430625916\n",
      "\t Training loss (single batch): 1.1680631637573242\n",
      "\t Training loss (single batch): 1.0769785642623901\n",
      "\t Training loss (single batch): 1.3284093141555786\n",
      "\t Training loss (single batch): 0.8564507365226746\n",
      "\t Training loss (single batch): 1.0278921127319336\n",
      "\t Training loss (single batch): 0.9945082664489746\n",
      "\t Training loss (single batch): 0.8978171348571777\n",
      "\t Training loss (single batch): 1.1592789888381958\n",
      "\t Training loss (single batch): 1.111138105392456\n",
      "\t Training loss (single batch): 1.4672796726226807\n",
      "\t Training loss (single batch): 0.9605157375335693\n",
      "\t Training loss (single batch): 1.4060879945755005\n",
      "\t Training loss (single batch): 1.4248428344726562\n",
      "\t Training loss (single batch): 1.395622968673706\n",
      "\t Training loss (single batch): 1.0054378509521484\n",
      "\t Training loss (single batch): 1.0191422700881958\n",
      "\t Training loss (single batch): 0.9062921404838562\n",
      "\t Training loss (single batch): 1.0230796337127686\n",
      "\t Training loss (single batch): 1.2786169052124023\n",
      "\t Training loss (single batch): 1.3772672414779663\n",
      "\t Training loss (single batch): 1.8027528524398804\n",
      "\t Training loss (single batch): 1.1732816696166992\n",
      "\t Training loss (single batch): 1.5681790113449097\n",
      "\t Training loss (single batch): 1.199142575263977\n",
      "\t Training loss (single batch): 1.5051182508468628\n",
      "\t Training loss (single batch): 1.6743741035461426\n",
      "\t Training loss (single batch): 1.6965296268463135\n",
      "\t Training loss (single batch): 2.0222551822662354\n",
      "\t Training loss (single batch): 1.1225521564483643\n",
      "\t Training loss (single batch): 1.2163114547729492\n",
      "\t Training loss (single batch): 1.1318682432174683\n",
      "\t Training loss (single batch): 0.9100255966186523\n",
      "\t Training loss (single batch): 1.3505938053131104\n",
      "\t Training loss (single batch): 0.9430162906646729\n",
      "\t Training loss (single batch): 0.9106837511062622\n",
      "\t Training loss (single batch): 1.4304239749908447\n",
      "\t Training loss (single batch): 1.644051194190979\n",
      "\t Training loss (single batch): 0.9736929535865784\n",
      "\t Training loss (single batch): 0.9321311116218567\n",
      "\t Training loss (single batch): 1.1947578191757202\n",
      "\t Training loss (single batch): 0.9241966605186462\n",
      "\t Training loss (single batch): 1.259453296661377\n",
      "\t Training loss (single batch): 1.3606834411621094\n",
      "\t Training loss (single batch): 1.2875587940216064\n",
      "\t Training loss (single batch): 1.1751863956451416\n",
      "\t Training loss (single batch): 1.1145766973495483\n",
      "\t Training loss (single batch): 1.5176422595977783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0005006790161133\n",
      "\t Training loss (single batch): 1.354007601737976\n",
      "\t Training loss (single batch): 1.1334760189056396\n",
      "\t Training loss (single batch): 1.2399356365203857\n",
      "\t Training loss (single batch): 1.1301357746124268\n",
      "\t Training loss (single batch): 0.9079363346099854\n",
      "\t Training loss (single batch): 1.0024899244308472\n",
      "\t Training loss (single batch): 1.4963018894195557\n",
      "\t Training loss (single batch): 1.3208953142166138\n",
      "\t Training loss (single batch): 1.8113138675689697\n",
      "\t Training loss (single batch): 1.2723498344421387\n",
      "\t Training loss (single batch): 1.4798851013183594\n",
      "\t Training loss (single batch): 1.153626561164856\n",
      "\t Training loss (single batch): 1.0823931694030762\n",
      "\t Training loss (single batch): 0.7500759959220886\n",
      "\t Training loss (single batch): 1.220844030380249\n",
      "\t Training loss (single batch): 1.1233681440353394\n",
      "\t Training loss (single batch): 1.4315617084503174\n",
      "\t Training loss (single batch): 0.9357467293739319\n",
      "\t Training loss (single batch): 1.1243606805801392\n",
      "\t Training loss (single batch): 1.1329083442687988\n",
      "\t Training loss (single batch): 1.577830195426941\n",
      "\t Training loss (single batch): 1.0751097202301025\n",
      "\t Training loss (single batch): 1.2631422281265259\n",
      "\t Training loss (single batch): 0.9505190253257751\n",
      "\t Training loss (single batch): 0.681000292301178\n",
      "\t Training loss (single batch): 1.1054792404174805\n",
      "\t Training loss (single batch): 1.4138448238372803\n",
      "\t Training loss (single batch): 1.1735368967056274\n",
      "\t Training loss (single batch): 1.2083641290664673\n",
      "\t Training loss (single batch): 1.0917783975601196\n",
      "\t Training loss (single batch): 1.3504775762557983\n",
      "\t Training loss (single batch): 1.3020002841949463\n",
      "\t Training loss (single batch): 1.3442920446395874\n",
      "\t Training loss (single batch): 0.6871191263198853\n",
      "\t Training loss (single batch): 0.8016031980514526\n",
      "\t Training loss (single batch): 0.9685555696487427\n",
      "\t Training loss (single batch): 1.7192435264587402\n",
      "\t Training loss (single batch): 1.1040159463882446\n",
      "\t Training loss (single batch): 1.3105847835540771\n",
      "\t Training loss (single batch): 0.9811151027679443\n",
      "\t Training loss (single batch): 1.1741740703582764\n",
      "\t Training loss (single batch): 0.9248365759849548\n",
      "\t Training loss (single batch): 1.2006397247314453\n",
      "\t Training loss (single batch): 1.3817192316055298\n",
      "\t Training loss (single batch): 1.2919472455978394\n",
      "\t Training loss (single batch): 1.6218140125274658\n",
      "\t Training loss (single batch): 1.1003706455230713\n",
      "\t Training loss (single batch): 0.6565980911254883\n",
      "\t Training loss (single batch): 1.4355926513671875\n",
      "\t Training loss (single batch): 0.7171909809112549\n",
      "\t Training loss (single batch): 1.08562433719635\n",
      "\t Training loss (single batch): 1.6802845001220703\n",
      "\t Training loss (single batch): 1.3623807430267334\n",
      "\t Training loss (single batch): 1.0046626329421997\n",
      "\t Training loss (single batch): 1.7816524505615234\n",
      "\t Training loss (single batch): 1.512616515159607\n",
      "\t Training loss (single batch): 1.6823259592056274\n",
      "\t Training loss (single batch): 1.172343134880066\n",
      "\t Training loss (single batch): 1.4750725030899048\n",
      "\t Training loss (single batch): 1.185168743133545\n",
      "\t Training loss (single batch): 1.1555224657058716\n",
      "\t Training loss (single batch): 1.434616208076477\n",
      "\t Training loss (single batch): 1.3957844972610474\n",
      "\t Training loss (single batch): 1.426480770111084\n",
      "\t Training loss (single batch): 1.5291714668273926\n",
      "\t Training loss (single batch): 0.934264063835144\n",
      "\t Training loss (single batch): 1.1084107160568237\n",
      "\t Training loss (single batch): 0.9736367464065552\n",
      "\t Training loss (single batch): 1.0108939409255981\n",
      "\t Training loss (single batch): 1.3149505853652954\n",
      "\t Training loss (single batch): 1.1746821403503418\n",
      "\t Training loss (single batch): 1.2102677822113037\n",
      "\t Training loss (single batch): 1.5668389797210693\n",
      "\t Training loss (single batch): 1.3038233518600464\n",
      "\t Training loss (single batch): 0.8318232297897339\n",
      "\t Training loss (single batch): 1.2098554372787476\n",
      "\t Training loss (single batch): 1.0699067115783691\n",
      "\t Training loss (single batch): 1.2409017086029053\n",
      "\t Training loss (single batch): 1.252025842666626\n",
      "\t Training loss (single batch): 1.9007896184921265\n",
      "\t Training loss (single batch): 1.0952483415603638\n",
      "\t Training loss (single batch): 1.4418820142745972\n",
      "\t Training loss (single batch): 1.1253015995025635\n",
      "\t Training loss (single batch): 1.3434991836547852\n",
      "\t Training loss (single batch): 1.0493953227996826\n",
      "\t Training loss (single batch): 0.9092448949813843\n",
      "\t Training loss (single batch): 1.0827093124389648\n",
      "\t Training loss (single batch): 1.6204383373260498\n",
      "\t Training loss (single batch): 1.295483946800232\n",
      "\t Training loss (single batch): 0.9219309091567993\n",
      "\t Training loss (single batch): 1.515380620956421\n",
      "\t Training loss (single batch): 1.335787057876587\n",
      "\t Training loss (single batch): 1.157699465751648\n",
      "\t Training loss (single batch): 0.8519735336303711\n",
      "\t Training loss (single batch): 1.184605360031128\n",
      "\t Training loss (single batch): 1.0314090251922607\n",
      "\t Training loss (single batch): 1.2517002820968628\n",
      "\t Training loss (single batch): 0.9070643782615662\n",
      "\t Training loss (single batch): 0.6513922214508057\n",
      "\t Training loss (single batch): 1.2492848634719849\n",
      "\t Training loss (single batch): 1.104109764099121\n",
      "\t Training loss (single batch): 1.124984622001648\n",
      "\t Training loss (single batch): 0.8812459707260132\n",
      "\t Training loss (single batch): 0.9092549085617065\n",
      "\t Training loss (single batch): 0.7974923849105835\n",
      "\t Training loss (single batch): 0.9838494062423706\n",
      "\t Training loss (single batch): 0.84956294298172\n",
      "\t Training loss (single batch): 0.786849319934845\n",
      "\t Training loss (single batch): 0.6401456594467163\n",
      "\t Training loss (single batch): 1.0730645656585693\n",
      "\t Training loss (single batch): 1.0964223146438599\n",
      "\t Training loss (single batch): 0.6949973106384277\n",
      "\t Training loss (single batch): 1.5877256393432617\n",
      "\t Training loss (single batch): 0.9740878939628601\n",
      "\t Training loss (single batch): 0.8762931227684021\n",
      "\t Training loss (single batch): 1.3536516427993774\n",
      "\t Training loss (single batch): 1.1277954578399658\n",
      "\t Training loss (single batch): 0.9912365674972534\n",
      "\t Training loss (single batch): 1.9870185852050781\n",
      "\t Training loss (single batch): 1.2005622386932373\n",
      "\t Training loss (single batch): 1.0527758598327637\n",
      "\t Training loss (single batch): 0.8987348079681396\n",
      "\t Training loss (single batch): 1.5207709074020386\n",
      "\t Training loss (single batch): 1.0253318548202515\n",
      "\t Training loss (single batch): 1.695723295211792\n",
      "\t Training loss (single batch): 1.2511341571807861\n",
      "\t Training loss (single batch): 1.1187026500701904\n",
      "\t Training loss (single batch): 1.2746920585632324\n",
      "\t Training loss (single batch): 1.5902689695358276\n",
      "\t Training loss (single batch): 1.0215370655059814\n",
      "\t Training loss (single batch): 1.552734375\n",
      "\t Training loss (single batch): 1.1520732641220093\n",
      "\t Training loss (single batch): 0.7591883540153503\n",
      "\t Training loss (single batch): 1.2588366270065308\n",
      "\t Training loss (single batch): 1.2583394050598145\n",
      "\t Training loss (single batch): 1.1747716665267944\n",
      "\t Training loss (single batch): 1.1285216808319092\n",
      "\t Training loss (single batch): 1.432084321975708\n",
      "\t Training loss (single batch): 1.2074044942855835\n",
      "\t Training loss (single batch): 0.9477577209472656\n",
      "\t Training loss (single batch): 0.8475038409233093\n",
      "\t Training loss (single batch): 1.1556529998779297\n",
      "\t Training loss (single batch): 1.0626461505889893\n",
      "\t Training loss (single batch): 1.1207300424575806\n",
      "\t Training loss (single batch): 1.0252268314361572\n",
      "\t Training loss (single batch): 1.4718104600906372\n",
      "\t Training loss (single batch): 0.8813825845718384\n",
      "\t Training loss (single batch): 0.988152027130127\n",
      "\t Training loss (single batch): 1.600015640258789\n",
      "\t Training loss (single batch): 1.2664316892623901\n",
      "\t Training loss (single batch): 1.1694077253341675\n",
      "\t Training loss (single batch): 1.1423004865646362\n",
      "\t Training loss (single batch): 1.2561076879501343\n",
      "\t Training loss (single batch): 1.311784029006958\n",
      "\t Training loss (single batch): 0.8883509635925293\n",
      "\t Training loss (single batch): 1.0972094535827637\n",
      "\t Training loss (single batch): 1.0392520427703857\n",
      "\t Training loss (single batch): 0.9560523629188538\n",
      "\t Training loss (single batch): 1.5398163795471191\n",
      "\t Training loss (single batch): 1.2770867347717285\n",
      "\t Training loss (single batch): 1.4947606325149536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.31293785572052\n",
      "\t Training loss (single batch): 1.0697543621063232\n",
      "\t Training loss (single batch): 1.5166914463043213\n",
      "\t Training loss (single batch): 1.3466657400131226\n",
      "\t Training loss (single batch): 1.2640838623046875\n",
      "\t Training loss (single batch): 1.4563106298446655\n",
      "\t Training loss (single batch): 1.1683785915374756\n",
      "\t Training loss (single batch): 0.933059573173523\n",
      "\t Training loss (single batch): 1.3532696962356567\n",
      "\t Training loss (single batch): 1.05372953414917\n",
      "\t Training loss (single batch): 1.1735705137252808\n",
      "\t Training loss (single batch): 1.8640005588531494\n",
      "\t Training loss (single batch): 1.1915130615234375\n",
      "\t Training loss (single batch): 0.8321548700332642\n",
      "\t Training loss (single batch): 1.2365168333053589\n",
      "\t Training loss (single batch): 1.2064933776855469\n",
      "\t Training loss (single batch): 0.7919397950172424\n",
      "\t Training loss (single batch): 0.9533681273460388\n",
      "\t Training loss (single batch): 1.5886681079864502\n",
      "\t Training loss (single batch): 1.1618938446044922\n",
      "\t Training loss (single batch): 1.5079339742660522\n",
      "\t Training loss (single batch): 0.9428141713142395\n",
      "\t Training loss (single batch): 0.9336739182472229\n",
      "\t Training loss (single batch): 1.3685786724090576\n",
      "\t Training loss (single batch): 1.3418868780136108\n",
      "\t Training loss (single batch): 1.2972768545150757\n",
      "\t Training loss (single batch): 1.279856562614441\n",
      "\t Training loss (single batch): 0.8322752118110657\n",
      "\t Training loss (single batch): 1.327348232269287\n",
      "\t Training loss (single batch): 1.467187762260437\n",
      "\t Training loss (single batch): 1.158402919769287\n",
      "\t Training loss (single batch): 1.2523365020751953\n",
      "\t Training loss (single batch): 0.9893193244934082\n",
      "\t Training loss (single batch): 0.878491997718811\n",
      "\t Training loss (single batch): 1.2489246129989624\n",
      "\t Training loss (single batch): 1.4711860418319702\n",
      "\t Training loss (single batch): 0.946158230304718\n",
      "\t Training loss (single batch): 0.9088957905769348\n",
      "\t Training loss (single batch): 1.2917537689208984\n",
      "\t Training loss (single batch): 1.3433942794799805\n",
      "\t Training loss (single batch): 1.0419718027114868\n",
      "\t Training loss (single batch): 1.275659441947937\n",
      "\t Training loss (single batch): 0.9066917300224304\n",
      "\t Training loss (single batch): 1.3166539669036865\n",
      "\t Training loss (single batch): 1.2529172897338867\n",
      "\t Training loss (single batch): 1.1517835855484009\n",
      "\t Training loss (single batch): 0.9809092283248901\n",
      "\t Training loss (single batch): 1.3892419338226318\n",
      "\t Training loss (single batch): 1.5101675987243652\n",
      "\t Training loss (single batch): 0.6465132832527161\n",
      "\t Training loss (single batch): 1.029259204864502\n",
      "\t Training loss (single batch): 0.7137743234634399\n",
      "\t Training loss (single batch): 1.6061264276504517\n",
      "\t Training loss (single batch): 1.2625747919082642\n",
      "\t Training loss (single batch): 1.4824212789535522\n",
      "\t Training loss (single batch): 1.0744192600250244\n",
      "\t Training loss (single batch): 0.6563248038291931\n",
      "\t Training loss (single batch): 1.0273137092590332\n",
      "\t Training loss (single batch): 1.1513327360153198\n",
      "\t Training loss (single batch): 0.7007353901863098\n",
      "\t Training loss (single batch): 0.8663799166679382\n",
      "\t Training loss (single batch): 1.7716294527053833\n",
      "\t Training loss (single batch): 2.0808961391448975\n",
      "\t Training loss (single batch): 2.218151807785034\n",
      "##################################\n",
      "## EPOCH 52\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1054519414901733\n",
      "\t Training loss (single batch): 1.4661297798156738\n",
      "\t Training loss (single batch): 1.339658498764038\n",
      "\t Training loss (single batch): 1.1769918203353882\n",
      "\t Training loss (single batch): 1.8012720346450806\n",
      "\t Training loss (single batch): 0.8809744715690613\n",
      "\t Training loss (single batch): 0.8411343097686768\n",
      "\t Training loss (single batch): 1.154908299446106\n",
      "\t Training loss (single batch): 1.3057059049606323\n",
      "\t Training loss (single batch): 0.9244614243507385\n",
      "\t Training loss (single batch): 1.481142282485962\n",
      "\t Training loss (single batch): 0.8280460834503174\n",
      "\t Training loss (single batch): 0.9403847455978394\n",
      "\t Training loss (single batch): 1.1305040121078491\n",
      "\t Training loss (single batch): 1.2510758638381958\n",
      "\t Training loss (single batch): 1.856813907623291\n",
      "\t Training loss (single batch): 1.4354420900344849\n",
      "\t Training loss (single batch): 0.8800894021987915\n",
      "\t Training loss (single batch): 0.9844527244567871\n",
      "\t Training loss (single batch): 1.48335599899292\n",
      "\t Training loss (single batch): 1.0564043521881104\n",
      "\t Training loss (single batch): 1.177078366279602\n",
      "\t Training loss (single batch): 1.0913689136505127\n",
      "\t Training loss (single batch): 1.3190722465515137\n",
      "\t Training loss (single batch): 1.7100260257720947\n",
      "\t Training loss (single batch): 1.0968117713928223\n",
      "\t Training loss (single batch): 1.093558430671692\n",
      "\t Training loss (single batch): 0.8182309865951538\n",
      "\t Training loss (single batch): 1.1666021347045898\n",
      "\t Training loss (single batch): 0.9481279253959656\n",
      "\t Training loss (single batch): 1.3198670148849487\n",
      "\t Training loss (single batch): 1.063713550567627\n",
      "\t Training loss (single batch): 0.7750614881515503\n",
      "\t Training loss (single batch): 1.0678242444992065\n",
      "\t Training loss (single batch): 1.5867961645126343\n",
      "\t Training loss (single batch): 1.2352802753448486\n",
      "\t Training loss (single batch): 1.2553988695144653\n",
      "\t Training loss (single batch): 1.058384656906128\n",
      "\t Training loss (single batch): 1.4509971141815186\n",
      "\t Training loss (single batch): 1.1464731693267822\n",
      "\t Training loss (single batch): 1.4222649335861206\n",
      "\t Training loss (single batch): 1.10065495967865\n",
      "\t Training loss (single batch): 1.2646689414978027\n",
      "\t Training loss (single batch): 1.3801472187042236\n",
      "\t Training loss (single batch): 1.4695920944213867\n",
      "\t Training loss (single batch): 1.295914888381958\n",
      "\t Training loss (single batch): 1.4439547061920166\n",
      "\t Training loss (single batch): 1.274746060371399\n",
      "\t Training loss (single batch): 1.3650107383728027\n",
      "\t Training loss (single batch): 1.0045363903045654\n",
      "\t Training loss (single batch): 1.2987394332885742\n",
      "\t Training loss (single batch): 1.2417691946029663\n",
      "\t Training loss (single batch): 0.9310782551765442\n",
      "\t Training loss (single batch): 1.1947003602981567\n",
      "\t Training loss (single batch): 1.5172935724258423\n",
      "\t Training loss (single batch): 0.8178893327713013\n",
      "\t Training loss (single batch): 1.4144644737243652\n",
      "\t Training loss (single batch): 1.5162651538848877\n",
      "\t Training loss (single batch): 1.1414638757705688\n",
      "\t Training loss (single batch): 1.1250978708267212\n",
      "\t Training loss (single batch): 1.0604327917099\n",
      "\t Training loss (single batch): 0.966763973236084\n",
      "\t Training loss (single batch): 0.861783504486084\n",
      "\t Training loss (single batch): 1.2649734020233154\n",
      "\t Training loss (single batch): 1.4788001775741577\n",
      "\t Training loss (single batch): 1.020095944404602\n",
      "\t Training loss (single batch): 1.188299536705017\n",
      "\t Training loss (single batch): 1.0765297412872314\n",
      "\t Training loss (single batch): 0.9890973567962646\n",
      "\t Training loss (single batch): 1.453438401222229\n",
      "\t Training loss (single batch): 1.3706272840499878\n",
      "\t Training loss (single batch): 1.0887805223464966\n",
      "\t Training loss (single batch): 1.0945392847061157\n",
      "\t Training loss (single batch): 0.9368941187858582\n",
      "\t Training loss (single batch): 1.1441131830215454\n",
      "\t Training loss (single batch): 1.3837716579437256\n",
      "\t Training loss (single batch): 1.2700953483581543\n",
      "\t Training loss (single batch): 0.9428982138633728\n",
      "\t Training loss (single batch): 1.086297869682312\n",
      "\t Training loss (single batch): 0.8170484900474548\n",
      "\t Training loss (single batch): 1.1933929920196533\n",
      "\t Training loss (single batch): 0.8811302185058594\n",
      "\t Training loss (single batch): 1.2880033254623413\n",
      "\t Training loss (single batch): 1.4770686626434326\n",
      "\t Training loss (single batch): 1.8582707643508911\n",
      "\t Training loss (single batch): 1.0482484102249146\n",
      "\t Training loss (single batch): 1.022769808769226\n",
      "\t Training loss (single batch): 1.0915699005126953\n",
      "\t Training loss (single batch): 1.247856616973877\n",
      "\t Training loss (single batch): 1.2733488082885742\n",
      "\t Training loss (single batch): 1.1037309169769287\n",
      "\t Training loss (single batch): 1.1835976839065552\n",
      "\t Training loss (single batch): 1.1613030433654785\n",
      "\t Training loss (single batch): 1.0372110605239868\n",
      "\t Training loss (single batch): 1.0403378009796143\n",
      "\t Training loss (single batch): 1.249094009399414\n",
      "\t Training loss (single batch): 0.9299370646476746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0216048955917358\n",
      "\t Training loss (single batch): 0.9862779974937439\n",
      "\t Training loss (single batch): 1.7665917873382568\n",
      "\t Training loss (single batch): 1.1383013725280762\n",
      "\t Training loss (single batch): 1.3437397480010986\n",
      "\t Training loss (single batch): 0.879727840423584\n",
      "\t Training loss (single batch): 1.1040962934494019\n",
      "\t Training loss (single batch): 1.3015599250793457\n",
      "\t Training loss (single batch): 1.2561674118041992\n",
      "\t Training loss (single batch): 0.9331316351890564\n",
      "\t Training loss (single batch): 1.7104682922363281\n",
      "\t Training loss (single batch): 0.9504881501197815\n",
      "\t Training loss (single batch): 1.072432518005371\n",
      "\t Training loss (single batch): 1.6717149019241333\n",
      "\t Training loss (single batch): 1.5410770177841187\n",
      "\t Training loss (single batch): 0.8082603812217712\n",
      "\t Training loss (single batch): 1.617133617401123\n",
      "\t Training loss (single batch): 1.4312800168991089\n",
      "\t Training loss (single batch): 0.9629606604576111\n",
      "\t Training loss (single batch): 0.9635803699493408\n",
      "\t Training loss (single batch): 1.1830836534500122\n",
      "\t Training loss (single batch): 0.8978246450424194\n",
      "\t Training loss (single batch): 1.5425068140029907\n",
      "\t Training loss (single batch): 1.3714510202407837\n",
      "\t Training loss (single batch): 1.4556519985198975\n",
      "\t Training loss (single batch): 1.4744848012924194\n",
      "\t Training loss (single batch): 1.210752010345459\n",
      "\t Training loss (single batch): 0.9700920581817627\n",
      "\t Training loss (single batch): 1.0548092126846313\n",
      "\t Training loss (single batch): 1.301397442817688\n",
      "\t Training loss (single batch): 1.181555151939392\n",
      "\t Training loss (single batch): 1.4113895893096924\n",
      "\t Training loss (single batch): 1.167382001876831\n",
      "\t Training loss (single batch): 1.0790650844573975\n",
      "\t Training loss (single batch): 1.1598236560821533\n",
      "\t Training loss (single batch): 1.2623542547225952\n",
      "\t Training loss (single batch): 0.8338663578033447\n",
      "\t Training loss (single batch): 0.7805618643760681\n",
      "\t Training loss (single batch): 1.2056342363357544\n",
      "\t Training loss (single batch): 1.1152729988098145\n",
      "\t Training loss (single batch): 0.9329243898391724\n",
      "\t Training loss (single batch): 1.478117823600769\n",
      "\t Training loss (single batch): 1.1828584671020508\n",
      "\t Training loss (single batch): 1.3178825378417969\n",
      "\t Training loss (single batch): 1.1959927082061768\n",
      "\t Training loss (single batch): 0.8721425533294678\n",
      "\t Training loss (single batch): 1.1993117332458496\n",
      "\t Training loss (single batch): 0.8559637069702148\n",
      "\t Training loss (single batch): 0.8906416296958923\n",
      "\t Training loss (single batch): 0.934397280216217\n",
      "\t Training loss (single batch): 1.76220703125\n",
      "\t Training loss (single batch): 1.219344973564148\n",
      "\t Training loss (single batch): 1.6984363794326782\n",
      "\t Training loss (single batch): 1.5326526165008545\n",
      "\t Training loss (single batch): 1.1222232580184937\n",
      "\t Training loss (single batch): 1.5316214561462402\n",
      "\t Training loss (single batch): 1.1778465509414673\n",
      "\t Training loss (single batch): 1.0572311878204346\n",
      "\t Training loss (single batch): 1.0329538583755493\n",
      "\t Training loss (single batch): 1.3254855871200562\n",
      "\t Training loss (single batch): 0.9861891269683838\n",
      "\t Training loss (single batch): 1.460888147354126\n",
      "\t Training loss (single batch): 1.3710273504257202\n",
      "\t Training loss (single batch): 1.3613108396530151\n",
      "\t Training loss (single batch): 1.468376874923706\n",
      "\t Training loss (single batch): 1.465005874633789\n",
      "\t Training loss (single batch): 1.156327724456787\n",
      "\t Training loss (single batch): 1.5871269702911377\n",
      "\t Training loss (single batch): 0.9652381539344788\n",
      "\t Training loss (single batch): 0.914322018623352\n",
      "\t Training loss (single batch): 1.2088713645935059\n",
      "\t Training loss (single batch): 1.3814759254455566\n",
      "\t Training loss (single batch): 1.245809555053711\n",
      "\t Training loss (single batch): 1.650283932685852\n",
      "\t Training loss (single batch): 1.128684639930725\n",
      "\t Training loss (single batch): 0.7548718452453613\n",
      "\t Training loss (single batch): 0.8918450474739075\n",
      "\t Training loss (single batch): 0.7153879404067993\n",
      "\t Training loss (single batch): 1.1118600368499756\n",
      "\t Training loss (single batch): 0.9246174693107605\n",
      "\t Training loss (single batch): 1.114052414894104\n",
      "\t Training loss (single batch): 1.6151961088180542\n",
      "\t Training loss (single batch): 1.1712208986282349\n",
      "\t Training loss (single batch): 0.7689124941825867\n",
      "\t Training loss (single batch): 0.8414846658706665\n",
      "\t Training loss (single batch): 1.2320069074630737\n",
      "\t Training loss (single batch): 1.705173134803772\n",
      "\t Training loss (single batch): 1.0647778511047363\n",
      "\t Training loss (single batch): 1.0555660724639893\n",
      "\t Training loss (single batch): 0.9872570037841797\n",
      "\t Training loss (single batch): 1.6666117906570435\n",
      "\t Training loss (single batch): 1.2343817949295044\n",
      "\t Training loss (single batch): 1.504506230354309\n",
      "\t Training loss (single batch): 1.5009808540344238\n",
      "\t Training loss (single batch): 1.3462576866149902\n",
      "\t Training loss (single batch): 1.2588822841644287\n",
      "\t Training loss (single batch): 1.140113115310669\n",
      "\t Training loss (single batch): 1.1963552236557007\n",
      "\t Training loss (single batch): 1.767795443534851\n",
      "\t Training loss (single batch): 1.0624350309371948\n",
      "\t Training loss (single batch): 1.3844919204711914\n",
      "\t Training loss (single batch): 1.2845721244812012\n",
      "\t Training loss (single batch): 1.4161934852600098\n",
      "\t Training loss (single batch): 0.8014744520187378\n",
      "\t Training loss (single batch): 0.8883581757545471\n",
      "\t Training loss (single batch): 0.8276422619819641\n",
      "\t Training loss (single batch): 0.947827935218811\n",
      "\t Training loss (single batch): 1.189828634262085\n",
      "\t Training loss (single batch): 1.5060192346572876\n",
      "\t Training loss (single batch): 1.3001751899719238\n",
      "\t Training loss (single batch): 1.4222661256790161\n",
      "\t Training loss (single batch): 1.410419225692749\n",
      "\t Training loss (single batch): 1.0030999183654785\n",
      "\t Training loss (single batch): 1.1335222721099854\n",
      "\t Training loss (single batch): 1.6474474668502808\n",
      "\t Training loss (single batch): 1.4392603635787964\n",
      "\t Training loss (single batch): 1.0519914627075195\n",
      "\t Training loss (single batch): 1.5359365940093994\n",
      "\t Training loss (single batch): 1.1078028678894043\n",
      "\t Training loss (single batch): 1.2189512252807617\n",
      "\t Training loss (single batch): 1.2368860244750977\n",
      "\t Training loss (single batch): 1.3452361822128296\n",
      "\t Training loss (single batch): 1.3772203922271729\n",
      "\t Training loss (single batch): 1.2252339124679565\n",
      "\t Training loss (single batch): 1.4222131967544556\n",
      "\t Training loss (single batch): 1.272749423980713\n",
      "\t Training loss (single batch): 1.0497320890426636\n",
      "\t Training loss (single batch): 1.0989707708358765\n",
      "\t Training loss (single batch): 1.1541318893432617\n",
      "\t Training loss (single batch): 0.96820068359375\n",
      "\t Training loss (single batch): 1.4942035675048828\n",
      "\t Training loss (single batch): 1.0193157196044922\n",
      "\t Training loss (single batch): 1.21857750415802\n",
      "\t Training loss (single batch): 1.1460380554199219\n",
      "\t Training loss (single batch): 1.0650757551193237\n",
      "\t Training loss (single batch): 1.5894758701324463\n",
      "\t Training loss (single batch): 1.9156094789505005\n",
      "\t Training loss (single batch): 1.508893609046936\n",
      "\t Training loss (single batch): 1.4235247373580933\n",
      "\t Training loss (single batch): 1.0621737241744995\n",
      "\t Training loss (single batch): 0.7269548773765564\n",
      "\t Training loss (single batch): 1.5662989616394043\n",
      "\t Training loss (single batch): 1.3681285381317139\n",
      "\t Training loss (single batch): 1.3009577989578247\n",
      "\t Training loss (single batch): 1.2243369817733765\n",
      "\t Training loss (single batch): 1.2319406270980835\n",
      "\t Training loss (single batch): 1.2390620708465576\n",
      "\t Training loss (single batch): 0.9120791554450989\n",
      "\t Training loss (single batch): 1.1745349168777466\n",
      "\t Training loss (single batch): 1.2476270198822021\n",
      "\t Training loss (single batch): 1.1444835662841797\n",
      "\t Training loss (single batch): 1.1298856735229492\n",
      "\t Training loss (single batch): 1.1505701541900635\n",
      "\t Training loss (single batch): 1.7861438989639282\n",
      "\t Training loss (single batch): 1.5145021677017212\n",
      "\t Training loss (single batch): 1.151686429977417\n",
      "\t Training loss (single batch): 0.9812513589859009\n",
      "\t Training loss (single batch): 1.4778493642807007\n",
      "\t Training loss (single batch): 1.2048362493515015\n",
      "\t Training loss (single batch): 1.1081303358078003\n",
      "\t Training loss (single batch): 0.8836707472801208\n",
      "\t Training loss (single batch): 1.7475377321243286\n",
      "\t Training loss (single batch): 1.2193493843078613\n",
      "\t Training loss (single batch): 1.3771401643753052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6826379299163818\n",
      "\t Training loss (single batch): 1.2761309146881104\n",
      "\t Training loss (single batch): 0.9914769530296326\n",
      "\t Training loss (single batch): 1.021991491317749\n",
      "\t Training loss (single batch): 1.0208625793457031\n",
      "\t Training loss (single batch): 1.0098824501037598\n",
      "\t Training loss (single batch): 1.4248539209365845\n",
      "\t Training loss (single batch): 0.9958016276359558\n",
      "\t Training loss (single batch): 1.0284446477890015\n",
      "\t Training loss (single batch): 0.9381939768791199\n",
      "\t Training loss (single batch): 1.013866662979126\n",
      "\t Training loss (single batch): 1.1916142702102661\n",
      "\t Training loss (single batch): 1.1155668497085571\n",
      "\t Training loss (single batch): 1.7677000761032104\n",
      "\t Training loss (single batch): 1.1998226642608643\n",
      "\t Training loss (single batch): 1.2164517641067505\n",
      "\t Training loss (single batch): 1.6828502416610718\n",
      "\t Training loss (single batch): 1.3056172132492065\n",
      "\t Training loss (single batch): 0.9660003185272217\n",
      "\t Training loss (single batch): 1.1156455278396606\n",
      "\t Training loss (single batch): 1.1822580099105835\n",
      "\t Training loss (single batch): 0.967967689037323\n",
      "\t Training loss (single batch): 1.4571733474731445\n",
      "\t Training loss (single batch): 1.2149676084518433\n",
      "\t Training loss (single batch): 0.9058704376220703\n",
      "\t Training loss (single batch): 0.898819088935852\n",
      "\t Training loss (single batch): 1.2487379312515259\n",
      "\t Training loss (single batch): 1.1669425964355469\n",
      "\t Training loss (single batch): 1.222618818283081\n",
      "\t Training loss (single batch): 1.0742757320404053\n",
      "\t Training loss (single batch): 1.3613077402114868\n",
      "\t Training loss (single batch): 1.342917799949646\n",
      "\t Training loss (single batch): 1.025562047958374\n",
      "\t Training loss (single batch): 1.1180574893951416\n",
      "\t Training loss (single batch): 0.787675678730011\n",
      "\t Training loss (single batch): 1.060428500175476\n",
      "\t Training loss (single batch): 0.8807190656661987\n",
      "\t Training loss (single batch): 1.3288524150848389\n",
      "\t Training loss (single batch): 1.604606032371521\n",
      "\t Training loss (single batch): 1.3031030893325806\n",
      "\t Training loss (single batch): 0.9890221357345581\n",
      "\t Training loss (single batch): 1.2057288885116577\n",
      "\t Training loss (single batch): 1.3404128551483154\n",
      "\t Training loss (single batch): 1.3305224180221558\n",
      "\t Training loss (single batch): 1.5861924886703491\n",
      "\t Training loss (single batch): 1.0953471660614014\n",
      "\t Training loss (single batch): 1.4125772714614868\n",
      "\t Training loss (single batch): 1.3609391450881958\n",
      "\t Training loss (single batch): 1.2749203443527222\n",
      "\t Training loss (single batch): 1.2238534688949585\n",
      "\t Training loss (single batch): 1.2970300912857056\n",
      "\t Training loss (single batch): 1.7013216018676758\n",
      "\t Training loss (single batch): 1.2515032291412354\n",
      "\t Training loss (single batch): 1.4804418087005615\n",
      "\t Training loss (single batch): 1.103913426399231\n",
      "\t Training loss (single batch): 0.7425903677940369\n",
      "##################################\n",
      "## EPOCH 53\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4824782609939575\n",
      "\t Training loss (single batch): 1.2306922674179077\n",
      "\t Training loss (single batch): 1.3288469314575195\n",
      "\t Training loss (single batch): 0.9659375548362732\n",
      "\t Training loss (single batch): 1.26212739944458\n",
      "\t Training loss (single batch): 0.9015275239944458\n",
      "\t Training loss (single batch): 0.8822513818740845\n",
      "\t Training loss (single batch): 0.9676250219345093\n",
      "\t Training loss (single batch): 0.9844076633453369\n",
      "\t Training loss (single batch): 2.009646415710449\n",
      "\t Training loss (single batch): 1.0249228477478027\n",
      "\t Training loss (single batch): 1.3073546886444092\n",
      "\t Training loss (single batch): 1.2128751277923584\n",
      "\t Training loss (single batch): 1.440414547920227\n",
      "\t Training loss (single batch): 1.1093322038650513\n",
      "\t Training loss (single batch): 1.3874131441116333\n",
      "\t Training loss (single batch): 1.1655516624450684\n",
      "\t Training loss (single batch): 1.2641834020614624\n",
      "\t Training loss (single batch): 1.444149374961853\n",
      "\t Training loss (single batch): 1.09243905544281\n",
      "\t Training loss (single batch): 1.4209651947021484\n",
      "\t Training loss (single batch): 1.400774359703064\n",
      "\t Training loss (single batch): 0.8240948915481567\n",
      "\t Training loss (single batch): 0.9316365122795105\n",
      "\t Training loss (single batch): 1.130936622619629\n",
      "\t Training loss (single batch): 1.2113317251205444\n",
      "\t Training loss (single batch): 1.1706424951553345\n",
      "\t Training loss (single batch): 1.3247202634811401\n",
      "\t Training loss (single batch): 1.3229469060897827\n",
      "\t Training loss (single batch): 1.6847856044769287\n",
      "\t Training loss (single batch): 1.559430480003357\n",
      "\t Training loss (single batch): 1.1969877481460571\n",
      "\t Training loss (single batch): 1.01633620262146\n",
      "\t Training loss (single batch): 1.3242413997650146\n",
      "\t Training loss (single batch): 1.2088137865066528\n",
      "\t Training loss (single batch): 1.170652985572815\n",
      "\t Training loss (single batch): 1.6116362810134888\n",
      "\t Training loss (single batch): 1.286016583442688\n",
      "\t Training loss (single batch): 1.2534931898117065\n",
      "\t Training loss (single batch): 1.1635414361953735\n",
      "\t Training loss (single batch): 1.3175355195999146\n",
      "\t Training loss (single batch): 1.3133854866027832\n",
      "\t Training loss (single batch): 1.375381588935852\n",
      "\t Training loss (single batch): 1.3798683881759644\n",
      "\t Training loss (single batch): 1.472875952720642\n",
      "\t Training loss (single batch): 1.237435221672058\n",
      "\t Training loss (single batch): 1.16049063205719\n",
      "\t Training loss (single batch): 1.400186538696289\n",
      "\t Training loss (single batch): 1.4953936338424683\n",
      "\t Training loss (single batch): 1.1433746814727783\n",
      "\t Training loss (single batch): 1.2548447847366333\n",
      "\t Training loss (single batch): 1.1470574140548706\n",
      "\t Training loss (single batch): 1.328974962234497\n",
      "\t Training loss (single batch): 1.1277729272842407\n",
      "\t Training loss (single batch): 1.2077285051345825\n",
      "\t Training loss (single batch): 1.09352445602417\n",
      "\t Training loss (single batch): 1.384617805480957\n",
      "\t Training loss (single batch): 1.1819895505905151\n",
      "\t Training loss (single batch): 1.5649489164352417\n",
      "\t Training loss (single batch): 0.9879739880561829\n",
      "\t Training loss (single batch): 1.0822230577468872\n",
      "\t Training loss (single batch): 0.8650614619255066\n",
      "\t Training loss (single batch): 1.369767665863037\n",
      "\t Training loss (single batch): 1.5234214067459106\n",
      "\t Training loss (single batch): 1.1620701551437378\n",
      "\t Training loss (single batch): 1.0705039501190186\n",
      "\t Training loss (single batch): 1.1269311904907227\n",
      "\t Training loss (single batch): 1.0305057764053345\n",
      "\t Training loss (single batch): 1.0954171419143677\n",
      "\t Training loss (single batch): 1.0118012428283691\n",
      "\t Training loss (single batch): 0.6881012320518494\n",
      "\t Training loss (single batch): 0.7581804990768433\n",
      "\t Training loss (single batch): 1.1465529203414917\n",
      "\t Training loss (single batch): 1.243950605392456\n",
      "\t Training loss (single batch): 1.147156834602356\n",
      "\t Training loss (single batch): 0.8068042993545532\n",
      "\t Training loss (single batch): 0.837415337562561\n",
      "\t Training loss (single batch): 0.8733786344528198\n",
      "\t Training loss (single batch): 1.456953525543213\n",
      "\t Training loss (single batch): 1.0831913948059082\n",
      "\t Training loss (single batch): 1.2302180528640747\n",
      "\t Training loss (single batch): 1.2910276651382446\n",
      "\t Training loss (single batch): 1.256130576133728\n",
      "\t Training loss (single batch): 1.2010647058486938\n",
      "\t Training loss (single batch): 1.1413096189498901\n",
      "\t Training loss (single batch): 1.3868862390518188\n",
      "\t Training loss (single batch): 0.9846488237380981\n",
      "\t Training loss (single batch): 1.067453384399414\n",
      "\t Training loss (single batch): 1.253922939300537\n",
      "\t Training loss (single batch): 1.0812071561813354\n",
      "\t Training loss (single batch): 0.8797804117202759\n",
      "\t Training loss (single batch): 1.278218150138855\n",
      "\t Training loss (single batch): 1.314705491065979\n",
      "\t Training loss (single batch): 1.4402344226837158\n",
      "\t Training loss (single batch): 1.1729074716567993\n",
      "\t Training loss (single batch): 1.135480284690857\n",
      "\t Training loss (single batch): 0.7449860572814941\n",
      "\t Training loss (single batch): 0.9288407564163208\n",
      "\t Training loss (single batch): 1.5507241487503052\n",
      "\t Training loss (single batch): 0.9189507365226746\n",
      "\t Training loss (single batch): 1.0856109857559204\n",
      "\t Training loss (single batch): 0.8301388621330261\n",
      "\t Training loss (single batch): 0.989715039730072\n",
      "\t Training loss (single batch): 0.9942833781242371\n",
      "\t Training loss (single batch): 0.9422156810760498\n",
      "\t Training loss (single batch): 1.0061334371566772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.994745135307312\n",
      "\t Training loss (single batch): 1.5188088417053223\n",
      "\t Training loss (single batch): 1.1069023609161377\n",
      "\t Training loss (single batch): 0.634125292301178\n",
      "\t Training loss (single batch): 1.4461379051208496\n",
      "\t Training loss (single batch): 1.0128544569015503\n",
      "\t Training loss (single batch): 1.0641366243362427\n",
      "\t Training loss (single batch): 1.3398901224136353\n",
      "\t Training loss (single batch): 1.2754106521606445\n",
      "\t Training loss (single batch): 1.021782636642456\n",
      "\t Training loss (single batch): 1.24140202999115\n",
      "\t Training loss (single batch): 1.479653000831604\n",
      "\t Training loss (single batch): 1.0213606357574463\n",
      "\t Training loss (single batch): 1.1975058317184448\n",
      "\t Training loss (single batch): 0.882076621055603\n",
      "\t Training loss (single batch): 1.2722984552383423\n",
      "\t Training loss (single batch): 1.0458719730377197\n",
      "\t Training loss (single batch): 1.0472376346588135\n",
      "\t Training loss (single batch): 0.9053106307983398\n",
      "\t Training loss (single batch): 1.6509746313095093\n",
      "\t Training loss (single batch): 1.3703256845474243\n",
      "\t Training loss (single batch): 1.6230547428131104\n",
      "\t Training loss (single batch): 1.309396505355835\n",
      "\t Training loss (single batch): 1.4728199243545532\n",
      "\t Training loss (single batch): 1.1966270208358765\n",
      "\t Training loss (single batch): 1.2095338106155396\n",
      "\t Training loss (single batch): 0.8640933632850647\n",
      "\t Training loss (single batch): 1.2956374883651733\n",
      "\t Training loss (single batch): 1.2530841827392578\n",
      "\t Training loss (single batch): 1.0098248720169067\n",
      "\t Training loss (single batch): 1.3041445016860962\n",
      "\t Training loss (single batch): 1.3561145067214966\n",
      "\t Training loss (single batch): 1.7311139106750488\n",
      "\t Training loss (single batch): 1.5405101776123047\n",
      "\t Training loss (single batch): 1.1696020364761353\n",
      "\t Training loss (single batch): 1.304634690284729\n",
      "\t Training loss (single batch): 1.0554282665252686\n",
      "\t Training loss (single batch): 1.6852879524230957\n",
      "\t Training loss (single batch): 1.4438185691833496\n",
      "\t Training loss (single batch): 0.9247862100601196\n",
      "\t Training loss (single batch): 1.2632538080215454\n",
      "\t Training loss (single batch): 1.6703131198883057\n",
      "\t Training loss (single batch): 1.422128438949585\n",
      "\t Training loss (single batch): 1.0748306512832642\n",
      "\t Training loss (single batch): 0.9752634167671204\n",
      "\t Training loss (single batch): 1.3121752738952637\n",
      "\t Training loss (single batch): 0.8453377485275269\n",
      "\t Training loss (single batch): 1.2217837572097778\n",
      "\t Training loss (single batch): 1.1366959810256958\n",
      "\t Training loss (single batch): 1.1389751434326172\n",
      "\t Training loss (single batch): 1.401085615158081\n",
      "\t Training loss (single batch): 0.9803129434585571\n",
      "\t Training loss (single batch): 1.2183268070220947\n",
      "\t Training loss (single batch): 1.1238347291946411\n",
      "\t Training loss (single batch): 1.463098168373108\n",
      "\t Training loss (single batch): 1.0848537683486938\n",
      "\t Training loss (single batch): 1.1208858489990234\n",
      "\t Training loss (single batch): 1.4694550037384033\n",
      "\t Training loss (single batch): 0.9848989844322205\n",
      "\t Training loss (single batch): 0.9403966069221497\n",
      "\t Training loss (single batch): 0.8973615765571594\n",
      "\t Training loss (single batch): 1.314550518989563\n",
      "\t Training loss (single batch): 0.909301221370697\n",
      "\t Training loss (single batch): 1.105584740638733\n",
      "\t Training loss (single batch): 1.2142817974090576\n",
      "\t Training loss (single batch): 1.2690540552139282\n",
      "\t Training loss (single batch): 1.390514850616455\n",
      "\t Training loss (single batch): 1.5088129043579102\n",
      "\t Training loss (single batch): 1.5742136240005493\n",
      "\t Training loss (single batch): 0.9174999594688416\n",
      "\t Training loss (single batch): 1.3341783285140991\n",
      "\t Training loss (single batch): 1.0046955347061157\n",
      "\t Training loss (single batch): 1.4007662534713745\n",
      "\t Training loss (single batch): 1.2827073335647583\n",
      "\t Training loss (single batch): 1.316622018814087\n",
      "\t Training loss (single batch): 1.3215789794921875\n",
      "\t Training loss (single batch): 1.4332481622695923\n",
      "\t Training loss (single batch): 1.0146839618682861\n",
      "\t Training loss (single batch): 1.3030112981796265\n",
      "\t Training loss (single batch): 0.9912346601486206\n",
      "\t Training loss (single batch): 1.4706400632858276\n",
      "\t Training loss (single batch): 1.3155982494354248\n",
      "\t Training loss (single batch): 1.2910064458847046\n",
      "\t Training loss (single batch): 0.9543343782424927\n",
      "\t Training loss (single batch): 0.9523317217826843\n",
      "\t Training loss (single batch): 1.121933102607727\n",
      "\t Training loss (single batch): 1.2103420495986938\n",
      "\t Training loss (single batch): 1.2790532112121582\n",
      "\t Training loss (single batch): 1.0482512712478638\n",
      "\t Training loss (single batch): 1.1493436098098755\n",
      "\t Training loss (single batch): 1.0552762746810913\n",
      "\t Training loss (single batch): 1.482397198677063\n",
      "\t Training loss (single batch): 1.7210544347763062\n",
      "\t Training loss (single batch): 1.0560996532440186\n",
      "\t Training loss (single batch): 0.8380476832389832\n",
      "\t Training loss (single batch): 0.9234254956245422\n",
      "\t Training loss (single batch): 1.738578200340271\n",
      "\t Training loss (single batch): 1.279774785041809\n",
      "\t Training loss (single batch): 1.0655561685562134\n",
      "\t Training loss (single batch): 1.013595461845398\n",
      "\t Training loss (single batch): 1.9974037408828735\n",
      "\t Training loss (single batch): 1.4635814428329468\n",
      "\t Training loss (single batch): 1.8925048112869263\n",
      "\t Training loss (single batch): 1.3931232690811157\n",
      "\t Training loss (single batch): 1.4218233823776245\n",
      "\t Training loss (single batch): 1.2590546607971191\n",
      "\t Training loss (single batch): 0.9604201912879944\n",
      "\t Training loss (single batch): 1.2244271039962769\n",
      "\t Training loss (single batch): 1.1675829887390137\n",
      "\t Training loss (single batch): 1.0117883682250977\n",
      "\t Training loss (single batch): 1.4885402917861938\n",
      "\t Training loss (single batch): 0.913741946220398\n",
      "\t Training loss (single batch): 1.4778803586959839\n",
      "\t Training loss (single batch): 0.9407185912132263\n",
      "\t Training loss (single batch): 0.8535475730895996\n",
      "\t Training loss (single batch): 1.0413870811462402\n",
      "\t Training loss (single batch): 1.4785826206207275\n",
      "\t Training loss (single batch): 2.2531421184539795\n",
      "\t Training loss (single batch): 1.182539939880371\n",
      "\t Training loss (single batch): 1.3714120388031006\n",
      "\t Training loss (single batch): 0.8693176507949829\n",
      "\t Training loss (single batch): 1.3704441785812378\n",
      "\t Training loss (single batch): 1.1754950284957886\n",
      "\t Training loss (single batch): 1.6702361106872559\n",
      "\t Training loss (single batch): 1.26887845993042\n",
      "\t Training loss (single batch): 1.1676158905029297\n",
      "\t Training loss (single batch): 1.2811411619186401\n",
      "\t Training loss (single batch): 1.2168464660644531\n",
      "\t Training loss (single batch): 0.9069628715515137\n",
      "\t Training loss (single batch): 1.0996310710906982\n",
      "\t Training loss (single batch): 1.2315495014190674\n",
      "\t Training loss (single batch): 1.1736042499542236\n",
      "\t Training loss (single batch): 1.3578542470932007\n",
      "\t Training loss (single batch): 0.6551195979118347\n",
      "\t Training loss (single batch): 1.15121328830719\n",
      "\t Training loss (single batch): 1.5342341661453247\n",
      "\t Training loss (single batch): 1.459683895111084\n",
      "\t Training loss (single batch): 1.265682578086853\n",
      "\t Training loss (single batch): 0.811277449131012\n",
      "\t Training loss (single batch): 0.8527741432189941\n",
      "\t Training loss (single batch): 1.0820976495742798\n",
      "\t Training loss (single batch): 1.2379484176635742\n",
      "\t Training loss (single batch): 1.4715958833694458\n",
      "\t Training loss (single batch): 1.1687588691711426\n",
      "\t Training loss (single batch): 1.5703685283660889\n",
      "\t Training loss (single batch): 1.2960644960403442\n",
      "\t Training loss (single batch): 1.16270112991333\n",
      "\t Training loss (single batch): 1.2032415866851807\n",
      "\t Training loss (single batch): 1.3493399620056152\n",
      "\t Training loss (single batch): 0.9051474928855896\n",
      "\t Training loss (single batch): 1.7158063650131226\n",
      "\t Training loss (single batch): 1.171266794204712\n",
      "\t Training loss (single batch): 1.248166561126709\n",
      "\t Training loss (single batch): 1.2083868980407715\n",
      "\t Training loss (single batch): 1.1065340042114258\n",
      "\t Training loss (single batch): 0.8562601804733276\n",
      "\t Training loss (single batch): 0.9703004360198975\n",
      "\t Training loss (single batch): 1.5831025838851929\n",
      "\t Training loss (single batch): 1.1547774076461792\n",
      "\t Training loss (single batch): 1.0383667945861816\n",
      "\t Training loss (single batch): 1.5103040933609009\n",
      "\t Training loss (single batch): 1.2171217203140259\n",
      "\t Training loss (single batch): 1.3230879306793213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.941783607006073\n",
      "\t Training loss (single batch): 1.13273286819458\n",
      "\t Training loss (single batch): 1.2319177389144897\n",
      "\t Training loss (single batch): 1.3636425733566284\n",
      "\t Training loss (single batch): 1.2463959455490112\n",
      "\t Training loss (single batch): 1.734241008758545\n",
      "\t Training loss (single batch): 1.0484275817871094\n",
      "\t Training loss (single batch): 0.8215256929397583\n",
      "\t Training loss (single batch): 1.1017217636108398\n",
      "\t Training loss (single batch): 1.512952446937561\n",
      "\t Training loss (single batch): 1.833971619606018\n",
      "\t Training loss (single batch): 0.820192277431488\n",
      "\t Training loss (single batch): 1.411540150642395\n",
      "\t Training loss (single batch): 1.8776686191558838\n",
      "\t Training loss (single batch): 1.3690295219421387\n",
      "\t Training loss (single batch): 1.3329195976257324\n",
      "\t Training loss (single batch): 1.0628148317337036\n",
      "\t Training loss (single batch): 0.8279539346694946\n",
      "\t Training loss (single batch): 1.4432272911071777\n",
      "\t Training loss (single batch): 1.0163074731826782\n",
      "\t Training loss (single batch): 1.170298457145691\n",
      "\t Training loss (single batch): 1.109586477279663\n",
      "\t Training loss (single batch): 0.9892606139183044\n",
      "\t Training loss (single batch): 1.525821566581726\n",
      "\t Training loss (single batch): 1.3192994594573975\n",
      "\t Training loss (single batch): 0.8943264484405518\n",
      "\t Training loss (single batch): 1.1335029602050781\n",
      "\t Training loss (single batch): 1.2306550741195679\n",
      "\t Training loss (single batch): 1.1675516366958618\n",
      "\t Training loss (single batch): 0.894801676273346\n",
      "\t Training loss (single batch): 1.3564833402633667\n",
      "\t Training loss (single batch): 0.9850569367408752\n",
      "\t Training loss (single batch): 0.9985384941101074\n",
      "\t Training loss (single batch): 1.0378836393356323\n",
      "\t Training loss (single batch): 1.0597007274627686\n",
      "\t Training loss (single batch): 0.7304423451423645\n",
      "\t Training loss (single batch): 1.6443456411361694\n",
      "\t Training loss (single batch): 1.419183373451233\n",
      "\t Training loss (single batch): 1.0078967809677124\n",
      "\t Training loss (single batch): 1.2828887701034546\n",
      "\t Training loss (single batch): 1.6862740516662598\n",
      "\t Training loss (single batch): 1.4531887769699097\n",
      "\t Training loss (single batch): 0.8954786062240601\n",
      "\t Training loss (single batch): 1.1729776859283447\n",
      "\t Training loss (single batch): 0.9537645578384399\n",
      "\t Training loss (single batch): 0.7906021475791931\n",
      "\t Training loss (single batch): 1.2507327795028687\n",
      "\t Training loss (single batch): 0.7113037705421448\n",
      "##################################\n",
      "## EPOCH 54\n",
      "##################################\n",
      "\t Training loss (single batch): 1.013787031173706\n",
      "\t Training loss (single batch): 1.1985774040222168\n",
      "\t Training loss (single batch): 0.9395331144332886\n",
      "\t Training loss (single batch): 1.641248106956482\n",
      "\t Training loss (single batch): 0.8142397999763489\n",
      "\t Training loss (single batch): 0.94076007604599\n",
      "\t Training loss (single batch): 0.8066734075546265\n",
      "\t Training loss (single batch): 1.2194907665252686\n",
      "\t Training loss (single batch): 1.0508711338043213\n",
      "\t Training loss (single batch): 1.2143810987472534\n",
      "\t Training loss (single batch): 0.9383553862571716\n",
      "\t Training loss (single batch): 1.4361547231674194\n",
      "\t Training loss (single batch): 1.4961298704147339\n",
      "\t Training loss (single batch): 1.2001386880874634\n",
      "\t Training loss (single batch): 1.341537356376648\n",
      "\t Training loss (single batch): 1.1552729606628418\n",
      "\t Training loss (single batch): 1.0822912454605103\n",
      "\t Training loss (single batch): 1.0493288040161133\n",
      "\t Training loss (single batch): 1.3154946565628052\n",
      "\t Training loss (single batch): 1.3187845945358276\n",
      "\t Training loss (single batch): 1.4041563272476196\n",
      "\t Training loss (single batch): 1.2375447750091553\n",
      "\t Training loss (single batch): 1.2940863370895386\n",
      "\t Training loss (single batch): 1.1127756834030151\n",
      "\t Training loss (single batch): 1.019288182258606\n",
      "\t Training loss (single batch): 1.2163524627685547\n",
      "\t Training loss (single batch): 1.047306776046753\n",
      "\t Training loss (single batch): 1.4043478965759277\n",
      "\t Training loss (single batch): 1.1795216798782349\n",
      "\t Training loss (single batch): 1.1500831842422485\n",
      "\t Training loss (single batch): 1.2581557035446167\n",
      "\t Training loss (single batch): 1.5374882221221924\n",
      "\t Training loss (single batch): 1.0609560012817383\n",
      "\t Training loss (single batch): 0.9940991401672363\n",
      "\t Training loss (single batch): 0.9351487755775452\n",
      "\t Training loss (single batch): 1.2834374904632568\n",
      "\t Training loss (single batch): 1.1396660804748535\n",
      "\t Training loss (single batch): 0.9735220074653625\n",
      "\t Training loss (single batch): 1.0265389680862427\n",
      "\t Training loss (single batch): 1.2901036739349365\n",
      "\t Training loss (single batch): 1.0796685218811035\n",
      "\t Training loss (single batch): 1.1068627834320068\n",
      "\t Training loss (single batch): 1.4256902933120728\n",
      "\t Training loss (single batch): 1.7065678834915161\n",
      "\t Training loss (single batch): 1.5210075378417969\n",
      "\t Training loss (single batch): 1.3907618522644043\n",
      "\t Training loss (single batch): 1.0902754068374634\n",
      "\t Training loss (single batch): 1.2236087322235107\n",
      "\t Training loss (single batch): 0.9913238883018494\n",
      "\t Training loss (single batch): 0.9656458497047424\n",
      "\t Training loss (single batch): 1.3776063919067383\n",
      "\t Training loss (single batch): 1.0829122066497803\n",
      "\t Training loss (single batch): 1.6093848943710327\n",
      "\t Training loss (single batch): 1.0359526872634888\n",
      "\t Training loss (single batch): 0.9503241777420044\n",
      "\t Training loss (single batch): 1.3648885488510132\n",
      "\t Training loss (single batch): 1.4927419424057007\n",
      "\t Training loss (single batch): 1.4763928651809692\n",
      "\t Training loss (single batch): 1.6902053356170654\n",
      "\t Training loss (single batch): 1.3050875663757324\n",
      "\t Training loss (single batch): 1.2083994150161743\n",
      "\t Training loss (single batch): 1.3321112394332886\n",
      "\t Training loss (single batch): 1.5924021005630493\n",
      "\t Training loss (single batch): 1.341049075126648\n",
      "\t Training loss (single batch): 1.424544334411621\n",
      "\t Training loss (single batch): 1.3059947490692139\n",
      "\t Training loss (single batch): 1.2252663373947144\n",
      "\t Training loss (single batch): 1.7452527284622192\n",
      "\t Training loss (single batch): 0.9975998401641846\n",
      "\t Training loss (single batch): 0.6490005850791931\n",
      "\t Training loss (single batch): 1.0188665390014648\n",
      "\t Training loss (single batch): 0.961879551410675\n",
      "\t Training loss (single batch): 1.5958421230316162\n",
      "\t Training loss (single batch): 1.338001012802124\n",
      "\t Training loss (single batch): 1.2667933702468872\n",
      "\t Training loss (single batch): 1.4861119985580444\n",
      "\t Training loss (single batch): 1.1371867656707764\n",
      "\t Training loss (single batch): 1.2239168882369995\n",
      "\t Training loss (single batch): 1.1332546472549438\n",
      "\t Training loss (single batch): 1.0603306293487549\n",
      "\t Training loss (single batch): 1.0527241230010986\n",
      "\t Training loss (single batch): 1.0080852508544922\n",
      "\t Training loss (single batch): 1.1284087896347046\n",
      "\t Training loss (single batch): 1.2433873414993286\n",
      "\t Training loss (single batch): 1.896225929260254\n",
      "\t Training loss (single batch): 1.0414334535598755\n",
      "\t Training loss (single batch): 1.57725989818573\n",
      "\t Training loss (single batch): 1.1662503480911255\n",
      "\t Training loss (single batch): 1.3110545873641968\n",
      "\t Training loss (single batch): 0.9997055530548096\n",
      "\t Training loss (single batch): 1.0963335037231445\n",
      "\t Training loss (single batch): 1.359591007232666\n",
      "\t Training loss (single batch): 1.402698278427124\n",
      "\t Training loss (single batch): 1.1045607328414917\n",
      "\t Training loss (single batch): 1.250646948814392\n",
      "\t Training loss (single batch): 1.0532487630844116\n",
      "\t Training loss (single batch): 1.3279019594192505\n",
      "\t Training loss (single batch): 1.5519349575042725\n",
      "\t Training loss (single batch): 1.1567316055297852\n",
      "\t Training loss (single batch): 1.0443345308303833\n",
      "\t Training loss (single batch): 1.2767714262008667\n",
      "\t Training loss (single batch): 1.245003342628479\n",
      "\t Training loss (single batch): 1.2626841068267822\n",
      "\t Training loss (single batch): 1.0987403392791748\n",
      "\t Training loss (single batch): 1.199265718460083\n",
      "\t Training loss (single batch): 1.2218505144119263\n",
      "\t Training loss (single batch): 1.0256978273391724\n",
      "\t Training loss (single batch): 1.44234299659729\n",
      "\t Training loss (single batch): 1.4852381944656372\n",
      "\t Training loss (single batch): 1.853808879852295\n",
      "\t Training loss (single batch): 1.4460340738296509\n",
      "\t Training loss (single batch): 1.0063488483428955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6727524995803833\n",
      "\t Training loss (single batch): 1.1581331491470337\n",
      "\t Training loss (single batch): 0.8190143704414368\n",
      "\t Training loss (single batch): 1.0353906154632568\n",
      "\t Training loss (single batch): 1.4014904499053955\n",
      "\t Training loss (single batch): 1.0612568855285645\n",
      "\t Training loss (single batch): 1.6875697374343872\n",
      "\t Training loss (single batch): 0.8818625211715698\n",
      "\t Training loss (single batch): 1.3988723754882812\n",
      "\t Training loss (single batch): 0.8727036714553833\n",
      "\t Training loss (single batch): 1.4455825090408325\n",
      "\t Training loss (single batch): 1.4203237295150757\n",
      "\t Training loss (single batch): 1.3028947114944458\n",
      "\t Training loss (single batch): 0.8337107300758362\n",
      "\t Training loss (single batch): 1.168214201927185\n",
      "\t Training loss (single batch): 1.2607160806655884\n",
      "\t Training loss (single batch): 1.8386973142623901\n",
      "\t Training loss (single batch): 1.203782081604004\n",
      "\t Training loss (single batch): 0.93638676404953\n",
      "\t Training loss (single batch): 0.9988771080970764\n",
      "\t Training loss (single batch): 1.351588249206543\n",
      "\t Training loss (single batch): 1.039888858795166\n",
      "\t Training loss (single batch): 1.229988932609558\n",
      "\t Training loss (single batch): 1.0915403366088867\n",
      "\t Training loss (single batch): 0.9926984906196594\n",
      "\t Training loss (single batch): 0.8467313647270203\n",
      "\t Training loss (single batch): 1.2485311031341553\n",
      "\t Training loss (single batch): 0.9220198392868042\n",
      "\t Training loss (single batch): 1.2328234910964966\n",
      "\t Training loss (single batch): 1.271790623664856\n",
      "\t Training loss (single batch): 1.2822258472442627\n",
      "\t Training loss (single batch): 1.6307601928710938\n",
      "\t Training loss (single batch): 1.55902898311615\n",
      "\t Training loss (single batch): 0.7593218684196472\n",
      "\t Training loss (single batch): 0.6319786310195923\n",
      "\t Training loss (single batch): 0.9724855422973633\n",
      "\t Training loss (single batch): 0.9551780223846436\n",
      "\t Training loss (single batch): 0.8382831811904907\n",
      "\t Training loss (single batch): 1.2154878377914429\n",
      "\t Training loss (single batch): 1.2215780019760132\n",
      "\t Training loss (single batch): 1.2691409587860107\n",
      "\t Training loss (single batch): 1.0449907779693604\n",
      "\t Training loss (single batch): 1.7022278308868408\n",
      "\t Training loss (single batch): 0.9534555673599243\n",
      "\t Training loss (single batch): 1.0707647800445557\n",
      "\t Training loss (single batch): 1.1958260536193848\n",
      "\t Training loss (single batch): 1.3402553796768188\n",
      "\t Training loss (single batch): 1.2213749885559082\n",
      "\t Training loss (single batch): 0.8995739221572876\n",
      "\t Training loss (single batch): 1.5469027757644653\n",
      "\t Training loss (single batch): 1.5444892644882202\n",
      "\t Training loss (single batch): 1.4943594932556152\n",
      "\t Training loss (single batch): 0.945261538028717\n",
      "\t Training loss (single batch): 1.441590666770935\n",
      "\t Training loss (single batch): 1.0864031314849854\n",
      "\t Training loss (single batch): 1.0401568412780762\n",
      "\t Training loss (single batch): 1.0553810596466064\n",
      "\t Training loss (single batch): 1.216499924659729\n",
      "\t Training loss (single batch): 1.0829757452011108\n",
      "\t Training loss (single batch): 1.1165838241577148\n",
      "\t Training loss (single batch): 1.4108195304870605\n",
      "\t Training loss (single batch): 1.3737279176712036\n",
      "\t Training loss (single batch): 1.00368332862854\n",
      "\t Training loss (single batch): 1.332512617111206\n",
      "\t Training loss (single batch): 1.6331361532211304\n",
      "\t Training loss (single batch): 1.1705034971237183\n",
      "\t Training loss (single batch): 1.12729012966156\n",
      "\t Training loss (single batch): 0.8227294683456421\n",
      "\t Training loss (single batch): 0.9869033694267273\n",
      "\t Training loss (single batch): 1.3590723276138306\n",
      "\t Training loss (single batch): 1.2499371767044067\n",
      "\t Training loss (single batch): 1.1057182550430298\n",
      "\t Training loss (single batch): 1.2426884174346924\n",
      "\t Training loss (single batch): 1.3400425910949707\n",
      "\t Training loss (single batch): 1.7958317995071411\n",
      "\t Training loss (single batch): 1.1825016736984253\n",
      "\t Training loss (single batch): 0.6611852645874023\n",
      "\t Training loss (single batch): 1.074380874633789\n",
      "\t Training loss (single batch): 1.080930471420288\n",
      "\t Training loss (single batch): 1.6867939233779907\n",
      "\t Training loss (single batch): 1.6918772459030151\n",
      "\t Training loss (single batch): 1.7491834163665771\n",
      "\t Training loss (single batch): 1.0029264688491821\n",
      "\t Training loss (single batch): 1.3116750717163086\n",
      "\t Training loss (single batch): 1.0395853519439697\n",
      "\t Training loss (single batch): 1.2730754613876343\n",
      "\t Training loss (single batch): 1.0349774360656738\n",
      "\t Training loss (single batch): 1.5959500074386597\n",
      "\t Training loss (single batch): 1.1450386047363281\n",
      "\t Training loss (single batch): 1.2588425874710083\n",
      "\t Training loss (single batch): 1.28949773311615\n",
      "\t Training loss (single batch): 1.0701000690460205\n",
      "\t Training loss (single batch): 1.5561518669128418\n",
      "\t Training loss (single batch): 1.2217403650283813\n",
      "\t Training loss (single batch): 1.2626069784164429\n",
      "\t Training loss (single batch): 1.1608831882476807\n",
      "\t Training loss (single batch): 0.9785879850387573\n",
      "\t Training loss (single batch): 1.1100865602493286\n",
      "\t Training loss (single batch): 1.4320110082626343\n",
      "\t Training loss (single batch): 1.1283577680587769\n",
      "\t Training loss (single batch): 1.2468076944351196\n",
      "\t Training loss (single batch): 1.0119085311889648\n",
      "\t Training loss (single batch): 1.1723577976226807\n",
      "\t Training loss (single batch): 1.1176660060882568\n",
      "\t Training loss (single batch): 1.508093237876892\n",
      "\t Training loss (single batch): 1.065110683441162\n",
      "\t Training loss (single batch): 1.5937448740005493\n",
      "\t Training loss (single batch): 1.1355739831924438\n",
      "\t Training loss (single batch): 1.310925841331482\n",
      "\t Training loss (single batch): 1.2530889511108398\n",
      "\t Training loss (single batch): 1.1667187213897705\n",
      "\t Training loss (single batch): 0.989672839641571\n",
      "\t Training loss (single batch): 1.0089625120162964\n",
      "\t Training loss (single batch): 1.317300796508789\n",
      "\t Training loss (single batch): 1.314682126045227\n",
      "\t Training loss (single batch): 1.3011701107025146\n",
      "\t Training loss (single batch): 0.9725106358528137\n",
      "\t Training loss (single batch): 1.0698610544204712\n",
      "\t Training loss (single batch): 1.486663579940796\n",
      "\t Training loss (single batch): 1.2842777967453003\n",
      "\t Training loss (single batch): 1.2133969068527222\n",
      "\t Training loss (single batch): 1.149766445159912\n",
      "\t Training loss (single batch): 1.3606820106506348\n",
      "\t Training loss (single batch): 0.9613638520240784\n",
      "\t Training loss (single batch): 1.41719651222229\n",
      "\t Training loss (single batch): 1.1497899293899536\n",
      "\t Training loss (single batch): 1.0598459243774414\n",
      "\t Training loss (single batch): 1.7922840118408203\n",
      "\t Training loss (single batch): 0.9469895958900452\n",
      "\t Training loss (single batch): 1.2229642868041992\n",
      "\t Training loss (single batch): 1.2577983140945435\n",
      "\t Training loss (single batch): 0.9437601566314697\n",
      "\t Training loss (single batch): 1.37643301486969\n",
      "\t Training loss (single batch): 1.1217517852783203\n",
      "\t Training loss (single batch): 0.779553234577179\n",
      "\t Training loss (single batch): 1.347943663597107\n",
      "\t Training loss (single batch): 1.1197162866592407\n",
      "\t Training loss (single batch): 1.5575273036956787\n",
      "\t Training loss (single batch): 1.4348108768463135\n",
      "\t Training loss (single batch): 1.1270095109939575\n",
      "\t Training loss (single batch): 1.3886308670043945\n",
      "\t Training loss (single batch): 0.9193972945213318\n",
      "\t Training loss (single batch): 1.0881588459014893\n",
      "\t Training loss (single batch): 1.539917230606079\n",
      "\t Training loss (single batch): 1.2912718057632446\n",
      "\t Training loss (single batch): 0.8671444058418274\n",
      "\t Training loss (single batch): 1.3797426223754883\n",
      "\t Training loss (single batch): 1.2338308095932007\n",
      "\t Training loss (single batch): 1.1216129064559937\n",
      "\t Training loss (single batch): 1.1554826498031616\n",
      "\t Training loss (single batch): 1.0561555624008179\n",
      "\t Training loss (single batch): 0.7326061129570007\n",
      "\t Training loss (single batch): 0.7763423323631287\n",
      "\t Training loss (single batch): 1.3140519857406616\n",
      "\t Training loss (single batch): 0.9839553236961365\n",
      "\t Training loss (single batch): 0.8721789717674255\n",
      "\t Training loss (single batch): 1.0937557220458984\n",
      "\t Training loss (single batch): 0.8657916784286499\n",
      "\t Training loss (single batch): 1.4002028703689575\n",
      "\t Training loss (single batch): 1.5095103979110718\n",
      "\t Training loss (single batch): 1.2819244861602783\n",
      "\t Training loss (single batch): 1.560194969177246\n",
      "\t Training loss (single batch): 0.684901773929596\n",
      "\t Training loss (single batch): 1.046567440032959\n",
      "\t Training loss (single batch): 1.0564849376678467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5049265623092651\n",
      "\t Training loss (single batch): 1.0377118587493896\n",
      "\t Training loss (single batch): 0.9511319398880005\n",
      "\t Training loss (single batch): 1.440363883972168\n",
      "\t Training loss (single batch): 1.2223016023635864\n",
      "\t Training loss (single batch): 0.7678977251052856\n",
      "\t Training loss (single batch): 1.1753039360046387\n",
      "\t Training loss (single batch): 1.6899365186691284\n",
      "\t Training loss (single batch): 1.457218885421753\n",
      "\t Training loss (single batch): 1.198034644126892\n",
      "\t Training loss (single batch): 0.7507227659225464\n",
      "\t Training loss (single batch): 1.147469401359558\n",
      "\t Training loss (single batch): 1.0295379161834717\n",
      "\t Training loss (single batch): 1.3321205377578735\n",
      "\t Training loss (single batch): 1.034912347793579\n",
      "\t Training loss (single batch): 1.7480134963989258\n",
      "\t Training loss (single batch): 1.5366706848144531\n",
      "\t Training loss (single batch): 0.818041980266571\n",
      "\t Training loss (single batch): 1.489963173866272\n",
      "\t Training loss (single batch): 0.7108598947525024\n",
      "\t Training loss (single batch): 0.7123675346374512\n",
      "\t Training loss (single batch): 0.9593397378921509\n",
      "\t Training loss (single batch): 1.011798620223999\n",
      "\t Training loss (single batch): 0.7362285256385803\n",
      "\t Training loss (single batch): 1.201327919960022\n",
      "\t Training loss (single batch): 0.8541340827941895\n",
      "\t Training loss (single batch): 1.6183695793151855\n",
      "\t Training loss (single batch): 1.5433508157730103\n",
      "\t Training loss (single batch): 0.7621306777000427\n",
      "\t Training loss (single batch): 0.9225841760635376\n",
      "\t Training loss (single batch): 1.4807627201080322\n",
      "\t Training loss (single batch): 1.0894407033920288\n",
      "\t Training loss (single batch): 1.0316067934036255\n",
      "\t Training loss (single batch): 1.5515005588531494\n",
      "\t Training loss (single batch): 1.3156028985977173\n",
      "\t Training loss (single batch): 1.837645173072815\n",
      "\t Training loss (single batch): 1.507391095161438\n",
      "\t Training loss (single batch): 1.4666894674301147\n",
      "\t Training loss (single batch): 1.0716668367385864\n",
      "\t Training loss (single batch): 0.6318780779838562\n",
      "##################################\n",
      "## EPOCH 55\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7725069522857666\n",
      "\t Training loss (single batch): 1.0534136295318604\n",
      "\t Training loss (single batch): 1.074494481086731\n",
      "\t Training loss (single batch): 0.7781144976615906\n",
      "\t Training loss (single batch): 1.9122917652130127\n",
      "\t Training loss (single batch): 1.0564155578613281\n",
      "\t Training loss (single batch): 1.6884630918502808\n",
      "\t Training loss (single batch): 0.8213958740234375\n",
      "\t Training loss (single batch): 1.095971703529358\n",
      "\t Training loss (single batch): 1.3708440065383911\n",
      "\t Training loss (single batch): 1.2187265157699585\n",
      "\t Training loss (single batch): 1.1699306964874268\n",
      "\t Training loss (single batch): 1.1131359338760376\n",
      "\t Training loss (single batch): 1.091605305671692\n",
      "\t Training loss (single batch): 0.7293041348457336\n",
      "\t Training loss (single batch): 1.1884785890579224\n",
      "\t Training loss (single batch): 0.8091505765914917\n",
      "\t Training loss (single batch): 0.9309788346290588\n",
      "\t Training loss (single batch): 1.4950157403945923\n",
      "\t Training loss (single batch): 1.4888721704483032\n",
      "\t Training loss (single batch): 1.700130820274353\n",
      "\t Training loss (single batch): 0.9407199025154114\n",
      "\t Training loss (single batch): 1.5366402864456177\n",
      "\t Training loss (single batch): 1.620126724243164\n",
      "\t Training loss (single batch): 1.4721583127975464\n",
      "\t Training loss (single batch): 0.9591430425643921\n",
      "\t Training loss (single batch): 1.717969298362732\n",
      "\t Training loss (single batch): 0.9515854716300964\n",
      "\t Training loss (single batch): 1.2867625951766968\n",
      "\t Training loss (single batch): 1.3960707187652588\n",
      "\t Training loss (single batch): 1.4931272268295288\n",
      "\t Training loss (single batch): 1.0096209049224854\n",
      "\t Training loss (single batch): 1.2990474700927734\n",
      "\t Training loss (single batch): 1.0665466785430908\n",
      "\t Training loss (single batch): 0.9201484322547913\n",
      "\t Training loss (single batch): 1.1214436292648315\n",
      "\t Training loss (single batch): 1.3682395219802856\n",
      "\t Training loss (single batch): 1.1406291723251343\n",
      "\t Training loss (single batch): 1.684093713760376\n",
      "\t Training loss (single batch): 0.6758538484573364\n",
      "\t Training loss (single batch): 0.9241058230400085\n",
      "\t Training loss (single batch): 1.946136236190796\n",
      "\t Training loss (single batch): 1.0820666551589966\n",
      "\t Training loss (single batch): 1.4455859661102295\n",
      "\t Training loss (single batch): 1.3724496364593506\n",
      "\t Training loss (single batch): 1.626110315322876\n",
      "\t Training loss (single batch): 1.1197826862335205\n",
      "\t Training loss (single batch): 1.4638292789459229\n",
      "\t Training loss (single batch): 1.2388674020767212\n",
      "\t Training loss (single batch): 0.9931609034538269\n",
      "\t Training loss (single batch): 1.3272303342819214\n",
      "\t Training loss (single batch): 0.8396382331848145\n",
      "\t Training loss (single batch): 1.5747801065444946\n",
      "\t Training loss (single batch): 1.1141301393508911\n",
      "\t Training loss (single batch): 1.276702642440796\n",
      "\t Training loss (single batch): 0.8547356724739075\n",
      "\t Training loss (single batch): 1.1360784769058228\n",
      "\t Training loss (single batch): 1.640910029411316\n",
      "\t Training loss (single batch): 1.050134301185608\n",
      "\t Training loss (single batch): 1.2896826267242432\n",
      "\t Training loss (single batch): 1.259621262550354\n",
      "\t Training loss (single batch): 1.2451214790344238\n",
      "\t Training loss (single batch): 1.3427753448486328\n",
      "\t Training loss (single batch): 0.8325456380844116\n",
      "\t Training loss (single batch): 1.3895255327224731\n",
      "\t Training loss (single batch): 1.7205588817596436\n",
      "\t Training loss (single batch): 1.3734745979309082\n",
      "\t Training loss (single batch): 0.926866888999939\n",
      "\t Training loss (single batch): 1.3902560472488403\n",
      "\t Training loss (single batch): 1.1772431135177612\n",
      "\t Training loss (single batch): 1.2699227333068848\n",
      "\t Training loss (single batch): 1.2710474729537964\n",
      "\t Training loss (single batch): 1.312453031539917\n",
      "\t Training loss (single batch): 1.5361065864562988\n",
      "\t Training loss (single batch): 0.7917154431343079\n",
      "\t Training loss (single batch): 0.8957747220993042\n",
      "\t Training loss (single batch): 0.9025033712387085\n",
      "\t Training loss (single batch): 0.8475356698036194\n",
      "\t Training loss (single batch): 1.0833628177642822\n",
      "\t Training loss (single batch): 1.7885327339172363\n",
      "\t Training loss (single batch): 1.1764347553253174\n",
      "\t Training loss (single batch): 1.222298502922058\n",
      "\t Training loss (single batch): 1.498841404914856\n",
      "\t Training loss (single batch): 0.9589529037475586\n",
      "\t Training loss (single batch): 1.0234779119491577\n",
      "\t Training loss (single batch): 1.1270891427993774\n",
      "\t Training loss (single batch): 1.316145896911621\n",
      "\t Training loss (single batch): 1.4765962362289429\n",
      "\t Training loss (single batch): 1.4671882390975952\n",
      "\t Training loss (single batch): 1.2986425161361694\n",
      "\t Training loss (single batch): 1.2493129968643188\n",
      "\t Training loss (single batch): 0.8159196972846985\n",
      "\t Training loss (single batch): 0.9892057180404663\n",
      "\t Training loss (single batch): 0.9572727680206299\n",
      "\t Training loss (single batch): 0.9591215252876282\n",
      "\t Training loss (single batch): 1.5498745441436768\n",
      "\t Training loss (single batch): 1.3320645093917847\n",
      "\t Training loss (single batch): 1.0110247135162354\n",
      "\t Training loss (single batch): 1.5968068838119507\n",
      "\t Training loss (single batch): 1.040057897567749\n",
      "\t Training loss (single batch): 1.2439578771591187\n",
      "\t Training loss (single batch): 1.2688168287277222\n",
      "\t Training loss (single batch): 1.1676347255706787\n",
      "\t Training loss (single batch): 1.4350614547729492\n",
      "\t Training loss (single batch): 1.0523186922073364\n",
      "\t Training loss (single batch): 1.3371504545211792\n",
      "\t Training loss (single batch): 1.0797008275985718\n",
      "\t Training loss (single batch): 1.4561572074890137\n",
      "\t Training loss (single batch): 1.3799411058425903\n",
      "\t Training loss (single batch): 1.2183387279510498\n",
      "\t Training loss (single batch): 0.8736769556999207\n",
      "\t Training loss (single batch): 1.1180737018585205\n",
      "\t Training loss (single batch): 1.6941148042678833\n",
      "\t Training loss (single batch): 1.1745707988739014\n",
      "\t Training loss (single batch): 1.5911908149719238\n",
      "\t Training loss (single batch): 0.8759562373161316\n",
      "\t Training loss (single batch): 1.3267713785171509\n",
      "\t Training loss (single batch): 1.588937759399414\n",
      "\t Training loss (single batch): 0.6141785383224487\n",
      "\t Training loss (single batch): 1.0521703958511353\n",
      "\t Training loss (single batch): 0.9568811058998108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1900087594985962\n",
      "\t Training loss (single batch): 1.1840972900390625\n",
      "\t Training loss (single batch): 0.6672486066818237\n",
      "\t Training loss (single batch): 1.3857728242874146\n",
      "\t Training loss (single batch): 1.0632264614105225\n",
      "\t Training loss (single batch): 0.5978596210479736\n",
      "\t Training loss (single batch): 1.2078545093536377\n",
      "\t Training loss (single batch): 1.0795652866363525\n",
      "\t Training loss (single batch): 1.2042814493179321\n",
      "\t Training loss (single batch): 0.9625605344772339\n",
      "\t Training loss (single batch): 1.0454332828521729\n",
      "\t Training loss (single batch): 1.0697963237762451\n",
      "\t Training loss (single batch): 1.1718841791152954\n",
      "\t Training loss (single batch): 0.8672384023666382\n",
      "\t Training loss (single batch): 1.597663402557373\n",
      "\t Training loss (single batch): 1.256844162940979\n",
      "\t Training loss (single batch): 0.9349243640899658\n",
      "\t Training loss (single batch): 1.259011149406433\n",
      "\t Training loss (single batch): 1.019339680671692\n",
      "\t Training loss (single batch): 1.2770594358444214\n",
      "\t Training loss (single batch): 0.8562536239624023\n",
      "\t Training loss (single batch): 1.2416772842407227\n",
      "\t Training loss (single batch): 1.5069266557693481\n",
      "\t Training loss (single batch): 1.3778003454208374\n",
      "\t Training loss (single batch): 0.7996877431869507\n",
      "\t Training loss (single batch): 1.9990280866622925\n",
      "\t Training loss (single batch): 1.3083865642547607\n",
      "\t Training loss (single batch): 1.1585980653762817\n",
      "\t Training loss (single batch): 0.878532886505127\n",
      "\t Training loss (single batch): 0.8190873265266418\n",
      "\t Training loss (single batch): 1.5697214603424072\n",
      "\t Training loss (single batch): 0.929081380367279\n",
      "\t Training loss (single batch): 1.0350730419158936\n",
      "\t Training loss (single batch): 1.4846440553665161\n",
      "\t Training loss (single batch): 1.3026057481765747\n",
      "\t Training loss (single batch): 1.4488141536712646\n",
      "\t Training loss (single batch): 1.4742884635925293\n",
      "\t Training loss (single batch): 0.9298373460769653\n",
      "\t Training loss (single batch): 2.1160993576049805\n",
      "\t Training loss (single batch): 1.0618988275527954\n",
      "\t Training loss (single batch): 1.0306766033172607\n",
      "\t Training loss (single batch): 1.360991358757019\n",
      "\t Training loss (single batch): 1.0959335565567017\n",
      "\t Training loss (single batch): 1.1627596616744995\n",
      "\t Training loss (single batch): 1.4225572347640991\n",
      "\t Training loss (single batch): 1.2277348041534424\n",
      "\t Training loss (single batch): 1.5812349319458008\n",
      "\t Training loss (single batch): 1.170852780342102\n",
      "\t Training loss (single batch): 1.3031048774719238\n",
      "\t Training loss (single batch): 1.012596845626831\n",
      "\t Training loss (single batch): 0.9344348311424255\n",
      "\t Training loss (single batch): 1.228753685951233\n",
      "\t Training loss (single batch): 1.2594300508499146\n",
      "\t Training loss (single batch): 1.2401787042617798\n",
      "\t Training loss (single batch): 1.3446673154830933\n",
      "\t Training loss (single batch): 0.7647640705108643\n",
      "\t Training loss (single batch): 1.3146649599075317\n",
      "\t Training loss (single batch): 1.995459794998169\n",
      "\t Training loss (single batch): 1.1308050155639648\n",
      "\t Training loss (single batch): 1.0423394441604614\n",
      "\t Training loss (single batch): 1.0217905044555664\n",
      "\t Training loss (single batch): 1.22077214717865\n",
      "\t Training loss (single batch): 0.7901576161384583\n",
      "\t Training loss (single batch): 1.4884065389633179\n",
      "\t Training loss (single batch): 1.6707446575164795\n",
      "\t Training loss (single batch): 1.8803529739379883\n",
      "\t Training loss (single batch): 0.7917255759239197\n",
      "\t Training loss (single batch): 0.9820025563240051\n",
      "\t Training loss (single batch): 1.8131734132766724\n",
      "\t Training loss (single batch): 0.802126407623291\n",
      "\t Training loss (single batch): 0.9359777569770813\n",
      "\t Training loss (single batch): 1.2251826524734497\n",
      "\t Training loss (single batch): 1.39885675907135\n",
      "\t Training loss (single batch): 1.0894712209701538\n",
      "\t Training loss (single batch): 1.1219816207885742\n",
      "\t Training loss (single batch): 1.5445159673690796\n",
      "\t Training loss (single batch): 0.841064453125\n",
      "\t Training loss (single batch): 1.0690282583236694\n",
      "\t Training loss (single batch): 1.026092529296875\n",
      "\t Training loss (single batch): 1.5869258642196655\n",
      "\t Training loss (single batch): 1.5559362173080444\n",
      "\t Training loss (single batch): 1.3314849138259888\n",
      "\t Training loss (single batch): 1.6217339038848877\n",
      "\t Training loss (single batch): 0.8370908498764038\n",
      "\t Training loss (single batch): 1.6534239053726196\n",
      "\t Training loss (single batch): 0.7877605557441711\n",
      "\t Training loss (single batch): 1.391176700592041\n",
      "\t Training loss (single batch): 1.1200028657913208\n",
      "\t Training loss (single batch): 1.1433029174804688\n",
      "\t Training loss (single batch): 0.7980751395225525\n",
      "\t Training loss (single batch): 1.190255045890808\n",
      "\t Training loss (single batch): 1.2181912660598755\n",
      "\t Training loss (single batch): 1.1843262910842896\n",
      "\t Training loss (single batch): 0.9340360760688782\n",
      "\t Training loss (single batch): 1.4641566276550293\n",
      "\t Training loss (single batch): 1.6375285387039185\n",
      "\t Training loss (single batch): 0.9953053593635559\n",
      "\t Training loss (single batch): 1.54728102684021\n",
      "\t Training loss (single batch): 0.9628821611404419\n",
      "\t Training loss (single batch): 1.4417563676834106\n",
      "\t Training loss (single batch): 1.0223406553268433\n",
      "\t Training loss (single batch): 1.2940315008163452\n",
      "\t Training loss (single batch): 1.0411734580993652\n",
      "\t Training loss (single batch): 1.1336414813995361\n",
      "\t Training loss (single batch): 1.054464340209961\n",
      "\t Training loss (single batch): 0.8810836672782898\n",
      "\t Training loss (single batch): 1.0989406108856201\n",
      "\t Training loss (single batch): 1.1587703227996826\n",
      "\t Training loss (single batch): 1.4508750438690186\n",
      "\t Training loss (single batch): 0.9100283980369568\n",
      "\t Training loss (single batch): 1.1325390338897705\n",
      "\t Training loss (single batch): 0.9417003989219666\n",
      "\t Training loss (single batch): 1.3638885021209717\n",
      "\t Training loss (single batch): 0.891470193862915\n",
      "\t Training loss (single batch): 1.1015260219573975\n",
      "\t Training loss (single batch): 1.5627418756484985\n",
      "\t Training loss (single batch): 1.324249267578125\n",
      "\t Training loss (single batch): 2.02649188041687\n",
      "\t Training loss (single batch): 1.4456862211227417\n",
      "\t Training loss (single batch): 2.044764995574951\n",
      "\t Training loss (single batch): 0.9166517853736877\n",
      "\t Training loss (single batch): 1.1447744369506836\n",
      "\t Training loss (single batch): 0.9516422748565674\n",
      "\t Training loss (single batch): 1.2162024974822998\n",
      "\t Training loss (single batch): 1.8486011028289795\n",
      "\t Training loss (single batch): 1.2573492527008057\n",
      "\t Training loss (single batch): 1.103018045425415\n",
      "\t Training loss (single batch): 1.0896373987197876\n",
      "\t Training loss (single batch): 1.1455092430114746\n",
      "\t Training loss (single batch): 1.4407507181167603\n",
      "\t Training loss (single batch): 0.9164250493049622\n",
      "\t Training loss (single batch): 1.0775673389434814\n",
      "\t Training loss (single batch): 0.9516372680664062\n",
      "\t Training loss (single batch): 0.6321573257446289\n",
      "\t Training loss (single batch): 1.156199336051941\n",
      "\t Training loss (single batch): 1.2426518201828003\n",
      "\t Training loss (single batch): 1.3074500560760498\n",
      "\t Training loss (single batch): 1.3648024797439575\n",
      "\t Training loss (single batch): 0.9676530957221985\n",
      "\t Training loss (single batch): 1.5349863767623901\n",
      "\t Training loss (single batch): 1.167799949645996\n",
      "\t Training loss (single batch): 1.1520965099334717\n",
      "\t Training loss (single batch): 0.56853848695755\n",
      "\t Training loss (single batch): 1.1158136129379272\n",
      "\t Training loss (single batch): 1.4678919315338135\n",
      "\t Training loss (single batch): 1.3148179054260254\n",
      "\t Training loss (single batch): 1.5565845966339111\n",
      "\t Training loss (single batch): 1.2354300022125244\n",
      "\t Training loss (single batch): 1.2342162132263184\n",
      "\t Training loss (single batch): 1.1930967569351196\n",
      "\t Training loss (single batch): 1.0918138027191162\n",
      "\t Training loss (single batch): 1.139910101890564\n",
      "\t Training loss (single batch): 1.357823133468628\n",
      "\t Training loss (single batch): 1.1961143016815186\n",
      "\t Training loss (single batch): 1.9963070154190063\n",
      "\t Training loss (single batch): 1.3685029745101929\n",
      "\t Training loss (single batch): 0.9618703126907349\n",
      "\t Training loss (single batch): 1.3860268592834473\n",
      "\t Training loss (single batch): 1.3045032024383545\n",
      "\t Training loss (single batch): 1.4424755573272705\n",
      "\t Training loss (single batch): 0.8862795829772949\n",
      "\t Training loss (single batch): 0.9856924414634705\n",
      "\t Training loss (single batch): 1.0467232465744019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0168497562408447\n",
      "\t Training loss (single batch): 0.9297761917114258\n",
      "\t Training loss (single batch): 0.9907116293907166\n",
      "\t Training loss (single batch): 1.052140235900879\n",
      "\t Training loss (single batch): 1.2630358934402466\n",
      "\t Training loss (single batch): 1.3609414100646973\n",
      "\t Training loss (single batch): 0.9412133097648621\n",
      "\t Training loss (single batch): 1.16706383228302\n",
      "\t Training loss (single batch): 1.0773082971572876\n",
      "\t Training loss (single batch): 1.5058746337890625\n",
      "\t Training loss (single batch): 1.1171268224716187\n",
      "\t Training loss (single batch): 1.4186521768569946\n",
      "\t Training loss (single batch): 1.4632668495178223\n",
      "\t Training loss (single batch): 1.1418322324752808\n",
      "\t Training loss (single batch): 0.966229259967804\n",
      "\t Training loss (single batch): 1.5370948314666748\n",
      "\t Training loss (single batch): 1.7040479183197021\n",
      "\t Training loss (single batch): 1.4171050786972046\n",
      "\t Training loss (single batch): 0.747413694858551\n",
      "\t Training loss (single batch): 1.1670068502426147\n",
      "\t Training loss (single batch): 1.3841570615768433\n",
      "\t Training loss (single batch): 1.4658854007720947\n",
      "\t Training loss (single batch): 1.4106460809707642\n",
      "\t Training loss (single batch): 1.5035943984985352\n",
      "\t Training loss (single batch): 1.1749985218048096\n",
      "\t Training loss (single batch): 1.4046865701675415\n",
      "\t Training loss (single batch): 1.169698715209961\n",
      "\t Training loss (single batch): 0.95145183801651\n",
      "\t Training loss (single batch): 1.2313867807388306\n",
      "\t Training loss (single batch): 1.310531497001648\n",
      "\t Training loss (single batch): 1.6427747011184692\n",
      "\t Training loss (single batch): 1.3646483421325684\n",
      "\t Training loss (single batch): 0.5209819078445435\n",
      "##################################\n",
      "## EPOCH 56\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0503268241882324\n",
      "\t Training loss (single batch): 1.050781488418579\n",
      "\t Training loss (single batch): 1.721774697303772\n",
      "\t Training loss (single batch): 1.1839531660079956\n",
      "\t Training loss (single batch): 1.2075039148330688\n",
      "\t Training loss (single batch): 1.1009819507598877\n",
      "\t Training loss (single batch): 0.7896527051925659\n",
      "\t Training loss (single batch): 1.375868558883667\n",
      "\t Training loss (single batch): 1.652268886566162\n",
      "\t Training loss (single batch): 1.2302581071853638\n",
      "\t Training loss (single batch): 1.007531762123108\n",
      "\t Training loss (single batch): 1.3647938966751099\n",
      "\t Training loss (single batch): 1.14640474319458\n",
      "\t Training loss (single batch): 1.0589107275009155\n",
      "\t Training loss (single batch): 0.7889074087142944\n",
      "\t Training loss (single batch): 1.0438898801803589\n",
      "\t Training loss (single batch): 0.9943336248397827\n",
      "\t Training loss (single batch): 1.530056357383728\n",
      "\t Training loss (single batch): 1.074912190437317\n",
      "\t Training loss (single batch): 1.1743940114974976\n",
      "\t Training loss (single batch): 1.1134443283081055\n",
      "\t Training loss (single batch): 0.8099554777145386\n",
      "\t Training loss (single batch): 1.2027939558029175\n",
      "\t Training loss (single batch): 1.156256914138794\n",
      "\t Training loss (single batch): 1.2268003225326538\n",
      "\t Training loss (single batch): 1.032043218612671\n",
      "\t Training loss (single batch): 1.0566493272781372\n",
      "\t Training loss (single batch): 1.3501297235488892\n",
      "\t Training loss (single batch): 1.3719536066055298\n",
      "\t Training loss (single batch): 1.2900919914245605\n",
      "\t Training loss (single batch): 0.699835479259491\n",
      "\t Training loss (single batch): 1.4149236679077148\n",
      "\t Training loss (single batch): 1.5559529066085815\n",
      "\t Training loss (single batch): 1.7114050388336182\n",
      "\t Training loss (single batch): 1.2557997703552246\n",
      "\t Training loss (single batch): 1.011298418045044\n",
      "\t Training loss (single batch): 1.1920256614685059\n",
      "\t Training loss (single batch): 1.453627109527588\n",
      "\t Training loss (single batch): 0.9370116591453552\n",
      "\t Training loss (single batch): 1.1522163152694702\n",
      "\t Training loss (single batch): 0.9020229578018188\n",
      "\t Training loss (single batch): 1.4931659698486328\n",
      "\t Training loss (single batch): 1.4932588338851929\n",
      "\t Training loss (single batch): 0.9898871779441833\n",
      "\t Training loss (single batch): 1.3053534030914307\n",
      "\t Training loss (single batch): 1.1052734851837158\n",
      "\t Training loss (single batch): 1.0130921602249146\n",
      "\t Training loss (single batch): 1.3001363277435303\n",
      "\t Training loss (single batch): 1.4158817529678345\n",
      "\t Training loss (single batch): 1.1231541633605957\n",
      "\t Training loss (single batch): 1.59254789352417\n",
      "\t Training loss (single batch): 1.2278790473937988\n",
      "\t Training loss (single batch): 1.4716192483901978\n",
      "\t Training loss (single batch): 1.4648213386535645\n",
      "\t Training loss (single batch): 1.0259790420532227\n",
      "\t Training loss (single batch): 1.1532243490219116\n",
      "\t Training loss (single batch): 1.215215802192688\n",
      "\t Training loss (single batch): 1.070961833000183\n",
      "\t Training loss (single batch): 1.1891374588012695\n",
      "\t Training loss (single batch): 0.9659281373023987\n",
      "\t Training loss (single batch): 1.334734320640564\n",
      "\t Training loss (single batch): 1.0508133172988892\n",
      "\t Training loss (single batch): 0.7533124089241028\n",
      "\t Training loss (single batch): 1.1083627939224243\n",
      "\t Training loss (single batch): 1.0192296504974365\n",
      "\t Training loss (single batch): 0.9490883350372314\n",
      "\t Training loss (single batch): 1.4755581617355347\n",
      "\t Training loss (single batch): 1.4625242948532104\n",
      "\t Training loss (single batch): 1.1422985792160034\n",
      "\t Training loss (single batch): 1.006673812866211\n",
      "\t Training loss (single batch): 0.9424996972084045\n",
      "\t Training loss (single batch): 0.8230003118515015\n",
      "\t Training loss (single batch): 1.459006428718567\n",
      "\t Training loss (single batch): 1.270424246788025\n",
      "\t Training loss (single batch): 0.9533045291900635\n",
      "\t Training loss (single batch): 1.5525401830673218\n",
      "\t Training loss (single batch): 0.8996299505233765\n",
      "\t Training loss (single batch): 1.108822226524353\n",
      "\t Training loss (single batch): 1.303602695465088\n",
      "\t Training loss (single batch): 1.7568858861923218\n",
      "\t Training loss (single batch): 0.9959231615066528\n",
      "\t Training loss (single batch): 1.4108631610870361\n",
      "\t Training loss (single batch): 1.783577799797058\n",
      "\t Training loss (single batch): 1.655120849609375\n",
      "\t Training loss (single batch): 1.2657238245010376\n",
      "\t Training loss (single batch): 0.9817317724227905\n",
      "\t Training loss (single batch): 0.9578281044960022\n",
      "\t Training loss (single batch): 1.1552963256835938\n",
      "\t Training loss (single batch): 1.1835575103759766\n",
      "\t Training loss (single batch): 1.352307677268982\n",
      "\t Training loss (single batch): 1.0809252262115479\n",
      "\t Training loss (single batch): 1.673474669456482\n",
      "\t Training loss (single batch): 1.0884169340133667\n",
      "\t Training loss (single batch): 1.3624005317687988\n",
      "\t Training loss (single batch): 1.3670254945755005\n",
      "\t Training loss (single batch): 0.9523493051528931\n",
      "\t Training loss (single batch): 1.0018937587738037\n",
      "\t Training loss (single batch): 1.0088465213775635\n",
      "\t Training loss (single batch): 1.1931744813919067\n",
      "\t Training loss (single batch): 1.5423449277877808\n",
      "\t Training loss (single batch): 1.5403393507003784\n",
      "\t Training loss (single batch): 0.9689777493476868\n",
      "\t Training loss (single batch): 1.4536168575286865\n",
      "\t Training loss (single batch): 1.158639669418335\n",
      "\t Training loss (single batch): 1.5076185464859009\n",
      "\t Training loss (single batch): 1.427789330482483\n",
      "\t Training loss (single batch): 0.8084829449653625\n",
      "\t Training loss (single batch): 1.2499812841415405\n",
      "\t Training loss (single batch): 1.0858739614486694\n",
      "\t Training loss (single batch): 0.9324595928192139\n",
      "\t Training loss (single batch): 1.5623486042022705\n",
      "\t Training loss (single batch): 1.1136173009872437\n",
      "\t Training loss (single batch): 0.6694548726081848\n",
      "\t Training loss (single batch): 1.3963381052017212\n",
      "\t Training loss (single batch): 0.9823387861251831\n",
      "\t Training loss (single batch): 1.1226873397827148\n",
      "\t Training loss (single batch): 1.2404478788375854\n",
      "\t Training loss (single batch): 1.8697481155395508\n",
      "\t Training loss (single batch): 1.3193230628967285\n",
      "\t Training loss (single batch): 1.62204110622406\n",
      "\t Training loss (single batch): 0.8779630064964294\n",
      "\t Training loss (single batch): 1.4233487844467163\n",
      "\t Training loss (single batch): 1.119592547416687\n",
      "\t Training loss (single batch): 1.2471905946731567\n",
      "\t Training loss (single batch): 1.5788037776947021\n",
      "\t Training loss (single batch): 1.2348552942276\n",
      "\t Training loss (single batch): 0.7811444401741028\n",
      "\t Training loss (single batch): 1.1741580963134766\n",
      "\t Training loss (single batch): 0.9888908863067627\n",
      "\t Training loss (single batch): 1.436076045036316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1249940395355225\n",
      "\t Training loss (single batch): 1.2296619415283203\n",
      "\t Training loss (single batch): 1.4403609037399292\n",
      "\t Training loss (single batch): 0.9305828213691711\n",
      "\t Training loss (single batch): 1.118041753768921\n",
      "\t Training loss (single batch): 1.4214065074920654\n",
      "\t Training loss (single batch): 1.5342655181884766\n",
      "\t Training loss (single batch): 1.4709477424621582\n",
      "\t Training loss (single batch): 0.7092606425285339\n",
      "\t Training loss (single batch): 1.5992921590805054\n",
      "\t Training loss (single batch): 1.1213352680206299\n",
      "\t Training loss (single batch): 0.9778032898902893\n",
      "\t Training loss (single batch): 1.2177554368972778\n",
      "\t Training loss (single batch): 1.174350380897522\n",
      "\t Training loss (single batch): 1.0875506401062012\n",
      "\t Training loss (single batch): 1.5561432838439941\n",
      "\t Training loss (single batch): 1.3095682859420776\n",
      "\t Training loss (single batch): 0.8918784856796265\n",
      "\t Training loss (single batch): 1.9187331199645996\n",
      "\t Training loss (single batch): 1.1391127109527588\n",
      "\t Training loss (single batch): 1.1840637922286987\n",
      "\t Training loss (single batch): 1.328104019165039\n",
      "\t Training loss (single batch): 1.6722512245178223\n",
      "\t Training loss (single batch): 1.166441798210144\n",
      "\t Training loss (single batch): 1.1560840606689453\n",
      "\t Training loss (single batch): 1.7905844449996948\n",
      "\t Training loss (single batch): 1.1617300510406494\n",
      "\t Training loss (single batch): 1.467230200767517\n",
      "\t Training loss (single batch): 1.5542515516281128\n",
      "\t Training loss (single batch): 1.1377007961273193\n",
      "\t Training loss (single batch): 1.4785317182540894\n",
      "\t Training loss (single batch): 1.0276637077331543\n",
      "\t Training loss (single batch): 1.4317094087600708\n",
      "\t Training loss (single batch): 1.0645737648010254\n",
      "\t Training loss (single batch): 1.3215551376342773\n",
      "\t Training loss (single batch): 1.2133500576019287\n",
      "\t Training loss (single batch): 1.262305498123169\n",
      "\t Training loss (single batch): 1.194645643234253\n",
      "\t Training loss (single batch): 1.0611110925674438\n",
      "\t Training loss (single batch): 0.8795820474624634\n",
      "\t Training loss (single batch): 1.1592724323272705\n",
      "\t Training loss (single batch): 1.2509582042694092\n",
      "\t Training loss (single batch): 1.2873101234436035\n",
      "\t Training loss (single batch): 1.0939377546310425\n",
      "\t Training loss (single batch): 1.2768170833587646\n",
      "\t Training loss (single batch): 1.486604928970337\n",
      "\t Training loss (single batch): 1.2600758075714111\n",
      "\t Training loss (single batch): 1.9901487827301025\n",
      "\t Training loss (single batch): 1.1143243312835693\n",
      "\t Training loss (single batch): 1.0507593154907227\n",
      "\t Training loss (single batch): 1.3738025426864624\n",
      "\t Training loss (single batch): 1.1562579870224\n",
      "\t Training loss (single batch): 1.114900827407837\n",
      "\t Training loss (single batch): 1.465657114982605\n",
      "\t Training loss (single batch): 1.2829928398132324\n",
      "\t Training loss (single batch): 1.1040540933609009\n",
      "\t Training loss (single batch): 1.3026992082595825\n",
      "\t Training loss (single batch): 1.259043574333191\n",
      "\t Training loss (single batch): 1.4995641708374023\n",
      "\t Training loss (single batch): 1.1280888319015503\n",
      "\t Training loss (single batch): 0.7991756200790405\n",
      "\t Training loss (single batch): 1.037873387336731\n",
      "\t Training loss (single batch): 1.18850576877594\n",
      "\t Training loss (single batch): 1.4720088243484497\n",
      "\t Training loss (single batch): 1.2083117961883545\n",
      "\t Training loss (single batch): 1.4265779256820679\n",
      "\t Training loss (single batch): 1.2321537733078003\n",
      "\t Training loss (single batch): 1.3140692710876465\n",
      "\t Training loss (single batch): 1.060380220413208\n",
      "\t Training loss (single batch): 0.9942365288734436\n",
      "\t Training loss (single batch): 0.9228172898292542\n",
      "\t Training loss (single batch): 1.4990977048873901\n",
      "\t Training loss (single batch): 0.9231048226356506\n",
      "\t Training loss (single batch): 0.8514130711555481\n",
      "\t Training loss (single batch): 0.8341597318649292\n",
      "\t Training loss (single batch): 1.3477749824523926\n",
      "\t Training loss (single batch): 1.5721715688705444\n",
      "\t Training loss (single batch): 1.511669397354126\n",
      "\t Training loss (single batch): 0.9309242367744446\n",
      "\t Training loss (single batch): 1.2988500595092773\n",
      "\t Training loss (single batch): 1.7931162118911743\n",
      "\t Training loss (single batch): 1.4355179071426392\n",
      "\t Training loss (single batch): 1.13508141040802\n",
      "\t Training loss (single batch): 1.1698122024536133\n",
      "\t Training loss (single batch): 0.7741702795028687\n",
      "\t Training loss (single batch): 1.4109433889389038\n",
      "\t Training loss (single batch): 1.6445884704589844\n",
      "\t Training loss (single batch): 1.551127314567566\n",
      "\t Training loss (single batch): 1.1681420803070068\n",
      "\t Training loss (single batch): 0.8289884328842163\n",
      "\t Training loss (single batch): 1.166740894317627\n",
      "\t Training loss (single batch): 1.162992000579834\n",
      "\t Training loss (single batch): 0.9778079986572266\n",
      "\t Training loss (single batch): 0.9504187703132629\n",
      "\t Training loss (single batch): 1.0075476169586182\n",
      "\t Training loss (single batch): 1.131957769393921\n",
      "\t Training loss (single batch): 1.095944881439209\n",
      "\t Training loss (single batch): 1.275275707244873\n",
      "\t Training loss (single batch): 1.2339175939559937\n",
      "\t Training loss (single batch): 0.992615818977356\n",
      "\t Training loss (single batch): 1.4803348779678345\n",
      "\t Training loss (single batch): 1.4099360704421997\n",
      "\t Training loss (single batch): 1.2312161922454834\n",
      "\t Training loss (single batch): 1.2258031368255615\n",
      "\t Training loss (single batch): 0.9120771884918213\n",
      "\t Training loss (single batch): 1.0711143016815186\n",
      "\t Training loss (single batch): 1.4955766201019287\n",
      "\t Training loss (single batch): 1.1893421411514282\n",
      "\t Training loss (single batch): 0.958367645740509\n",
      "\t Training loss (single batch): 1.0519589185714722\n",
      "\t Training loss (single batch): 1.2885584831237793\n",
      "\t Training loss (single batch): 1.4095977544784546\n",
      "\t Training loss (single batch): 0.9712799191474915\n",
      "\t Training loss (single batch): 0.818669855594635\n",
      "\t Training loss (single batch): 1.169742465019226\n",
      "\t Training loss (single batch): 1.6079919338226318\n",
      "\t Training loss (single batch): 1.171078085899353\n",
      "\t Training loss (single batch): 0.9294578433036804\n",
      "\t Training loss (single batch): 1.0395958423614502\n",
      "\t Training loss (single batch): 1.7225360870361328\n",
      "\t Training loss (single batch): 1.3546737432479858\n",
      "\t Training loss (single batch): 1.222576379776001\n",
      "\t Training loss (single batch): 1.4377778768539429\n",
      "\t Training loss (single batch): 1.7182304859161377\n",
      "\t Training loss (single batch): 1.5375114679336548\n",
      "\t Training loss (single batch): 1.2322585582733154\n",
      "\t Training loss (single batch): 1.2892416715621948\n",
      "\t Training loss (single batch): 1.6987130641937256\n",
      "\t Training loss (single batch): 1.7540520429611206\n",
      "\t Training loss (single batch): 1.0023810863494873\n",
      "\t Training loss (single batch): 1.1410621404647827\n",
      "\t Training loss (single batch): 1.491848349571228\n",
      "\t Training loss (single batch): 1.60099458694458\n",
      "\t Training loss (single batch): 1.4236987829208374\n",
      "\t Training loss (single batch): 0.9861409068107605\n",
      "\t Training loss (single batch): 0.9819865822792053\n",
      "\t Training loss (single batch): 1.6140133142471313\n",
      "\t Training loss (single batch): 1.007596492767334\n",
      "\t Training loss (single batch): 1.1152515411376953\n",
      "\t Training loss (single batch): 0.7875065207481384\n",
      "\t Training loss (single batch): 1.0485570430755615\n",
      "\t Training loss (single batch): 1.448135256767273\n",
      "\t Training loss (single batch): 0.8820943832397461\n",
      "\t Training loss (single batch): 0.9738118052482605\n",
      "\t Training loss (single batch): 1.4777556657791138\n",
      "\t Training loss (single batch): 1.2518465518951416\n",
      "\t Training loss (single batch): 1.101929783821106\n",
      "\t Training loss (single batch): 1.7432459592819214\n",
      "\t Training loss (single batch): 0.9056715965270996\n",
      "\t Training loss (single batch): 1.3720710277557373\n",
      "\t Training loss (single batch): 1.2514538764953613\n",
      "\t Training loss (single batch): 1.600229263305664\n",
      "\t Training loss (single batch): 0.9792296290397644\n",
      "\t Training loss (single batch): 1.0559678077697754\n",
      "\t Training loss (single batch): 1.8565218448638916\n",
      "\t Training loss (single batch): 1.7663196325302124\n",
      "\t Training loss (single batch): 1.199028730392456\n",
      "\t Training loss (single batch): 1.2759876251220703\n",
      "\t Training loss (single batch): 1.3691872358322144\n",
      "\t Training loss (single batch): 1.154718279838562\n",
      "\t Training loss (single batch): 0.9507954716682434\n",
      "\t Training loss (single batch): 1.0391757488250732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9554178714752197\n",
      "\t Training loss (single batch): 1.4007028341293335\n",
      "\t Training loss (single batch): 1.4717129468917847\n",
      "\t Training loss (single batch): 0.868675172328949\n",
      "\t Training loss (single batch): 0.9650440216064453\n",
      "\t Training loss (single batch): 0.9656899571418762\n",
      "\t Training loss (single batch): 1.1013522148132324\n",
      "\t Training loss (single batch): 0.9736601114273071\n",
      "\t Training loss (single batch): 1.148856282234192\n",
      "\t Training loss (single batch): 1.2529489994049072\n",
      "\t Training loss (single batch): 1.392700433731079\n",
      "\t Training loss (single batch): 1.6595028638839722\n",
      "\t Training loss (single batch): 1.277904748916626\n",
      "\t Training loss (single batch): 1.344672679901123\n",
      "\t Training loss (single batch): 1.6238421201705933\n",
      "\t Training loss (single batch): 1.0646858215332031\n",
      "\t Training loss (single batch): 1.5180219411849976\n",
      "\t Training loss (single batch): 1.2264946699142456\n",
      "\t Training loss (single batch): 1.4907453060150146\n",
      "\t Training loss (single batch): 1.5640333890914917\n",
      "\t Training loss (single batch): 0.9625288248062134\n",
      "\t Training loss (single batch): 0.7939440011978149\n",
      "\t Training loss (single batch): 1.104438304901123\n",
      "\t Training loss (single batch): 1.0821532011032104\n",
      "\t Training loss (single batch): 2.35504150390625\n",
      "##################################\n",
      "## EPOCH 57\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8774462342262268\n",
      "\t Training loss (single batch): 1.1111313104629517\n",
      "\t Training loss (single batch): 1.104475975036621\n",
      "\t Training loss (single batch): 1.543379545211792\n",
      "\t Training loss (single batch): 1.3054498434066772\n",
      "\t Training loss (single batch): 1.146233081817627\n",
      "\t Training loss (single batch): 1.6229747533798218\n",
      "\t Training loss (single batch): 1.3961378335952759\n",
      "\t Training loss (single batch): 1.0589778423309326\n",
      "\t Training loss (single batch): 0.8532925248146057\n",
      "\t Training loss (single batch): 1.5652520656585693\n",
      "\t Training loss (single batch): 1.155231237411499\n",
      "\t Training loss (single batch): 1.0631459951400757\n",
      "\t Training loss (single batch): 0.9443439841270447\n",
      "\t Training loss (single batch): 1.780919075012207\n",
      "\t Training loss (single batch): 1.1422301530838013\n",
      "\t Training loss (single batch): 1.2059154510498047\n",
      "\t Training loss (single batch): 1.270896553993225\n",
      "\t Training loss (single batch): 1.9762767553329468\n",
      "\t Training loss (single batch): 1.1502199172973633\n",
      "\t Training loss (single batch): 1.3280718326568604\n",
      "\t Training loss (single batch): 0.744676947593689\n",
      "\t Training loss (single batch): 1.507519006729126\n",
      "\t Training loss (single batch): 1.3239223957061768\n",
      "\t Training loss (single batch): 1.2107264995574951\n",
      "\t Training loss (single batch): 1.3946380615234375\n",
      "\t Training loss (single batch): 1.1384962797164917\n",
      "\t Training loss (single batch): 0.9553689956665039\n",
      "\t Training loss (single batch): 1.0722640752792358\n",
      "\t Training loss (single batch): 1.1370331048965454\n",
      "\t Training loss (single batch): 0.9409757256507874\n",
      "\t Training loss (single batch): 0.7611340880393982\n",
      "\t Training loss (single batch): 0.9107524156570435\n",
      "\t Training loss (single batch): 0.8947499990463257\n",
      "\t Training loss (single batch): 1.2675281763076782\n",
      "\t Training loss (single batch): 1.127677083015442\n",
      "\t Training loss (single batch): 1.492992877960205\n",
      "\t Training loss (single batch): 1.1738135814666748\n",
      "\t Training loss (single batch): 1.581347107887268\n",
      "\t Training loss (single batch): 1.4260896444320679\n",
      "\t Training loss (single batch): 1.4897810220718384\n",
      "\t Training loss (single batch): 1.2936327457427979\n",
      "\t Training loss (single batch): 1.2171833515167236\n",
      "\t Training loss (single batch): 1.3605570793151855\n",
      "\t Training loss (single batch): 1.544814109802246\n",
      "\t Training loss (single batch): 1.614261507987976\n",
      "\t Training loss (single batch): 1.1552108526229858\n",
      "\t Training loss (single batch): 1.2094306945800781\n",
      "\t Training loss (single batch): 1.22446608543396\n",
      "\t Training loss (single batch): 0.7626367807388306\n",
      "\t Training loss (single batch): 1.0961023569107056\n",
      "\t Training loss (single batch): 1.2488073110580444\n",
      "\t Training loss (single batch): 1.4788070917129517\n",
      "\t Training loss (single batch): 1.3408265113830566\n",
      "\t Training loss (single batch): 0.9477731585502625\n",
      "\t Training loss (single batch): 0.9443309307098389\n",
      "\t Training loss (single batch): 1.176465392112732\n",
      "\t Training loss (single batch): 1.213349461555481\n",
      "\t Training loss (single batch): 1.0825144052505493\n",
      "\t Training loss (single batch): 1.309468150138855\n",
      "\t Training loss (single batch): 0.8632067441940308\n",
      "\t Training loss (single batch): 0.9672879576683044\n",
      "\t Training loss (single batch): 1.1216868162155151\n",
      "\t Training loss (single batch): 1.0293530225753784\n",
      "\t Training loss (single batch): 1.291144609451294\n",
      "\t Training loss (single batch): 1.4103883504867554\n",
      "\t Training loss (single batch): 1.0997391939163208\n",
      "\t Training loss (single batch): 1.3239785432815552\n",
      "\t Training loss (single batch): 1.2304186820983887\n",
      "\t Training loss (single batch): 1.141895055770874\n",
      "\t Training loss (single batch): 0.8748375177383423\n",
      "\t Training loss (single batch): 1.2849328517913818\n",
      "\t Training loss (single batch): 0.7904199361801147\n",
      "\t Training loss (single batch): 1.1556648015975952\n",
      "\t Training loss (single batch): 1.8390411138534546\n",
      "\t Training loss (single batch): 0.8569467067718506\n",
      "\t Training loss (single batch): 1.1359769105911255\n",
      "\t Training loss (single batch): 1.2819914817810059\n",
      "\t Training loss (single batch): 0.7347197532653809\n",
      "\t Training loss (single batch): 1.2313181161880493\n",
      "\t Training loss (single batch): 1.2963637113571167\n",
      "\t Training loss (single batch): 1.0473653078079224\n",
      "\t Training loss (single batch): 1.2714831829071045\n",
      "\t Training loss (single batch): 0.6343651413917542\n",
      "\t Training loss (single batch): 1.1610356569290161\n",
      "\t Training loss (single batch): 0.8759382367134094\n",
      "\t Training loss (single batch): 1.4898226261138916\n",
      "\t Training loss (single batch): 1.4298584461212158\n",
      "\t Training loss (single batch): 0.8736791610717773\n",
      "\t Training loss (single batch): 1.1038172245025635\n",
      "\t Training loss (single batch): 1.383047342300415\n",
      "\t Training loss (single batch): 1.3043924570083618\n",
      "\t Training loss (single batch): 1.0923199653625488\n",
      "\t Training loss (single batch): 1.0608179569244385\n",
      "\t Training loss (single batch): 1.4260095357894897\n",
      "\t Training loss (single batch): 1.254631519317627\n",
      "\t Training loss (single batch): 1.071600079536438\n",
      "\t Training loss (single batch): 1.263908863067627\n",
      "\t Training loss (single batch): 1.1158158779144287\n",
      "\t Training loss (single batch): 0.7469649910926819\n",
      "\t Training loss (single batch): 1.1724472045898438\n",
      "\t Training loss (single batch): 1.2708536386489868\n",
      "\t Training loss (single batch): 0.7664352655410767\n",
      "\t Training loss (single batch): 0.6326339244842529\n",
      "\t Training loss (single batch): 1.5128792524337769\n",
      "\t Training loss (single batch): 1.2027307748794556\n",
      "\t Training loss (single batch): 1.1773501634597778\n",
      "\t Training loss (single batch): 0.8387104868888855\n",
      "\t Training loss (single batch): 0.9822830557823181\n",
      "\t Training loss (single batch): 1.5555446147918701\n",
      "\t Training loss (single batch): 0.9395270347595215\n",
      "\t Training loss (single batch): 1.3017081022262573\n",
      "\t Training loss (single batch): 1.2527786493301392\n",
      "\t Training loss (single batch): 1.2173129320144653\n",
      "\t Training loss (single batch): 1.7660988569259644\n",
      "\t Training loss (single batch): 1.531637191772461\n",
      "\t Training loss (single batch): 1.500603437423706\n",
      "\t Training loss (single batch): 1.8722965717315674\n",
      "\t Training loss (single batch): 1.5422437191009521\n",
      "\t Training loss (single batch): 1.6149282455444336\n",
      "\t Training loss (single batch): 0.787885844707489\n",
      "\t Training loss (single batch): 0.9675148725509644\n",
      "\t Training loss (single batch): 0.923039436340332\n",
      "\t Training loss (single batch): 1.0678666830062866\n",
      "\t Training loss (single batch): 1.3231128454208374\n",
      "\t Training loss (single batch): 1.029376745223999\n",
      "\t Training loss (single batch): 1.524613380432129\n",
      "\t Training loss (single batch): 1.350548505783081\n",
      "\t Training loss (single batch): 1.6128672361373901\n",
      "\t Training loss (single batch): 1.3343935012817383\n",
      "\t Training loss (single batch): 0.8595263361930847\n",
      "\t Training loss (single batch): 1.0387781858444214\n",
      "\t Training loss (single batch): 1.654115915298462\n",
      "\t Training loss (single batch): 1.2051770687103271\n",
      "\t Training loss (single batch): 1.2206860780715942\n",
      "\t Training loss (single batch): 1.600947618484497\n",
      "\t Training loss (single batch): 1.1608810424804688\n",
      "\t Training loss (single batch): 1.0369837284088135\n",
      "\t Training loss (single batch): 1.459843635559082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8049632906913757\n",
      "\t Training loss (single batch): 1.5347468852996826\n",
      "\t Training loss (single batch): 1.015613317489624\n",
      "\t Training loss (single batch): 1.3761658668518066\n",
      "\t Training loss (single batch): 0.9523242712020874\n",
      "\t Training loss (single batch): 1.1549524068832397\n",
      "\t Training loss (single batch): 1.5312591791152954\n",
      "\t Training loss (single batch): 0.9492781758308411\n",
      "\t Training loss (single batch): 2.0364091396331787\n",
      "\t Training loss (single batch): 1.5569920539855957\n",
      "\t Training loss (single batch): 1.1019178628921509\n",
      "\t Training loss (single batch): 1.0446135997772217\n",
      "\t Training loss (single batch): 1.7026458978652954\n",
      "\t Training loss (single batch): 1.292816162109375\n",
      "\t Training loss (single batch): 0.8119761943817139\n",
      "\t Training loss (single batch): 0.8664388060569763\n",
      "\t Training loss (single batch): 1.4479162693023682\n",
      "\t Training loss (single batch): 0.9154965877532959\n",
      "\t Training loss (single batch): 1.1654205322265625\n",
      "\t Training loss (single batch): 1.021793246269226\n",
      "\t Training loss (single batch): 0.851231038570404\n",
      "\t Training loss (single batch): 0.9578484892845154\n",
      "\t Training loss (single batch): 1.1313409805297852\n",
      "\t Training loss (single batch): 0.8878370523452759\n",
      "\t Training loss (single batch): 1.1091697216033936\n",
      "\t Training loss (single batch): 0.9604775309562683\n",
      "\t Training loss (single batch): 1.0998764038085938\n",
      "\t Training loss (single batch): 1.383148193359375\n",
      "\t Training loss (single batch): 1.1909914016723633\n",
      "\t Training loss (single batch): 1.2764737606048584\n",
      "\t Training loss (single batch): 1.2763633728027344\n",
      "\t Training loss (single batch): 1.242917537689209\n",
      "\t Training loss (single batch): 0.882483184337616\n",
      "\t Training loss (single batch): 1.133766770362854\n",
      "\t Training loss (single batch): 1.1092913150787354\n",
      "\t Training loss (single batch): 0.9651934504508972\n",
      "\t Training loss (single batch): 0.9433525204658508\n",
      "\t Training loss (single batch): 1.1564199924468994\n",
      "\t Training loss (single batch): 1.331529974937439\n",
      "\t Training loss (single batch): 1.0379561185836792\n",
      "\t Training loss (single batch): 1.1596723794937134\n",
      "\t Training loss (single batch): 1.6480311155319214\n",
      "\t Training loss (single batch): 1.4317245483398438\n",
      "\t Training loss (single batch): 1.0087096691131592\n",
      "\t Training loss (single batch): 1.1138676404953003\n",
      "\t Training loss (single batch): 1.2689498662948608\n",
      "\t Training loss (single batch): 1.3889389038085938\n",
      "\t Training loss (single batch): 0.856892466545105\n",
      "\t Training loss (single batch): 1.2914315462112427\n",
      "\t Training loss (single batch): 1.5196189880371094\n",
      "\t Training loss (single batch): 1.1067867279052734\n",
      "\t Training loss (single batch): 1.1177688837051392\n",
      "\t Training loss (single batch): 1.6326336860656738\n",
      "\t Training loss (single batch): 1.3185451030731201\n",
      "\t Training loss (single batch): 1.2552114725112915\n",
      "\t Training loss (single batch): 1.1165558099746704\n",
      "\t Training loss (single batch): 1.1251524686813354\n",
      "\t Training loss (single batch): 0.8649427890777588\n",
      "\t Training loss (single batch): 1.1386255025863647\n",
      "\t Training loss (single batch): 0.8866424560546875\n",
      "\t Training loss (single batch): 1.3539564609527588\n",
      "\t Training loss (single batch): 1.0981295108795166\n",
      "\t Training loss (single batch): 1.2887142896652222\n",
      "\t Training loss (single batch): 1.4870394468307495\n",
      "\t Training loss (single batch): 0.8708871603012085\n",
      "\t Training loss (single batch): 1.6614283323287964\n",
      "\t Training loss (single batch): 0.8737767338752747\n",
      "\t Training loss (single batch): 1.4571906328201294\n",
      "\t Training loss (single batch): 0.8941602110862732\n",
      "\t Training loss (single batch): 1.5430550575256348\n",
      "\t Training loss (single batch): 0.9498165845870972\n",
      "\t Training loss (single batch): 1.1990712881088257\n",
      "\t Training loss (single batch): 1.2571876049041748\n",
      "\t Training loss (single batch): 1.7216229438781738\n",
      "\t Training loss (single batch): 1.2686105966567993\n",
      "\t Training loss (single batch): 1.0397019386291504\n",
      "\t Training loss (single batch): 0.9740778207778931\n",
      "\t Training loss (single batch): 1.544613242149353\n",
      "\t Training loss (single batch): 1.2980858087539673\n",
      "\t Training loss (single batch): 1.4667142629623413\n",
      "\t Training loss (single batch): 1.1022770404815674\n",
      "\t Training loss (single batch): 0.9183610081672668\n",
      "\t Training loss (single batch): 1.0119954347610474\n",
      "\t Training loss (single batch): 1.5161174535751343\n",
      "\t Training loss (single batch): 1.0489261150360107\n",
      "\t Training loss (single batch): 1.1966919898986816\n",
      "\t Training loss (single batch): 1.3827276229858398\n",
      "\t Training loss (single batch): 2.0728728771209717\n",
      "\t Training loss (single batch): 0.9034290909767151\n",
      "\t Training loss (single batch): 1.0147931575775146\n",
      "\t Training loss (single batch): 1.1516916751861572\n",
      "\t Training loss (single batch): 1.0288305282592773\n",
      "\t Training loss (single batch): 1.481749176979065\n",
      "\t Training loss (single batch): 0.8468080163002014\n",
      "\t Training loss (single batch): 1.026463508605957\n",
      "\t Training loss (single batch): 1.1925381422042847\n",
      "\t Training loss (single batch): 1.3424351215362549\n",
      "\t Training loss (single batch): 1.4165616035461426\n",
      "\t Training loss (single batch): 1.8005365133285522\n",
      "\t Training loss (single batch): 0.919439971446991\n",
      "\t Training loss (single batch): 1.4026628732681274\n",
      "\t Training loss (single batch): 1.6297129392623901\n",
      "\t Training loss (single batch): 1.627961277961731\n",
      "\t Training loss (single batch): 1.6406351327896118\n",
      "\t Training loss (single batch): 1.0149489641189575\n",
      "\t Training loss (single batch): 0.9153155088424683\n",
      "\t Training loss (single batch): 1.026310682296753\n",
      "\t Training loss (single batch): 1.0503907203674316\n",
      "\t Training loss (single batch): 0.9477572441101074\n",
      "\t Training loss (single batch): 0.8539704084396362\n",
      "\t Training loss (single batch): 1.1126607656478882\n",
      "\t Training loss (single batch): 1.383421778678894\n",
      "\t Training loss (single batch): 0.7723349928855896\n",
      "\t Training loss (single batch): 0.9748890399932861\n",
      "\t Training loss (single batch): 1.062140941619873\n",
      "\t Training loss (single batch): 1.2660651206970215\n",
      "\t Training loss (single batch): 1.1048201322555542\n",
      "\t Training loss (single batch): 1.188366413116455\n",
      "\t Training loss (single batch): 1.0113753080368042\n",
      "\t Training loss (single batch): 1.18574059009552\n",
      "\t Training loss (single batch): 0.9184064269065857\n",
      "\t Training loss (single batch): 1.5649901628494263\n",
      "\t Training loss (single batch): 0.849547803401947\n",
      "\t Training loss (single batch): 1.356532096862793\n",
      "\t Training loss (single batch): 1.480334758758545\n",
      "\t Training loss (single batch): 1.4645633697509766\n",
      "\t Training loss (single batch): 1.4950084686279297\n",
      "\t Training loss (single batch): 1.2345836162567139\n",
      "\t Training loss (single batch): 0.9172327518463135\n",
      "\t Training loss (single batch): 0.8823152780532837\n",
      "\t Training loss (single batch): 0.9260240197181702\n",
      "\t Training loss (single batch): 1.4111720323562622\n",
      "\t Training loss (single batch): 1.4873017072677612\n",
      "\t Training loss (single batch): 1.4456076622009277\n",
      "\t Training loss (single batch): 1.40223228931427\n",
      "\t Training loss (single batch): 0.7955422401428223\n",
      "\t Training loss (single batch): 1.5077215433120728\n",
      "\t Training loss (single batch): 1.3850538730621338\n",
      "\t Training loss (single batch): 1.0421063899993896\n",
      "\t Training loss (single batch): 1.852906346321106\n",
      "\t Training loss (single batch): 1.800185203552246\n",
      "\t Training loss (single batch): 1.5852532386779785\n",
      "\t Training loss (single batch): 1.2180712223052979\n",
      "\t Training loss (single batch): 1.2540987730026245\n",
      "\t Training loss (single batch): 1.3204883337020874\n",
      "\t Training loss (single batch): 1.035685420036316\n",
      "\t Training loss (single batch): 1.4143041372299194\n",
      "\t Training loss (single batch): 1.1344268321990967\n",
      "\t Training loss (single batch): 1.417981743812561\n",
      "\t Training loss (single batch): 1.1052589416503906\n",
      "\t Training loss (single batch): 0.8819687962532043\n",
      "\t Training loss (single batch): 1.3529846668243408\n",
      "\t Training loss (single batch): 1.258119821548462\n",
      "\t Training loss (single batch): 1.2837591171264648\n",
      "\t Training loss (single batch): 1.6318427324295044\n",
      "\t Training loss (single batch): 1.4600961208343506\n",
      "\t Training loss (single batch): 1.461238145828247\n",
      "\t Training loss (single batch): 0.9072421789169312\n",
      "\t Training loss (single batch): 1.5466179847717285\n",
      "\t Training loss (single batch): 0.950188934803009\n",
      "\t Training loss (single batch): 1.1352801322937012\n",
      "\t Training loss (single batch): 0.7139831185340881\n",
      "\t Training loss (single batch): 1.3534018993377686\n",
      "\t Training loss (single batch): 0.8844748139381409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0787678956985474\n",
      "\t Training loss (single batch): 0.9651788473129272\n",
      "\t Training loss (single batch): 1.2855514287948608\n",
      "\t Training loss (single batch): 1.329759955406189\n",
      "\t Training loss (single batch): 1.3658828735351562\n",
      "\t Training loss (single batch): 0.9405649304389954\n",
      "\t Training loss (single batch): 1.3922948837280273\n",
      "\t Training loss (single batch): 1.8075149059295654\n",
      "\t Training loss (single batch): 1.5541598796844482\n",
      "\t Training loss (single batch): 1.3277435302734375\n",
      "\t Training loss (single batch): 1.2792258262634277\n",
      "\t Training loss (single batch): 1.0190969705581665\n",
      "\t Training loss (single batch): 1.2743470668792725\n",
      "\t Training loss (single batch): 2.7787492275238037\n",
      "##################################\n",
      "## EPOCH 58\n",
      "##################################\n",
      "\t Training loss (single batch): 1.578452467918396\n",
      "\t Training loss (single batch): 1.0757825374603271\n",
      "\t Training loss (single batch): 1.1829854249954224\n",
      "\t Training loss (single batch): 1.6031290292739868\n",
      "\t Training loss (single batch): 1.0984679460525513\n",
      "\t Training loss (single batch): 1.6260710954666138\n",
      "\t Training loss (single batch): 1.1425899267196655\n",
      "\t Training loss (single batch): 1.430173397064209\n",
      "\t Training loss (single batch): 1.1212013959884644\n",
      "\t Training loss (single batch): 1.3178188800811768\n",
      "\t Training loss (single batch): 0.9299440383911133\n",
      "\t Training loss (single batch): 1.1474381685256958\n",
      "\t Training loss (single batch): 1.1059770584106445\n",
      "\t Training loss (single batch): 0.8778746128082275\n",
      "\t Training loss (single batch): 0.764796257019043\n",
      "\t Training loss (single batch): 1.1009029150009155\n",
      "\t Training loss (single batch): 0.9286989569664001\n",
      "\t Training loss (single batch): 1.5015658140182495\n",
      "\t Training loss (single batch): 0.9742038249969482\n",
      "\t Training loss (single batch): 0.9166337251663208\n",
      "\t Training loss (single batch): 1.3074201345443726\n",
      "\t Training loss (single batch): 1.007353663444519\n",
      "\t Training loss (single batch): 1.1367968320846558\n",
      "\t Training loss (single batch): 1.3258286714553833\n",
      "\t Training loss (single batch): 1.0178505182266235\n",
      "\t Training loss (single batch): 1.3021867275238037\n",
      "\t Training loss (single batch): 0.8174995183944702\n",
      "\t Training loss (single batch): 1.2716447114944458\n",
      "\t Training loss (single batch): 1.3578535318374634\n",
      "\t Training loss (single batch): 1.052582859992981\n",
      "\t Training loss (single batch): 1.3192394971847534\n",
      "\t Training loss (single batch): 0.9669913649559021\n",
      "\t Training loss (single batch): 1.438855528831482\n",
      "\t Training loss (single batch): 0.7564735412597656\n",
      "\t Training loss (single batch): 1.1976337432861328\n",
      "\t Training loss (single batch): 1.3308322429656982\n",
      "\t Training loss (single batch): 1.488571047782898\n",
      "\t Training loss (single batch): 1.2095357179641724\n",
      "\t Training loss (single batch): 1.2840025424957275\n",
      "\t Training loss (single batch): 0.7386719584465027\n",
      "\t Training loss (single batch): 0.8624808192253113\n",
      "\t Training loss (single batch): 1.43864107131958\n",
      "\t Training loss (single batch): 1.3182326555252075\n",
      "\t Training loss (single batch): 1.06136953830719\n",
      "\t Training loss (single batch): 1.304465413093567\n",
      "\t Training loss (single batch): 1.002805233001709\n",
      "\t Training loss (single batch): 1.8065640926361084\n",
      "\t Training loss (single batch): 1.7410706281661987\n",
      "\t Training loss (single batch): 0.8172858357429504\n",
      "\t Training loss (single batch): 1.254124641418457\n",
      "\t Training loss (single batch): 1.465315580368042\n",
      "\t Training loss (single batch): 0.7444449663162231\n",
      "\t Training loss (single batch): 0.7216329574584961\n",
      "\t Training loss (single batch): 0.9162203073501587\n",
      "\t Training loss (single batch): 1.316057562828064\n",
      "\t Training loss (single batch): 1.131107211112976\n",
      "\t Training loss (single batch): 0.9983220100402832\n",
      "\t Training loss (single batch): 1.1222100257873535\n",
      "\t Training loss (single batch): 1.2480604648590088\n",
      "\t Training loss (single batch): 1.146652340888977\n",
      "\t Training loss (single batch): 1.0395070314407349\n",
      "\t Training loss (single batch): 0.5602867603302002\n",
      "\t Training loss (single batch): 1.4871896505355835\n",
      "\t Training loss (single batch): 1.489128589630127\n",
      "\t Training loss (single batch): 1.3985674381256104\n",
      "\t Training loss (single batch): 1.195505142211914\n",
      "\t Training loss (single batch): 1.2948037385940552\n",
      "\t Training loss (single batch): 0.8100410103797913\n",
      "\t Training loss (single batch): 1.473260760307312\n",
      "\t Training loss (single batch): 1.0681862831115723\n",
      "\t Training loss (single batch): 1.303388237953186\n",
      "\t Training loss (single batch): 1.1926047801971436\n",
      "\t Training loss (single batch): 1.0997341871261597\n",
      "\t Training loss (single batch): 1.467873215675354\n",
      "\t Training loss (single batch): 1.0712072849273682\n",
      "\t Training loss (single batch): 1.356553316116333\n",
      "\t Training loss (single batch): 1.4766240119934082\n",
      "\t Training loss (single batch): 1.4528120756149292\n",
      "\t Training loss (single batch): 1.076267957687378\n",
      "\t Training loss (single batch): 1.5548521280288696\n",
      "\t Training loss (single batch): 0.894758403301239\n",
      "\t Training loss (single batch): 1.611923098564148\n",
      "\t Training loss (single batch): 1.3460536003112793\n",
      "\t Training loss (single batch): 1.2677010297775269\n",
      "\t Training loss (single batch): 1.530764102935791\n",
      "\t Training loss (single batch): 1.3059804439544678\n",
      "\t Training loss (single batch): 0.8252882957458496\n",
      "\t Training loss (single batch): 1.3375205993652344\n",
      "\t Training loss (single batch): 1.019176959991455\n",
      "\t Training loss (single batch): 1.4444327354431152\n",
      "\t Training loss (single batch): 1.1347042322158813\n",
      "\t Training loss (single batch): 0.9559629559516907\n",
      "\t Training loss (single batch): 1.0860506296157837\n",
      "\t Training loss (single batch): 0.7461888790130615\n",
      "\t Training loss (single batch): 0.8845822811126709\n",
      "\t Training loss (single batch): 1.2554707527160645\n",
      "\t Training loss (single batch): 1.4168586730957031\n",
      "\t Training loss (single batch): 0.8115581274032593\n",
      "\t Training loss (single batch): 0.9199957847595215\n",
      "\t Training loss (single batch): 0.9848644137382507\n",
      "\t Training loss (single batch): 0.9673109650611877\n",
      "\t Training loss (single batch): 1.2056314945220947\n",
      "\t Training loss (single batch): 1.0051772594451904\n",
      "\t Training loss (single batch): 0.8700661063194275\n",
      "\t Training loss (single batch): 0.9526969194412231\n",
      "\t Training loss (single batch): 1.7961654663085938\n",
      "\t Training loss (single batch): 1.6576141119003296\n",
      "\t Training loss (single batch): 0.9917364716529846\n",
      "\t Training loss (single batch): 1.096620798110962\n",
      "\t Training loss (single batch): 1.410958170890808\n",
      "\t Training loss (single batch): 1.0795913934707642\n",
      "\t Training loss (single batch): 1.0839083194732666\n",
      "\t Training loss (single batch): 0.5842099785804749\n",
      "\t Training loss (single batch): 0.8109661936759949\n",
      "\t Training loss (single batch): 1.1876782178878784\n",
      "\t Training loss (single batch): 1.7910423278808594\n",
      "\t Training loss (single batch): 1.2324109077453613\n",
      "\t Training loss (single batch): 1.166408896446228\n",
      "\t Training loss (single batch): 1.1169980764389038\n",
      "\t Training loss (single batch): 1.4102638959884644\n",
      "\t Training loss (single batch): 0.7931742668151855\n",
      "\t Training loss (single batch): 1.2091044187545776\n",
      "\t Training loss (single batch): 1.201398491859436\n",
      "\t Training loss (single batch): 0.9855380058288574\n",
      "\t Training loss (single batch): 1.4443252086639404\n",
      "\t Training loss (single batch): 1.128793716430664\n",
      "\t Training loss (single batch): 1.1462621688842773\n",
      "\t Training loss (single batch): 1.521571397781372\n",
      "\t Training loss (single batch): 1.262156367301941\n",
      "\t Training loss (single batch): 1.335579752922058\n",
      "\t Training loss (single batch): 1.3483521938323975\n",
      "\t Training loss (single batch): 1.1330974102020264\n",
      "\t Training loss (single batch): 1.6573442220687866\n",
      "\t Training loss (single batch): 1.3427700996398926\n",
      "\t Training loss (single batch): 2.4028873443603516\n",
      "\t Training loss (single batch): 1.1816534996032715\n",
      "\t Training loss (single batch): 0.9719963669776917\n",
      "\t Training loss (single batch): 1.4079017639160156\n",
      "\t Training loss (single batch): 1.5341652631759644\n",
      "\t Training loss (single batch): 1.0971884727478027\n",
      "\t Training loss (single batch): 1.7524973154067993\n",
      "\t Training loss (single batch): 0.9869019985198975\n",
      "\t Training loss (single batch): 1.6566859483718872\n",
      "\t Training loss (single batch): 1.2226319313049316\n",
      "\t Training loss (single batch): 1.3030833005905151\n",
      "\t Training loss (single batch): 1.4560374021530151\n",
      "\t Training loss (single batch): 1.3431452512741089\n",
      "\t Training loss (single batch): 1.332041621208191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0725288391113281\n",
      "\t Training loss (single batch): 1.2137130498886108\n",
      "\t Training loss (single batch): 1.3012417554855347\n",
      "\t Training loss (single batch): 1.3628430366516113\n",
      "\t Training loss (single batch): 0.7132771611213684\n",
      "\t Training loss (single batch): 1.1385067701339722\n",
      "\t Training loss (single batch): 1.2842276096343994\n",
      "\t Training loss (single batch): 1.1781502962112427\n",
      "\t Training loss (single batch): 1.4964481592178345\n",
      "\t Training loss (single batch): 1.5131721496582031\n",
      "\t Training loss (single batch): 1.3744920492172241\n",
      "\t Training loss (single batch): 1.1942437887191772\n",
      "\t Training loss (single batch): 1.9394599199295044\n",
      "\t Training loss (single batch): 1.2576059103012085\n",
      "\t Training loss (single batch): 1.2433247566223145\n",
      "\t Training loss (single batch): 1.541998267173767\n",
      "\t Training loss (single batch): 0.8847514986991882\n",
      "\t Training loss (single batch): 0.98566734790802\n",
      "\t Training loss (single batch): 1.2002887725830078\n",
      "\t Training loss (single batch): 0.9113830924034119\n",
      "\t Training loss (single batch): 0.8412365913391113\n",
      "\t Training loss (single batch): 1.6451143026351929\n",
      "\t Training loss (single batch): 0.7609267234802246\n",
      "\t Training loss (single batch): 1.0466827154159546\n",
      "\t Training loss (single batch): 0.8798931241035461\n",
      "\t Training loss (single batch): 0.9977753162384033\n",
      "\t Training loss (single batch): 1.1583073139190674\n",
      "\t Training loss (single batch): 1.0487607717514038\n",
      "\t Training loss (single batch): 1.0029995441436768\n",
      "\t Training loss (single batch): 1.4884668588638306\n",
      "\t Training loss (single batch): 1.2134952545166016\n",
      "\t Training loss (single batch): 0.9923039078712463\n",
      "\t Training loss (single batch): 1.2440295219421387\n",
      "\t Training loss (single batch): 0.6287793517112732\n",
      "\t Training loss (single batch): 1.0891953706741333\n",
      "\t Training loss (single batch): 1.438916563987732\n",
      "\t Training loss (single batch): 1.3962253332138062\n",
      "\t Training loss (single batch): 0.9771710634231567\n",
      "\t Training loss (single batch): 1.4789478778839111\n",
      "\t Training loss (single batch): 1.127088189125061\n",
      "\t Training loss (single batch): 1.7423927783966064\n",
      "\t Training loss (single batch): 1.165435791015625\n",
      "\t Training loss (single batch): 1.2924848794937134\n",
      "\t Training loss (single batch): 1.3070294857025146\n",
      "\t Training loss (single batch): 1.2361048460006714\n",
      "\t Training loss (single batch): 1.2719230651855469\n",
      "\t Training loss (single batch): 1.4584500789642334\n",
      "\t Training loss (single batch): 1.275492548942566\n",
      "\t Training loss (single batch): 0.6845953464508057\n",
      "\t Training loss (single batch): 1.369565725326538\n",
      "\t Training loss (single batch): 1.1349492073059082\n",
      "\t Training loss (single batch): 1.591840147972107\n",
      "\t Training loss (single batch): 1.518783450126648\n",
      "\t Training loss (single batch): 1.5192360877990723\n",
      "\t Training loss (single batch): 1.3952336311340332\n",
      "\t Training loss (single batch): 1.1233500242233276\n",
      "\t Training loss (single batch): 1.0252866744995117\n",
      "\t Training loss (single batch): 0.9104353189468384\n",
      "\t Training loss (single batch): 1.0203360319137573\n",
      "\t Training loss (single batch): 0.8854328989982605\n",
      "\t Training loss (single batch): 0.948057234287262\n",
      "\t Training loss (single batch): 0.9958686828613281\n",
      "\t Training loss (single batch): 0.9263734221458435\n",
      "\t Training loss (single batch): 1.2046995162963867\n",
      "\t Training loss (single batch): 1.168189287185669\n",
      "\t Training loss (single batch): 1.14585542678833\n",
      "\t Training loss (single batch): 0.9219723343849182\n",
      "\t Training loss (single batch): 1.0262150764465332\n",
      "\t Training loss (single batch): 0.8987105488777161\n",
      "\t Training loss (single batch): 1.065459132194519\n",
      "\t Training loss (single batch): 1.2893790006637573\n",
      "\t Training loss (single batch): 1.0853270292282104\n",
      "\t Training loss (single batch): 1.4971609115600586\n",
      "\t Training loss (single batch): 1.5118086338043213\n",
      "\t Training loss (single batch): 1.3355029821395874\n",
      "\t Training loss (single batch): 0.892008900642395\n",
      "\t Training loss (single batch): 1.6485769748687744\n",
      "\t Training loss (single batch): 1.8030729293823242\n",
      "\t Training loss (single batch): 1.7861274480819702\n",
      "\t Training loss (single batch): 1.0344390869140625\n",
      "\t Training loss (single batch): 1.415803074836731\n",
      "\t Training loss (single batch): 1.0801609754562378\n",
      "\t Training loss (single batch): 1.2960479259490967\n",
      "\t Training loss (single batch): 1.3002361059188843\n",
      "\t Training loss (single batch): 1.99476957321167\n",
      "\t Training loss (single batch): 1.1123461723327637\n",
      "\t Training loss (single batch): 1.0626919269561768\n",
      "\t Training loss (single batch): 1.155142068862915\n",
      "\t Training loss (single batch): 1.0288265943527222\n",
      "\t Training loss (single batch): 1.8438400030136108\n",
      "\t Training loss (single batch): 1.3101422786712646\n",
      "\t Training loss (single batch): 1.5942692756652832\n",
      "\t Training loss (single batch): 1.226374864578247\n",
      "\t Training loss (single batch): 1.2523016929626465\n",
      "\t Training loss (single batch): 1.3871575593948364\n",
      "\t Training loss (single batch): 1.3702702522277832\n",
      "\t Training loss (single batch): 1.028783917427063\n",
      "\t Training loss (single batch): 1.8272360563278198\n",
      "\t Training loss (single batch): 1.3444602489471436\n",
      "\t Training loss (single batch): 0.9934751987457275\n",
      "\t Training loss (single batch): 0.9438233375549316\n",
      "\t Training loss (single batch): 1.2451539039611816\n",
      "\t Training loss (single batch): 1.3852217197418213\n",
      "\t Training loss (single batch): 0.9378551244735718\n",
      "\t Training loss (single batch): 1.2471537590026855\n",
      "\t Training loss (single batch): 1.0431983470916748\n",
      "\t Training loss (single batch): 1.2890070676803589\n",
      "\t Training loss (single batch): 1.186876893043518\n",
      "\t Training loss (single batch): 1.3240487575531006\n",
      "\t Training loss (single batch): 1.350405216217041\n",
      "\t Training loss (single batch): 1.1738051176071167\n",
      "\t Training loss (single batch): 1.4621163606643677\n",
      "\t Training loss (single batch): 1.313030481338501\n",
      "\t Training loss (single batch): 1.2224725484848022\n",
      "\t Training loss (single batch): 1.4858019351959229\n",
      "\t Training loss (single batch): 1.033270001411438\n",
      "\t Training loss (single batch): 1.275314211845398\n",
      "\t Training loss (single batch): 1.5399425029754639\n",
      "\t Training loss (single batch): 1.245299220085144\n",
      "\t Training loss (single batch): 1.0218217372894287\n",
      "\t Training loss (single batch): 0.8095680475234985\n",
      "\t Training loss (single batch): 0.9895626902580261\n",
      "\t Training loss (single batch): 0.9254595637321472\n",
      "\t Training loss (single batch): 0.9689564108848572\n",
      "\t Training loss (single batch): 0.799485981464386\n",
      "\t Training loss (single batch): 1.4931046962738037\n",
      "\t Training loss (single batch): 1.1960787773132324\n",
      "\t Training loss (single batch): 1.1967973709106445\n",
      "\t Training loss (single batch): 1.1528290510177612\n",
      "\t Training loss (single batch): 1.5632565021514893\n",
      "\t Training loss (single batch): 1.5411958694458008\n",
      "\t Training loss (single batch): 0.8661748170852661\n",
      "\t Training loss (single batch): 1.814759612083435\n",
      "\t Training loss (single batch): 1.513899803161621\n",
      "\t Training loss (single batch): 1.168060064315796\n",
      "\t Training loss (single batch): 1.5875507593154907\n",
      "\t Training loss (single batch): 1.5639793872833252\n",
      "\t Training loss (single batch): 1.0565403699874878\n",
      "\t Training loss (single batch): 1.389461874961853\n",
      "\t Training loss (single batch): 0.8558221459388733\n",
      "\t Training loss (single batch): 0.8601114749908447\n",
      "\t Training loss (single batch): 0.8236100077629089\n",
      "\t Training loss (single batch): 1.8238478899002075\n",
      "\t Training loss (single batch): 1.3044462203979492\n",
      "\t Training loss (single batch): 1.1777663230895996\n",
      "\t Training loss (single batch): 0.8420619368553162\n",
      "\t Training loss (single batch): 0.9592182636260986\n",
      "\t Training loss (single batch): 1.2265256643295288\n",
      "\t Training loss (single batch): 1.2826000452041626\n",
      "\t Training loss (single batch): 1.072906255722046\n",
      "\t Training loss (single batch): 1.490795373916626\n",
      "\t Training loss (single batch): 1.0910450220108032\n",
      "\t Training loss (single batch): 1.461880087852478\n",
      "\t Training loss (single batch): 1.1877676248550415\n",
      "\t Training loss (single batch): 1.2655096054077148\n",
      "\t Training loss (single batch): 1.4534059762954712\n",
      "\t Training loss (single batch): 0.9493404030799866\n",
      "\t Training loss (single batch): 1.3249894380569458\n",
      "\t Training loss (single batch): 1.3345519304275513\n",
      "\t Training loss (single batch): 0.7685738801956177\n",
      "\t Training loss (single batch): 1.5252556800842285\n",
      "\t Training loss (single batch): 1.6656684875488281\n",
      "\t Training loss (single batch): 0.9868732690811157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4159499406814575\n",
      "\t Training loss (single batch): 0.8694252967834473\n",
      "\t Training loss (single batch): 1.1475096940994263\n",
      "\t Training loss (single batch): 0.9540296196937561\n",
      "\t Training loss (single batch): 1.569280743598938\n",
      "\t Training loss (single batch): 0.4352248013019562\n",
      "##################################\n",
      "## EPOCH 59\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0703171491622925\n",
      "\t Training loss (single batch): 0.9000556468963623\n",
      "\t Training loss (single batch): 1.1342004537582397\n",
      "\t Training loss (single batch): 1.14686918258667\n",
      "\t Training loss (single batch): 1.276068925857544\n",
      "\t Training loss (single batch): 1.031367540359497\n",
      "\t Training loss (single batch): 1.0298322439193726\n",
      "\t Training loss (single batch): 0.9839604496955872\n",
      "\t Training loss (single batch): 1.402534008026123\n",
      "\t Training loss (single batch): 0.9636068344116211\n",
      "\t Training loss (single batch): 1.5514390468597412\n",
      "\t Training loss (single batch): 1.2841858863830566\n",
      "\t Training loss (single batch): 1.215643048286438\n",
      "\t Training loss (single batch): 1.0477521419525146\n",
      "\t Training loss (single batch): 1.1121855974197388\n",
      "\t Training loss (single batch): 1.450073480606079\n",
      "\t Training loss (single batch): 1.707183837890625\n",
      "\t Training loss (single batch): 0.9263083338737488\n",
      "\t Training loss (single batch): 1.2536958456039429\n",
      "\t Training loss (single batch): 1.016323208808899\n",
      "\t Training loss (single batch): 1.6196190118789673\n",
      "\t Training loss (single batch): 1.237958550453186\n",
      "\t Training loss (single batch): 1.1688933372497559\n",
      "\t Training loss (single batch): 0.9750452041625977\n",
      "\t Training loss (single batch): 1.3361629247665405\n",
      "\t Training loss (single batch): 1.844853162765503\n",
      "\t Training loss (single batch): 0.9218789339065552\n",
      "\t Training loss (single batch): 1.5047523975372314\n",
      "\t Training loss (single batch): 1.091834545135498\n",
      "\t Training loss (single batch): 1.4567077159881592\n",
      "\t Training loss (single batch): 1.1651579141616821\n",
      "\t Training loss (single batch): 1.0112502574920654\n",
      "\t Training loss (single batch): 1.5632942914962769\n",
      "\t Training loss (single batch): 1.3284275531768799\n",
      "\t Training loss (single batch): 0.8499652743339539\n",
      "\t Training loss (single batch): 1.1982014179229736\n",
      "\t Training loss (single batch): 1.3141071796417236\n",
      "\t Training loss (single batch): 0.8404622077941895\n",
      "\t Training loss (single batch): 1.0661660432815552\n",
      "\t Training loss (single batch): 1.080716609954834\n",
      "\t Training loss (single batch): 0.9619958996772766\n",
      "\t Training loss (single batch): 1.2036185264587402\n",
      "\t Training loss (single batch): 0.8981528878211975\n",
      "\t Training loss (single batch): 1.468271017074585\n",
      "\t Training loss (single batch): 1.1624716520309448\n",
      "\t Training loss (single batch): 1.1142230033874512\n",
      "\t Training loss (single batch): 1.1271909475326538\n",
      "\t Training loss (single batch): 1.0947359800338745\n",
      "\t Training loss (single batch): 0.89723140001297\n",
      "\t Training loss (single batch): 0.9755441546440125\n",
      "\t Training loss (single batch): 0.9766061305999756\n",
      "\t Training loss (single batch): 1.2258483171463013\n",
      "\t Training loss (single batch): 0.7846920490264893\n",
      "\t Training loss (single batch): 1.1994565725326538\n",
      "\t Training loss (single batch): 0.7357577085494995\n",
      "\t Training loss (single batch): 1.216828465461731\n",
      "\t Training loss (single batch): 1.099030613899231\n",
      "\t Training loss (single batch): 1.0091681480407715\n",
      "\t Training loss (single batch): 1.2920478582382202\n",
      "\t Training loss (single batch): 1.1319025754928589\n",
      "\t Training loss (single batch): 1.134982705116272\n",
      "\t Training loss (single batch): 0.992448627948761\n",
      "\t Training loss (single batch): 1.0121103525161743\n",
      "\t Training loss (single batch): 1.0969429016113281\n",
      "\t Training loss (single batch): 1.4905102252960205\n",
      "\t Training loss (single batch): 1.6121188402175903\n",
      "\t Training loss (single batch): 1.333775520324707\n",
      "\t Training loss (single batch): 0.901212215423584\n",
      "\t Training loss (single batch): 0.9860339164733887\n",
      "\t Training loss (single batch): 1.5989110469818115\n",
      "\t Training loss (single batch): 1.49760901927948\n",
      "\t Training loss (single batch): 1.2428383827209473\n",
      "\t Training loss (single batch): 1.2334413528442383\n",
      "\t Training loss (single batch): 1.357610821723938\n",
      "\t Training loss (single batch): 1.5589206218719482\n",
      "\t Training loss (single batch): 1.407202959060669\n",
      "\t Training loss (single batch): 1.2274808883666992\n",
      "\t Training loss (single batch): 1.65692138671875\n",
      "\t Training loss (single batch): 1.0500917434692383\n",
      "\t Training loss (single batch): 0.9991377592086792\n",
      "\t Training loss (single batch): 0.7018404006958008\n",
      "\t Training loss (single batch): 1.329932451248169\n",
      "\t Training loss (single batch): 1.2022690773010254\n",
      "\t Training loss (single batch): 0.831275463104248\n",
      "\t Training loss (single batch): 1.604979157447815\n",
      "\t Training loss (single batch): 1.2620484828948975\n",
      "\t Training loss (single batch): 1.0035922527313232\n",
      "\t Training loss (single batch): 1.2258708477020264\n",
      "\t Training loss (single batch): 1.1339995861053467\n",
      "\t Training loss (single batch): 1.157455563545227\n",
      "\t Training loss (single batch): 1.405576467514038\n",
      "\t Training loss (single batch): 1.3718924522399902\n",
      "\t Training loss (single batch): 1.4701498746871948\n",
      "\t Training loss (single batch): 1.1146154403686523\n",
      "\t Training loss (single batch): 1.0989426374435425\n",
      "\t Training loss (single batch): 1.3440158367156982\n",
      "\t Training loss (single batch): 1.1459804773330688\n",
      "\t Training loss (single batch): 1.4560378789901733\n",
      "\t Training loss (single batch): 1.6175389289855957\n",
      "\t Training loss (single batch): 1.2312779426574707\n",
      "\t Training loss (single batch): 1.1688637733459473\n",
      "\t Training loss (single batch): 0.8747715950012207\n",
      "\t Training loss (single batch): 1.0240155458450317\n",
      "\t Training loss (single batch): 1.4888925552368164\n",
      "\t Training loss (single batch): 1.6790845394134521\n",
      "\t Training loss (single batch): 1.4151500463485718\n",
      "\t Training loss (single batch): 1.4896429777145386\n",
      "\t Training loss (single batch): 1.2311625480651855\n",
      "\t Training loss (single batch): 1.1336872577667236\n",
      "\t Training loss (single batch): 0.8005055785179138\n",
      "\t Training loss (single batch): 1.31477689743042\n",
      "\t Training loss (single batch): 1.1170408725738525\n",
      "\t Training loss (single batch): 1.3249536752700806\n",
      "\t Training loss (single batch): 1.136505126953125\n",
      "\t Training loss (single batch): 1.4044430255889893\n",
      "\t Training loss (single batch): 1.0382038354873657\n",
      "\t Training loss (single batch): 1.2661585807800293\n",
      "\t Training loss (single batch): 0.995785117149353\n",
      "\t Training loss (single batch): 1.4488859176635742\n",
      "\t Training loss (single batch): 1.263801097869873\n",
      "\t Training loss (single batch): 1.0979928970336914\n",
      "\t Training loss (single batch): 0.8937339186668396\n",
      "\t Training loss (single batch): 1.1152690649032593\n",
      "\t Training loss (single batch): 1.2006399631500244\n",
      "\t Training loss (single batch): 1.2287089824676514\n",
      "\t Training loss (single batch): 1.265851378440857\n",
      "\t Training loss (single batch): 1.209844946861267\n",
      "\t Training loss (single batch): 1.0041667222976685\n",
      "\t Training loss (single batch): 0.7616116404533386\n",
      "\t Training loss (single batch): 1.0899683237075806\n",
      "\t Training loss (single batch): 1.4983118772506714\n",
      "\t Training loss (single batch): 1.4339015483856201\n",
      "\t Training loss (single batch): 1.3567079305648804\n",
      "\t Training loss (single batch): 1.003213882446289\n",
      "\t Training loss (single batch): 2.042579174041748\n",
      "\t Training loss (single batch): 0.8358661532402039\n",
      "\t Training loss (single batch): 1.1164966821670532\n",
      "\t Training loss (single batch): 1.2960573434829712\n",
      "\t Training loss (single batch): 1.7909561395645142\n",
      "\t Training loss (single batch): 1.5015695095062256\n",
      "\t Training loss (single batch): 1.171182632446289\n",
      "\t Training loss (single batch): 1.0327447652816772\n",
      "\t Training loss (single batch): 1.3516353368759155\n",
      "\t Training loss (single batch): 0.9145752787590027\n",
      "\t Training loss (single batch): 1.342638373374939\n",
      "\t Training loss (single batch): 1.5539510250091553\n",
      "\t Training loss (single batch): 1.0234148502349854\n",
      "\t Training loss (single batch): 1.3344115018844604\n",
      "\t Training loss (single batch): 1.5917634963989258\n",
      "\t Training loss (single batch): 1.2643500566482544\n",
      "\t Training loss (single batch): 1.148815393447876\n",
      "\t Training loss (single batch): 1.3952382802963257\n",
      "\t Training loss (single batch): 0.8665533065795898\n",
      "\t Training loss (single batch): 1.9148730039596558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.422413945198059\n",
      "\t Training loss (single batch): 1.6154417991638184\n",
      "\t Training loss (single batch): 1.2917087078094482\n",
      "\t Training loss (single batch): 1.0204710960388184\n",
      "\t Training loss (single batch): 1.6414779424667358\n",
      "\t Training loss (single batch): 1.0053954124450684\n",
      "\t Training loss (single batch): 1.4314610958099365\n",
      "\t Training loss (single batch): 0.9936285018920898\n",
      "\t Training loss (single batch): 1.6257553100585938\n",
      "\t Training loss (single batch): 1.3678615093231201\n",
      "\t Training loss (single batch): 0.9332382678985596\n",
      "\t Training loss (single batch): 1.2403110265731812\n",
      "\t Training loss (single batch): 1.1702771186828613\n",
      "\t Training loss (single batch): 1.113929271697998\n",
      "\t Training loss (single batch): 1.6646777391433716\n",
      "\t Training loss (single batch): 1.5756412744522095\n",
      "\t Training loss (single batch): 1.2242403030395508\n",
      "\t Training loss (single batch): 1.1960433721542358\n",
      "\t Training loss (single batch): 1.1905754804611206\n",
      "\t Training loss (single batch): 1.4119752645492554\n",
      "\t Training loss (single batch): 1.0823091268539429\n",
      "\t Training loss (single batch): 1.159688949584961\n",
      "\t Training loss (single batch): 1.417228102684021\n",
      "\t Training loss (single batch): 1.3540480136871338\n",
      "\t Training loss (single batch): 1.07523512840271\n",
      "\t Training loss (single batch): 1.0511939525604248\n",
      "\t Training loss (single batch): 1.1802563667297363\n",
      "\t Training loss (single batch): 1.1209068298339844\n",
      "\t Training loss (single batch): 0.9137285947799683\n",
      "\t Training loss (single batch): 0.7894443273544312\n",
      "\t Training loss (single batch): 0.7958781123161316\n",
      "\t Training loss (single batch): 1.5199230909347534\n",
      "\t Training loss (single batch): 0.8867377042770386\n",
      "\t Training loss (single batch): 1.0572404861450195\n",
      "\t Training loss (single batch): 1.6354798078536987\n",
      "\t Training loss (single batch): 0.8284798860549927\n",
      "\t Training loss (single batch): 1.5479326248168945\n",
      "\t Training loss (single batch): 1.2198163270950317\n",
      "\t Training loss (single batch): 0.9570558667182922\n",
      "\t Training loss (single batch): 1.532263159751892\n",
      "\t Training loss (single batch): 0.8256082534790039\n",
      "\t Training loss (single batch): 0.817973792552948\n",
      "\t Training loss (single batch): 1.446589469909668\n",
      "\t Training loss (single batch): 1.5732614994049072\n",
      "\t Training loss (single batch): 1.1712825298309326\n",
      "\t Training loss (single batch): 1.5156666040420532\n",
      "\t Training loss (single batch): 1.2700594663619995\n",
      "\t Training loss (single batch): 0.9709867835044861\n",
      "\t Training loss (single batch): 0.8526743054389954\n",
      "\t Training loss (single batch): 1.2721223831176758\n",
      "\t Training loss (single batch): 1.3112276792526245\n",
      "\t Training loss (single batch): 0.9796205759048462\n",
      "\t Training loss (single batch): 1.0926378965377808\n",
      "\t Training loss (single batch): 1.6948168277740479\n",
      "\t Training loss (single batch): 1.1143509149551392\n",
      "\t Training loss (single batch): 1.034185528755188\n",
      "\t Training loss (single batch): 1.33979070186615\n",
      "\t Training loss (single batch): 1.504889726638794\n",
      "\t Training loss (single batch): 1.164890170097351\n",
      "\t Training loss (single batch): 0.958050012588501\n",
      "\t Training loss (single batch): 1.0431007146835327\n",
      "\t Training loss (single batch): 1.045670747756958\n",
      "\t Training loss (single batch): 0.7951211929321289\n",
      "\t Training loss (single batch): 1.0987931489944458\n",
      "\t Training loss (single batch): 1.2964107990264893\n",
      "\t Training loss (single batch): 1.0146516561508179\n",
      "\t Training loss (single batch): 1.0500918626785278\n",
      "\t Training loss (single batch): 1.3046960830688477\n",
      "\t Training loss (single batch): 1.2239073514938354\n",
      "\t Training loss (single batch): 1.119121789932251\n",
      "\t Training loss (single batch): 1.4077438116073608\n",
      "\t Training loss (single batch): 1.4110751152038574\n",
      "\t Training loss (single batch): 1.1715009212493896\n",
      "\t Training loss (single batch): 1.094226598739624\n",
      "\t Training loss (single batch): 1.334355115890503\n",
      "\t Training loss (single batch): 1.5867499113082886\n",
      "\t Training loss (single batch): 1.6455509662628174\n",
      "\t Training loss (single batch): 1.0229346752166748\n",
      "\t Training loss (single batch): 0.9250518679618835\n",
      "\t Training loss (single batch): 1.4099870920181274\n",
      "\t Training loss (single batch): 0.9672808051109314\n",
      "\t Training loss (single batch): 1.3298782110214233\n",
      "\t Training loss (single batch): 1.1409831047058105\n",
      "\t Training loss (single batch): 0.9085313677787781\n",
      "\t Training loss (single batch): 0.9580089449882507\n",
      "\t Training loss (single batch): 1.4975550174713135\n",
      "\t Training loss (single batch): 1.2866674661636353\n",
      "\t Training loss (single batch): 0.8416135907173157\n",
      "\t Training loss (single batch): 1.1630195379257202\n",
      "\t Training loss (single batch): 1.317234754562378\n",
      "\t Training loss (single batch): 1.0420451164245605\n",
      "\t Training loss (single batch): 1.1462794542312622\n",
      "\t Training loss (single batch): 1.1339718103408813\n",
      "\t Training loss (single batch): 1.0775048732757568\n",
      "\t Training loss (single batch): 1.4458955526351929\n",
      "\t Training loss (single batch): 1.061607003211975\n",
      "\t Training loss (single batch): 0.8458530306816101\n",
      "\t Training loss (single batch): 1.2503206729888916\n",
      "\t Training loss (single batch): 1.0631877183914185\n",
      "\t Training loss (single batch): 0.8572348356246948\n",
      "\t Training loss (single batch): 1.3222538232803345\n",
      "\t Training loss (single batch): 1.228287935256958\n",
      "\t Training loss (single batch): 1.41995370388031\n",
      "\t Training loss (single batch): 1.4224474430084229\n",
      "\t Training loss (single batch): 1.1413887739181519\n",
      "\t Training loss (single batch): 0.7317094802856445\n",
      "\t Training loss (single batch): 0.8530505299568176\n",
      "\t Training loss (single batch): 1.20361328125\n",
      "\t Training loss (single batch): 1.3689755201339722\n",
      "\t Training loss (single batch): 0.7217778563499451\n",
      "\t Training loss (single batch): 1.243029236793518\n",
      "\t Training loss (single batch): 1.3309029340744019\n",
      "\t Training loss (single batch): 1.6494717597961426\n",
      "\t Training loss (single batch): 1.6980549097061157\n",
      "\t Training loss (single batch): 1.4790462255477905\n",
      "\t Training loss (single batch): 1.3566627502441406\n",
      "\t Training loss (single batch): 1.174607276916504\n",
      "\t Training loss (single batch): 0.7997592091560364\n",
      "\t Training loss (single batch): 1.3378976583480835\n",
      "\t Training loss (single batch): 1.216574788093567\n",
      "\t Training loss (single batch): 1.3468562364578247\n",
      "\t Training loss (single batch): 1.2633813619613647\n",
      "\t Training loss (single batch): 0.9342374205589294\n",
      "\t Training loss (single batch): 1.9927164316177368\n",
      "\t Training loss (single batch): 1.1932835578918457\n",
      "\t Training loss (single batch): 1.2963966131210327\n",
      "\t Training loss (single batch): 0.9713535308837891\n",
      "\t Training loss (single batch): 0.829059362411499\n",
      "\t Training loss (single batch): 1.1225402355194092\n",
      "\t Training loss (single batch): 1.50163733959198\n",
      "\t Training loss (single batch): 1.4042205810546875\n",
      "\t Training loss (single batch): 1.5526372194290161\n",
      "\t Training loss (single batch): 1.4153459072113037\n",
      "\t Training loss (single batch): 1.143042802810669\n",
      "\t Training loss (single batch): 1.2447774410247803\n",
      "\t Training loss (single batch): 1.6037083864212036\n",
      "\t Training loss (single batch): 0.9424271583557129\n",
      "\t Training loss (single batch): 1.6788560152053833\n",
      "\t Training loss (single batch): 1.9837677478790283\n",
      "\t Training loss (single batch): 0.942495584487915\n",
      "\t Training loss (single batch): 1.6154953241348267\n",
      "\t Training loss (single batch): 1.4797766208648682\n",
      "\t Training loss (single batch): 1.2896215915679932\n",
      "\t Training loss (single batch): 1.312304973602295\n",
      "\t Training loss (single batch): 1.4535200595855713\n",
      "\t Training loss (single batch): 1.261023759841919\n",
      "\t Training loss (single batch): 1.090527892112732\n",
      "\t Training loss (single batch): 0.9940137267112732\n",
      "\t Training loss (single batch): 1.186700701713562\n",
      "\t Training loss (single batch): 1.5329136848449707\n",
      "\t Training loss (single batch): 1.1718544960021973\n",
      "\t Training loss (single batch): 0.7347658276557922\n",
      "\t Training loss (single batch): 0.919784426689148\n",
      "\t Training loss (single batch): 1.4157098531723022\n",
      "\t Training loss (single batch): 1.18789803981781\n",
      "\t Training loss (single batch): 0.9400137662887573\n",
      "\t Training loss (single batch): 1.2066929340362549\n",
      "\t Training loss (single batch): 1.0201842784881592\n",
      "\t Training loss (single batch): 1.1412392854690552\n",
      "\t Training loss (single batch): 1.0640144348144531\n",
      "\t Training loss (single batch): 1.4332010746002197\n",
      "\t Training loss (single batch): 0.9032835960388184\n",
      "\t Training loss (single batch): 2.2843775749206543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "## EPOCH 60\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3444887399673462\n",
      "\t Training loss (single batch): 0.8252349495887756\n",
      "\t Training loss (single batch): 1.0345399379730225\n",
      "\t Training loss (single batch): 1.7642561197280884\n",
      "\t Training loss (single batch): 1.0233975648880005\n",
      "\t Training loss (single batch): 1.442104458808899\n",
      "\t Training loss (single batch): 0.9123082160949707\n",
      "\t Training loss (single batch): 1.1463955640792847\n",
      "\t Training loss (single batch): 1.3486825227737427\n",
      "\t Training loss (single batch): 1.2009851932525635\n",
      "\t Training loss (single batch): 1.239894151687622\n",
      "\t Training loss (single batch): 1.0296238660812378\n",
      "\t Training loss (single batch): 1.6562308073043823\n",
      "\t Training loss (single batch): 0.9454483985900879\n",
      "\t Training loss (single batch): 1.0681766271591187\n",
      "\t Training loss (single batch): 1.3503092527389526\n",
      "\t Training loss (single batch): 0.7944432497024536\n",
      "\t Training loss (single batch): 1.122527837753296\n",
      "\t Training loss (single batch): 1.18159818649292\n",
      "\t Training loss (single batch): 1.112709879875183\n",
      "\t Training loss (single batch): 0.9309641718864441\n",
      "\t Training loss (single batch): 0.8679525256156921\n",
      "\t Training loss (single batch): 1.1541224718093872\n",
      "\t Training loss (single batch): 1.6995550394058228\n",
      "\t Training loss (single batch): 0.9567139744758606\n",
      "\t Training loss (single batch): 1.6656215190887451\n",
      "\t Training loss (single batch): 0.874671459197998\n",
      "\t Training loss (single batch): 1.0803879499435425\n",
      "\t Training loss (single batch): 1.2902995347976685\n",
      "\t Training loss (single batch): 1.0699193477630615\n",
      "\t Training loss (single batch): 1.1056181192398071\n",
      "\t Training loss (single batch): 1.2431340217590332\n",
      "\t Training loss (single batch): 1.2000291347503662\n",
      "\t Training loss (single batch): 1.203413724899292\n",
      "\t Training loss (single batch): 1.00079345703125\n",
      "\t Training loss (single batch): 1.000557780265808\n",
      "\t Training loss (single batch): 0.996973991394043\n",
      "\t Training loss (single batch): 1.3192459344863892\n",
      "\t Training loss (single batch): 1.710530161857605\n",
      "\t Training loss (single batch): 1.3059426546096802\n",
      "\t Training loss (single batch): 1.137297511100769\n",
      "\t Training loss (single batch): 1.1778782606124878\n",
      "\t Training loss (single batch): 1.0661555528640747\n",
      "\t Training loss (single batch): 1.0213239192962646\n",
      "\t Training loss (single batch): 1.4166353940963745\n",
      "\t Training loss (single batch): 1.5331625938415527\n",
      "\t Training loss (single batch): 1.431957721710205\n",
      "\t Training loss (single batch): 0.9924699068069458\n",
      "\t Training loss (single batch): 1.5742236375808716\n",
      "\t Training loss (single batch): 1.2989881038665771\n",
      "\t Training loss (single batch): 1.769573450088501\n",
      "\t Training loss (single batch): 1.385470986366272\n",
      "\t Training loss (single batch): 1.4606497287750244\n",
      "\t Training loss (single batch): 1.2940713167190552\n",
      "\t Training loss (single batch): 1.2774338722229004\n",
      "\t Training loss (single batch): 1.233538031578064\n",
      "\t Training loss (single batch): 1.121366024017334\n",
      "\t Training loss (single batch): 1.4171172380447388\n",
      "\t Training loss (single batch): 1.4157243967056274\n",
      "\t Training loss (single batch): 1.400145411491394\n",
      "\t Training loss (single batch): 1.2127188444137573\n",
      "\t Training loss (single batch): 1.4947854280471802\n",
      "\t Training loss (single batch): 1.1392146348953247\n",
      "\t Training loss (single batch): 1.0923306941986084\n",
      "\t Training loss (single batch): 1.5110735893249512\n",
      "\t Training loss (single batch): 0.9644286632537842\n",
      "\t Training loss (single batch): 0.8937230706214905\n",
      "\t Training loss (single batch): 1.4441454410552979\n",
      "\t Training loss (single batch): 1.1043699979782104\n",
      "\t Training loss (single batch): 0.9982252717018127\n",
      "\t Training loss (single batch): 1.5705394744873047\n",
      "\t Training loss (single batch): 1.0630080699920654\n",
      "\t Training loss (single batch): 1.7983431816101074\n",
      "\t Training loss (single batch): 1.1913771629333496\n",
      "\t Training loss (single batch): 0.622890055179596\n",
      "\t Training loss (single batch): 1.6253553628921509\n",
      "\t Training loss (single batch): 1.3490922451019287\n",
      "\t Training loss (single batch): 0.9448341131210327\n",
      "\t Training loss (single batch): 1.5154789686203003\n",
      "\t Training loss (single batch): 1.2052325010299683\n",
      "\t Training loss (single batch): 1.2361729145050049\n",
      "\t Training loss (single batch): 1.130024790763855\n",
      "\t Training loss (single batch): 1.196589469909668\n",
      "\t Training loss (single batch): 1.4576280117034912\n",
      "\t Training loss (single batch): 1.3457942008972168\n",
      "\t Training loss (single batch): 1.0607503652572632\n",
      "\t Training loss (single batch): 1.3983681201934814\n",
      "\t Training loss (single batch): 1.4618353843688965\n",
      "\t Training loss (single batch): 0.834418535232544\n",
      "\t Training loss (single batch): 0.9309645891189575\n",
      "\t Training loss (single batch): 1.2197973728179932\n",
      "\t Training loss (single batch): 0.9877046942710876\n",
      "\t Training loss (single batch): 1.260403037071228\n",
      "\t Training loss (single batch): 0.9961366653442383\n",
      "\t Training loss (single batch): 1.1150082349777222\n",
      "\t Training loss (single batch): 1.4437057971954346\n",
      "\t Training loss (single batch): 0.9769167304039001\n",
      "\t Training loss (single batch): 0.8596847653388977\n",
      "\t Training loss (single batch): 1.743484377861023\n",
      "\t Training loss (single batch): 1.1304762363433838\n",
      "\t Training loss (single batch): 1.6279613971710205\n",
      "\t Training loss (single batch): 1.0083032846450806\n",
      "\t Training loss (single batch): 1.0668156147003174\n",
      "\t Training loss (single batch): 1.510103464126587\n",
      "\t Training loss (single batch): 1.4158087968826294\n",
      "\t Training loss (single batch): 1.3048527240753174\n",
      "\t Training loss (single batch): 1.0229154825210571\n",
      "\t Training loss (single batch): 1.0920764207839966\n",
      "\t Training loss (single batch): 1.5852643251419067\n",
      "\t Training loss (single batch): 0.9515689611434937\n",
      "\t Training loss (single batch): 0.8569469451904297\n",
      "\t Training loss (single batch): 0.828519880771637\n",
      "\t Training loss (single batch): 1.0323081016540527\n",
      "\t Training loss (single batch): 1.2500593662261963\n",
      "\t Training loss (single batch): 2.160895347595215\n",
      "\t Training loss (single batch): 1.5471137762069702\n",
      "\t Training loss (single batch): 1.6602624654769897\n",
      "\t Training loss (single batch): 1.1527801752090454\n",
      "\t Training loss (single batch): 1.055673599243164\n",
      "\t Training loss (single batch): 1.0392242670059204\n",
      "\t Training loss (single batch): 0.8794283270835876\n",
      "\t Training loss (single batch): 1.0880738496780396\n",
      "\t Training loss (single batch): 1.2221429347991943\n",
      "\t Training loss (single batch): 1.6881210803985596\n",
      "\t Training loss (single batch): 1.01176118850708\n",
      "\t Training loss (single batch): 1.5333064794540405\n",
      "\t Training loss (single batch): 1.569873332977295\n",
      "\t Training loss (single batch): 1.458048701286316\n",
      "\t Training loss (single batch): 1.6773700714111328\n",
      "\t Training loss (single batch): 1.1809470653533936\n",
      "\t Training loss (single batch): 1.5540285110473633\n",
      "\t Training loss (single batch): 1.0070598125457764\n",
      "\t Training loss (single batch): 1.7854866981506348\n",
      "\t Training loss (single batch): 1.3218408823013306\n",
      "\t Training loss (single batch): 1.480284571647644\n",
      "\t Training loss (single batch): 0.9290898442268372\n",
      "\t Training loss (single batch): 1.7357771396636963\n",
      "\t Training loss (single batch): 1.4592095613479614\n",
      "\t Training loss (single batch): 1.0280354022979736\n",
      "\t Training loss (single batch): 1.2190496921539307\n",
      "\t Training loss (single batch): 1.4468106031417847\n",
      "\t Training loss (single batch): 1.2563221454620361\n",
      "\t Training loss (single batch): 1.278515338897705\n",
      "\t Training loss (single batch): 0.8845025897026062\n",
      "\t Training loss (single batch): 1.2502374649047852\n",
      "\t Training loss (single batch): 0.7468369007110596\n",
      "\t Training loss (single batch): 0.8554324507713318\n",
      "\t Training loss (single batch): 1.303918480873108\n",
      "\t Training loss (single batch): 1.3513051271438599\n",
      "\t Training loss (single batch): 1.4448504447937012\n",
      "\t Training loss (single batch): 1.2957143783569336\n",
      "\t Training loss (single batch): 1.2460860013961792\n",
      "\t Training loss (single batch): 1.2477319240570068\n",
      "\t Training loss (single batch): 1.03310227394104\n",
      "\t Training loss (single batch): 0.7182098031044006\n",
      "\t Training loss (single batch): 1.211326241493225\n",
      "\t Training loss (single batch): 1.1785461902618408\n",
      "\t Training loss (single batch): 0.9986458420753479\n",
      "\t Training loss (single batch): 1.249075174331665\n",
      "\t Training loss (single batch): 1.0123711824417114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0826783180236816\n",
      "\t Training loss (single batch): 1.2429965734481812\n",
      "\t Training loss (single batch): 1.091054081916809\n",
      "\t Training loss (single batch): 1.8558577299118042\n",
      "\t Training loss (single batch): 1.1133993864059448\n",
      "\t Training loss (single batch): 1.4377005100250244\n",
      "\t Training loss (single batch): 0.9658535718917847\n",
      "\t Training loss (single batch): 1.1611926555633545\n",
      "\t Training loss (single batch): 1.3721165657043457\n",
      "\t Training loss (single batch): 0.8270686864852905\n",
      "\t Training loss (single batch): 1.8195102214813232\n",
      "\t Training loss (single batch): 1.008073329925537\n",
      "\t Training loss (single batch): 1.1987032890319824\n",
      "\t Training loss (single batch): 1.9332566261291504\n",
      "\t Training loss (single batch): 1.0373107194900513\n",
      "\t Training loss (single batch): 0.9385033249855042\n",
      "\t Training loss (single batch): 1.3614964485168457\n",
      "\t Training loss (single batch): 1.5782480239868164\n",
      "\t Training loss (single batch): 1.0195958614349365\n",
      "\t Training loss (single batch): 1.064100980758667\n",
      "\t Training loss (single batch): 1.50179922580719\n",
      "\t Training loss (single batch): 1.336897373199463\n",
      "\t Training loss (single batch): 1.0922390222549438\n",
      "\t Training loss (single batch): 1.0433814525604248\n",
      "\t Training loss (single batch): 1.0621768236160278\n",
      "\t Training loss (single batch): 1.0832079648971558\n",
      "\t Training loss (single batch): 1.0323030948638916\n",
      "\t Training loss (single batch): 1.1755836009979248\n",
      "\t Training loss (single batch): 1.213634729385376\n",
      "\t Training loss (single batch): 0.9690108299255371\n",
      "\t Training loss (single batch): 1.4578346014022827\n",
      "\t Training loss (single batch): 1.7838608026504517\n",
      "\t Training loss (single batch): 1.1016031503677368\n",
      "\t Training loss (single batch): 1.3266372680664062\n",
      "\t Training loss (single batch): 1.034173607826233\n",
      "\t Training loss (single batch): 1.4072142839431763\n",
      "\t Training loss (single batch): 0.8475908041000366\n",
      "\t Training loss (single batch): 1.1429827213287354\n",
      "\t Training loss (single batch): 1.0605498552322388\n",
      "\t Training loss (single batch): 0.7997012734413147\n",
      "\t Training loss (single batch): 1.3531371355056763\n",
      "\t Training loss (single batch): 1.3905978202819824\n",
      "\t Training loss (single batch): 1.4721208810806274\n",
      "\t Training loss (single batch): 1.1125833988189697\n",
      "\t Training loss (single batch): 1.24907386302948\n",
      "\t Training loss (single batch): 0.804643452167511\n",
      "\t Training loss (single batch): 1.3689699172973633\n",
      "\t Training loss (single batch): 0.8701955080032349\n",
      "\t Training loss (single batch): 1.3533539772033691\n",
      "\t Training loss (single batch): 1.26850163936615\n",
      "\t Training loss (single batch): 1.8893135786056519\n",
      "\t Training loss (single batch): 1.699264645576477\n",
      "\t Training loss (single batch): 1.3835740089416504\n",
      "\t Training loss (single batch): 1.09282648563385\n",
      "\t Training loss (single batch): 0.9883368611335754\n",
      "\t Training loss (single batch): 1.0266642570495605\n",
      "\t Training loss (single batch): 1.5196352005004883\n",
      "\t Training loss (single batch): 1.3483844995498657\n",
      "\t Training loss (single batch): 1.6211525201797485\n",
      "\t Training loss (single batch): 0.9136640429496765\n",
      "\t Training loss (single batch): 0.9069024324417114\n",
      "\t Training loss (single batch): 1.5491842031478882\n",
      "\t Training loss (single batch): 1.4096227884292603\n",
      "\t Training loss (single batch): 1.2611677646636963\n",
      "\t Training loss (single batch): 1.6128168106079102\n",
      "\t Training loss (single batch): 1.1260217428207397\n",
      "\t Training loss (single batch): 1.168483853340149\n",
      "\t Training loss (single batch): 1.0880221128463745\n",
      "\t Training loss (single batch): 0.8811399936676025\n",
      "\t Training loss (single batch): 1.7926615476608276\n",
      "\t Training loss (single batch): 1.282403588294983\n",
      "\t Training loss (single batch): 1.3255780935287476\n",
      "\t Training loss (single batch): 1.3400461673736572\n",
      "\t Training loss (single batch): 1.3961082696914673\n",
      "\t Training loss (single batch): 1.0510095357894897\n",
      "\t Training loss (single batch): 1.033174991607666\n",
      "\t Training loss (single batch): 1.2069311141967773\n",
      "\t Training loss (single batch): 1.4679826498031616\n",
      "\t Training loss (single batch): 0.9213157296180725\n",
      "\t Training loss (single batch): 1.105804204940796\n",
      "\t Training loss (single batch): 1.2840020656585693\n",
      "\t Training loss (single batch): 0.9205543994903564\n",
      "\t Training loss (single batch): 1.7829840183258057\n",
      "\t Training loss (single batch): 0.8225088119506836\n",
      "\t Training loss (single batch): 0.7398146390914917\n",
      "\t Training loss (single batch): 1.4894180297851562\n",
      "\t Training loss (single batch): 1.5025585889816284\n",
      "\t Training loss (single batch): 0.9332350492477417\n",
      "\t Training loss (single batch): 0.9661577343940735\n",
      "\t Training loss (single batch): 1.5434075593948364\n",
      "\t Training loss (single batch): 1.017886757850647\n",
      "\t Training loss (single batch): 1.2116942405700684\n",
      "\t Training loss (single batch): 0.7400836944580078\n",
      "\t Training loss (single batch): 1.218044638633728\n",
      "\t Training loss (single batch): 1.3226215839385986\n",
      "\t Training loss (single batch): 0.7262547612190247\n",
      "\t Training loss (single batch): 1.315809965133667\n",
      "\t Training loss (single batch): 1.355038046836853\n",
      "\t Training loss (single batch): 1.0265871286392212\n",
      "\t Training loss (single batch): 0.9729642868041992\n",
      "\t Training loss (single batch): 1.2502819299697876\n",
      "\t Training loss (single batch): 0.9268671870231628\n",
      "\t Training loss (single batch): 1.0967415571212769\n",
      "\t Training loss (single batch): 1.4982192516326904\n",
      "\t Training loss (single batch): 1.203896164894104\n",
      "\t Training loss (single batch): 1.56536066532135\n",
      "\t Training loss (single batch): 1.555925726890564\n",
      "\t Training loss (single batch): 1.3003747463226318\n",
      "\t Training loss (single batch): 1.6662085056304932\n",
      "\t Training loss (single batch): 1.2346549034118652\n",
      "\t Training loss (single batch): 1.439448595046997\n",
      "\t Training loss (single batch): 1.572098731994629\n",
      "\t Training loss (single batch): 1.460368275642395\n",
      "\t Training loss (single batch): 1.9461438655853271\n",
      "\t Training loss (single batch): 1.381015658378601\n",
      "\t Training loss (single batch): 1.0837138891220093\n",
      "\t Training loss (single batch): 1.3048256635665894\n",
      "\t Training loss (single batch): 1.4052681922912598\n",
      "\t Training loss (single batch): 1.3134784698486328\n",
      "\t Training loss (single batch): 1.1060212850570679\n",
      "\t Training loss (single batch): 1.2725414037704468\n",
      "\t Training loss (single batch): 1.200408697128296\n",
      "\t Training loss (single batch): 1.284975528717041\n",
      "\t Training loss (single batch): 1.1339274644851685\n",
      "\t Training loss (single batch): 1.1684918403625488\n",
      "\t Training loss (single batch): 0.7064692378044128\n",
      "\t Training loss (single batch): 1.356141448020935\n",
      "\t Training loss (single batch): 1.675143837928772\n",
      "\t Training loss (single batch): 0.7874670624732971\n",
      "\t Training loss (single batch): 1.0279027223587036\n",
      "\t Training loss (single batch): 1.3384623527526855\n",
      "\t Training loss (single batch): 0.9806169867515564\n",
      "\t Training loss (single batch): 1.1374225616455078\n",
      "\t Training loss (single batch): 1.6785577535629272\n",
      "\t Training loss (single batch): 0.7780367136001587\n",
      "\t Training loss (single batch): 1.8447338342666626\n",
      "\t Training loss (single batch): 1.3692617416381836\n",
      "\t Training loss (single batch): 1.0117933750152588\n",
      "\t Training loss (single batch): 1.4902559518814087\n",
      "\t Training loss (single batch): 1.3618372678756714\n",
      "\t Training loss (single batch): 0.9111927151679993\n",
      "\t Training loss (single batch): 1.0235495567321777\n",
      "\t Training loss (single batch): 1.055597186088562\n",
      "\t Training loss (single batch): 1.1754417419433594\n",
      "\t Training loss (single batch): 0.8069536089897156\n",
      "\t Training loss (single batch): 0.9558535814285278\n",
      "\t Training loss (single batch): 0.8878911137580872\n",
      "\t Training loss (single batch): 1.3606888055801392\n",
      "\t Training loss (single batch): 0.8399932980537415\n",
      "\t Training loss (single batch): 1.4675867557525635\n",
      "\t Training loss (single batch): 1.0670324563980103\n",
      "\t Training loss (single batch): 1.0141081809997559\n",
      "\t Training loss (single batch): 1.8838108777999878\n",
      "\t Training loss (single batch): 1.097243309020996\n",
      "\t Training loss (single batch): 0.8911102414131165\n",
      "\t Training loss (single batch): 1.2758722305297852\n",
      "\t Training loss (single batch): 0.6129036545753479\n",
      "##################################\n",
      "## EPOCH 61\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6971206665039062\n",
      "\t Training loss (single batch): 1.6393589973449707\n",
      "\t Training loss (single batch): 0.9811610579490662\n",
      "\t Training loss (single batch): 1.0973210334777832\n",
      "\t Training loss (single batch): 1.8140724897384644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.7382969260215759\n",
      "\t Training loss (single batch): 0.972356915473938\n",
      "\t Training loss (single batch): 0.9060459733009338\n",
      "\t Training loss (single batch): 1.268897533416748\n",
      "\t Training loss (single batch): 1.2124053239822388\n",
      "\t Training loss (single batch): 1.3180885314941406\n",
      "\t Training loss (single batch): 1.348142385482788\n",
      "\t Training loss (single batch): 1.114494800567627\n",
      "\t Training loss (single batch): 0.8914186358451843\n",
      "\t Training loss (single batch): 1.1765611171722412\n",
      "\t Training loss (single batch): 1.864324927330017\n",
      "\t Training loss (single batch): 1.0742312669754028\n",
      "\t Training loss (single batch): 0.9171531200408936\n",
      "\t Training loss (single batch): 0.8636494874954224\n",
      "\t Training loss (single batch): 1.432146430015564\n",
      "\t Training loss (single batch): 1.2285598516464233\n",
      "\t Training loss (single batch): 1.4117604494094849\n",
      "\t Training loss (single batch): 1.1680210828781128\n",
      "\t Training loss (single batch): 1.45640230178833\n",
      "\t Training loss (single batch): 1.7374111413955688\n",
      "\t Training loss (single batch): 1.27642822265625\n",
      "\t Training loss (single batch): 1.2524051666259766\n",
      "\t Training loss (single batch): 1.2822175025939941\n",
      "\t Training loss (single batch): 1.2850165367126465\n",
      "\t Training loss (single batch): 1.0536683797836304\n",
      "\t Training loss (single batch): 1.258733868598938\n",
      "\t Training loss (single batch): 1.0497947931289673\n",
      "\t Training loss (single batch): 0.9870631098747253\n",
      "\t Training loss (single batch): 1.1689722537994385\n",
      "\t Training loss (single batch): 1.4828053712844849\n",
      "\t Training loss (single batch): 1.0085560083389282\n",
      "\t Training loss (single batch): 0.8657273650169373\n",
      "\t Training loss (single batch): 1.1820473670959473\n",
      "\t Training loss (single batch): 1.0383801460266113\n",
      "\t Training loss (single batch): 1.4927351474761963\n",
      "\t Training loss (single batch): 0.9289464354515076\n",
      "\t Training loss (single batch): 1.097788691520691\n",
      "\t Training loss (single batch): 0.8595060110092163\n",
      "\t Training loss (single batch): 0.9867613911628723\n",
      "\t Training loss (single batch): 0.9742453694343567\n",
      "\t Training loss (single batch): 1.126775860786438\n",
      "\t Training loss (single batch): 1.1458262205123901\n",
      "\t Training loss (single batch): 1.3438154458999634\n",
      "\t Training loss (single batch): 1.3201855421066284\n",
      "\t Training loss (single batch): 1.3864810466766357\n",
      "\t Training loss (single batch): 1.0104304552078247\n",
      "\t Training loss (single batch): 1.323292851448059\n",
      "\t Training loss (single batch): 1.4249634742736816\n",
      "\t Training loss (single batch): 1.569917917251587\n",
      "\t Training loss (single batch): 0.6950356960296631\n",
      "\t Training loss (single batch): 1.168629765510559\n",
      "\t Training loss (single batch): 1.5562673807144165\n",
      "\t Training loss (single batch): 1.354250431060791\n",
      "\t Training loss (single batch): 0.9373530745506287\n",
      "\t Training loss (single batch): 1.6831600666046143\n",
      "\t Training loss (single batch): 1.1720235347747803\n",
      "\t Training loss (single batch): 0.7945336103439331\n",
      "\t Training loss (single batch): 1.2580522298812866\n",
      "\t Training loss (single batch): 1.0118818283081055\n",
      "\t Training loss (single batch): 0.7059531211853027\n",
      "\t Training loss (single batch): 1.2392936944961548\n",
      "\t Training loss (single batch): 0.9758275151252747\n",
      "\t Training loss (single batch): 1.1586627960205078\n",
      "\t Training loss (single batch): 0.902144193649292\n",
      "\t Training loss (single batch): 1.1653721332550049\n",
      "\t Training loss (single batch): 0.7911775708198547\n",
      "\t Training loss (single batch): 1.477448582649231\n",
      "\t Training loss (single batch): 1.5688748359680176\n",
      "\t Training loss (single batch): 1.5103250741958618\n",
      "\t Training loss (single batch): 1.2650771141052246\n",
      "\t Training loss (single batch): 1.1192947626113892\n",
      "\t Training loss (single batch): 0.7828751802444458\n",
      "\t Training loss (single batch): 1.1222351789474487\n",
      "\t Training loss (single batch): 0.7616446018218994\n",
      "\t Training loss (single batch): 1.5443570613861084\n",
      "\t Training loss (single batch): 1.5403913259506226\n",
      "\t Training loss (single batch): 1.192496418952942\n",
      "\t Training loss (single batch): 0.9379938840866089\n",
      "\t Training loss (single batch): 0.9689590930938721\n",
      "\t Training loss (single batch): 0.9430974721908569\n",
      "\t Training loss (single batch): 1.4200968742370605\n",
      "\t Training loss (single batch): 1.4724045991897583\n",
      "\t Training loss (single batch): 0.6396129131317139\n",
      "\t Training loss (single batch): 0.6459123492240906\n",
      "\t Training loss (single batch): 0.9269751310348511\n",
      "\t Training loss (single batch): 1.123083233833313\n",
      "\t Training loss (single batch): 1.7548396587371826\n",
      "\t Training loss (single batch): 1.243932843208313\n",
      "\t Training loss (single batch): 1.2412844896316528\n",
      "\t Training loss (single batch): 1.3543224334716797\n",
      "\t Training loss (single batch): 1.339608073234558\n",
      "\t Training loss (single batch): 1.6344584226608276\n",
      "\t Training loss (single batch): 1.5626670122146606\n",
      "\t Training loss (single batch): 1.2579134702682495\n",
      "\t Training loss (single batch): 1.5580538511276245\n",
      "\t Training loss (single batch): 1.745057225227356\n",
      "\t Training loss (single batch): 1.4819538593292236\n",
      "\t Training loss (single batch): 1.161686658859253\n",
      "\t Training loss (single batch): 0.7443527579307556\n",
      "\t Training loss (single batch): 1.4451662302017212\n",
      "\t Training loss (single batch): 1.0476008653640747\n",
      "\t Training loss (single batch): 0.872915506362915\n",
      "\t Training loss (single batch): 1.4463223218917847\n",
      "\t Training loss (single batch): 0.9841870069503784\n",
      "\t Training loss (single batch): 1.1524890661239624\n",
      "\t Training loss (single batch): 1.1068756580352783\n",
      "\t Training loss (single batch): 1.677411437034607\n",
      "\t Training loss (single batch): 0.9611318707466125\n",
      "\t Training loss (single batch): 1.6489580869674683\n",
      "\t Training loss (single batch): 0.899291455745697\n",
      "\t Training loss (single batch): 1.3478965759277344\n",
      "\t Training loss (single batch): 1.6033164262771606\n",
      "\t Training loss (single batch): 0.7679351568222046\n",
      "\t Training loss (single batch): 0.9365929961204529\n",
      "\t Training loss (single batch): 1.1861536502838135\n",
      "\t Training loss (single batch): 1.2424020767211914\n",
      "\t Training loss (single batch): 1.1983113288879395\n",
      "\t Training loss (single batch): 1.0773558616638184\n",
      "\t Training loss (single batch): 1.2552810907363892\n",
      "\t Training loss (single batch): 1.045527458190918\n",
      "\t Training loss (single batch): 1.2614729404449463\n",
      "\t Training loss (single batch): 1.5517243146896362\n",
      "\t Training loss (single batch): 1.473767638206482\n",
      "\t Training loss (single batch): 1.2968872785568237\n",
      "\t Training loss (single batch): 1.6847785711288452\n",
      "\t Training loss (single batch): 1.0846707820892334\n",
      "\t Training loss (single batch): 1.2367331981658936\n",
      "\t Training loss (single batch): 1.2850490808486938\n",
      "\t Training loss (single batch): 1.211314082145691\n",
      "\t Training loss (single batch): 1.1817984580993652\n",
      "\t Training loss (single batch): 1.2601776123046875\n",
      "\t Training loss (single batch): 1.3685507774353027\n",
      "\t Training loss (single batch): 0.9518693089485168\n",
      "\t Training loss (single batch): 1.6026132106781006\n",
      "\t Training loss (single batch): 1.0631035566329956\n",
      "\t Training loss (single batch): 1.1104503870010376\n",
      "\t Training loss (single batch): 1.0355507135391235\n",
      "\t Training loss (single batch): 1.3316452503204346\n",
      "\t Training loss (single batch): 0.6938772797584534\n",
      "\t Training loss (single batch): 0.7951499223709106\n",
      "\t Training loss (single batch): 1.3298594951629639\n",
      "\t Training loss (single batch): 1.2285199165344238\n",
      "\t Training loss (single batch): 1.0308125019073486\n",
      "\t Training loss (single batch): 0.980377197265625\n",
      "\t Training loss (single batch): 1.247573733329773\n",
      "\t Training loss (single batch): 1.1999207735061646\n",
      "\t Training loss (single batch): 0.6796053051948547\n",
      "\t Training loss (single batch): 1.4079008102416992\n",
      "\t Training loss (single batch): 1.3615221977233887\n",
      "\t Training loss (single batch): 1.2148412466049194\n",
      "\t Training loss (single batch): 1.4702798128128052\n",
      "\t Training loss (single batch): 1.3535786867141724\n",
      "\t Training loss (single batch): 1.0029892921447754\n",
      "\t Training loss (single batch): 1.2620298862457275\n",
      "\t Training loss (single batch): 1.2363293170928955\n",
      "\t Training loss (single batch): 1.004970669746399\n",
      "\t Training loss (single batch): 1.0046473741531372\n",
      "\t Training loss (single batch): 1.2786189317703247\n",
      "\t Training loss (single batch): 1.21840500831604\n",
      "\t Training loss (single batch): 1.856939435005188\n",
      "\t Training loss (single batch): 1.8613983392715454\n",
      "\t Training loss (single batch): 1.1165767908096313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0030171871185303\n",
      "\t Training loss (single batch): 1.2713687419891357\n",
      "\t Training loss (single batch): 1.0168631076812744\n",
      "\t Training loss (single batch): 1.035048484802246\n",
      "\t Training loss (single batch): 1.4128518104553223\n",
      "\t Training loss (single batch): 1.5945996046066284\n",
      "\t Training loss (single batch): 1.3436179161071777\n",
      "\t Training loss (single batch): 1.2943978309631348\n",
      "\t Training loss (single batch): 1.8604806661605835\n",
      "\t Training loss (single batch): 1.9804891347885132\n",
      "\t Training loss (single batch): 0.9607693552970886\n",
      "\t Training loss (single batch): 1.6250866651535034\n",
      "\t Training loss (single batch): 1.2433481216430664\n",
      "\t Training loss (single batch): 1.2471815347671509\n",
      "\t Training loss (single batch): 1.2520235776901245\n",
      "\t Training loss (single batch): 1.3780163526535034\n",
      "\t Training loss (single batch): 0.9755169153213501\n",
      "\t Training loss (single batch): 1.6447466611862183\n",
      "\t Training loss (single batch): 0.8885350227355957\n",
      "\t Training loss (single batch): 1.418752670288086\n",
      "\t Training loss (single batch): 1.3146538734436035\n",
      "\t Training loss (single batch): 1.2530443668365479\n",
      "\t Training loss (single batch): 1.1404417753219604\n",
      "\t Training loss (single batch): 1.3043948411941528\n",
      "\t Training loss (single batch): 1.2340524196624756\n",
      "\t Training loss (single batch): 1.0129516124725342\n",
      "\t Training loss (single batch): 1.246835708618164\n",
      "\t Training loss (single batch): 1.4258582592010498\n",
      "\t Training loss (single batch): 0.7985435724258423\n",
      "\t Training loss (single batch): 1.2717665433883667\n",
      "\t Training loss (single batch): 1.179896593093872\n",
      "\t Training loss (single batch): 1.0200556516647339\n",
      "\t Training loss (single batch): 1.0764532089233398\n",
      "\t Training loss (single batch): 1.0245460271835327\n",
      "\t Training loss (single batch): 1.3139058351516724\n",
      "\t Training loss (single batch): 0.8964943289756775\n",
      "\t Training loss (single batch): 1.3129050731658936\n",
      "\t Training loss (single batch): 0.8654288053512573\n",
      "\t Training loss (single batch): 1.1089264154434204\n",
      "\t Training loss (single batch): 0.9981937408447266\n",
      "\t Training loss (single batch): 1.0998543500900269\n",
      "\t Training loss (single batch): 1.4336670637130737\n",
      "\t Training loss (single batch): 0.6486179232597351\n",
      "\t Training loss (single batch): 0.7924553751945496\n",
      "\t Training loss (single batch): 1.165974497795105\n",
      "\t Training loss (single batch): 1.1440298557281494\n",
      "\t Training loss (single batch): 1.327130913734436\n",
      "\t Training loss (single batch): 1.3582510948181152\n",
      "\t Training loss (single batch): 1.226150631904602\n",
      "\t Training loss (single batch): 1.5728667974472046\n",
      "\t Training loss (single batch): 1.619770884513855\n",
      "\t Training loss (single batch): 1.246274709701538\n",
      "\t Training loss (single batch): 1.4316213130950928\n",
      "\t Training loss (single batch): 1.301016092300415\n",
      "\t Training loss (single batch): 1.1509469747543335\n",
      "\t Training loss (single batch): 1.455361008644104\n",
      "\t Training loss (single batch): 1.2065773010253906\n",
      "\t Training loss (single batch): 1.1946964263916016\n",
      "\t Training loss (single batch): 1.1223443746566772\n",
      "\t Training loss (single batch): 1.5461881160736084\n",
      "\t Training loss (single batch): 0.9823987483978271\n",
      "\t Training loss (single batch): 1.2070828676223755\n",
      "\t Training loss (single batch): 0.8363777995109558\n",
      "\t Training loss (single batch): 1.4771819114685059\n",
      "\t Training loss (single batch): 1.022878885269165\n",
      "\t Training loss (single batch): 1.0990360975265503\n",
      "\t Training loss (single batch): 1.2306578159332275\n",
      "\t Training loss (single batch): 1.7932605743408203\n",
      "\t Training loss (single batch): 1.05557119846344\n",
      "\t Training loss (single batch): 1.4749908447265625\n",
      "\t Training loss (single batch): 1.8724926710128784\n",
      "\t Training loss (single batch): 1.260374665260315\n",
      "\t Training loss (single batch): 1.3261899948120117\n",
      "\t Training loss (single batch): 0.9458476305007935\n",
      "\t Training loss (single batch): 1.4595305919647217\n",
      "\t Training loss (single batch): 1.0773983001708984\n",
      "\t Training loss (single batch): 0.9971355199813843\n",
      "\t Training loss (single batch): 1.2089664936065674\n",
      "\t Training loss (single batch): 1.0554782152175903\n",
      "\t Training loss (single batch): 1.5838546752929688\n",
      "\t Training loss (single batch): 1.7344774007797241\n",
      "\t Training loss (single batch): 1.37031090259552\n",
      "\t Training loss (single batch): 1.3288180828094482\n",
      "\t Training loss (single batch): 0.6402567028999329\n",
      "\t Training loss (single batch): 1.135914921760559\n",
      "\t Training loss (single batch): 1.4572049379348755\n",
      "\t Training loss (single batch): 1.0464295148849487\n",
      "\t Training loss (single batch): 1.9272414445877075\n",
      "\t Training loss (single batch): 1.2701091766357422\n",
      "\t Training loss (single batch): 1.8339229822158813\n",
      "\t Training loss (single batch): 1.2807542085647583\n",
      "\t Training loss (single batch): 0.8098787069320679\n",
      "\t Training loss (single batch): 1.2053419351577759\n",
      "\t Training loss (single batch): 1.0132360458374023\n",
      "\t Training loss (single batch): 0.843813419342041\n",
      "\t Training loss (single batch): 0.9787012338638306\n",
      "\t Training loss (single batch): 0.9310689568519592\n",
      "\t Training loss (single batch): 1.2983863353729248\n",
      "\t Training loss (single batch): 1.2831387519836426\n",
      "\t Training loss (single batch): 1.1308773756027222\n",
      "\t Training loss (single batch): 0.7582619786262512\n",
      "\t Training loss (single batch): 0.9995853900909424\n",
      "\t Training loss (single batch): 1.1037498712539673\n",
      "\t Training loss (single batch): 1.6248489618301392\n",
      "\t Training loss (single batch): 1.1397058963775635\n",
      "\t Training loss (single batch): 1.3290997743606567\n",
      "\t Training loss (single batch): 1.1297253370285034\n",
      "\t Training loss (single batch): 1.1594696044921875\n",
      "\t Training loss (single batch): 1.3223493099212646\n",
      "\t Training loss (single batch): 1.2064629793167114\n",
      "\t Training loss (single batch): 1.6690988540649414\n",
      "\t Training loss (single batch): 1.2313534021377563\n",
      "\t Training loss (single batch): 1.242447853088379\n",
      "\t Training loss (single batch): 1.0932830572128296\n",
      "\t Training loss (single batch): 1.1727052927017212\n",
      "\t Training loss (single batch): 0.9510501623153687\n",
      "\t Training loss (single batch): 1.283491849899292\n",
      "\t Training loss (single batch): 1.0382652282714844\n",
      "\t Training loss (single batch): 1.053246021270752\n",
      "\t Training loss (single batch): 0.9113513231277466\n",
      "\t Training loss (single batch): 1.2193893194198608\n",
      "\t Training loss (single batch): 1.580061674118042\n",
      "\t Training loss (single batch): 1.4618549346923828\n",
      "\t Training loss (single batch): 1.2050917148590088\n",
      "\t Training loss (single batch): 0.9357684254646301\n",
      "\t Training loss (single batch): 1.5590115785598755\n",
      "\t Training loss (single batch): 1.6616390943527222\n",
      "\t Training loss (single batch): 0.9951391816139221\n",
      "\t Training loss (single batch): 1.2883827686309814\n",
      "\t Training loss (single batch): 1.6420971155166626\n",
      "\t Training loss (single batch): 1.697725772857666\n",
      "\t Training loss (single batch): 1.1618647575378418\n",
      "\t Training loss (single batch): 1.2883323431015015\n",
      "\t Training loss (single batch): 1.437954068183899\n",
      "\t Training loss (single batch): 1.2910810708999634\n",
      "\t Training loss (single batch): 1.511854648590088\n",
      "\t Training loss (single batch): 0.7957825660705566\n",
      "\t Training loss (single batch): 0.6998023390769958\n",
      "\t Training loss (single batch): 1.3894213438034058\n",
      "\t Training loss (single batch): 1.20540452003479\n",
      "\t Training loss (single batch): 1.0246672630310059\n",
      "\t Training loss (single batch): 1.4759615659713745\n",
      "\t Training loss (single batch): 1.1023423671722412\n",
      "\t Training loss (single batch): 1.007842779159546\n",
      "\t Training loss (single batch): 1.108293890953064\n",
      "\t Training loss (single batch): 1.0261269807815552\n",
      "\t Training loss (single batch): 1.6944226026535034\n",
      "\t Training loss (single batch): 1.0859322547912598\n",
      "\t Training loss (single batch): 1.438431739807129\n",
      "\t Training loss (single batch): 2.7546346187591553\n",
      "##################################\n",
      "## EPOCH 62\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1062315702438354\n",
      "\t Training loss (single batch): 1.2531405687332153\n",
      "\t Training loss (single batch): 1.0146560668945312\n",
      "\t Training loss (single batch): 1.124435305595398\n",
      "\t Training loss (single batch): 1.2186139822006226\n",
      "\t Training loss (single batch): 1.1160128116607666\n",
      "\t Training loss (single batch): 1.2792385816574097\n",
      "\t Training loss (single batch): 1.1449906826019287\n",
      "\t Training loss (single batch): 0.9331843852996826\n",
      "\t Training loss (single batch): 1.2047693729400635\n",
      "\t Training loss (single batch): 1.308964490890503\n",
      "\t Training loss (single batch): 1.0732793807983398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9732989072799683\n",
      "\t Training loss (single batch): 1.1436482667922974\n",
      "\t Training loss (single batch): 1.5212862491607666\n",
      "\t Training loss (single batch): 1.1369199752807617\n",
      "\t Training loss (single batch): 1.7313592433929443\n",
      "\t Training loss (single batch): 1.2929611206054688\n",
      "\t Training loss (single batch): 1.2785929441452026\n",
      "\t Training loss (single batch): 1.383796215057373\n",
      "\t Training loss (single batch): 0.9530463218688965\n",
      "\t Training loss (single batch): 1.9478110074996948\n",
      "\t Training loss (single batch): 1.6324570178985596\n",
      "\t Training loss (single batch): 1.1991126537322998\n",
      "\t Training loss (single batch): 1.5968705415725708\n",
      "\t Training loss (single batch): 1.32980215549469\n",
      "\t Training loss (single batch): 1.0641603469848633\n",
      "\t Training loss (single batch): 1.1704589128494263\n",
      "\t Training loss (single batch): 0.8015815019607544\n",
      "\t Training loss (single batch): 1.197137713432312\n",
      "\t Training loss (single batch): 1.7367053031921387\n",
      "\t Training loss (single batch): 1.5023341178894043\n",
      "\t Training loss (single batch): 1.2471028566360474\n",
      "\t Training loss (single batch): 1.194726824760437\n",
      "\t Training loss (single batch): 0.910636842250824\n",
      "\t Training loss (single batch): 0.8646848797798157\n",
      "\t Training loss (single batch): 1.151647686958313\n",
      "\t Training loss (single batch): 1.3365620374679565\n",
      "\t Training loss (single batch): 1.1311495304107666\n",
      "\t Training loss (single batch): 0.9256812930107117\n",
      "\t Training loss (single batch): 1.9344103336334229\n",
      "\t Training loss (single batch): 1.393088459968567\n",
      "\t Training loss (single batch): 1.406915545463562\n",
      "\t Training loss (single batch): 1.1740977764129639\n",
      "\t Training loss (single batch): 1.1304460763931274\n",
      "\t Training loss (single batch): 1.1474487781524658\n",
      "\t Training loss (single batch): 0.8674460053443909\n",
      "\t Training loss (single batch): 1.2577409744262695\n",
      "\t Training loss (single batch): 1.1329149007797241\n",
      "\t Training loss (single batch): 1.1915007829666138\n",
      "\t Training loss (single batch): 1.613311767578125\n",
      "\t Training loss (single batch): 1.503172516822815\n",
      "\t Training loss (single batch): 0.7928072214126587\n",
      "\t Training loss (single batch): 1.6693140268325806\n",
      "\t Training loss (single batch): 1.2107537984848022\n",
      "\t Training loss (single batch): 1.437312126159668\n",
      "\t Training loss (single batch): 1.5169190168380737\n",
      "\t Training loss (single batch): 1.2517683506011963\n",
      "\t Training loss (single batch): 1.2648152112960815\n",
      "\t Training loss (single batch): 1.1733418703079224\n",
      "\t Training loss (single batch): 1.043843388557434\n",
      "\t Training loss (single batch): 1.2679908275604248\n",
      "\t Training loss (single batch): 0.9648624658584595\n",
      "\t Training loss (single batch): 1.4794611930847168\n",
      "\t Training loss (single batch): 1.4041333198547363\n",
      "\t Training loss (single batch): 1.6287144422531128\n",
      "\t Training loss (single batch): 1.1217641830444336\n",
      "\t Training loss (single batch): 0.8571673631668091\n",
      "\t Training loss (single batch): 1.0644763708114624\n",
      "\t Training loss (single batch): 1.0936720371246338\n",
      "\t Training loss (single batch): 1.1231396198272705\n",
      "\t Training loss (single batch): 0.8813968300819397\n",
      "\t Training loss (single batch): 1.3938872814178467\n",
      "\t Training loss (single batch): 1.0972901582717896\n",
      "\t Training loss (single batch): 1.0668522119522095\n",
      "\t Training loss (single batch): 1.4886173009872437\n",
      "\t Training loss (single batch): 1.276354432106018\n",
      "\t Training loss (single batch): 1.0402129888534546\n",
      "\t Training loss (single batch): 1.5048553943634033\n",
      "\t Training loss (single batch): 1.0092434883117676\n",
      "\t Training loss (single batch): 1.40064537525177\n",
      "\t Training loss (single batch): 1.215047001838684\n",
      "\t Training loss (single batch): 1.3644222021102905\n",
      "\t Training loss (single batch): 0.8955768346786499\n",
      "\t Training loss (single batch): 1.3075528144836426\n",
      "\t Training loss (single batch): 1.2297329902648926\n",
      "\t Training loss (single batch): 1.2834566831588745\n",
      "\t Training loss (single batch): 1.056477665901184\n",
      "\t Training loss (single batch): 1.8954955339431763\n",
      "\t Training loss (single batch): 1.1357526779174805\n",
      "\t Training loss (single batch): 1.2238414287567139\n",
      "\t Training loss (single batch): 1.2752552032470703\n",
      "\t Training loss (single batch): 0.8063479661941528\n",
      "\t Training loss (single batch): 1.1326137781143188\n",
      "\t Training loss (single batch): 1.3620719909667969\n",
      "\t Training loss (single batch): 0.8749979734420776\n",
      "\t Training loss (single batch): 2.1023783683776855\n",
      "\t Training loss (single batch): 1.338111162185669\n",
      "\t Training loss (single batch): 1.000309944152832\n",
      "\t Training loss (single batch): 1.7237260341644287\n",
      "\t Training loss (single batch): 1.3986912965774536\n",
      "\t Training loss (single batch): 1.2828402519226074\n",
      "\t Training loss (single batch): 0.931806743144989\n",
      "\t Training loss (single batch): 1.2787024974822998\n",
      "\t Training loss (single batch): 1.3524525165557861\n",
      "\t Training loss (single batch): 1.0904326438903809\n",
      "\t Training loss (single batch): 1.3646690845489502\n",
      "\t Training loss (single batch): 1.3239226341247559\n",
      "\t Training loss (single batch): 1.1360801458358765\n",
      "\t Training loss (single batch): 1.002692699432373\n",
      "\t Training loss (single batch): 1.2302113771438599\n",
      "\t Training loss (single batch): 0.9321073889732361\n",
      "\t Training loss (single batch): 1.358169436454773\n",
      "\t Training loss (single batch): 1.2216928005218506\n",
      "\t Training loss (single batch): 0.8291942477226257\n",
      "\t Training loss (single batch): 1.0146527290344238\n",
      "\t Training loss (single batch): 0.8166218400001526\n",
      "\t Training loss (single batch): 1.4276618957519531\n",
      "\t Training loss (single batch): 1.7039910554885864\n",
      "\t Training loss (single batch): 1.1092162132263184\n",
      "\t Training loss (single batch): 1.2901971340179443\n",
      "\t Training loss (single batch): 1.425321340560913\n",
      "\t Training loss (single batch): 0.803887128829956\n",
      "\t Training loss (single batch): 0.9606884121894836\n",
      "\t Training loss (single batch): 0.9896394610404968\n",
      "\t Training loss (single batch): 1.3754818439483643\n",
      "\t Training loss (single batch): 1.082001805305481\n",
      "\t Training loss (single batch): 1.6081520318984985\n",
      "\t Training loss (single batch): 0.8467381000518799\n",
      "\t Training loss (single batch): 0.8683497905731201\n",
      "\t Training loss (single batch): 1.2194511890411377\n",
      "\t Training loss (single batch): 1.7578797340393066\n",
      "\t Training loss (single batch): 1.3599061965942383\n",
      "\t Training loss (single batch): 1.3882986307144165\n",
      "\t Training loss (single batch): 0.8388319611549377\n",
      "\t Training loss (single batch): 1.174614429473877\n",
      "\t Training loss (single batch): 1.0585241317749023\n",
      "\t Training loss (single batch): 1.7801909446716309\n",
      "\t Training loss (single batch): 1.0166736841201782\n",
      "\t Training loss (single batch): 1.0662827491760254\n",
      "\t Training loss (single batch): 1.6445817947387695\n",
      "\t Training loss (single batch): 1.5692964792251587\n",
      "\t Training loss (single batch): 1.495467185974121\n",
      "\t Training loss (single batch): 1.0577781200408936\n",
      "\t Training loss (single batch): 1.3746263980865479\n",
      "\t Training loss (single batch): 1.313424825668335\n",
      "\t Training loss (single batch): 1.038448691368103\n",
      "\t Training loss (single batch): 0.9410573840141296\n",
      "\t Training loss (single batch): 1.0650607347488403\n",
      "\t Training loss (single batch): 1.004514217376709\n",
      "\t Training loss (single batch): 1.2863032817840576\n",
      "\t Training loss (single batch): 1.0645599365234375\n",
      "\t Training loss (single batch): 0.9977355599403381\n",
      "\t Training loss (single batch): 1.4259321689605713\n",
      "\t Training loss (single batch): 1.1248520612716675\n",
      "\t Training loss (single batch): 1.2540239095687866\n",
      "\t Training loss (single batch): 1.1935737133026123\n",
      "\t Training loss (single batch): 1.1610115766525269\n",
      "\t Training loss (single batch): 1.1511644124984741\n",
      "\t Training loss (single batch): 1.3337182998657227\n",
      "\t Training loss (single batch): 1.0384695529937744\n",
      "\t Training loss (single batch): 1.0932267904281616\n",
      "\t Training loss (single batch): 1.3207978010177612\n",
      "\t Training loss (single batch): 1.0467809438705444\n",
      "\t Training loss (single batch): 0.8428331017494202\n",
      "\t Training loss (single batch): 0.982769787311554\n",
      "\t Training loss (single batch): 0.8066906332969666\n",
      "\t Training loss (single batch): 1.5603814125061035\n",
      "\t Training loss (single batch): 0.96712726354599\n",
      "\t Training loss (single batch): 1.190775752067566\n",
      "\t Training loss (single batch): 0.9550255537033081\n",
      "\t Training loss (single batch): 1.2908300161361694\n",
      "\t Training loss (single batch): 1.0213899612426758\n",
      "\t Training loss (single batch): 0.7983286380767822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0286645889282227\n",
      "\t Training loss (single batch): 1.0661062002182007\n",
      "\t Training loss (single batch): 1.4961256980895996\n",
      "\t Training loss (single batch): 1.2247284650802612\n",
      "\t Training loss (single batch): 1.157601237297058\n",
      "\t Training loss (single batch): 0.9013085961341858\n",
      "\t Training loss (single batch): 1.0636705160140991\n",
      "\t Training loss (single batch): 1.5894919633865356\n",
      "\t Training loss (single batch): 0.9678911566734314\n",
      "\t Training loss (single batch): 1.329155683517456\n",
      "\t Training loss (single batch): 1.1309269666671753\n",
      "\t Training loss (single batch): 1.214202642440796\n",
      "\t Training loss (single batch): 1.0620535612106323\n",
      "\t Training loss (single batch): 1.2195202112197876\n",
      "\t Training loss (single batch): 0.9069969058036804\n",
      "\t Training loss (single batch): 0.8515062928199768\n",
      "\t Training loss (single batch): 0.993400514125824\n",
      "\t Training loss (single batch): 0.9466241598129272\n",
      "\t Training loss (single batch): 1.7518872022628784\n",
      "\t Training loss (single batch): 1.1612660884857178\n",
      "\t Training loss (single batch): 1.0755054950714111\n",
      "\t Training loss (single batch): 1.264760971069336\n",
      "\t Training loss (single batch): 2.017317771911621\n",
      "\t Training loss (single batch): 0.8900300860404968\n",
      "\t Training loss (single batch): 1.5126097202301025\n",
      "\t Training loss (single batch): 1.3789069652557373\n",
      "\t Training loss (single batch): 1.7920925617218018\n",
      "\t Training loss (single batch): 1.3372385501861572\n",
      "\t Training loss (single batch): 0.7975051403045654\n",
      "\t Training loss (single batch): 0.9654713273048401\n",
      "\t Training loss (single batch): 0.8497718572616577\n",
      "\t Training loss (single batch): 1.3516515493392944\n",
      "\t Training loss (single batch): 1.6201432943344116\n",
      "\t Training loss (single batch): 1.1444114446640015\n",
      "\t Training loss (single batch): 0.7506963610649109\n",
      "\t Training loss (single batch): 1.2508955001831055\n",
      "\t Training loss (single batch): 1.4093906879425049\n",
      "\t Training loss (single batch): 1.1861718893051147\n",
      "\t Training loss (single batch): 0.941778838634491\n",
      "\t Training loss (single batch): 1.5310518741607666\n",
      "\t Training loss (single batch): 1.2464250326156616\n",
      "\t Training loss (single batch): 1.080886721611023\n",
      "\t Training loss (single batch): 1.462831735610962\n",
      "\t Training loss (single batch): 1.3232184648513794\n",
      "\t Training loss (single batch): 0.832586944103241\n",
      "\t Training loss (single batch): 1.1894924640655518\n",
      "\t Training loss (single batch): 1.4602147340774536\n",
      "\t Training loss (single batch): 0.897416889667511\n",
      "\t Training loss (single batch): 1.368025541305542\n",
      "\t Training loss (single batch): 1.4120173454284668\n",
      "\t Training loss (single batch): 1.666871428489685\n",
      "\t Training loss (single batch): 0.9952610731124878\n",
      "\t Training loss (single batch): 0.7065449953079224\n",
      "\t Training loss (single batch): 0.9626200199127197\n",
      "\t Training loss (single batch): 1.4963833093643188\n",
      "\t Training loss (single batch): 0.9564092755317688\n",
      "\t Training loss (single batch): 1.0928462743759155\n",
      "\t Training loss (single batch): 1.034281611442566\n",
      "\t Training loss (single batch): 1.3372581005096436\n",
      "\t Training loss (single batch): 0.9453656673431396\n",
      "\t Training loss (single batch): 1.2655444145202637\n",
      "\t Training loss (single batch): 1.2313787937164307\n",
      "\t Training loss (single batch): 1.022368311882019\n",
      "\t Training loss (single batch): 1.7666900157928467\n",
      "\t Training loss (single batch): 1.1111114025115967\n",
      "\t Training loss (single batch): 1.3155492544174194\n",
      "\t Training loss (single batch): 1.4121661186218262\n",
      "\t Training loss (single batch): 1.5636259317398071\n",
      "\t Training loss (single batch): 1.0772300958633423\n",
      "\t Training loss (single batch): 1.0136574506759644\n",
      "\t Training loss (single batch): 1.3098758459091187\n",
      "\t Training loss (single batch): 1.6885137557983398\n",
      "\t Training loss (single batch): 1.1945559978485107\n",
      "\t Training loss (single batch): 1.4153763055801392\n",
      "\t Training loss (single batch): 1.3418223857879639\n",
      "\t Training loss (single batch): 1.1868255138397217\n",
      "\t Training loss (single batch): 0.8429448008537292\n",
      "\t Training loss (single batch): 1.3304190635681152\n",
      "\t Training loss (single batch): 1.4436920881271362\n",
      "\t Training loss (single batch): 1.219012975692749\n",
      "\t Training loss (single batch): 1.1038953065872192\n",
      "\t Training loss (single batch): 1.2218563556671143\n",
      "\t Training loss (single batch): 1.0617942810058594\n",
      "\t Training loss (single batch): 0.9923005104064941\n",
      "\t Training loss (single batch): 0.7255645990371704\n",
      "\t Training loss (single batch): 0.9515528678894043\n",
      "\t Training loss (single batch): 0.9644137620925903\n",
      "\t Training loss (single batch): 1.6521642208099365\n",
      "\t Training loss (single batch): 1.0844056606292725\n",
      "\t Training loss (single batch): 1.3069878816604614\n",
      "\t Training loss (single batch): 0.8906249403953552\n",
      "\t Training loss (single batch): 1.1383342742919922\n",
      "\t Training loss (single batch): 0.9972527623176575\n",
      "\t Training loss (single batch): 1.3092803955078125\n",
      "\t Training loss (single batch): 0.8897202610969543\n",
      "\t Training loss (single batch): 1.1425540447235107\n",
      "\t Training loss (single batch): 1.3002229928970337\n",
      "\t Training loss (single batch): 0.8004817962646484\n",
      "\t Training loss (single batch): 1.0011769533157349\n",
      "\t Training loss (single batch): 1.5029093027114868\n",
      "\t Training loss (single batch): 1.1132004261016846\n",
      "\t Training loss (single batch): 1.4021042585372925\n",
      "\t Training loss (single batch): 1.0800561904907227\n",
      "\t Training loss (single batch): 0.7794886827468872\n",
      "\t Training loss (single batch): 1.345406413078308\n",
      "\t Training loss (single batch): 1.4153817892074585\n",
      "\t Training loss (single batch): 1.2713122367858887\n",
      "\t Training loss (single batch): 1.2674336433410645\n",
      "\t Training loss (single batch): 1.3279023170471191\n",
      "\t Training loss (single batch): 1.1070767641067505\n",
      "\t Training loss (single batch): 1.1787919998168945\n",
      "\t Training loss (single batch): 1.3102411031723022\n",
      "\t Training loss (single batch): 1.136494517326355\n",
      "\t Training loss (single batch): 1.4966439008712769\n",
      "\t Training loss (single batch): 0.9173133373260498\n",
      "\t Training loss (single batch): 1.3259577751159668\n",
      "\t Training loss (single batch): 1.2558248043060303\n",
      "\t Training loss (single batch): 1.0894030332565308\n",
      "\t Training loss (single batch): 1.2903228998184204\n",
      "\t Training loss (single batch): 1.2219879627227783\n",
      "\t Training loss (single batch): 1.1954848766326904\n",
      "\t Training loss (single batch): 1.4498937129974365\n",
      "\t Training loss (single batch): 1.103166937828064\n",
      "\t Training loss (single batch): 1.0328346490859985\n",
      "\t Training loss (single batch): 1.2404531240463257\n",
      "\t Training loss (single batch): 0.9409793615341187\n",
      "\t Training loss (single batch): 1.121569037437439\n",
      "\t Training loss (single batch): 1.6964298486709595\n",
      "\t Training loss (single batch): 1.463689923286438\n",
      "\t Training loss (single batch): 1.1903016567230225\n",
      "\t Training loss (single batch): 1.0101617574691772\n",
      "\t Training loss (single batch): 0.9191211462020874\n",
      "\t Training loss (single batch): 1.002092957496643\n",
      "\t Training loss (single batch): 1.3739533424377441\n",
      "\t Training loss (single batch): 1.3056923151016235\n",
      "\t Training loss (single batch): 2.0082666873931885\n",
      "\t Training loss (single batch): 1.368165135383606\n",
      "\t Training loss (single batch): 0.9993681311607361\n",
      "\t Training loss (single batch): 1.0172816514968872\n",
      "\t Training loss (single batch): 0.67835932970047\n",
      "\t Training loss (single batch): 1.2205920219421387\n",
      "\t Training loss (single batch): 1.4673583507537842\n",
      "\t Training loss (single batch): 0.35249149799346924\n",
      "##################################\n",
      "## EPOCH 63\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5825129747390747\n",
      "\t Training loss (single batch): 0.9164683818817139\n",
      "\t Training loss (single batch): 1.260785460472107\n",
      "\t Training loss (single batch): 0.8834133148193359\n",
      "\t Training loss (single batch): 0.999930202960968\n",
      "\t Training loss (single batch): 1.191898226737976\n",
      "\t Training loss (single batch): 1.3123422861099243\n",
      "\t Training loss (single batch): 1.1489942073822021\n",
      "\t Training loss (single batch): 2.1417338848114014\n",
      "\t Training loss (single batch): 1.0728782415390015\n",
      "\t Training loss (single batch): 1.54264497756958\n",
      "\t Training loss (single batch): 1.1083735227584839\n",
      "\t Training loss (single batch): 0.9977423548698425\n",
      "\t Training loss (single batch): 0.9358776211738586\n",
      "\t Training loss (single batch): 1.0180675983428955\n",
      "\t Training loss (single batch): 1.2798082828521729\n",
      "\t Training loss (single batch): 1.3311021327972412\n",
      "\t Training loss (single batch): 1.4428642988204956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.065504550933838\n",
      "\t Training loss (single batch): 1.0246105194091797\n",
      "\t Training loss (single batch): 1.2829269170761108\n",
      "\t Training loss (single batch): 1.2523386478424072\n",
      "\t Training loss (single batch): 1.3611109256744385\n",
      "\t Training loss (single batch): 1.172386884689331\n",
      "\t Training loss (single batch): 0.9248819351196289\n",
      "\t Training loss (single batch): 1.5158718824386597\n",
      "\t Training loss (single batch): 1.5217779874801636\n",
      "\t Training loss (single batch): 1.2112418413162231\n",
      "\t Training loss (single batch): 1.1810933351516724\n",
      "\t Training loss (single batch): 1.0066773891448975\n",
      "\t Training loss (single batch): 1.323167324066162\n",
      "\t Training loss (single batch): 1.263208270072937\n",
      "\t Training loss (single batch): 1.0911706686019897\n",
      "\t Training loss (single batch): 0.7218468189239502\n",
      "\t Training loss (single batch): 1.2523471117019653\n",
      "\t Training loss (single batch): 1.2145968675613403\n",
      "\t Training loss (single batch): 0.8533793091773987\n",
      "\t Training loss (single batch): 1.1749776601791382\n",
      "\t Training loss (single batch): 1.3238046169281006\n",
      "\t Training loss (single batch): 1.0626096725463867\n",
      "\t Training loss (single batch): 0.9594932794570923\n",
      "\t Training loss (single batch): 1.1294524669647217\n",
      "\t Training loss (single batch): 1.6185626983642578\n",
      "\t Training loss (single batch): 1.2151354551315308\n",
      "\t Training loss (single batch): 1.0317221879959106\n",
      "\t Training loss (single batch): 1.2152178287506104\n",
      "\t Training loss (single batch): 1.5910283327102661\n",
      "\t Training loss (single batch): 1.1059919595718384\n",
      "\t Training loss (single batch): 1.4463571310043335\n",
      "\t Training loss (single batch): 1.2491220235824585\n",
      "\t Training loss (single batch): 1.2464652061462402\n",
      "\t Training loss (single batch): 1.1068248748779297\n",
      "\t Training loss (single batch): 1.4169968366622925\n",
      "\t Training loss (single batch): 0.9909546971321106\n",
      "\t Training loss (single batch): 1.3443416357040405\n",
      "\t Training loss (single batch): 1.0618716478347778\n",
      "\t Training loss (single batch): 0.8778882026672363\n",
      "\t Training loss (single batch): 1.5346121788024902\n",
      "\t Training loss (single batch): 1.1164250373840332\n",
      "\t Training loss (single batch): 1.286495327949524\n",
      "\t Training loss (single batch): 1.6279600858688354\n",
      "\t Training loss (single batch): 1.545891523361206\n",
      "\t Training loss (single batch): 0.6309908032417297\n",
      "\t Training loss (single batch): 0.9768499135971069\n",
      "\t Training loss (single batch): 1.626624345779419\n",
      "\t Training loss (single batch): 0.8267210721969604\n",
      "\t Training loss (single batch): 1.372656226158142\n",
      "\t Training loss (single batch): 1.2627685070037842\n",
      "\t Training loss (single batch): 1.0507429838180542\n",
      "\t Training loss (single batch): 1.2933709621429443\n",
      "\t Training loss (single batch): 1.308936595916748\n",
      "\t Training loss (single batch): 0.8215142488479614\n",
      "\t Training loss (single batch): 1.5573954582214355\n",
      "\t Training loss (single batch): 1.7129979133605957\n",
      "\t Training loss (single batch): 1.1031560897827148\n",
      "\t Training loss (single batch): 1.1436139345169067\n",
      "\t Training loss (single batch): 1.2566213607788086\n",
      "\t Training loss (single batch): 1.189447045326233\n",
      "\t Training loss (single batch): 1.2397867441177368\n",
      "\t Training loss (single batch): 1.3982630968093872\n",
      "\t Training loss (single batch): 0.9108704328536987\n",
      "\t Training loss (single batch): 1.3599746227264404\n",
      "\t Training loss (single batch): 1.0234589576721191\n",
      "\t Training loss (single batch): 1.279179573059082\n",
      "\t Training loss (single batch): 1.2396667003631592\n",
      "\t Training loss (single batch): 0.7471897006034851\n",
      "\t Training loss (single batch): 1.5842355489730835\n",
      "\t Training loss (single batch): 1.4679096937179565\n",
      "\t Training loss (single batch): 0.8579668998718262\n",
      "\t Training loss (single batch): 1.7097351551055908\n",
      "\t Training loss (single batch): 1.2016650438308716\n",
      "\t Training loss (single batch): 1.2164908647537231\n",
      "\t Training loss (single batch): 1.7851711511611938\n",
      "\t Training loss (single batch): 1.4269791841506958\n",
      "\t Training loss (single batch): 1.0989689826965332\n",
      "\t Training loss (single batch): 1.551077127456665\n",
      "\t Training loss (single batch): 1.1809693574905396\n",
      "\t Training loss (single batch): 1.2761248350143433\n",
      "\t Training loss (single batch): 1.0748237371444702\n",
      "\t Training loss (single batch): 1.2815184593200684\n",
      "\t Training loss (single batch): 1.089898705482483\n",
      "\t Training loss (single batch): 1.1756097078323364\n",
      "\t Training loss (single batch): 1.4178187847137451\n",
      "\t Training loss (single batch): 1.2312910556793213\n",
      "\t Training loss (single batch): 1.2545338869094849\n",
      "\t Training loss (single batch): 1.0013211965560913\n",
      "\t Training loss (single batch): 0.9852197170257568\n",
      "\t Training loss (single batch): 1.2165776491165161\n",
      "\t Training loss (single batch): 1.047521710395813\n",
      "\t Training loss (single batch): 1.0043158531188965\n",
      "\t Training loss (single batch): 1.0064862966537476\n",
      "\t Training loss (single batch): 1.1080039739608765\n",
      "\t Training loss (single batch): 0.9957765340805054\n",
      "\t Training loss (single batch): 1.3180603981018066\n",
      "\t Training loss (single batch): 1.1045559644699097\n",
      "\t Training loss (single batch): 1.4228976964950562\n",
      "\t Training loss (single batch): 1.800405740737915\n",
      "\t Training loss (single batch): 2.0309083461761475\n",
      "\t Training loss (single batch): 1.551445722579956\n",
      "\t Training loss (single batch): 1.384880542755127\n",
      "\t Training loss (single batch): 1.2500344514846802\n",
      "\t Training loss (single batch): 1.2252589464187622\n",
      "\t Training loss (single batch): 1.144789695739746\n",
      "\t Training loss (single batch): 1.3914002180099487\n",
      "\t Training loss (single batch): 1.167696475982666\n",
      "\t Training loss (single batch): 1.0029984712600708\n",
      "\t Training loss (single batch): 1.1264439821243286\n",
      "\t Training loss (single batch): 1.1003468036651611\n",
      "\t Training loss (single batch): 1.1242421865463257\n",
      "\t Training loss (single batch): 1.120539665222168\n",
      "\t Training loss (single batch): 1.318110466003418\n",
      "\t Training loss (single batch): 1.0096659660339355\n",
      "\t Training loss (single batch): 1.5262278318405151\n",
      "\t Training loss (single batch): 1.4802173376083374\n",
      "\t Training loss (single batch): 1.1812031269073486\n",
      "\t Training loss (single batch): 1.337460994720459\n",
      "\t Training loss (single batch): 1.9441903829574585\n",
      "\t Training loss (single batch): 1.1643624305725098\n",
      "\t Training loss (single batch): 1.2196040153503418\n",
      "\t Training loss (single batch): 1.0786648988723755\n",
      "\t Training loss (single batch): 1.5757759809494019\n",
      "\t Training loss (single batch): 1.0647544860839844\n",
      "\t Training loss (single batch): 1.1304384469985962\n",
      "\t Training loss (single batch): 1.220805048942566\n",
      "\t Training loss (single batch): 1.6423554420471191\n",
      "\t Training loss (single batch): 1.0009493827819824\n",
      "\t Training loss (single batch): 1.0388678312301636\n",
      "\t Training loss (single batch): 0.9577633142471313\n",
      "\t Training loss (single batch): 1.1728167533874512\n",
      "\t Training loss (single batch): 1.0819892883300781\n",
      "\t Training loss (single batch): 1.2348828315734863\n",
      "\t Training loss (single batch): 0.9828636646270752\n",
      "\t Training loss (single batch): 1.078899621963501\n",
      "\t Training loss (single batch): 1.2700473070144653\n",
      "\t Training loss (single batch): 1.2368017435073853\n",
      "\t Training loss (single batch): 1.6305288076400757\n",
      "\t Training loss (single batch): 1.3666870594024658\n",
      "\t Training loss (single batch): 1.5137150287628174\n",
      "\t Training loss (single batch): 1.051809310913086\n",
      "\t Training loss (single batch): 0.7295662760734558\n",
      "\t Training loss (single batch): 1.8427797555923462\n",
      "\t Training loss (single batch): 0.9682535529136658\n",
      "\t Training loss (single batch): 1.4032026529312134\n",
      "\t Training loss (single batch): 1.0271234512329102\n",
      "\t Training loss (single batch): 1.2164522409439087\n",
      "\t Training loss (single batch): 1.199468731880188\n",
      "\t Training loss (single batch): 0.9072674512863159\n",
      "\t Training loss (single batch): 1.5097146034240723\n",
      "\t Training loss (single batch): 1.045352578163147\n",
      "\t Training loss (single batch): 1.047148585319519\n",
      "\t Training loss (single batch): 0.9922485947608948\n",
      "\t Training loss (single batch): 1.5305334329605103\n",
      "\t Training loss (single batch): 1.1447477340698242\n",
      "\t Training loss (single batch): 1.0671074390411377\n",
      "\t Training loss (single batch): 0.9495846033096313\n",
      "\t Training loss (single batch): 1.0695172548294067\n",
      "\t Training loss (single batch): 1.3554582595825195\n",
      "\t Training loss (single batch): 2.0479226112365723\n",
      "\t Training loss (single batch): 1.011978268623352\n",
      "\t Training loss (single batch): 1.032361626625061\n",
      "\t Training loss (single batch): 1.6625757217407227\n",
      "\t Training loss (single batch): 0.9054311513900757\n",
      "\t Training loss (single batch): 1.409225583076477\n",
      "\t Training loss (single batch): 1.204057216644287\n",
      "\t Training loss (single batch): 1.5833178758621216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1295865774154663\n",
      "\t Training loss (single batch): 1.301297664642334\n",
      "\t Training loss (single batch): 1.556239128112793\n",
      "\t Training loss (single batch): 1.4444336891174316\n",
      "\t Training loss (single batch): 1.4584115743637085\n",
      "\t Training loss (single batch): 1.166306495666504\n",
      "\t Training loss (single batch): 1.1477868556976318\n",
      "\t Training loss (single batch): 0.8438556790351868\n",
      "\t Training loss (single batch): 1.0118557214736938\n",
      "\t Training loss (single batch): 1.1296348571777344\n",
      "\t Training loss (single batch): 1.000953197479248\n",
      "\t Training loss (single batch): 1.164120078086853\n",
      "\t Training loss (single batch): 1.9984978437423706\n",
      "\t Training loss (single batch): 1.0717774629592896\n",
      "\t Training loss (single batch): 1.5371129512786865\n",
      "\t Training loss (single batch): 1.027571201324463\n",
      "\t Training loss (single batch): 1.5497747659683228\n",
      "\t Training loss (single batch): 1.0679036378860474\n",
      "\t Training loss (single batch): 1.6470588445663452\n",
      "\t Training loss (single batch): 1.2030047178268433\n",
      "\t Training loss (single batch): 1.694293737411499\n",
      "\t Training loss (single batch): 0.9877411127090454\n",
      "\t Training loss (single batch): 1.510810375213623\n",
      "\t Training loss (single batch): 1.3135299682617188\n",
      "\t Training loss (single batch): 1.1189241409301758\n",
      "\t Training loss (single batch): 0.9332553744316101\n",
      "\t Training loss (single batch): 0.932774007320404\n",
      "\t Training loss (single batch): 1.044396996498108\n",
      "\t Training loss (single batch): 0.7397130727767944\n",
      "\t Training loss (single batch): 1.2363370656967163\n",
      "\t Training loss (single batch): 1.4663317203521729\n",
      "\t Training loss (single batch): 1.4981963634490967\n",
      "\t Training loss (single batch): 1.1055504083633423\n",
      "\t Training loss (single batch): 1.6385489702224731\n",
      "\t Training loss (single batch): 1.1454896926879883\n",
      "\t Training loss (single batch): 0.8753873705863953\n",
      "\t Training loss (single batch): 1.0729987621307373\n",
      "\t Training loss (single batch): 1.1500320434570312\n",
      "\t Training loss (single batch): 1.652459740638733\n",
      "\t Training loss (single batch): 1.623899221420288\n",
      "\t Training loss (single batch): 1.4927961826324463\n",
      "\t Training loss (single batch): 1.2938580513000488\n",
      "\t Training loss (single batch): 0.868789553642273\n",
      "\t Training loss (single batch): 1.2656937837600708\n",
      "\t Training loss (single batch): 1.1994645595550537\n",
      "\t Training loss (single batch): 1.2230629920959473\n",
      "\t Training loss (single batch): 1.1818889379501343\n",
      "\t Training loss (single batch): 1.394193410873413\n",
      "\t Training loss (single batch): 1.3865933418273926\n",
      "\t Training loss (single batch): 1.3036623001098633\n",
      "\t Training loss (single batch): 1.1258701086044312\n",
      "\t Training loss (single batch): 1.207826018333435\n",
      "\t Training loss (single batch): 1.1094979047775269\n",
      "\t Training loss (single batch): 1.2278852462768555\n",
      "\t Training loss (single batch): 1.330324649810791\n",
      "\t Training loss (single batch): 0.8623398542404175\n",
      "\t Training loss (single batch): 0.9545806646347046\n",
      "\t Training loss (single batch): 0.9626844525337219\n",
      "\t Training loss (single batch): 1.3546888828277588\n",
      "\t Training loss (single batch): 1.2213959693908691\n",
      "\t Training loss (single batch): 1.088254451751709\n",
      "\t Training loss (single batch): 1.6648889780044556\n",
      "\t Training loss (single batch): 1.1021143198013306\n",
      "\t Training loss (single batch): 0.9728187918663025\n",
      "\t Training loss (single batch): 1.4922425746917725\n",
      "\t Training loss (single batch): 1.0521551370620728\n",
      "\t Training loss (single batch): 1.595538854598999\n",
      "\t Training loss (single batch): 0.9555893540382385\n",
      "\t Training loss (single batch): 1.3481017351150513\n",
      "\t Training loss (single batch): 0.7501333951950073\n",
      "\t Training loss (single batch): 0.9092577695846558\n",
      "\t Training loss (single batch): 1.0401853322982788\n",
      "\t Training loss (single batch): 1.3637771606445312\n",
      "\t Training loss (single batch): 1.1847984790802002\n",
      "\t Training loss (single batch): 1.0955337285995483\n",
      "\t Training loss (single batch): 0.9149839282035828\n",
      "\t Training loss (single batch): 1.447438359260559\n",
      "\t Training loss (single batch): 1.4636939764022827\n",
      "\t Training loss (single batch): 0.9739379286766052\n",
      "\t Training loss (single batch): 1.5397340059280396\n",
      "\t Training loss (single batch): 1.2097114324569702\n",
      "\t Training loss (single batch): 1.299685001373291\n",
      "\t Training loss (single batch): 1.200992226600647\n",
      "\t Training loss (single batch): 1.1585843563079834\n",
      "\t Training loss (single batch): 1.566176414489746\n",
      "\t Training loss (single batch): 1.1073390245437622\n",
      "\t Training loss (single batch): 0.7519670724868774\n",
      "\t Training loss (single batch): 1.3875417709350586\n",
      "\t Training loss (single batch): 1.0803040266036987\n",
      "\t Training loss (single batch): 1.2034306526184082\n",
      "\t Training loss (single batch): 1.1954936981201172\n",
      "\t Training loss (single batch): 1.3292427062988281\n",
      "\t Training loss (single batch): 1.1179147958755493\n",
      "\t Training loss (single batch): 1.463182806968689\n",
      "\t Training loss (single batch): 1.220874547958374\n",
      "\t Training loss (single batch): 1.0476020574569702\n",
      "\t Training loss (single batch): 0.7527016997337341\n",
      "\t Training loss (single batch): 0.883246123790741\n",
      "\t Training loss (single batch): 1.0897157192230225\n",
      "\t Training loss (single batch): 1.4426349401474\n",
      "\t Training loss (single batch): 1.2369542121887207\n",
      "\t Training loss (single batch): 0.8783922791481018\n",
      "\t Training loss (single batch): 0.8996461629867554\n",
      "\t Training loss (single batch): 1.0699021816253662\n",
      "\t Training loss (single batch): 1.166161298751831\n",
      "\t Training loss (single batch): 0.7895208597183228\n",
      "\t Training loss (single batch): 1.153976321220398\n",
      "\t Training loss (single batch): 1.5695654153823853\n",
      "\t Training loss (single batch): 1.2447755336761475\n",
      "\t Training loss (single batch): 1.5731295347213745\n",
      "\t Training loss (single batch): 1.4094206094741821\n",
      "\t Training loss (single batch): 0.892238438129425\n",
      "\t Training loss (single batch): 1.014744520187378\n",
      "\t Training loss (single batch): 1.232316255569458\n",
      "\t Training loss (single batch): 1.1443809270858765\n",
      "\t Training loss (single batch): 1.1971012353897095\n",
      "\t Training loss (single batch): 0.970912754535675\n",
      "\t Training loss (single batch): 1.2387222051620483\n",
      "\t Training loss (single batch): 1.0257834196090698\n",
      "\t Training loss (single batch): 0.9796028733253479\n",
      "\t Training loss (single batch): 1.0339645147323608\n",
      "\t Training loss (single batch): 0.8999971747398376\n",
      "\t Training loss (single batch): 0.7586032152175903\n",
      "\t Training loss (single batch): 1.3731589317321777\n",
      "\t Training loss (single batch): 0.932095468044281\n",
      "\t Training loss (single batch): 1.340970754623413\n",
      "\t Training loss (single batch): 0.8952967524528503\n",
      "\t Training loss (single batch): 1.0804636478424072\n",
      "\t Training loss (single batch): 1.0696250200271606\n",
      "\t Training loss (single batch): 1.303882122039795\n",
      "\t Training loss (single batch): 0.9925104379653931\n",
      "\t Training loss (single batch): 0.6772111058235168\n",
      "##################################\n",
      "## EPOCH 64\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5500391721725464\n",
      "\t Training loss (single batch): 1.2452129125595093\n",
      "\t Training loss (single batch): 0.7973073124885559\n",
      "\t Training loss (single batch): 1.2061904668807983\n",
      "\t Training loss (single batch): 1.2204891443252563\n",
      "\t Training loss (single batch): 1.0903048515319824\n",
      "\t Training loss (single batch): 1.277641773223877\n",
      "\t Training loss (single batch): 1.1438430547714233\n",
      "\t Training loss (single batch): 1.1188583374023438\n",
      "\t Training loss (single batch): 1.1052035093307495\n",
      "\t Training loss (single batch): 1.3442354202270508\n",
      "\t Training loss (single batch): 1.467136025428772\n",
      "\t Training loss (single batch): 1.5431634187698364\n",
      "\t Training loss (single batch): 1.323857069015503\n",
      "\t Training loss (single batch): 1.1550318002700806\n",
      "\t Training loss (single batch): 1.3513290882110596\n",
      "\t Training loss (single batch): 1.1048858165740967\n",
      "\t Training loss (single batch): 1.3370238542556763\n",
      "\t Training loss (single batch): 1.1185994148254395\n",
      "\t Training loss (single batch): 1.148638367652893\n",
      "\t Training loss (single batch): 1.1168279647827148\n",
      "\t Training loss (single batch): 1.1712993383407593\n",
      "\t Training loss (single batch): 1.0183484554290771\n",
      "\t Training loss (single batch): 1.1728311777114868\n",
      "\t Training loss (single batch): 1.41618812084198\n",
      "\t Training loss (single batch): 1.181460976600647\n",
      "\t Training loss (single batch): 0.9623582363128662\n",
      "\t Training loss (single batch): 1.130961537361145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.553011178970337\n",
      "\t Training loss (single batch): 1.623547911643982\n",
      "\t Training loss (single batch): 1.0896775722503662\n",
      "\t Training loss (single batch): 0.9438608884811401\n",
      "\t Training loss (single batch): 0.7888994216918945\n",
      "\t Training loss (single batch): 0.8873862624168396\n",
      "\t Training loss (single batch): 0.687961757183075\n",
      "\t Training loss (single batch): 1.843051791191101\n",
      "\t Training loss (single batch): 1.6460829973220825\n",
      "\t Training loss (single batch): 1.444049596786499\n",
      "\t Training loss (single batch): 0.8506016135215759\n",
      "\t Training loss (single batch): 0.8884884715080261\n",
      "\t Training loss (single batch): 0.8788869976997375\n",
      "\t Training loss (single batch): 1.2140976190567017\n",
      "\t Training loss (single batch): 1.8775007724761963\n",
      "\t Training loss (single batch): 2.238715887069702\n",
      "\t Training loss (single batch): 1.6839241981506348\n",
      "\t Training loss (single batch): 1.3482192754745483\n",
      "\t Training loss (single batch): 0.8137190341949463\n",
      "\t Training loss (single batch): 1.3722259998321533\n",
      "\t Training loss (single batch): 1.0700639486312866\n",
      "\t Training loss (single batch): 1.7058449983596802\n",
      "\t Training loss (single batch): 1.0970205068588257\n",
      "\t Training loss (single batch): 1.2494059801101685\n",
      "\t Training loss (single batch): 1.3967795372009277\n",
      "\t Training loss (single batch): 1.309134602546692\n",
      "\t Training loss (single batch): 1.0461665391921997\n",
      "\t Training loss (single batch): 1.1015808582305908\n",
      "\t Training loss (single batch): 1.6325238943099976\n",
      "\t Training loss (single batch): 0.9570913314819336\n",
      "\t Training loss (single batch): 1.2784401178359985\n",
      "\t Training loss (single batch): 1.1103721857070923\n",
      "\t Training loss (single batch): 1.2448616027832031\n",
      "\t Training loss (single batch): 0.9155953526496887\n",
      "\t Training loss (single batch): 1.13999342918396\n",
      "\t Training loss (single batch): 0.9884997010231018\n",
      "\t Training loss (single batch): 1.590636968612671\n",
      "\t Training loss (single batch): 0.8293763399124146\n",
      "\t Training loss (single batch): 1.23823082447052\n",
      "\t Training loss (single batch): 1.2133901119232178\n",
      "\t Training loss (single batch): 0.9706583023071289\n",
      "\t Training loss (single batch): 1.0856088399887085\n",
      "\t Training loss (single batch): 1.3639869689941406\n",
      "\t Training loss (single batch): 0.9197903871536255\n",
      "\t Training loss (single batch): 0.8059922456741333\n",
      "\t Training loss (single batch): 1.403294324874878\n",
      "\t Training loss (single batch): 1.2948193550109863\n",
      "\t Training loss (single batch): 1.8614747524261475\n",
      "\t Training loss (single batch): 1.2554763555526733\n",
      "\t Training loss (single batch): 1.0642503499984741\n",
      "\t Training loss (single batch): 1.1318039894104004\n",
      "\t Training loss (single batch): 0.9901898503303528\n",
      "\t Training loss (single batch): 1.661266803741455\n",
      "\t Training loss (single batch): 1.2022669315338135\n",
      "\t Training loss (single batch): 1.340165138244629\n",
      "\t Training loss (single batch): 1.1228877305984497\n",
      "\t Training loss (single batch): 1.0617419481277466\n",
      "\t Training loss (single batch): 0.9372969269752502\n",
      "\t Training loss (single batch): 1.1475257873535156\n",
      "\t Training loss (single batch): 0.8893925547599792\n",
      "\t Training loss (single batch): 0.98944491147995\n",
      "\t Training loss (single batch): 1.1584395170211792\n",
      "\t Training loss (single batch): 1.4185364246368408\n",
      "\t Training loss (single batch): 1.121566653251648\n",
      "\t Training loss (single batch): 1.0121842622756958\n",
      "\t Training loss (single batch): 0.8743268847465515\n",
      "\t Training loss (single batch): 0.6110372543334961\n",
      "\t Training loss (single batch): 1.4763790369033813\n",
      "\t Training loss (single batch): 1.023026704788208\n",
      "\t Training loss (single batch): 1.193987488746643\n",
      "\t Training loss (single batch): 0.8275816440582275\n",
      "\t Training loss (single batch): 1.341671109199524\n",
      "\t Training loss (single batch): 0.994385838508606\n",
      "\t Training loss (single batch): 1.405364990234375\n",
      "\t Training loss (single batch): 1.272402048110962\n",
      "\t Training loss (single batch): 0.8724696636199951\n",
      "\t Training loss (single batch): 0.6919165253639221\n",
      "\t Training loss (single batch): 0.7772831916809082\n",
      "\t Training loss (single batch): 1.260713815689087\n",
      "\t Training loss (single batch): 1.3893362283706665\n",
      "\t Training loss (single batch): 1.3075767755508423\n",
      "\t Training loss (single batch): 1.3226044178009033\n",
      "\t Training loss (single batch): 1.2262389659881592\n",
      "\t Training loss (single batch): 0.8449980616569519\n",
      "\t Training loss (single batch): 0.8730435371398926\n",
      "\t Training loss (single batch): 1.4295498132705688\n",
      "\t Training loss (single batch): 1.4675663709640503\n",
      "\t Training loss (single batch): 0.8789688944816589\n",
      "\t Training loss (single batch): 1.1188879013061523\n",
      "\t Training loss (single batch): 1.049913763999939\n",
      "\t Training loss (single batch): 0.8655304312705994\n",
      "\t Training loss (single batch): 1.367126226425171\n",
      "\t Training loss (single batch): 1.0901148319244385\n",
      "\t Training loss (single batch): 1.5511746406555176\n",
      "\t Training loss (single batch): 1.2513775825500488\n",
      "\t Training loss (single batch): 1.0459973812103271\n",
      "\t Training loss (single batch): 1.3916348218917847\n",
      "\t Training loss (single batch): 2.327584981918335\n",
      "\t Training loss (single batch): 0.8912625908851624\n",
      "\t Training loss (single batch): 1.0158711671829224\n",
      "\t Training loss (single batch): 1.3123160600662231\n",
      "\t Training loss (single batch): 1.0892366170883179\n",
      "\t Training loss (single batch): 1.2179802656173706\n",
      "\t Training loss (single batch): 1.5026254653930664\n",
      "\t Training loss (single batch): 1.9532642364501953\n",
      "\t Training loss (single batch): 1.319738507270813\n",
      "\t Training loss (single batch): 1.5420855283737183\n",
      "\t Training loss (single batch): 1.1096022129058838\n",
      "\t Training loss (single batch): 1.04519522190094\n",
      "\t Training loss (single batch): 1.4021122455596924\n",
      "\t Training loss (single batch): 1.2878849506378174\n",
      "\t Training loss (single batch): 1.6920738220214844\n",
      "\t Training loss (single batch): 1.7247252464294434\n",
      "\t Training loss (single batch): 1.3427625894546509\n",
      "\t Training loss (single batch): 1.382628083229065\n",
      "\t Training loss (single batch): 0.9013734459877014\n",
      "\t Training loss (single batch): 0.539836049079895\n",
      "\t Training loss (single batch): 1.3594573736190796\n",
      "\t Training loss (single batch): 1.0742192268371582\n",
      "\t Training loss (single batch): 1.0792423486709595\n",
      "\t Training loss (single batch): 0.911037802696228\n",
      "\t Training loss (single batch): 1.3042864799499512\n",
      "\t Training loss (single batch): 1.1894855499267578\n",
      "\t Training loss (single batch): 1.4926481246948242\n",
      "\t Training loss (single batch): 0.7951120734214783\n",
      "\t Training loss (single batch): 1.0714493989944458\n",
      "\t Training loss (single batch): 1.5970768928527832\n",
      "\t Training loss (single batch): 0.9446176290512085\n",
      "\t Training loss (single batch): 1.554040551185608\n",
      "\t Training loss (single batch): 1.6829999685287476\n",
      "\t Training loss (single batch): 1.6467359066009521\n",
      "\t Training loss (single batch): 0.8564658164978027\n",
      "\t Training loss (single batch): 1.1698296070098877\n",
      "\t Training loss (single batch): 1.3745795488357544\n",
      "\t Training loss (single batch): 0.8915982842445374\n",
      "\t Training loss (single batch): 1.1943981647491455\n",
      "\t Training loss (single batch): 1.2967679500579834\n",
      "\t Training loss (single batch): 1.2193081378936768\n",
      "\t Training loss (single batch): 1.194944977760315\n",
      "\t Training loss (single batch): 1.3924205303192139\n",
      "\t Training loss (single batch): 0.8459390997886658\n",
      "\t Training loss (single batch): 0.9398906230926514\n",
      "\t Training loss (single batch): 1.308907389640808\n",
      "\t Training loss (single batch): 1.1337636709213257\n",
      "\t Training loss (single batch): 1.2016171216964722\n",
      "\t Training loss (single batch): 1.2720087766647339\n",
      "\t Training loss (single batch): 1.3407411575317383\n",
      "\t Training loss (single batch): 1.7897205352783203\n",
      "\t Training loss (single batch): 1.5607682466506958\n",
      "\t Training loss (single batch): 1.3554294109344482\n",
      "\t Training loss (single batch): 1.3832437992095947\n",
      "\t Training loss (single batch): 1.0929930210113525\n",
      "\t Training loss (single batch): 1.057382583618164\n",
      "\t Training loss (single batch): 1.588282823562622\n",
      "\t Training loss (single batch): 1.15123450756073\n",
      "\t Training loss (single batch): 1.204147219657898\n",
      "\t Training loss (single batch): 1.445476770401001\n",
      "\t Training loss (single batch): 1.1660890579223633\n",
      "\t Training loss (single batch): 1.2926408052444458\n",
      "\t Training loss (single batch): 0.9224051237106323\n",
      "\t Training loss (single batch): 1.2528163194656372\n",
      "\t Training loss (single batch): 1.614121913909912\n",
      "\t Training loss (single batch): 1.3089768886566162\n",
      "\t Training loss (single batch): 1.3106714487075806\n",
      "\t Training loss (single batch): 1.2501033544540405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9577888250350952\n",
      "\t Training loss (single batch): 1.1618908643722534\n",
      "\t Training loss (single batch): 1.1853153705596924\n",
      "\t Training loss (single batch): 1.0790014266967773\n",
      "\t Training loss (single batch): 1.2946616411209106\n",
      "\t Training loss (single batch): 0.9717698693275452\n",
      "\t Training loss (single batch): 1.162041425704956\n",
      "\t Training loss (single batch): 1.1869605779647827\n",
      "\t Training loss (single batch): 1.4878275394439697\n",
      "\t Training loss (single batch): 1.1138343811035156\n",
      "\t Training loss (single batch): 1.3720647096633911\n",
      "\t Training loss (single batch): 1.6243985891342163\n",
      "\t Training loss (single batch): 1.1648629903793335\n",
      "\t Training loss (single batch): 1.6175084114074707\n",
      "\t Training loss (single batch): 1.2208387851715088\n",
      "\t Training loss (single batch): 1.8789262771606445\n",
      "\t Training loss (single batch): 1.4443060159683228\n",
      "\t Training loss (single batch): 0.7694252729415894\n",
      "\t Training loss (single batch): 1.0131428241729736\n",
      "\t Training loss (single batch): 1.4000741243362427\n",
      "\t Training loss (single batch): 1.4027478694915771\n",
      "\t Training loss (single batch): 1.7597166299819946\n",
      "\t Training loss (single batch): 0.9074062705039978\n",
      "\t Training loss (single batch): 1.0741459131240845\n",
      "\t Training loss (single batch): 1.5111814737319946\n",
      "\t Training loss (single batch): 1.2605252265930176\n",
      "\t Training loss (single batch): 0.8669577240943909\n",
      "\t Training loss (single batch): 1.0785011053085327\n",
      "\t Training loss (single batch): 0.6206547617912292\n",
      "\t Training loss (single batch): 1.4400967359542847\n",
      "\t Training loss (single batch): 1.4279534816741943\n",
      "\t Training loss (single batch): 0.9150709509849548\n",
      "\t Training loss (single batch): 1.777419924736023\n",
      "\t Training loss (single batch): 1.353647232055664\n",
      "\t Training loss (single batch): 1.193401575088501\n",
      "\t Training loss (single batch): 1.5242900848388672\n",
      "\t Training loss (single batch): 1.225396990776062\n",
      "\t Training loss (single batch): 0.9655817747116089\n",
      "\t Training loss (single batch): 1.3060022592544556\n",
      "\t Training loss (single batch): 1.0203056335449219\n",
      "\t Training loss (single batch): 0.9457745552062988\n",
      "\t Training loss (single batch): 1.4516931772232056\n",
      "\t Training loss (single batch): 1.1093124151229858\n",
      "\t Training loss (single batch): 1.2554433345794678\n",
      "\t Training loss (single batch): 1.1945830583572388\n",
      "\t Training loss (single batch): 1.0678213834762573\n",
      "\t Training loss (single batch): 0.9578796029090881\n",
      "\t Training loss (single batch): 0.7630488276481628\n",
      "\t Training loss (single batch): 1.129115104675293\n",
      "\t Training loss (single batch): 1.350732445716858\n",
      "\t Training loss (single batch): 1.06143319606781\n",
      "\t Training loss (single batch): 1.066482663154602\n",
      "\t Training loss (single batch): 0.8970842361450195\n",
      "\t Training loss (single batch): 1.2983176708221436\n",
      "\t Training loss (single batch): 0.9089534878730774\n",
      "\t Training loss (single batch): 1.7470180988311768\n",
      "\t Training loss (single batch): 1.0601625442504883\n",
      "\t Training loss (single batch): 1.4479916095733643\n",
      "\t Training loss (single batch): 0.9471750259399414\n",
      "\t Training loss (single batch): 1.2113138437271118\n",
      "\t Training loss (single batch): 1.5044876337051392\n",
      "\t Training loss (single batch): 1.338527798652649\n",
      "\t Training loss (single batch): 0.8434935212135315\n",
      "\t Training loss (single batch): 0.9610213041305542\n",
      "\t Training loss (single batch): 1.0862406492233276\n",
      "\t Training loss (single batch): 1.5827336311340332\n",
      "\t Training loss (single batch): 1.2879267930984497\n",
      "\t Training loss (single batch): 1.0897102355957031\n",
      "\t Training loss (single batch): 1.2172272205352783\n",
      "\t Training loss (single batch): 1.2221485376358032\n",
      "\t Training loss (single batch): 1.0139117240905762\n",
      "\t Training loss (single batch): 1.3450816869735718\n",
      "\t Training loss (single batch): 1.4697036743164062\n",
      "\t Training loss (single batch): 1.1863436698913574\n",
      "\t Training loss (single batch): 1.2503650188446045\n",
      "\t Training loss (single batch): 1.1974745988845825\n",
      "\t Training loss (single batch): 1.4694347381591797\n",
      "\t Training loss (single batch): 0.7928837537765503\n",
      "\t Training loss (single batch): 1.413979172706604\n",
      "\t Training loss (single batch): 1.628803014755249\n",
      "\t Training loss (single batch): 1.1314231157302856\n",
      "\t Training loss (single batch): 0.8236485123634338\n",
      "\t Training loss (single batch): 1.0193527936935425\n",
      "\t Training loss (single batch): 0.9000121355056763\n",
      "\t Training loss (single batch): 1.4543862342834473\n",
      "\t Training loss (single batch): 0.9310818314552307\n",
      "\t Training loss (single batch): 0.7040818929672241\n",
      "\t Training loss (single batch): 0.7215529680252075\n",
      "\t Training loss (single batch): 1.6611062288284302\n",
      "\t Training loss (single batch): 1.4744716882705688\n",
      "\t Training loss (single batch): 1.4951233863830566\n",
      "\t Training loss (single batch): 1.0587358474731445\n",
      "\t Training loss (single batch): 1.504564642906189\n",
      "\t Training loss (single batch): 0.8455392122268677\n",
      "\t Training loss (single batch): 0.9703384041786194\n",
      "\t Training loss (single batch): 0.8033539056777954\n",
      "\t Training loss (single batch): 1.3678398132324219\n",
      "\t Training loss (single batch): 1.0768067836761475\n",
      "\t Training loss (single batch): 1.3379002809524536\n",
      "\t Training loss (single batch): 1.5493497848510742\n",
      "\t Training loss (single batch): 0.985392153263092\n",
      "\t Training loss (single batch): 0.9853479862213135\n",
      "\t Training loss (single batch): 1.0040690898895264\n",
      "\t Training loss (single batch): 1.0673205852508545\n",
      "\t Training loss (single batch): 1.2954412698745728\n",
      "\t Training loss (single batch): 1.1945748329162598\n",
      "\t Training loss (single batch): 1.0558979511260986\n",
      "\t Training loss (single batch): 1.0696029663085938\n",
      "\t Training loss (single batch): 1.387837290763855\n",
      "\t Training loss (single batch): 1.133378267288208\n",
      "\t Training loss (single batch): 1.1498918533325195\n",
      "\t Training loss (single batch): 1.4060451984405518\n",
      "\t Training loss (single batch): 1.2485944032669067\n",
      "\t Training loss (single batch): 1.3750704526901245\n",
      "\t Training loss (single batch): 1.2128413915634155\n",
      "\t Training loss (single batch): 0.7312226295471191\n",
      "\t Training loss (single batch): 1.3661680221557617\n",
      "\t Training loss (single batch): 0.9446691274642944\n",
      "\t Training loss (single batch): 0.7736214995384216\n",
      "\t Training loss (single batch): 1.1014658212661743\n",
      "\t Training loss (single batch): 1.008079171180725\n",
      "\t Training loss (single batch): 1.1617521047592163\n",
      "\t Training loss (single batch): 1.325463056564331\n",
      "\t Training loss (single batch): 1.0007026195526123\n",
      "##################################\n",
      "## EPOCH 65\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4524140357971191\n",
      "\t Training loss (single batch): 1.0742154121398926\n",
      "\t Training loss (single batch): 1.1847184896469116\n",
      "\t Training loss (single batch): 1.1683140993118286\n",
      "\t Training loss (single batch): 1.4819523096084595\n",
      "\t Training loss (single batch): 1.7702250480651855\n",
      "\t Training loss (single batch): 1.317294955253601\n",
      "\t Training loss (single batch): 1.779990792274475\n",
      "\t Training loss (single batch): 1.068500280380249\n",
      "\t Training loss (single batch): 0.771077573299408\n",
      "\t Training loss (single batch): 1.1068161725997925\n",
      "\t Training loss (single batch): 1.0157997608184814\n",
      "\t Training loss (single batch): 1.1894335746765137\n",
      "\t Training loss (single batch): 1.1170281171798706\n",
      "\t Training loss (single batch): 1.0861501693725586\n",
      "\t Training loss (single batch): 1.0802417993545532\n",
      "\t Training loss (single batch): 1.1915570497512817\n",
      "\t Training loss (single batch): 1.614293098449707\n",
      "\t Training loss (single batch): 1.0105808973312378\n",
      "\t Training loss (single batch): 0.9906991720199585\n",
      "\t Training loss (single batch): 1.1885157823562622\n",
      "\t Training loss (single batch): 1.1867741346359253\n",
      "\t Training loss (single batch): 1.4695754051208496\n",
      "\t Training loss (single batch): 1.164685606956482\n",
      "\t Training loss (single batch): 1.2868887186050415\n",
      "\t Training loss (single batch): 1.1986234188079834\n",
      "\t Training loss (single batch): 1.0273772478103638\n",
      "\t Training loss (single batch): 1.1042038202285767\n",
      "\t Training loss (single batch): 1.2307597398757935\n",
      "\t Training loss (single batch): 1.1912089586257935\n",
      "\t Training loss (single batch): 0.9785599708557129\n",
      "\t Training loss (single batch): 1.394797921180725\n",
      "\t Training loss (single batch): 1.2930940389633179\n",
      "\t Training loss (single batch): 1.3055317401885986\n",
      "\t Training loss (single batch): 1.3851205110549927\n",
      "\t Training loss (single batch): 1.1849932670593262\n",
      "\t Training loss (single batch): 0.8072420358657837\n",
      "\t Training loss (single batch): 1.1103097200393677\n",
      "\t Training loss (single batch): 1.0782161951065063\n",
      "\t Training loss (single batch): 1.2068883180618286\n",
      "\t Training loss (single batch): 0.9231869578361511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9570035934448242\n",
      "\t Training loss (single batch): 1.3790810108184814\n",
      "\t Training loss (single batch): 1.258131742477417\n",
      "\t Training loss (single batch): 0.9381124973297119\n",
      "\t Training loss (single batch): 0.9191509485244751\n",
      "\t Training loss (single batch): 0.8873620629310608\n",
      "\t Training loss (single batch): 1.2996339797973633\n",
      "\t Training loss (single batch): 1.059315800666809\n",
      "\t Training loss (single batch): 0.663670003414154\n",
      "\t Training loss (single batch): 1.1325675249099731\n",
      "\t Training loss (single batch): 0.745524525642395\n",
      "\t Training loss (single batch): 1.0891278982162476\n",
      "\t Training loss (single batch): 1.3125786781311035\n",
      "\t Training loss (single batch): 1.0086959600448608\n",
      "\t Training loss (single batch): 1.574302077293396\n",
      "\t Training loss (single batch): 1.2900115251541138\n",
      "\t Training loss (single batch): 1.1664663553237915\n",
      "\t Training loss (single batch): 1.1539100408554077\n",
      "\t Training loss (single batch): 1.4877607822418213\n",
      "\t Training loss (single batch): 1.0862056016921997\n",
      "\t Training loss (single batch): 1.2408483028411865\n",
      "\t Training loss (single batch): 1.1483995914459229\n",
      "\t Training loss (single batch): 1.3620187044143677\n",
      "\t Training loss (single batch): 1.2393943071365356\n",
      "\t Training loss (single batch): 0.7128551006317139\n",
      "\t Training loss (single batch): 0.987898051738739\n",
      "\t Training loss (single batch): 0.8624942302703857\n",
      "\t Training loss (single batch): 1.357105016708374\n",
      "\t Training loss (single batch): 1.3149943351745605\n",
      "\t Training loss (single batch): 1.5924359560012817\n",
      "\t Training loss (single batch): 0.7740790247917175\n",
      "\t Training loss (single batch): 1.3695998191833496\n",
      "\t Training loss (single batch): 0.883631706237793\n",
      "\t Training loss (single batch): 1.074244499206543\n",
      "\t Training loss (single batch): 0.9473035931587219\n",
      "\t Training loss (single batch): 1.3662853240966797\n",
      "\t Training loss (single batch): 1.4099615812301636\n",
      "\t Training loss (single batch): 0.9279177188873291\n",
      "\t Training loss (single batch): 1.1673474311828613\n",
      "\t Training loss (single batch): 0.9372499585151672\n",
      "\t Training loss (single batch): 1.4206879138946533\n",
      "\t Training loss (single batch): 1.4004614353179932\n",
      "\t Training loss (single batch): 1.1034705638885498\n",
      "\t Training loss (single batch): 0.8786349892616272\n",
      "\t Training loss (single batch): 1.2115026712417603\n",
      "\t Training loss (single batch): 1.4165842533111572\n",
      "\t Training loss (single batch): 1.1783381700515747\n",
      "\t Training loss (single batch): 1.0769814252853394\n",
      "\t Training loss (single batch): 1.3379132747650146\n",
      "\t Training loss (single batch): 0.8675355911254883\n",
      "\t Training loss (single batch): 1.0100181102752686\n",
      "\t Training loss (single batch): 0.9368795156478882\n",
      "\t Training loss (single batch): 1.3283506631851196\n",
      "\t Training loss (single batch): 1.2709890604019165\n",
      "\t Training loss (single batch): 1.0667660236358643\n",
      "\t Training loss (single batch): 1.2171725034713745\n",
      "\t Training loss (single batch): 0.7935196757316589\n",
      "\t Training loss (single batch): 1.5782723426818848\n",
      "\t Training loss (single batch): 0.8358917236328125\n",
      "\t Training loss (single batch): 1.2822827100753784\n",
      "\t Training loss (single batch): 1.4685781002044678\n",
      "\t Training loss (single batch): 1.3247767686843872\n",
      "\t Training loss (single batch): 0.844724714756012\n",
      "\t Training loss (single batch): 1.3487467765808105\n",
      "\t Training loss (single batch): 1.330918312072754\n",
      "\t Training loss (single batch): 0.8955649137496948\n",
      "\t Training loss (single batch): 0.9437270760536194\n",
      "\t Training loss (single batch): 1.134136438369751\n",
      "\t Training loss (single batch): 1.877742886543274\n",
      "\t Training loss (single batch): 1.3652291297912598\n",
      "\t Training loss (single batch): 1.2963266372680664\n",
      "\t Training loss (single batch): 1.3958282470703125\n",
      "\t Training loss (single batch): 0.9631996750831604\n",
      "\t Training loss (single batch): 1.9533884525299072\n",
      "\t Training loss (single batch): 0.7396751046180725\n",
      "\t Training loss (single batch): 1.2001006603240967\n",
      "\t Training loss (single batch): 1.1884957551956177\n",
      "\t Training loss (single batch): 1.1322637796401978\n",
      "\t Training loss (single batch): 1.4910334348678589\n",
      "\t Training loss (single batch): 1.3480974435806274\n",
      "\t Training loss (single batch): 1.6419466733932495\n",
      "\t Training loss (single batch): 1.4447681903839111\n",
      "\t Training loss (single batch): 1.5388973951339722\n",
      "\t Training loss (single batch): 0.9798180460929871\n",
      "\t Training loss (single batch): 1.2469946146011353\n",
      "\t Training loss (single batch): 1.3171371221542358\n",
      "\t Training loss (single batch): 1.2072583436965942\n",
      "\t Training loss (single batch): 1.3123502731323242\n",
      "\t Training loss (single batch): 0.9729625582695007\n",
      "\t Training loss (single batch): 1.2066396474838257\n",
      "\t Training loss (single batch): 1.520436406135559\n",
      "\t Training loss (single batch): 1.072651743888855\n",
      "\t Training loss (single batch): 1.2883659601211548\n",
      "\t Training loss (single batch): 0.9189968705177307\n",
      "\t Training loss (single batch): 1.506237506866455\n",
      "\t Training loss (single batch): 1.4029550552368164\n",
      "\t Training loss (single batch): 1.0433346033096313\n",
      "\t Training loss (single batch): 1.4955979585647583\n",
      "\t Training loss (single batch): 1.3132437467575073\n",
      "\t Training loss (single batch): 0.9700413346290588\n",
      "\t Training loss (single batch): 1.514833927154541\n",
      "\t Training loss (single batch): 1.239055871963501\n",
      "\t Training loss (single batch): 1.107656717300415\n",
      "\t Training loss (single batch): 1.2053287029266357\n",
      "\t Training loss (single batch): 1.358323335647583\n",
      "\t Training loss (single batch): 1.5827317237854004\n",
      "\t Training loss (single batch): 1.2014800310134888\n",
      "\t Training loss (single batch): 1.202282190322876\n",
      "\t Training loss (single batch): 1.2867323160171509\n",
      "\t Training loss (single batch): 1.1471387147903442\n",
      "\t Training loss (single batch): 0.8532359600067139\n",
      "\t Training loss (single batch): 0.8306607604026794\n",
      "\t Training loss (single batch): 0.9938287138938904\n",
      "\t Training loss (single batch): 1.2300293445587158\n",
      "\t Training loss (single batch): 1.1547826528549194\n",
      "\t Training loss (single batch): 1.0249629020690918\n",
      "\t Training loss (single batch): 1.1905441284179688\n",
      "\t Training loss (single batch): 1.5681757926940918\n",
      "\t Training loss (single batch): 1.9157745838165283\n",
      "\t Training loss (single batch): 1.1414635181427002\n",
      "\t Training loss (single batch): 1.0411211252212524\n",
      "\t Training loss (single batch): 1.1271226406097412\n",
      "\t Training loss (single batch): 1.1067466735839844\n",
      "\t Training loss (single batch): 1.3560428619384766\n",
      "\t Training loss (single batch): 1.2672843933105469\n",
      "\t Training loss (single batch): 1.237107515335083\n",
      "\t Training loss (single batch): 1.184056282043457\n",
      "\t Training loss (single batch): 1.1955403089523315\n",
      "\t Training loss (single batch): 1.6211708784103394\n",
      "\t Training loss (single batch): 1.3910821676254272\n",
      "\t Training loss (single batch): 1.7027041912078857\n",
      "\t Training loss (single batch): 1.341923475265503\n",
      "\t Training loss (single batch): 0.8761247992515564\n",
      "\t Training loss (single batch): 0.9290396571159363\n",
      "\t Training loss (single batch): 0.9148997664451599\n",
      "\t Training loss (single batch): 1.3125373125076294\n",
      "\t Training loss (single batch): 1.3520792722702026\n",
      "\t Training loss (single batch): 1.243874192237854\n",
      "\t Training loss (single batch): 1.1773637533187866\n",
      "\t Training loss (single batch): 0.9109960198402405\n",
      "\t Training loss (single batch): 1.6320558786392212\n",
      "\t Training loss (single batch): 1.2362984418869019\n",
      "\t Training loss (single batch): 1.0725785493850708\n",
      "\t Training loss (single batch): 1.4459609985351562\n",
      "\t Training loss (single batch): 1.270880103111267\n",
      "\t Training loss (single batch): 1.2826557159423828\n",
      "\t Training loss (single batch): 1.0824289321899414\n",
      "\t Training loss (single batch): 1.1361838579177856\n",
      "\t Training loss (single batch): 0.98175048828125\n",
      "\t Training loss (single batch): 1.3929064273834229\n",
      "\t Training loss (single batch): 1.4436103105545044\n",
      "\t Training loss (single batch): 1.5333075523376465\n",
      "\t Training loss (single batch): 1.2309973239898682\n",
      "\t Training loss (single batch): 1.0833089351654053\n",
      "\t Training loss (single batch): 1.642054557800293\n",
      "\t Training loss (single batch): 1.049124002456665\n",
      "\t Training loss (single batch): 1.2630046606063843\n",
      "\t Training loss (single batch): 1.6093047857284546\n",
      "\t Training loss (single batch): 1.1128308773040771\n",
      "\t Training loss (single batch): 1.3189518451690674\n",
      "\t Training loss (single batch): 0.7552207112312317\n",
      "\t Training loss (single batch): 1.6019083261489868\n",
      "\t Training loss (single batch): 0.891724705696106\n",
      "\t Training loss (single batch): 1.603530764579773\n",
      "\t Training loss (single batch): 1.42428719997406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3059180974960327\n",
      "\t Training loss (single batch): 0.9388466477394104\n",
      "\t Training loss (single batch): 0.898725688457489\n",
      "\t Training loss (single batch): 1.6635125875473022\n",
      "\t Training loss (single batch): 1.3220480680465698\n",
      "\t Training loss (single batch): 1.193694829940796\n",
      "\t Training loss (single batch): 1.0634530782699585\n",
      "\t Training loss (single batch): 1.1866782903671265\n",
      "\t Training loss (single batch): 1.0675352811813354\n",
      "\t Training loss (single batch): 0.9857175946235657\n",
      "\t Training loss (single batch): 1.3007614612579346\n",
      "\t Training loss (single batch): 1.359352469444275\n",
      "\t Training loss (single batch): 1.256976842880249\n",
      "\t Training loss (single batch): 1.2523237466812134\n",
      "\t Training loss (single batch): 1.3542954921722412\n",
      "\t Training loss (single batch): 1.2978355884552002\n",
      "\t Training loss (single batch): 1.3998342752456665\n",
      "\t Training loss (single batch): 1.5527979135513306\n",
      "\t Training loss (single batch): 1.5111011266708374\n",
      "\t Training loss (single batch): 1.4314278364181519\n",
      "\t Training loss (single batch): 1.4963325262069702\n",
      "\t Training loss (single batch): 1.1793067455291748\n",
      "\t Training loss (single batch): 0.9879299402236938\n",
      "\t Training loss (single batch): 0.9884708523750305\n",
      "\t Training loss (single batch): 1.517492413520813\n",
      "\t Training loss (single batch): 1.4416269063949585\n",
      "\t Training loss (single batch): 1.4350911378860474\n",
      "\t Training loss (single batch): 1.057185411453247\n",
      "\t Training loss (single batch): 1.1797277927398682\n",
      "\t Training loss (single batch): 1.5849658250808716\n",
      "\t Training loss (single batch): 0.999457061290741\n",
      "\t Training loss (single batch): 0.7040355801582336\n",
      "\t Training loss (single batch): 1.3493068218231201\n",
      "\t Training loss (single batch): 1.1344853639602661\n",
      "\t Training loss (single batch): 1.4595019817352295\n",
      "\t Training loss (single batch): 0.9073416590690613\n",
      "\t Training loss (single batch): 1.212334394454956\n",
      "\t Training loss (single batch): 1.8681575059890747\n",
      "\t Training loss (single batch): 1.085154414176941\n",
      "\t Training loss (single batch): 0.8797513246536255\n",
      "\t Training loss (single batch): 1.2798467874526978\n",
      "\t Training loss (single batch): 1.2134175300598145\n",
      "\t Training loss (single batch): 1.05455482006073\n",
      "\t Training loss (single batch): 0.7909889817237854\n",
      "\t Training loss (single batch): 0.9482691287994385\n",
      "\t Training loss (single batch): 1.0146154165267944\n",
      "\t Training loss (single batch): 1.1575533151626587\n",
      "\t Training loss (single batch): 2.0077242851257324\n",
      "\t Training loss (single batch): 0.7544729709625244\n",
      "\t Training loss (single batch): 1.0472276210784912\n",
      "\t Training loss (single batch): 1.3477697372436523\n",
      "\t Training loss (single batch): 1.4250646829605103\n",
      "\t Training loss (single batch): 1.3587111234664917\n",
      "\t Training loss (single batch): 1.1025257110595703\n",
      "\t Training loss (single batch): 1.1124699115753174\n",
      "\t Training loss (single batch): 1.3414862155914307\n",
      "\t Training loss (single batch): 0.9396762847900391\n",
      "\t Training loss (single batch): 1.4009599685668945\n",
      "\t Training loss (single batch): 1.7532334327697754\n",
      "\t Training loss (single batch): 1.1718002557754517\n",
      "\t Training loss (single batch): 1.3888258934020996\n",
      "\t Training loss (single batch): 1.5353611707687378\n",
      "\t Training loss (single batch): 1.3069393634796143\n",
      "\t Training loss (single batch): 1.0260882377624512\n",
      "\t Training loss (single batch): 0.896836519241333\n",
      "\t Training loss (single batch): 1.1782841682434082\n",
      "\t Training loss (single batch): 1.6438595056533813\n",
      "\t Training loss (single batch): 0.9165617227554321\n",
      "\t Training loss (single batch): 1.3232135772705078\n",
      "\t Training loss (single batch): 1.208969235420227\n",
      "\t Training loss (single batch): 0.9785873889923096\n",
      "\t Training loss (single batch): 0.987636387348175\n",
      "\t Training loss (single batch): 1.250646710395813\n",
      "\t Training loss (single batch): 1.5092931985855103\n",
      "\t Training loss (single batch): 1.451312780380249\n",
      "\t Training loss (single batch): 1.1201870441436768\n",
      "\t Training loss (single batch): 1.2231900691986084\n",
      "\t Training loss (single batch): 1.5055280923843384\n",
      "\t Training loss (single batch): 1.202318549156189\n",
      "\t Training loss (single batch): 1.1868904829025269\n",
      "\t Training loss (single batch): 1.2267996072769165\n",
      "\t Training loss (single batch): 1.370233416557312\n",
      "\t Training loss (single batch): 1.3524961471557617\n",
      "\t Training loss (single batch): 1.242376446723938\n",
      "\t Training loss (single batch): 1.4904624223709106\n",
      "\t Training loss (single batch): 1.0691756010055542\n",
      "\t Training loss (single batch): 1.3500782251358032\n",
      "\t Training loss (single batch): 0.988023042678833\n",
      "\t Training loss (single batch): 1.3736751079559326\n",
      "\t Training loss (single batch): 0.9539095759391785\n",
      "\t Training loss (single batch): 1.2415564060211182\n",
      "\t Training loss (single batch): 1.475437045097351\n",
      "\t Training loss (single batch): 1.3441144227981567\n",
      "\t Training loss (single batch): 1.4026026725769043\n",
      "\t Training loss (single batch): 1.0766593217849731\n",
      "\t Training loss (single batch): 1.346977949142456\n",
      "\t Training loss (single batch): 1.202711582183838\n",
      "\t Training loss (single batch): 1.2439090013504028\n",
      "\t Training loss (single batch): 1.0424656867980957\n",
      "\t Training loss (single batch): 0.8587945699691772\n",
      "\t Training loss (single batch): 1.3314309120178223\n",
      "\t Training loss (single batch): 1.5857694149017334\n",
      "\t Training loss (single batch): 0.8925710320472717\n",
      "\t Training loss (single batch): 1.300720453262329\n",
      "\t Training loss (single batch): 1.1760162115097046\n",
      "\t Training loss (single batch): 1.1433093547821045\n",
      "\t Training loss (single batch): 0.785370945930481\n",
      "\t Training loss (single batch): 1.3061308860778809\n",
      "\t Training loss (single batch): 1.0105143785476685\n",
      "\t Training loss (single batch): 1.1572703123092651\n",
      "\t Training loss (single batch): 1.6430972814559937\n",
      "##################################\n",
      "## EPOCH 66\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2757571935653687\n",
      "\t Training loss (single batch): 1.1393272876739502\n",
      "\t Training loss (single batch): 1.0444284677505493\n",
      "\t Training loss (single batch): 1.4552903175354004\n",
      "\t Training loss (single batch): 1.3438013792037964\n",
      "\t Training loss (single batch): 1.2816390991210938\n",
      "\t Training loss (single batch): 1.6157182455062866\n",
      "\t Training loss (single batch): 1.7489808797836304\n",
      "\t Training loss (single batch): 0.8447586894035339\n",
      "\t Training loss (single batch): 1.260352611541748\n",
      "\t Training loss (single batch): 0.9786604046821594\n",
      "\t Training loss (single batch): 1.2763720750808716\n",
      "\t Training loss (single batch): 1.2643051147460938\n",
      "\t Training loss (single batch): 1.356306791305542\n",
      "\t Training loss (single batch): 1.5795173645019531\n",
      "\t Training loss (single batch): 1.3436954021453857\n",
      "\t Training loss (single batch): 0.6880625486373901\n",
      "\t Training loss (single batch): 1.24907386302948\n",
      "\t Training loss (single batch): 1.537787675857544\n",
      "\t Training loss (single batch): 1.182129979133606\n",
      "\t Training loss (single batch): 0.9312893748283386\n",
      "\t Training loss (single batch): 0.8289316296577454\n",
      "\t Training loss (single batch): 1.0448646545410156\n",
      "\t Training loss (single batch): 1.0509114265441895\n",
      "\t Training loss (single batch): 1.3075218200683594\n",
      "\t Training loss (single batch): 1.0470174551010132\n",
      "\t Training loss (single batch): 0.8423100113868713\n",
      "\t Training loss (single batch): 0.955396831035614\n",
      "\t Training loss (single batch): 0.982288658618927\n",
      "\t Training loss (single batch): 1.1705926656723022\n",
      "\t Training loss (single batch): 1.3268849849700928\n",
      "\t Training loss (single batch): 0.8917480707168579\n",
      "\t Training loss (single batch): 1.2218562364578247\n",
      "\t Training loss (single batch): 1.3092374801635742\n",
      "\t Training loss (single batch): 1.454573154449463\n",
      "\t Training loss (single batch): 1.0193712711334229\n",
      "\t Training loss (single batch): 1.1702890396118164\n",
      "\t Training loss (single batch): 1.4037529230117798\n",
      "\t Training loss (single batch): 1.3283723592758179\n",
      "\t Training loss (single batch): 1.084670066833496\n",
      "\t Training loss (single batch): 0.890985369682312\n",
      "\t Training loss (single batch): 1.2388310432434082\n",
      "\t Training loss (single batch): 1.3551394939422607\n",
      "\t Training loss (single batch): 1.0027592182159424\n",
      "\t Training loss (single batch): 1.1650044918060303\n",
      "\t Training loss (single batch): 1.6170686483383179\n",
      "\t Training loss (single batch): 1.2676881551742554\n",
      "\t Training loss (single batch): 1.4165394306182861\n",
      "\t Training loss (single batch): 1.5689328908920288\n",
      "\t Training loss (single batch): 1.1424518823623657\n",
      "\t Training loss (single batch): 1.0450496673583984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0662349462509155\n",
      "\t Training loss (single batch): 1.0898796319961548\n",
      "\t Training loss (single batch): 0.845805823802948\n",
      "\t Training loss (single batch): 1.4129748344421387\n",
      "\t Training loss (single batch): 0.9142665863037109\n",
      "\t Training loss (single batch): 1.1451514959335327\n",
      "\t Training loss (single batch): 1.2672853469848633\n",
      "\t Training loss (single batch): 1.9702718257904053\n",
      "\t Training loss (single batch): 1.7866687774658203\n",
      "\t Training loss (single batch): 1.2544152736663818\n",
      "\t Training loss (single batch): 1.4050523042678833\n",
      "\t Training loss (single batch): 1.2924933433532715\n",
      "\t Training loss (single batch): 1.710732102394104\n",
      "\t Training loss (single batch): 1.4726402759552002\n",
      "\t Training loss (single batch): 1.3176454305648804\n",
      "\t Training loss (single batch): 1.0522918701171875\n",
      "\t Training loss (single batch): 0.9724084138870239\n",
      "\t Training loss (single batch): 1.1690536737442017\n",
      "\t Training loss (single batch): 1.0489195585250854\n",
      "\t Training loss (single batch): 1.5342426300048828\n",
      "\t Training loss (single batch): 1.2347667217254639\n",
      "\t Training loss (single batch): 1.1760436296463013\n",
      "\t Training loss (single batch): 1.1024264097213745\n",
      "\t Training loss (single batch): 1.1015119552612305\n",
      "\t Training loss (single batch): 1.4559788703918457\n",
      "\t Training loss (single batch): 1.1160471439361572\n",
      "\t Training loss (single batch): 1.112825632095337\n",
      "\t Training loss (single batch): 1.6053789854049683\n",
      "\t Training loss (single batch): 1.307121753692627\n",
      "\t Training loss (single batch): 1.2619348764419556\n",
      "\t Training loss (single batch): 1.118117332458496\n",
      "\t Training loss (single batch): 1.3597396612167358\n",
      "\t Training loss (single batch): 1.5636591911315918\n",
      "\t Training loss (single batch): 1.4292964935302734\n",
      "\t Training loss (single batch): 0.9322041273117065\n",
      "\t Training loss (single batch): 1.501583218574524\n",
      "\t Training loss (single batch): 0.871383011341095\n",
      "\t Training loss (single batch): 1.3598867654800415\n",
      "\t Training loss (single batch): 1.0864394903182983\n",
      "\t Training loss (single batch): 1.0762380361557007\n",
      "\t Training loss (single batch): 1.018974781036377\n",
      "\t Training loss (single batch): 0.9336921572685242\n",
      "\t Training loss (single batch): 1.5010991096496582\n",
      "\t Training loss (single batch): 1.2053977251052856\n",
      "\t Training loss (single batch): 1.150331974029541\n",
      "\t Training loss (single batch): 1.0988385677337646\n",
      "\t Training loss (single batch): 1.098924160003662\n",
      "\t Training loss (single batch): 1.422541618347168\n",
      "\t Training loss (single batch): 1.4367965459823608\n",
      "\t Training loss (single batch): 1.218069314956665\n",
      "\t Training loss (single batch): 1.047959327697754\n",
      "\t Training loss (single batch): 1.3458436727523804\n",
      "\t Training loss (single batch): 1.3095821142196655\n",
      "\t Training loss (single batch): 1.4324084520339966\n",
      "\t Training loss (single batch): 1.1410232782363892\n",
      "\t Training loss (single batch): 1.00668203830719\n",
      "\t Training loss (single batch): 1.2148481607437134\n",
      "\t Training loss (single batch): 1.1704559326171875\n",
      "\t Training loss (single batch): 0.9211075305938721\n",
      "\t Training loss (single batch): 1.3221074342727661\n",
      "\t Training loss (single batch): 0.998823881149292\n",
      "\t Training loss (single batch): 1.0539031028747559\n",
      "\t Training loss (single batch): 1.5035144090652466\n",
      "\t Training loss (single batch): 1.0669305324554443\n",
      "\t Training loss (single batch): 1.4230490922927856\n",
      "\t Training loss (single batch): 0.8341823220252991\n",
      "\t Training loss (single batch): 0.9179479479789734\n",
      "\t Training loss (single batch): 0.9985814094543457\n",
      "\t Training loss (single batch): 1.2291021347045898\n",
      "\t Training loss (single batch): 1.0537053346633911\n",
      "\t Training loss (single batch): 0.921183168888092\n",
      "\t Training loss (single batch): 1.4892823696136475\n",
      "\t Training loss (single batch): 1.438934564590454\n",
      "\t Training loss (single batch): 0.9958208203315735\n",
      "\t Training loss (single batch): 1.3852099180221558\n",
      "\t Training loss (single batch): 0.8954563736915588\n",
      "\t Training loss (single batch): 1.5125209093093872\n",
      "\t Training loss (single batch): 1.3597832918167114\n",
      "\t Training loss (single batch): 0.8668679594993591\n",
      "\t Training loss (single batch): 1.2247248888015747\n",
      "\t Training loss (single batch): 1.115696668624878\n",
      "\t Training loss (single batch): 1.2027664184570312\n",
      "\t Training loss (single batch): 1.4108327627182007\n",
      "\t Training loss (single batch): 1.532950758934021\n",
      "\t Training loss (single batch): 1.6139328479766846\n",
      "\t Training loss (single batch): 1.1054896116256714\n",
      "\t Training loss (single batch): 1.3894891738891602\n",
      "\t Training loss (single batch): 1.3174446821212769\n",
      "\t Training loss (single batch): 1.0112510919570923\n",
      "\t Training loss (single batch): 1.359616756439209\n",
      "\t Training loss (single batch): 1.3884632587432861\n",
      "\t Training loss (single batch): 1.5224248170852661\n",
      "\t Training loss (single batch): 1.0948070287704468\n",
      "\t Training loss (single batch): 1.3054583072662354\n",
      "\t Training loss (single batch): 1.4846571683883667\n",
      "\t Training loss (single batch): 1.2627333402633667\n",
      "\t Training loss (single batch): 1.0453451871871948\n",
      "\t Training loss (single batch): 1.0861117839813232\n",
      "\t Training loss (single batch): 1.1826928853988647\n",
      "\t Training loss (single batch): 1.1666041612625122\n",
      "\t Training loss (single batch): 1.3732562065124512\n",
      "\t Training loss (single batch): 0.9862526059150696\n",
      "\t Training loss (single batch): 0.9704100489616394\n",
      "\t Training loss (single batch): 0.8935537934303284\n",
      "\t Training loss (single batch): 1.2139956951141357\n",
      "\t Training loss (single batch): 1.3698002099990845\n",
      "\t Training loss (single batch): 1.158587098121643\n",
      "\t Training loss (single batch): 0.754355788230896\n",
      "\t Training loss (single batch): 1.1577844619750977\n",
      "\t Training loss (single batch): 1.6252508163452148\n",
      "\t Training loss (single batch): 2.020026206970215\n",
      "\t Training loss (single batch): 1.2567572593688965\n",
      "\t Training loss (single batch): 1.3946317434310913\n",
      "\t Training loss (single batch): 1.848788857460022\n",
      "\t Training loss (single batch): 1.2729331254959106\n",
      "\t Training loss (single batch): 0.9091567397117615\n",
      "\t Training loss (single batch): 1.0066474676132202\n",
      "\t Training loss (single batch): 1.1789382696151733\n",
      "\t Training loss (single batch): 1.111581802368164\n",
      "\t Training loss (single batch): 0.8698834776878357\n",
      "\t Training loss (single batch): 1.9154518842697144\n",
      "\t Training loss (single batch): 0.9569627046585083\n",
      "\t Training loss (single batch): 1.0246024131774902\n",
      "\t Training loss (single batch): 1.4887434244155884\n",
      "\t Training loss (single batch): 1.4212645292282104\n",
      "\t Training loss (single batch): 1.3470407724380493\n",
      "\t Training loss (single batch): 1.5374608039855957\n",
      "\t Training loss (single batch): 1.4531582593917847\n",
      "\t Training loss (single batch): 1.4622186422348022\n",
      "\t Training loss (single batch): 1.0048210620880127\n",
      "\t Training loss (single batch): 1.292040228843689\n",
      "\t Training loss (single batch): 1.0672436952590942\n",
      "\t Training loss (single batch): 1.0398670434951782\n",
      "\t Training loss (single batch): 1.230610728263855\n",
      "\t Training loss (single batch): 1.0537763833999634\n",
      "\t Training loss (single batch): 1.4377840757369995\n",
      "\t Training loss (single batch): 1.2135629653930664\n",
      "\t Training loss (single batch): 1.154052495956421\n",
      "\t Training loss (single batch): 0.9389965534210205\n",
      "\t Training loss (single batch): 1.4669631719589233\n",
      "\t Training loss (single batch): 0.9477010369300842\n",
      "\t Training loss (single batch): 1.2949168682098389\n",
      "\t Training loss (single batch): 1.6640586853027344\n",
      "\t Training loss (single batch): 0.9901725649833679\n",
      "\t Training loss (single batch): 0.8124554753303528\n",
      "\t Training loss (single batch): 1.6256818771362305\n",
      "\t Training loss (single batch): 1.2325528860092163\n",
      "\t Training loss (single batch): 0.6683979034423828\n",
      "\t Training loss (single batch): 1.288369059562683\n",
      "\t Training loss (single batch): 0.7228161096572876\n",
      "\t Training loss (single batch): 1.3696599006652832\n",
      "\t Training loss (single batch): 1.3744393587112427\n",
      "\t Training loss (single batch): 1.1036211252212524\n",
      "\t Training loss (single batch): 1.0035966634750366\n",
      "\t Training loss (single batch): 1.1693131923675537\n",
      "\t Training loss (single batch): 1.0787010192871094\n",
      "\t Training loss (single batch): 1.0476665496826172\n",
      "\t Training loss (single batch): 1.4634113311767578\n",
      "\t Training loss (single batch): 1.0966817140579224\n",
      "\t Training loss (single batch): 1.0552717447280884\n",
      "\t Training loss (single batch): 1.6122772693634033\n",
      "\t Training loss (single batch): 1.3353434801101685\n",
      "\t Training loss (single batch): 1.3451935052871704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8959918022155762\n",
      "\t Training loss (single batch): 1.1090856790542603\n",
      "\t Training loss (single batch): 1.3571045398712158\n",
      "\t Training loss (single batch): 0.8745508790016174\n",
      "\t Training loss (single batch): 1.1267362833023071\n",
      "\t Training loss (single batch): 1.220208764076233\n",
      "\t Training loss (single batch): 1.2253574132919312\n",
      "\t Training loss (single batch): 1.9449958801269531\n",
      "\t Training loss (single batch): 1.085555911064148\n",
      "\t Training loss (single batch): 1.0869752168655396\n",
      "\t Training loss (single batch): 1.3001923561096191\n",
      "\t Training loss (single batch): 0.968644380569458\n",
      "\t Training loss (single batch): 1.2083171606063843\n",
      "\t Training loss (single batch): 1.2096707820892334\n",
      "\t Training loss (single batch): 1.5039854049682617\n",
      "\t Training loss (single batch): 1.3643866777420044\n",
      "\t Training loss (single batch): 1.4694868326187134\n",
      "\t Training loss (single batch): 0.9568721055984497\n",
      "\t Training loss (single batch): 1.620166301727295\n",
      "\t Training loss (single batch): 1.1513270139694214\n",
      "\t Training loss (single batch): 1.5666394233703613\n",
      "\t Training loss (single batch): 1.3423919677734375\n",
      "\t Training loss (single batch): 1.505784034729004\n",
      "\t Training loss (single batch): 0.8765901923179626\n",
      "\t Training loss (single batch): 1.191264033317566\n",
      "\t Training loss (single batch): 1.355873942375183\n",
      "\t Training loss (single batch): 1.5237517356872559\n",
      "\t Training loss (single batch): 1.1517972946166992\n",
      "\t Training loss (single batch): 1.2622579336166382\n",
      "\t Training loss (single batch): 1.2630977630615234\n",
      "\t Training loss (single batch): 0.8152914047241211\n",
      "\t Training loss (single batch): 1.0444824695587158\n",
      "\t Training loss (single batch): 1.2352662086486816\n",
      "\t Training loss (single batch): 1.2879760265350342\n",
      "\t Training loss (single batch): 1.3893766403198242\n",
      "\t Training loss (single batch): 1.2015923261642456\n",
      "\t Training loss (single batch): 0.8440828919410706\n",
      "\t Training loss (single batch): 0.75960773229599\n",
      "\t Training loss (single batch): 1.7126059532165527\n",
      "\t Training loss (single batch): 0.9975405335426331\n",
      "\t Training loss (single batch): 1.288602352142334\n",
      "\t Training loss (single batch): 1.6017730236053467\n",
      "\t Training loss (single batch): 1.0634231567382812\n",
      "\t Training loss (single batch): 1.6517528295516968\n",
      "\t Training loss (single batch): 1.0728604793548584\n",
      "\t Training loss (single batch): 1.3567713499069214\n",
      "\t Training loss (single batch): 1.1722562313079834\n",
      "\t Training loss (single batch): 1.4105379581451416\n",
      "\t Training loss (single batch): 0.9733934998512268\n",
      "\t Training loss (single batch): 1.2205045223236084\n",
      "\t Training loss (single batch): 0.8053126931190491\n",
      "\t Training loss (single batch): 1.1499024629592896\n",
      "\t Training loss (single batch): 1.1350253820419312\n",
      "\t Training loss (single batch): 1.4442179203033447\n",
      "\t Training loss (single batch): 1.030178427696228\n",
      "\t Training loss (single batch): 1.4741097688674927\n",
      "\t Training loss (single batch): 0.9516294598579407\n",
      "\t Training loss (single batch): 1.5680162906646729\n",
      "\t Training loss (single batch): 1.1887869834899902\n",
      "\t Training loss (single batch): 0.8840618133544922\n",
      "\t Training loss (single batch): 1.0438201427459717\n",
      "\t Training loss (single batch): 0.6813472509384155\n",
      "\t Training loss (single batch): 1.356659173965454\n",
      "\t Training loss (single batch): 1.2674424648284912\n",
      "\t Training loss (single batch): 1.152992844581604\n",
      "\t Training loss (single batch): 1.067788004875183\n",
      "\t Training loss (single batch): 0.6284610629081726\n",
      "\t Training loss (single batch): 1.7835758924484253\n",
      "\t Training loss (single batch): 1.2873262166976929\n",
      "\t Training loss (single batch): 1.5076098442077637\n",
      "\t Training loss (single batch): 1.3389278650283813\n",
      "\t Training loss (single batch): 1.1546276807785034\n",
      "\t Training loss (single batch): 1.498063564300537\n",
      "\t Training loss (single batch): 1.485111117362976\n",
      "\t Training loss (single batch): 1.161368727684021\n",
      "\t Training loss (single batch): 1.7077386379241943\n",
      "\t Training loss (single batch): 1.2895134687423706\n",
      "\t Training loss (single batch): 1.1055105924606323\n",
      "\t Training loss (single batch): 1.5252876281738281\n",
      "\t Training loss (single batch): 1.6070815324783325\n",
      "\t Training loss (single batch): 1.0440443754196167\n",
      "\t Training loss (single batch): 1.2829673290252686\n",
      "\t Training loss (single batch): 1.0677598714828491\n",
      "\t Training loss (single batch): 1.3126095533370972\n",
      "\t Training loss (single batch): 1.3172186613082886\n",
      "\t Training loss (single batch): 0.9817330837249756\n",
      "\t Training loss (single batch): 1.4136289358139038\n",
      "\t Training loss (single batch): 0.7578043937683105\n",
      "\t Training loss (single batch): 0.7487712502479553\n",
      "\t Training loss (single batch): 1.374102234840393\n",
      "\t Training loss (single batch): 1.4611375331878662\n",
      "\t Training loss (single batch): 1.1021220684051514\n",
      "\t Training loss (single batch): 1.555374026298523\n",
      "\t Training loss (single batch): 0.8306063413619995\n",
      "\t Training loss (single batch): 1.0109456777572632\n",
      "\t Training loss (single batch): 1.0170978307724\n",
      "\t Training loss (single batch): 1.0948750972747803\n",
      "\t Training loss (single batch): 1.0935481786727905\n",
      "\t Training loss (single batch): 1.2434970140457153\n",
      "\t Training loss (single batch): 1.1405216455459595\n",
      "\t Training loss (single batch): 1.3262090682983398\n",
      "\t Training loss (single batch): 1.0789167881011963\n",
      "\t Training loss (single batch): 0.9343819618225098\n",
      "##################################\n",
      "## EPOCH 67\n",
      "##################################\n",
      "\t Training loss (single batch): 1.560562252998352\n",
      "\t Training loss (single batch): 1.2088477611541748\n",
      "\t Training loss (single batch): 0.8753964304924011\n",
      "\t Training loss (single batch): 1.1842972040176392\n",
      "\t Training loss (single batch): 0.6186495423316956\n",
      "\t Training loss (single batch): 0.9805724024772644\n",
      "\t Training loss (single batch): 1.5516433715820312\n",
      "\t Training loss (single batch): 0.848358690738678\n",
      "\t Training loss (single batch): 1.2851234674453735\n",
      "\t Training loss (single batch): 1.056800365447998\n",
      "\t Training loss (single batch): 1.047594666481018\n",
      "\t Training loss (single batch): 0.8970366716384888\n",
      "\t Training loss (single batch): 1.3835355043411255\n",
      "\t Training loss (single batch): 0.944929301738739\n",
      "\t Training loss (single batch): 1.2804116010665894\n",
      "\t Training loss (single batch): 1.1835896968841553\n",
      "\t Training loss (single batch): 1.4572890996932983\n",
      "\t Training loss (single batch): 0.6030407547950745\n",
      "\t Training loss (single batch): 1.572389841079712\n",
      "\t Training loss (single batch): 1.1071197986602783\n",
      "\t Training loss (single batch): 1.2364459037780762\n",
      "\t Training loss (single batch): 1.5234088897705078\n",
      "\t Training loss (single batch): 1.055086374282837\n",
      "\t Training loss (single batch): 1.4607936143875122\n",
      "\t Training loss (single batch): 0.9888147711753845\n",
      "\t Training loss (single batch): 1.5555158853530884\n",
      "\t Training loss (single batch): 1.567667841911316\n",
      "\t Training loss (single batch): 0.9681938886642456\n",
      "\t Training loss (single batch): 1.3161081075668335\n",
      "\t Training loss (single batch): 1.0653313398361206\n",
      "\t Training loss (single batch): 1.6064919233322144\n",
      "\t Training loss (single batch): 0.897022545337677\n",
      "\t Training loss (single batch): 1.0102463960647583\n",
      "\t Training loss (single batch): 1.31806480884552\n",
      "\t Training loss (single batch): 1.073783040046692\n",
      "\t Training loss (single batch): 1.2706525325775146\n",
      "\t Training loss (single batch): 1.3813568353652954\n",
      "\t Training loss (single batch): 1.3308920860290527\n",
      "\t Training loss (single batch): 0.9894497990608215\n",
      "\t Training loss (single batch): 1.4970812797546387\n",
      "\t Training loss (single batch): 1.4613295793533325\n",
      "\t Training loss (single batch): 1.590735673904419\n",
      "\t Training loss (single batch): 1.207059621810913\n",
      "\t Training loss (single batch): 1.2932300567626953\n",
      "\t Training loss (single batch): 1.3170872926712036\n",
      "\t Training loss (single batch): 0.8453534245491028\n",
      "\t Training loss (single batch): 0.9531094431877136\n",
      "\t Training loss (single batch): 1.2740890979766846\n",
      "\t Training loss (single batch): 1.6912225484848022\n",
      "\t Training loss (single batch): 1.636417031288147\n",
      "\t Training loss (single batch): 1.353725552558899\n",
      "\t Training loss (single batch): 2.013134002685547\n",
      "\t Training loss (single batch): 1.4713197946548462\n",
      "\t Training loss (single batch): 0.8558270931243896\n",
      "\t Training loss (single batch): 0.9351054430007935\n",
      "\t Training loss (single batch): 1.0860368013381958\n",
      "\t Training loss (single batch): 1.0934627056121826\n",
      "\t Training loss (single batch): 1.2468037605285645\n",
      "\t Training loss (single batch): 1.1042659282684326\n",
      "\t Training loss (single batch): 1.74668550491333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.060958981513977\n",
      "\t Training loss (single batch): 1.4509788751602173\n",
      "\t Training loss (single batch): 1.0324543714523315\n",
      "\t Training loss (single batch): 0.8730071783065796\n",
      "\t Training loss (single batch): 1.1307897567749023\n",
      "\t Training loss (single batch): 0.8747475147247314\n",
      "\t Training loss (single batch): 0.9821269512176514\n",
      "\t Training loss (single batch): 1.2465276718139648\n",
      "\t Training loss (single batch): 0.9660192728042603\n",
      "\t Training loss (single batch): 1.1069157123565674\n",
      "\t Training loss (single batch): 1.5043914318084717\n",
      "\t Training loss (single batch): 1.6183403730392456\n",
      "\t Training loss (single batch): 1.3298085927963257\n",
      "\t Training loss (single batch): 1.4203481674194336\n",
      "\t Training loss (single batch): 1.6445578336715698\n",
      "\t Training loss (single batch): 1.5903968811035156\n",
      "\t Training loss (single batch): 1.0807313919067383\n",
      "\t Training loss (single batch): 1.1687366962432861\n",
      "\t Training loss (single batch): 1.621524691581726\n",
      "\t Training loss (single batch): 1.389162302017212\n",
      "\t Training loss (single batch): 1.1238304376602173\n",
      "\t Training loss (single batch): 1.033508062362671\n",
      "\t Training loss (single batch): 1.0385384559631348\n",
      "\t Training loss (single batch): 1.0117452144622803\n",
      "\t Training loss (single batch): 0.9734039306640625\n",
      "\t Training loss (single batch): 1.2179805040359497\n",
      "\t Training loss (single batch): 1.1854952573776245\n",
      "\t Training loss (single batch): 1.005690574645996\n",
      "\t Training loss (single batch): 1.2222784757614136\n",
      "\t Training loss (single batch): 0.8731046319007874\n",
      "\t Training loss (single batch): 1.2518726587295532\n",
      "\t Training loss (single batch): 1.365891456604004\n",
      "\t Training loss (single batch): 1.6240047216415405\n",
      "\t Training loss (single batch): 0.9759668111801147\n",
      "\t Training loss (single batch): 1.0632063150405884\n",
      "\t Training loss (single batch): 1.3429666757583618\n",
      "\t Training loss (single batch): 1.1449611186981201\n",
      "\t Training loss (single batch): 1.0972219705581665\n",
      "\t Training loss (single batch): 1.4510438442230225\n",
      "\t Training loss (single batch): 1.758576512336731\n",
      "\t Training loss (single batch): 1.3164668083190918\n",
      "\t Training loss (single batch): 1.5667264461517334\n",
      "\t Training loss (single batch): 1.2883098125457764\n",
      "\t Training loss (single batch): 1.5969303846359253\n",
      "\t Training loss (single batch): 1.385817050933838\n",
      "\t Training loss (single batch): 1.5175776481628418\n",
      "\t Training loss (single batch): 1.358303427696228\n",
      "\t Training loss (single batch): 1.3653606176376343\n",
      "\t Training loss (single batch): 0.9081335067749023\n",
      "\t Training loss (single batch): 1.4232574701309204\n",
      "\t Training loss (single batch): 1.0270905494689941\n",
      "\t Training loss (single batch): 1.52437424659729\n",
      "\t Training loss (single batch): 1.1275227069854736\n",
      "\t Training loss (single batch): 1.1965785026550293\n",
      "\t Training loss (single batch): 1.5556550025939941\n",
      "\t Training loss (single batch): 0.9514696002006531\n",
      "\t Training loss (single batch): 1.2263563871383667\n",
      "\t Training loss (single batch): 1.2005527019500732\n",
      "\t Training loss (single batch): 1.2682750225067139\n",
      "\t Training loss (single batch): 1.155076265335083\n",
      "\t Training loss (single batch): 1.1818490028381348\n",
      "\t Training loss (single batch): 1.0841015577316284\n",
      "\t Training loss (single batch): 0.8223698735237122\n",
      "\t Training loss (single batch): 1.4482342004776\n",
      "\t Training loss (single batch): 1.9208588600158691\n",
      "\t Training loss (single batch): 0.9420586824417114\n",
      "\t Training loss (single batch): 1.0030605792999268\n",
      "\t Training loss (single batch): 1.1897588968276978\n",
      "\t Training loss (single batch): 1.1746474504470825\n",
      "\t Training loss (single batch): 0.9754030704498291\n",
      "\t Training loss (single batch): 1.5092374086380005\n",
      "\t Training loss (single batch): 1.3160871267318726\n",
      "\t Training loss (single batch): 1.3131113052368164\n",
      "\t Training loss (single batch): 1.5902475118637085\n",
      "\t Training loss (single batch): 1.3839342594146729\n",
      "\t Training loss (single batch): 1.2851358652114868\n",
      "\t Training loss (single batch): 1.297186255455017\n",
      "\t Training loss (single batch): 0.9270849227905273\n",
      "\t Training loss (single batch): 0.9673627018928528\n",
      "\t Training loss (single batch): 1.0870012044906616\n",
      "\t Training loss (single batch): 1.0229496955871582\n",
      "\t Training loss (single batch): 0.8208363056182861\n",
      "\t Training loss (single batch): 1.880845546722412\n",
      "\t Training loss (single batch): 1.1889805793762207\n",
      "\t Training loss (single batch): 1.3372701406478882\n",
      "\t Training loss (single batch): 1.1868517398834229\n",
      "\t Training loss (single batch): 1.035127878189087\n",
      "\t Training loss (single batch): 1.3750077486038208\n",
      "\t Training loss (single batch): 1.2605290412902832\n",
      "\t Training loss (single batch): 1.3403587341308594\n",
      "\t Training loss (single batch): 1.073847770690918\n",
      "\t Training loss (single batch): 1.3121522665023804\n",
      "\t Training loss (single batch): 1.2415902614593506\n",
      "\t Training loss (single batch): 1.1673128604888916\n",
      "\t Training loss (single batch): 0.8847688436508179\n",
      "\t Training loss (single batch): 1.1853852272033691\n",
      "\t Training loss (single batch): 1.2551921606063843\n",
      "\t Training loss (single batch): 1.4164459705352783\n",
      "\t Training loss (single batch): 0.8086279630661011\n",
      "\t Training loss (single batch): 1.572912573814392\n",
      "\t Training loss (single batch): 1.389591932296753\n",
      "\t Training loss (single batch): 0.9355365037918091\n",
      "\t Training loss (single batch): 1.3777005672454834\n",
      "\t Training loss (single batch): 0.7999973893165588\n",
      "\t Training loss (single batch): 1.2692172527313232\n",
      "\t Training loss (single batch): 0.840575635433197\n",
      "\t Training loss (single batch): 1.3380506038665771\n",
      "\t Training loss (single batch): 1.1767709255218506\n",
      "\t Training loss (single batch): 1.128514051437378\n",
      "\t Training loss (single batch): 1.2262617349624634\n",
      "\t Training loss (single batch): 1.3372340202331543\n",
      "\t Training loss (single batch): 1.0997451543807983\n",
      "\t Training loss (single batch): 1.069065809249878\n",
      "\t Training loss (single batch): 1.1801472902297974\n",
      "\t Training loss (single batch): 1.264876365661621\n",
      "\t Training loss (single batch): 1.140426754951477\n",
      "\t Training loss (single batch): 1.0474352836608887\n",
      "\t Training loss (single batch): 0.8086875081062317\n",
      "\t Training loss (single batch): 1.036099910736084\n",
      "\t Training loss (single batch): 1.582943081855774\n",
      "\t Training loss (single batch): 1.2292083501815796\n",
      "\t Training loss (single batch): 1.3258731365203857\n",
      "\t Training loss (single batch): 1.3499913215637207\n",
      "\t Training loss (single batch): 1.4509506225585938\n",
      "\t Training loss (single batch): 1.0958178043365479\n",
      "\t Training loss (single batch): 1.0089596509933472\n",
      "\t Training loss (single batch): 0.9817179441452026\n",
      "\t Training loss (single batch): 1.425480604171753\n",
      "\t Training loss (single batch): 1.5320740938186646\n",
      "\t Training loss (single batch): 0.9681425094604492\n",
      "\t Training loss (single batch): 1.1082618236541748\n",
      "\t Training loss (single batch): 1.284810185432434\n",
      "\t Training loss (single batch): 0.88797527551651\n",
      "\t Training loss (single batch): 1.9425967931747437\n",
      "\t Training loss (single batch): 1.3299847841262817\n",
      "\t Training loss (single batch): 0.8189594745635986\n",
      "\t Training loss (single batch): 1.386297583580017\n",
      "\t Training loss (single batch): 1.4252468347549438\n",
      "\t Training loss (single batch): 1.1328506469726562\n",
      "\t Training loss (single batch): 1.386960506439209\n",
      "\t Training loss (single batch): 0.8694571256637573\n",
      "\t Training loss (single batch): 1.2558330297470093\n",
      "\t Training loss (single batch): 1.2381962537765503\n",
      "\t Training loss (single batch): 1.418407678604126\n",
      "\t Training loss (single batch): 1.0773935317993164\n",
      "\t Training loss (single batch): 1.010886549949646\n",
      "\t Training loss (single batch): 1.5581376552581787\n",
      "\t Training loss (single batch): 1.1039202213287354\n",
      "\t Training loss (single batch): 1.0949962139129639\n",
      "\t Training loss (single batch): 0.8724700212478638\n",
      "\t Training loss (single batch): 1.1835579872131348\n",
      "\t Training loss (single batch): 1.480344295501709\n",
      "\t Training loss (single batch): 1.4930741786956787\n",
      "\t Training loss (single batch): 1.3098973035812378\n",
      "\t Training loss (single batch): 1.170646071434021\n",
      "\t Training loss (single batch): 1.2449160814285278\n",
      "\t Training loss (single batch): 1.1020935773849487\n",
      "\t Training loss (single batch): 1.1925417184829712\n",
      "\t Training loss (single batch): 1.2421529293060303\n",
      "\t Training loss (single batch): 1.3240522146224976\n",
      "\t Training loss (single batch): 1.2458369731903076\n",
      "\t Training loss (single batch): 0.8949707746505737\n",
      "\t Training loss (single batch): 1.245500922203064\n",
      "\t Training loss (single batch): 0.8049562573432922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1472833156585693\n",
      "\t Training loss (single batch): 1.4821377992630005\n",
      "\t Training loss (single batch): 1.5504902601242065\n",
      "\t Training loss (single batch): 1.4387893676757812\n",
      "\t Training loss (single batch): 1.4973342418670654\n",
      "\t Training loss (single batch): 1.674588918685913\n",
      "\t Training loss (single batch): 1.1756056547164917\n",
      "\t Training loss (single batch): 1.2205865383148193\n",
      "\t Training loss (single batch): 0.9699781537055969\n",
      "\t Training loss (single batch): 1.133178472518921\n",
      "\t Training loss (single batch): 1.634994387626648\n",
      "\t Training loss (single batch): 1.5740423202514648\n",
      "\t Training loss (single batch): 1.5034172534942627\n",
      "\t Training loss (single batch): 1.4021410942077637\n",
      "\t Training loss (single batch): 1.4943065643310547\n",
      "\t Training loss (single batch): 1.7604845762252808\n",
      "\t Training loss (single batch): 1.4023423194885254\n",
      "\t Training loss (single batch): 1.433619737625122\n",
      "\t Training loss (single batch): 1.0613378286361694\n",
      "\t Training loss (single batch): 0.7695623636245728\n",
      "\t Training loss (single batch): 0.7384769320487976\n",
      "\t Training loss (single batch): 1.147321105003357\n",
      "\t Training loss (single batch): 1.0186392068862915\n",
      "\t Training loss (single batch): 1.871932864189148\n",
      "\t Training loss (single batch): 1.110349178314209\n",
      "\t Training loss (single batch): 1.3398876190185547\n",
      "\t Training loss (single batch): 1.449100375175476\n",
      "\t Training loss (single batch): 0.8922624588012695\n",
      "\t Training loss (single batch): 1.6014387607574463\n",
      "\t Training loss (single batch): 1.230104684829712\n",
      "\t Training loss (single batch): 1.2430706024169922\n",
      "\t Training loss (single batch): 1.249399185180664\n",
      "\t Training loss (single batch): 1.3102928400039673\n",
      "\t Training loss (single batch): 0.8573749661445618\n",
      "\t Training loss (single batch): 1.5131621360778809\n",
      "\t Training loss (single batch): 1.045667290687561\n",
      "\t Training loss (single batch): 1.0787413120269775\n",
      "\t Training loss (single batch): 1.3467140197753906\n",
      "\t Training loss (single batch): 1.1869933605194092\n",
      "\t Training loss (single batch): 1.2569366693496704\n",
      "\t Training loss (single batch): 0.8221578001976013\n",
      "\t Training loss (single batch): 1.6811928749084473\n",
      "\t Training loss (single batch): 0.8869897127151489\n",
      "\t Training loss (single batch): 1.33411705493927\n",
      "\t Training loss (single batch): 0.9041988849639893\n",
      "\t Training loss (single batch): 1.469150185585022\n",
      "\t Training loss (single batch): 1.2213975191116333\n",
      "\t Training loss (single batch): 1.2826522588729858\n",
      "\t Training loss (single batch): 1.1321020126342773\n",
      "\t Training loss (single batch): 0.9882802963256836\n",
      "\t Training loss (single batch): 1.1359583139419556\n",
      "\t Training loss (single batch): 0.9052643179893494\n",
      "\t Training loss (single batch): 1.8131167888641357\n",
      "\t Training loss (single batch): 1.2188506126403809\n",
      "\t Training loss (single batch): 1.573379635810852\n",
      "\t Training loss (single batch): 1.0879477262496948\n",
      "\t Training loss (single batch): 1.123971700668335\n",
      "\t Training loss (single batch): 1.297214388847351\n",
      "\t Training loss (single batch): 1.3607980012893677\n",
      "\t Training loss (single batch): 1.5397223234176636\n",
      "\t Training loss (single batch): 1.2695201635360718\n",
      "\t Training loss (single batch): 1.1115045547485352\n",
      "\t Training loss (single batch): 1.035048246383667\n",
      "\t Training loss (single batch): 1.2189233303070068\n",
      "\t Training loss (single batch): 1.3886631727218628\n",
      "\t Training loss (single batch): 1.0910592079162598\n",
      "\t Training loss (single batch): 0.7989686727523804\n",
      "\t Training loss (single batch): 1.539389729499817\n",
      "\t Training loss (single batch): 0.7775329351425171\n",
      "\t Training loss (single batch): 1.3263968229293823\n",
      "\t Training loss (single batch): 1.239972472190857\n",
      "\t Training loss (single batch): 1.1857726573944092\n",
      "\t Training loss (single batch): 1.2905223369598389\n",
      "\t Training loss (single batch): 1.126023292541504\n",
      "\t Training loss (single batch): 1.4087040424346924\n",
      "\t Training loss (single batch): 1.280998945236206\n",
      "\t Training loss (single batch): 1.2816723585128784\n",
      "\t Training loss (single batch): 1.113032341003418\n",
      "\t Training loss (single batch): 0.9106169939041138\n",
      "\t Training loss (single batch): 1.168516755104065\n",
      "\t Training loss (single batch): 0.9170029759407043\n",
      "\t Training loss (single batch): 0.7856919169425964\n",
      "\t Training loss (single batch): 1.1513346433639526\n",
      "\t Training loss (single batch): 1.1074135303497314\n",
      "\t Training loss (single batch): 1.0472221374511719\n",
      "\t Training loss (single batch): 0.8878607749938965\n",
      "\t Training loss (single batch): 0.7928804159164429\n",
      "\t Training loss (single batch): 1.6963801383972168\n",
      "\t Training loss (single batch): 1.3647313117980957\n",
      "\t Training loss (single batch): 2.0963587760925293\n",
      "\t Training loss (single batch): 1.8269712924957275\n",
      "\t Training loss (single batch): 1.311682939529419\n",
      "\t Training loss (single batch): 0.6285034418106079\n",
      "##################################\n",
      "## EPOCH 68\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0105621814727783\n",
      "\t Training loss (single batch): 0.830327570438385\n",
      "\t Training loss (single batch): 0.9965550899505615\n",
      "\t Training loss (single batch): 0.9326545596122742\n",
      "\t Training loss (single batch): 1.3702174425125122\n",
      "\t Training loss (single batch): 1.458042860031128\n",
      "\t Training loss (single batch): 1.3478755950927734\n",
      "\t Training loss (single batch): 1.0560495853424072\n",
      "\t Training loss (single batch): 1.4440395832061768\n",
      "\t Training loss (single batch): 0.71439528465271\n",
      "\t Training loss (single batch): 1.638495922088623\n",
      "\t Training loss (single batch): 1.0235708951950073\n",
      "\t Training loss (single batch): 1.6366868019104004\n",
      "\t Training loss (single batch): 1.7177152633666992\n",
      "\t Training loss (single batch): 1.6149375438690186\n",
      "\t Training loss (single batch): 1.0957940816879272\n",
      "\t Training loss (single batch): 0.823716402053833\n",
      "\t Training loss (single batch): 1.4228090047836304\n",
      "\t Training loss (single batch): 1.270326018333435\n",
      "\t Training loss (single batch): 0.964043378829956\n",
      "\t Training loss (single batch): 1.7467715740203857\n",
      "\t Training loss (single batch): 1.545642375946045\n",
      "\t Training loss (single batch): 1.7607907056808472\n",
      "\t Training loss (single batch): 1.2138386964797974\n",
      "\t Training loss (single batch): 1.4656120538711548\n",
      "\t Training loss (single batch): 1.2446662187576294\n",
      "\t Training loss (single batch): 1.231611967086792\n",
      "\t Training loss (single batch): 0.9907696843147278\n",
      "\t Training loss (single batch): 1.0404592752456665\n",
      "\t Training loss (single batch): 1.113303542137146\n",
      "\t Training loss (single batch): 1.063646674156189\n",
      "\t Training loss (single batch): 0.984657883644104\n",
      "\t Training loss (single batch): 1.006787896156311\n",
      "\t Training loss (single batch): 1.478406548500061\n",
      "\t Training loss (single batch): 1.225406527519226\n",
      "\t Training loss (single batch): 1.6175882816314697\n",
      "\t Training loss (single batch): 1.0454469919204712\n",
      "\t Training loss (single batch): 0.7310154438018799\n",
      "\t Training loss (single batch): 1.019049048423767\n",
      "\t Training loss (single batch): 1.4023598432540894\n",
      "\t Training loss (single batch): 0.5774736404418945\n",
      "\t Training loss (single batch): 1.0460416078567505\n",
      "\t Training loss (single batch): 1.4700186252593994\n",
      "\t Training loss (single batch): 1.3016427755355835\n",
      "\t Training loss (single batch): 0.8893916010856628\n",
      "\t Training loss (single batch): 0.9938250780105591\n",
      "\t Training loss (single batch): 0.9168251156806946\n",
      "\t Training loss (single batch): 0.9024674296379089\n",
      "\t Training loss (single batch): 1.803829550743103\n",
      "\t Training loss (single batch): 0.7005353569984436\n",
      "\t Training loss (single batch): 1.178486943244934\n",
      "\t Training loss (single batch): 1.122103214263916\n",
      "\t Training loss (single batch): 1.2298835515975952\n",
      "\t Training loss (single batch): 0.6923003792762756\n",
      "\t Training loss (single batch): 0.9158775210380554\n",
      "\t Training loss (single batch): 1.3400996923446655\n",
      "\t Training loss (single batch): 1.189820647239685\n",
      "\t Training loss (single batch): 1.2762389183044434\n",
      "\t Training loss (single batch): 0.9287233352661133\n",
      "\t Training loss (single batch): 1.4444493055343628\n",
      "\t Training loss (single batch): 0.8812420964241028\n",
      "\t Training loss (single batch): 1.198261022567749\n",
      "\t Training loss (single batch): 1.0859694480895996\n",
      "\t Training loss (single batch): 1.1705366373062134\n",
      "\t Training loss (single batch): 1.1164758205413818\n",
      "\t Training loss (single batch): 1.5842686891555786\n",
      "\t Training loss (single batch): 1.1636918783187866\n",
      "\t Training loss (single batch): 1.4205989837646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.176389217376709\n",
      "\t Training loss (single batch): 1.013282299041748\n",
      "\t Training loss (single batch): 1.1839697360992432\n",
      "\t Training loss (single batch): 1.131633996963501\n",
      "\t Training loss (single batch): 1.259031891822815\n",
      "\t Training loss (single batch): 1.1368263959884644\n",
      "\t Training loss (single batch): 1.097044825553894\n",
      "\t Training loss (single batch): 1.4793726205825806\n",
      "\t Training loss (single batch): 0.9191602468490601\n",
      "\t Training loss (single batch): 1.15048348903656\n",
      "\t Training loss (single batch): 1.0903818607330322\n",
      "\t Training loss (single batch): 1.4279441833496094\n",
      "\t Training loss (single batch): 1.249298095703125\n",
      "\t Training loss (single batch): 1.186957597732544\n",
      "\t Training loss (single batch): 1.3807891607284546\n",
      "\t Training loss (single batch): 1.1351532936096191\n",
      "\t Training loss (single batch): 0.8961554765701294\n",
      "\t Training loss (single batch): 0.9875019192695618\n",
      "\t Training loss (single batch): 1.3026493787765503\n",
      "\t Training loss (single batch): 1.3781744241714478\n",
      "\t Training loss (single batch): 1.2641040086746216\n",
      "\t Training loss (single batch): 1.0142267942428589\n",
      "\t Training loss (single batch): 1.1029372215270996\n",
      "\t Training loss (single batch): 1.0283399820327759\n",
      "\t Training loss (single batch): 0.7773944735527039\n",
      "\t Training loss (single batch): 0.670131504535675\n",
      "\t Training loss (single batch): 0.8953883647918701\n",
      "\t Training loss (single batch): 1.5501257181167603\n",
      "\t Training loss (single batch): 1.1951165199279785\n",
      "\t Training loss (single batch): 1.33588445186615\n",
      "\t Training loss (single batch): 1.0091825723648071\n",
      "\t Training loss (single batch): 1.2685214281082153\n",
      "\t Training loss (single batch): 1.3101849555969238\n",
      "\t Training loss (single batch): 1.2397218942642212\n",
      "\t Training loss (single batch): 1.164908766746521\n",
      "\t Training loss (single batch): 1.038901448249817\n",
      "\t Training loss (single batch): 1.2677980661392212\n",
      "\t Training loss (single batch): 1.5600775480270386\n",
      "\t Training loss (single batch): 0.8560121059417725\n",
      "\t Training loss (single batch): 1.5577031373977661\n",
      "\t Training loss (single batch): 1.7717770338058472\n",
      "\t Training loss (single batch): 1.0142040252685547\n",
      "\t Training loss (single batch): 1.7112443447113037\n",
      "\t Training loss (single batch): 1.5085481405258179\n",
      "\t Training loss (single batch): 1.2007222175598145\n",
      "\t Training loss (single batch): 1.1113500595092773\n",
      "\t Training loss (single batch): 1.288441777229309\n",
      "\t Training loss (single batch): 0.7717556357383728\n",
      "\t Training loss (single batch): 1.0264356136322021\n",
      "\t Training loss (single batch): 1.620925784111023\n",
      "\t Training loss (single batch): 1.1543679237365723\n",
      "\t Training loss (single batch): 1.3843903541564941\n",
      "\t Training loss (single batch): 0.9673680663108826\n",
      "\t Training loss (single batch): 1.321155309677124\n",
      "\t Training loss (single batch): 1.0414952039718628\n",
      "\t Training loss (single batch): 0.7825068235397339\n",
      "\t Training loss (single batch): 1.1497080326080322\n",
      "\t Training loss (single batch): 1.1722830533981323\n",
      "\t Training loss (single batch): 1.1921720504760742\n",
      "\t Training loss (single batch): 1.1629366874694824\n",
      "\t Training loss (single batch): 0.9902927279472351\n",
      "\t Training loss (single batch): 1.330086350440979\n",
      "\t Training loss (single batch): 1.1084908246994019\n",
      "\t Training loss (single batch): 1.0684809684753418\n",
      "\t Training loss (single batch): 1.290097713470459\n",
      "\t Training loss (single batch): 0.7947094440460205\n",
      "\t Training loss (single batch): 0.8167752623558044\n",
      "\t Training loss (single batch): 1.6187610626220703\n",
      "\t Training loss (single batch): 1.3334161043167114\n",
      "\t Training loss (single batch): 1.4775691032409668\n",
      "\t Training loss (single batch): 0.6901480555534363\n",
      "\t Training loss (single batch): 0.8941628932952881\n",
      "\t Training loss (single batch): 1.2619154453277588\n",
      "\t Training loss (single batch): 2.0875160694122314\n",
      "\t Training loss (single batch): 1.2950506210327148\n",
      "\t Training loss (single batch): 1.0976943969726562\n",
      "\t Training loss (single batch): 1.6525598764419556\n",
      "\t Training loss (single batch): 1.2421752214431763\n",
      "\t Training loss (single batch): 1.2283140420913696\n",
      "\t Training loss (single batch): 1.1001076698303223\n",
      "\t Training loss (single batch): 0.9927019476890564\n",
      "\t Training loss (single batch): 1.4354627132415771\n",
      "\t Training loss (single batch): 1.4060707092285156\n",
      "\t Training loss (single batch): 1.7281063795089722\n",
      "\t Training loss (single batch): 1.1708126068115234\n",
      "\t Training loss (single batch): 1.8790279626846313\n",
      "\t Training loss (single batch): 1.4596835374832153\n",
      "\t Training loss (single batch): 1.3602577447891235\n",
      "\t Training loss (single batch): 1.1656497716903687\n",
      "\t Training loss (single batch): 1.4335224628448486\n",
      "\t Training loss (single batch): 1.303151249885559\n",
      "\t Training loss (single batch): 1.4623935222625732\n",
      "\t Training loss (single batch): 1.142953872680664\n",
      "\t Training loss (single batch): 1.1672402620315552\n",
      "\t Training loss (single batch): 1.1216493844985962\n",
      "\t Training loss (single batch): 1.3015625476837158\n",
      "\t Training loss (single batch): 1.3169093132019043\n",
      "\t Training loss (single batch): 1.165615439414978\n",
      "\t Training loss (single batch): 0.9714055061340332\n",
      "\t Training loss (single batch): 1.0115700960159302\n",
      "\t Training loss (single batch): 0.8085690140724182\n",
      "\t Training loss (single batch): 0.7804051041603088\n",
      "\t Training loss (single batch): 1.4657589197158813\n",
      "\t Training loss (single batch): 1.1371760368347168\n",
      "\t Training loss (single batch): 1.4528452157974243\n",
      "\t Training loss (single batch): 1.482345700263977\n",
      "\t Training loss (single batch): 1.1979228258132935\n",
      "\t Training loss (single batch): 1.1538612842559814\n",
      "\t Training loss (single batch): 0.9975113868713379\n",
      "\t Training loss (single batch): 1.1793522834777832\n",
      "\t Training loss (single batch): 1.0914002656936646\n",
      "\t Training loss (single batch): 1.0285546779632568\n",
      "\t Training loss (single batch): 1.072859287261963\n",
      "\t Training loss (single batch): 0.8546562790870667\n",
      "\t Training loss (single batch): 1.078214406967163\n",
      "\t Training loss (single batch): 1.4489467144012451\n",
      "\t Training loss (single batch): 0.8279404640197754\n",
      "\t Training loss (single batch): 1.2806627750396729\n",
      "\t Training loss (single batch): 1.7054905891418457\n",
      "\t Training loss (single batch): 1.2523300647735596\n",
      "\t Training loss (single batch): 1.3803364038467407\n",
      "\t Training loss (single batch): 1.7009642124176025\n",
      "\t Training loss (single batch): 0.8542284369468689\n",
      "\t Training loss (single batch): 1.0743112564086914\n",
      "\t Training loss (single batch): 1.072839379310608\n",
      "\t Training loss (single batch): 1.4008723497390747\n",
      "\t Training loss (single batch): 0.9008321762084961\n",
      "\t Training loss (single batch): 1.1594997644424438\n",
      "\t Training loss (single batch): 0.7929065823554993\n",
      "\t Training loss (single batch): 1.538610816001892\n",
      "\t Training loss (single batch): 1.1404986381530762\n",
      "\t Training loss (single batch): 1.8978304862976074\n",
      "\t Training loss (single batch): 0.8927965760231018\n",
      "\t Training loss (single batch): 1.457739233970642\n",
      "\t Training loss (single batch): 0.5426266193389893\n",
      "\t Training loss (single batch): 1.4111579656600952\n",
      "\t Training loss (single batch): 1.1406316757202148\n",
      "\t Training loss (single batch): 1.3628056049346924\n",
      "\t Training loss (single batch): 1.4206242561340332\n",
      "\t Training loss (single batch): 1.580623984336853\n",
      "\t Training loss (single batch): 1.0630100965499878\n",
      "\t Training loss (single batch): 1.2701705694198608\n",
      "\t Training loss (single batch): 1.0183883905410767\n",
      "\t Training loss (single batch): 1.1080741882324219\n",
      "\t Training loss (single batch): 1.070995569229126\n",
      "\t Training loss (single batch): 0.8447819352149963\n",
      "\t Training loss (single batch): 1.0760196447372437\n",
      "\t Training loss (single batch): 0.677695631980896\n",
      "\t Training loss (single batch): 0.8352472186088562\n",
      "\t Training loss (single batch): 1.077021837234497\n",
      "\t Training loss (single batch): 1.0998492240905762\n",
      "\t Training loss (single batch): 1.4422049522399902\n",
      "\t Training loss (single batch): 1.5284996032714844\n",
      "\t Training loss (single batch): 1.5633838176727295\n",
      "\t Training loss (single batch): 1.9260504245758057\n",
      "\t Training loss (single batch): 1.38869309425354\n",
      "\t Training loss (single batch): 1.4536458253860474\n",
      "\t Training loss (single batch): 0.6400677561759949\n",
      "\t Training loss (single batch): 1.1204415559768677\n",
      "\t Training loss (single batch): 0.9952499270439148\n",
      "\t Training loss (single batch): 1.1840537786483765\n",
      "\t Training loss (single batch): 1.3339629173278809\n",
      "\t Training loss (single batch): 1.0290182828903198\n",
      "\t Training loss (single batch): 1.117238163948059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0315682888031006\n",
      "\t Training loss (single batch): 1.2743980884552002\n",
      "\t Training loss (single batch): 1.3969249725341797\n",
      "\t Training loss (single batch): 1.168826699256897\n",
      "\t Training loss (single batch): 1.0014426708221436\n",
      "\t Training loss (single batch): 0.7031721472740173\n",
      "\t Training loss (single batch): 1.6770555973052979\n",
      "\t Training loss (single batch): 0.9256691336631775\n",
      "\t Training loss (single batch): 1.161381721496582\n",
      "\t Training loss (single batch): 0.9423649907112122\n",
      "\t Training loss (single batch): 1.3056156635284424\n",
      "\t Training loss (single batch): 1.3199951648712158\n",
      "\t Training loss (single batch): 1.1476759910583496\n",
      "\t Training loss (single batch): 0.9266245365142822\n",
      "\t Training loss (single batch): 1.1865895986557007\n",
      "\t Training loss (single batch): 0.7854200005531311\n",
      "\t Training loss (single batch): 1.3215513229370117\n",
      "\t Training loss (single batch): 1.4466042518615723\n",
      "\t Training loss (single batch): 1.3211064338684082\n",
      "\t Training loss (single batch): 1.3471451997756958\n",
      "\t Training loss (single batch): 1.3516778945922852\n",
      "\t Training loss (single batch): 0.9146577715873718\n",
      "\t Training loss (single batch): 1.5269696712493896\n",
      "\t Training loss (single batch): 1.0856751203536987\n",
      "\t Training loss (single batch): 1.1121138334274292\n",
      "\t Training loss (single batch): 1.2353347539901733\n",
      "\t Training loss (single batch): 0.8141363859176636\n",
      "\t Training loss (single batch): 1.762003779411316\n",
      "\t Training loss (single batch): 0.9281859993934631\n",
      "\t Training loss (single batch): 1.575408935546875\n",
      "\t Training loss (single batch): 1.0428611040115356\n",
      "\t Training loss (single batch): 0.8990257978439331\n",
      "\t Training loss (single batch): 1.1917122602462769\n",
      "\t Training loss (single batch): 1.484732985496521\n",
      "\t Training loss (single batch): 1.1368036270141602\n",
      "\t Training loss (single batch): 1.2200244665145874\n",
      "\t Training loss (single batch): 1.14003324508667\n",
      "\t Training loss (single batch): 1.239194393157959\n",
      "\t Training loss (single batch): 0.8859562873840332\n",
      "\t Training loss (single batch): 0.9483904242515564\n",
      "\t Training loss (single batch): 1.1286897659301758\n",
      "\t Training loss (single batch): 1.4860835075378418\n",
      "\t Training loss (single batch): 2.0070767402648926\n",
      "\t Training loss (single batch): 1.1986768245697021\n",
      "\t Training loss (single batch): 1.3341370820999146\n",
      "\t Training loss (single batch): 0.9420986771583557\n",
      "\t Training loss (single batch): 1.299899935722351\n",
      "\t Training loss (single batch): 0.980912446975708\n",
      "\t Training loss (single batch): 1.254186749458313\n",
      "\t Training loss (single batch): 0.9751911759376526\n",
      "\t Training loss (single batch): 1.2475618124008179\n",
      "\t Training loss (single batch): 1.5070141553878784\n",
      "\t Training loss (single batch): 1.20400869846344\n",
      "\t Training loss (single batch): 0.9331424236297607\n",
      "\t Training loss (single batch): 1.3344253301620483\n",
      "\t Training loss (single batch): 1.6345614194869995\n",
      "\t Training loss (single batch): 0.9974523782730103\n",
      "\t Training loss (single batch): 1.243232250213623\n",
      "\t Training loss (single batch): 1.1387685537338257\n",
      "\t Training loss (single batch): 0.7929466962814331\n",
      "\t Training loss (single batch): 1.2009955644607544\n",
      "\t Training loss (single batch): 1.3835110664367676\n",
      "\t Training loss (single batch): 1.1674110889434814\n",
      "\t Training loss (single batch): 1.441988468170166\n",
      "\t Training loss (single batch): 1.0229911804199219\n",
      "\t Training loss (single batch): 1.2213823795318604\n",
      "\t Training loss (single batch): 1.0623977184295654\n",
      "\t Training loss (single batch): 1.564708948135376\n",
      "\t Training loss (single batch): 1.6739399433135986\n",
      "\t Training loss (single batch): 0.8694499135017395\n",
      "\t Training loss (single batch): 1.2153929471969604\n",
      "\t Training loss (single batch): 0.8797914981842041\n",
      "\t Training loss (single batch): 1.0311343669891357\n",
      "\t Training loss (single batch): 1.4073333740234375\n",
      "\t Training loss (single batch): 1.0244516134262085\n",
      "\t Training loss (single batch): 1.1749049425125122\n",
      "\t Training loss (single batch): 1.2581498622894287\n",
      "\t Training loss (single batch): 1.0906833410263062\n",
      "\t Training loss (single batch): 1.1524063348770142\n",
      "\t Training loss (single batch): 1.2516785860061646\n",
      "\t Training loss (single batch): 0.7821305990219116\n",
      "\t Training loss (single batch): 1.1300431489944458\n",
      "\t Training loss (single batch): 1.230069637298584\n",
      "\t Training loss (single batch): 1.9116212129592896\n",
      "\t Training loss (single batch): 1.035183310508728\n",
      "##################################\n",
      "## EPOCH 69\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7148397564888\n",
      "\t Training loss (single batch): 1.2638272047042847\n",
      "\t Training loss (single batch): 1.0208793878555298\n",
      "\t Training loss (single batch): 1.3828624486923218\n",
      "\t Training loss (single batch): 1.4925081729888916\n",
      "\t Training loss (single batch): 0.9986395835876465\n",
      "\t Training loss (single batch): 1.5511984825134277\n",
      "\t Training loss (single batch): 1.5617023706436157\n",
      "\t Training loss (single batch): 1.1601876020431519\n",
      "\t Training loss (single batch): 0.8919103145599365\n",
      "\t Training loss (single batch): 1.2663427591323853\n",
      "\t Training loss (single batch): 0.8775960803031921\n",
      "\t Training loss (single batch): 0.9944342970848083\n",
      "\t Training loss (single batch): 1.4012199640274048\n",
      "\t Training loss (single batch): 1.1828354597091675\n",
      "\t Training loss (single batch): 0.973803699016571\n",
      "\t Training loss (single batch): 1.1148602962493896\n",
      "\t Training loss (single batch): 0.778414249420166\n",
      "\t Training loss (single batch): 1.2941924333572388\n",
      "\t Training loss (single batch): 1.1209467649459839\n",
      "\t Training loss (single batch): 0.8530479073524475\n",
      "\t Training loss (single batch): 1.5065964460372925\n",
      "\t Training loss (single batch): 0.9907814264297485\n",
      "\t Training loss (single batch): 0.7159726023674011\n",
      "\t Training loss (single batch): 0.9381234645843506\n",
      "\t Training loss (single batch): 0.7645348310470581\n",
      "\t Training loss (single batch): 1.3296911716461182\n",
      "\t Training loss (single batch): 1.3674308061599731\n",
      "\t Training loss (single batch): 1.469535231590271\n",
      "\t Training loss (single batch): 0.8694562315940857\n",
      "\t Training loss (single batch): 0.9211701154708862\n",
      "\t Training loss (single batch): 1.107727289199829\n",
      "\t Training loss (single batch): 1.090975046157837\n",
      "\t Training loss (single batch): 1.254274845123291\n",
      "\t Training loss (single batch): 1.5086984634399414\n",
      "\t Training loss (single batch): 2.003434896469116\n",
      "\t Training loss (single batch): 1.1162832975387573\n",
      "\t Training loss (single batch): 1.7236229181289673\n",
      "\t Training loss (single batch): 0.8253927230834961\n",
      "\t Training loss (single batch): 1.4068219661712646\n",
      "\t Training loss (single batch): 1.1708433628082275\n",
      "\t Training loss (single batch): 1.2577953338623047\n",
      "\t Training loss (single batch): 1.39913809299469\n",
      "\t Training loss (single batch): 1.1453677415847778\n",
      "\t Training loss (single batch): 1.2465882301330566\n",
      "\t Training loss (single batch): 1.3497350215911865\n",
      "\t Training loss (single batch): 1.0128439664840698\n",
      "\t Training loss (single batch): 1.4226675033569336\n",
      "\t Training loss (single batch): 1.1819193363189697\n",
      "\t Training loss (single batch): 1.0767015218734741\n",
      "\t Training loss (single batch): 1.3070709705352783\n",
      "\t Training loss (single batch): 0.8499470353126526\n",
      "\t Training loss (single batch): 1.5785295963287354\n",
      "\t Training loss (single batch): 0.9557735323905945\n",
      "\t Training loss (single batch): 1.0716813802719116\n",
      "\t Training loss (single batch): 1.180282711982727\n",
      "\t Training loss (single batch): 0.9946467280387878\n",
      "\t Training loss (single batch): 1.7446786165237427\n",
      "\t Training loss (single batch): 1.141198992729187\n",
      "\t Training loss (single batch): 1.0656864643096924\n",
      "\t Training loss (single batch): 1.0928188562393188\n",
      "\t Training loss (single batch): 0.7414565086364746\n",
      "\t Training loss (single batch): 0.7999474406242371\n",
      "\t Training loss (single batch): 1.342334508895874\n",
      "\t Training loss (single batch): 1.007912278175354\n",
      "\t Training loss (single batch): 1.4537990093231201\n",
      "\t Training loss (single batch): 0.9898253679275513\n",
      "\t Training loss (single batch): 0.7522761821746826\n",
      "\t Training loss (single batch): 0.8736327290534973\n",
      "\t Training loss (single batch): 1.049955129623413\n",
      "\t Training loss (single batch): 1.267129898071289\n",
      "\t Training loss (single batch): 1.1243672370910645\n",
      "\t Training loss (single batch): 1.3075261116027832\n",
      "\t Training loss (single batch): 1.1116286516189575\n",
      "\t Training loss (single batch): 1.2541452646255493\n",
      "\t Training loss (single batch): 1.2673265933990479\n",
      "\t Training loss (single batch): 0.9821285009384155\n",
      "\t Training loss (single batch): 1.174203872680664\n",
      "\t Training loss (single batch): 1.0051636695861816\n",
      "\t Training loss (single batch): 1.1752744913101196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0359526872634888\n",
      "\t Training loss (single batch): 1.1919630765914917\n",
      "\t Training loss (single batch): 1.4898858070373535\n",
      "\t Training loss (single batch): 1.2179653644561768\n",
      "\t Training loss (single batch): 1.1622267961502075\n",
      "\t Training loss (single batch): 1.4381425380706787\n",
      "\t Training loss (single batch): 1.642209529876709\n",
      "\t Training loss (single batch): 1.0274604558944702\n",
      "\t Training loss (single batch): 1.416165828704834\n",
      "\t Training loss (single batch): 1.1798954010009766\n",
      "\t Training loss (single batch): 1.5394989252090454\n",
      "\t Training loss (single batch): 1.089084267616272\n",
      "\t Training loss (single batch): 1.2321727275848389\n",
      "\t Training loss (single batch): 0.9900816082954407\n",
      "\t Training loss (single batch): 1.7140517234802246\n",
      "\t Training loss (single batch): 1.1747487783432007\n",
      "\t Training loss (single batch): 1.1648404598236084\n",
      "\t Training loss (single batch): 1.2029246091842651\n",
      "\t Training loss (single batch): 1.459591269493103\n",
      "\t Training loss (single batch): 1.4835896492004395\n",
      "\t Training loss (single batch): 1.5995136499404907\n",
      "\t Training loss (single batch): 1.1898624897003174\n",
      "\t Training loss (single batch): 0.832625687122345\n",
      "\t Training loss (single batch): 1.1078646183013916\n",
      "\t Training loss (single batch): 1.3075116872787476\n",
      "\t Training loss (single batch): 1.1375048160552979\n",
      "\t Training loss (single batch): 1.3181546926498413\n",
      "\t Training loss (single batch): 1.3808488845825195\n",
      "\t Training loss (single batch): 0.7055942416191101\n",
      "\t Training loss (single batch): 1.2547986507415771\n",
      "\t Training loss (single batch): 1.4197677373886108\n",
      "\t Training loss (single batch): 1.5593030452728271\n",
      "\t Training loss (single batch): 1.7736403942108154\n",
      "\t Training loss (single batch): 1.1419947147369385\n",
      "\t Training loss (single batch): 1.3632107973098755\n",
      "\t Training loss (single batch): 1.3579039573669434\n",
      "\t Training loss (single batch): 1.095040202140808\n",
      "\t Training loss (single batch): 1.0248057842254639\n",
      "\t Training loss (single batch): 1.4349920749664307\n",
      "\t Training loss (single batch): 1.5977774858474731\n",
      "\t Training loss (single batch): 1.2518740892410278\n",
      "\t Training loss (single batch): 1.1376161575317383\n",
      "\t Training loss (single batch): 1.38671875\n",
      "\t Training loss (single batch): 1.3551632165908813\n",
      "\t Training loss (single batch): 1.0311354398727417\n",
      "\t Training loss (single batch): 0.9172763228416443\n",
      "\t Training loss (single batch): 0.946208655834198\n",
      "\t Training loss (single batch): 1.5126203298568726\n",
      "\t Training loss (single batch): 0.9316011071205139\n",
      "\t Training loss (single batch): 1.0911667346954346\n",
      "\t Training loss (single batch): 0.7056668400764465\n",
      "\t Training loss (single batch): 0.9207648634910583\n",
      "\t Training loss (single batch): 0.6768054962158203\n",
      "\t Training loss (single batch): 1.439415454864502\n",
      "\t Training loss (single batch): 1.794095516204834\n",
      "\t Training loss (single batch): 0.8762366771697998\n",
      "\t Training loss (single batch): 1.7040834426879883\n",
      "\t Training loss (single batch): 1.5070624351501465\n",
      "\t Training loss (single batch): 0.7796860337257385\n",
      "\t Training loss (single batch): 0.9173786640167236\n",
      "\t Training loss (single batch): 1.1505829095840454\n",
      "\t Training loss (single batch): 1.080249309539795\n",
      "\t Training loss (single batch): 1.2181063890457153\n",
      "\t Training loss (single batch): 1.3291926383972168\n",
      "\t Training loss (single batch): 1.0170561075210571\n",
      "\t Training loss (single batch): 1.3723371028900146\n",
      "\t Training loss (single batch): 1.4113763570785522\n",
      "\t Training loss (single batch): 1.0414284467697144\n",
      "\t Training loss (single batch): 1.2080055475234985\n",
      "\t Training loss (single batch): 1.4914060831069946\n",
      "\t Training loss (single batch): 1.438336730003357\n",
      "\t Training loss (single batch): 0.7330266237258911\n",
      "\t Training loss (single batch): 1.1480183601379395\n",
      "\t Training loss (single batch): 1.418111801147461\n",
      "\t Training loss (single batch): 0.8785629272460938\n",
      "\t Training loss (single batch): 1.0884658098220825\n",
      "\t Training loss (single batch): 1.6438192129135132\n",
      "\t Training loss (single batch): 1.3541957139968872\n",
      "\t Training loss (single batch): 1.3187490701675415\n",
      "\t Training loss (single batch): 1.2749035358428955\n",
      "\t Training loss (single batch): 1.3506932258605957\n",
      "\t Training loss (single batch): 1.582599401473999\n",
      "\t Training loss (single batch): 0.8650859594345093\n",
      "\t Training loss (single batch): 1.2349120378494263\n",
      "\t Training loss (single batch): 0.8594923615455627\n",
      "\t Training loss (single batch): 0.9169024229049683\n",
      "\t Training loss (single batch): 1.263324499130249\n",
      "\t Training loss (single batch): 1.2321408987045288\n",
      "\t Training loss (single batch): 0.9084368944168091\n",
      "\t Training loss (single batch): 1.3106164932250977\n",
      "\t Training loss (single batch): 1.239747405052185\n",
      "\t Training loss (single batch): 1.118483066558838\n",
      "\t Training loss (single batch): 1.109029769897461\n",
      "\t Training loss (single batch): 1.3053689002990723\n",
      "\t Training loss (single batch): 0.6044318675994873\n",
      "\t Training loss (single batch): 0.9013947248458862\n",
      "\t Training loss (single batch): 1.3937071561813354\n",
      "\t Training loss (single batch): 1.3715606927871704\n",
      "\t Training loss (single batch): 1.9768600463867188\n",
      "\t Training loss (single batch): 1.342163324356079\n",
      "\t Training loss (single batch): 1.0824840068817139\n",
      "\t Training loss (single batch): 1.3994301557540894\n",
      "\t Training loss (single batch): 0.7272936105728149\n",
      "\t Training loss (single batch): 0.9082694053649902\n",
      "\t Training loss (single batch): 1.2829370498657227\n",
      "\t Training loss (single batch): 1.3903567790985107\n",
      "\t Training loss (single batch): 1.2876179218292236\n",
      "\t Training loss (single batch): 1.2318997383117676\n",
      "\t Training loss (single batch): 1.02875554561615\n",
      "\t Training loss (single batch): 1.1213377714157104\n",
      "\t Training loss (single batch): 1.04229736328125\n",
      "\t Training loss (single batch): 1.0960032939910889\n",
      "\t Training loss (single batch): 1.6754417419433594\n",
      "\t Training loss (single batch): 1.570923089981079\n",
      "\t Training loss (single batch): 1.3361234664916992\n",
      "\t Training loss (single batch): 1.4175622463226318\n",
      "\t Training loss (single batch): 1.3400890827178955\n",
      "\t Training loss (single batch): 1.2020890712738037\n",
      "\t Training loss (single batch): 0.9881394505500793\n",
      "\t Training loss (single batch): 1.259230613708496\n",
      "\t Training loss (single batch): 1.717848777770996\n",
      "\t Training loss (single batch): 0.9501288533210754\n",
      "\t Training loss (single batch): 0.9300723671913147\n",
      "\t Training loss (single batch): 0.8985471725463867\n",
      "\t Training loss (single batch): 1.3619965314865112\n",
      "\t Training loss (single batch): 1.2129079103469849\n",
      "\t Training loss (single batch): 0.7589549422264099\n",
      "\t Training loss (single batch): 1.384890079498291\n",
      "\t Training loss (single batch): 0.8755733966827393\n",
      "\t Training loss (single batch): 1.1104507446289062\n",
      "\t Training loss (single batch): 1.0082485675811768\n",
      "\t Training loss (single batch): 1.2564836740493774\n",
      "\t Training loss (single batch): 1.6956138610839844\n",
      "\t Training loss (single batch): 1.2420110702514648\n",
      "\t Training loss (single batch): 1.3297646045684814\n",
      "\t Training loss (single batch): 1.2212053537368774\n",
      "\t Training loss (single batch): 1.2603009939193726\n",
      "\t Training loss (single batch): 1.5557241439819336\n",
      "\t Training loss (single batch): 0.8803354501724243\n",
      "\t Training loss (single batch): 1.234567642211914\n",
      "\t Training loss (single batch): 1.4860509634017944\n",
      "\t Training loss (single batch): 1.1373316049575806\n",
      "\t Training loss (single batch): 1.0771818161010742\n",
      "\t Training loss (single batch): 1.2598719596862793\n",
      "\t Training loss (single batch): 1.1602195501327515\n",
      "\t Training loss (single batch): 1.3499356508255005\n",
      "\t Training loss (single batch): 1.4995967149734497\n",
      "\t Training loss (single batch): 1.5544767379760742\n",
      "\t Training loss (single batch): 0.9817482233047485\n",
      "\t Training loss (single batch): 1.2578047513961792\n",
      "\t Training loss (single batch): 1.3975452184677124\n",
      "\t Training loss (single batch): 0.8905693888664246\n",
      "\t Training loss (single batch): 0.805103063583374\n",
      "\t Training loss (single batch): 1.2641353607177734\n",
      "\t Training loss (single batch): 0.9596505761146545\n",
      "\t Training loss (single batch): 1.3046795129776\n",
      "\t Training loss (single batch): 1.471197247505188\n",
      "\t Training loss (single batch): 0.8200915455818176\n",
      "\t Training loss (single batch): 1.3148860931396484\n",
      "\t Training loss (single batch): 1.1078616380691528\n",
      "\t Training loss (single batch): 0.9351961612701416\n",
      "\t Training loss (single batch): 1.748346209526062\n",
      "\t Training loss (single batch): 1.4035242795944214\n",
      "\t Training loss (single batch): 1.0021531581878662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8160034418106079\n",
      "\t Training loss (single batch): 1.0099995136260986\n",
      "\t Training loss (single batch): 1.2973977327346802\n",
      "\t Training loss (single batch): 1.0691630840301514\n",
      "\t Training loss (single batch): 1.3099836111068726\n",
      "\t Training loss (single batch): 1.015331745147705\n",
      "\t Training loss (single batch): 1.1386127471923828\n",
      "\t Training loss (single batch): 1.1873633861541748\n",
      "\t Training loss (single batch): 0.9227063059806824\n",
      "\t Training loss (single batch): 1.1036112308502197\n",
      "\t Training loss (single batch): 1.0090867280960083\n",
      "\t Training loss (single batch): 1.403901219367981\n",
      "\t Training loss (single batch): 1.2556023597717285\n",
      "\t Training loss (single batch): 1.197440505027771\n",
      "\t Training loss (single batch): 1.6362134218215942\n",
      "\t Training loss (single batch): 1.4138511419296265\n",
      "\t Training loss (single batch): 1.3826603889465332\n",
      "\t Training loss (single batch): 1.5466279983520508\n",
      "\t Training loss (single batch): 0.8878976702690125\n",
      "\t Training loss (single batch): 1.4928566217422485\n",
      "\t Training loss (single batch): 1.1625511646270752\n",
      "\t Training loss (single batch): 1.023452639579773\n",
      "\t Training loss (single batch): 1.3235411643981934\n",
      "\t Training loss (single batch): 0.915185809135437\n",
      "\t Training loss (single batch): 0.8156318664550781\n",
      "\t Training loss (single batch): 1.3568344116210938\n",
      "\t Training loss (single batch): 1.2024047374725342\n",
      "\t Training loss (single batch): 0.9823556542396545\n",
      "\t Training loss (single batch): 1.102576494216919\n",
      "\t Training loss (single batch): 2.036717653274536\n",
      "\t Training loss (single batch): 1.4381251335144043\n",
      "\t Training loss (single batch): 1.0959174633026123\n",
      "\t Training loss (single batch): 1.1932297945022583\n",
      "\t Training loss (single batch): 1.1769142150878906\n",
      "\t Training loss (single batch): 1.3067114353179932\n",
      "\t Training loss (single batch): 0.9105174541473389\n",
      "\t Training loss (single batch): 1.2243664264678955\n",
      "\t Training loss (single batch): 1.0891972780227661\n",
      "\t Training loss (single batch): 1.1288385391235352\n",
      "\t Training loss (single batch): 1.3247486352920532\n",
      "\t Training loss (single batch): 1.1627885103225708\n",
      "\t Training loss (single batch): 1.1366389989852905\n",
      "\t Training loss (single batch): 1.186790108680725\n",
      "\t Training loss (single batch): 1.20221745967865\n",
      "\t Training loss (single batch): 1.4844282865524292\n",
      "\t Training loss (single batch): 1.1331143379211426\n",
      "\t Training loss (single batch): 0.9780507683753967\n",
      "\t Training loss (single batch): 0.9399226307868958\n",
      "\t Training loss (single batch): 0.9869608879089355\n",
      "\t Training loss (single batch): 0.7953735589981079\n",
      "\t Training loss (single batch): 0.8199156522750854\n",
      "\t Training loss (single batch): 1.153239130973816\n",
      "\t Training loss (single batch): 1.0466840267181396\n",
      "\t Training loss (single batch): 1.6189777851104736\n",
      "\t Training loss (single batch): 0.998702883720398\n",
      "\t Training loss (single batch): 1.2069404125213623\n",
      "\t Training loss (single batch): 1.0159896612167358\n",
      "\t Training loss (single batch): 1.9061931371688843\n",
      "\t Training loss (single batch): 0.9539834856987\n",
      "\t Training loss (single batch): 1.1153316497802734\n",
      "\t Training loss (single batch): 1.088339924812317\n",
      "\t Training loss (single batch): 1.580588698387146\n",
      "\t Training loss (single batch): 1.1089603900909424\n",
      "\t Training loss (single batch): 1.3600021600723267\n",
      "\t Training loss (single batch): 1.0401990413665771\n",
      "\t Training loss (single batch): 1.2729952335357666\n",
      "\t Training loss (single batch): 1.3384292125701904\n",
      "\t Training loss (single batch): 1.2275310754776\n",
      "\t Training loss (single batch): 1.088839054107666\n",
      "\t Training loss (single batch): 1.1016024351119995\n",
      "\t Training loss (single batch): 0.8797438740730286\n",
      "\t Training loss (single batch): 1.1882123947143555\n",
      "\t Training loss (single batch): 2.5348949432373047\n",
      "##################################\n",
      "## EPOCH 70\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4838275909423828\n",
      "\t Training loss (single batch): 1.0058655738830566\n",
      "\t Training loss (single batch): 0.6786617040634155\n",
      "\t Training loss (single batch): 1.1723148822784424\n",
      "\t Training loss (single batch): 1.179044246673584\n",
      "\t Training loss (single batch): 1.0943574905395508\n",
      "\t Training loss (single batch): 1.0920391082763672\n",
      "\t Training loss (single batch): 1.1087827682495117\n",
      "\t Training loss (single batch): 0.8596212267875671\n",
      "\t Training loss (single batch): 1.6993197202682495\n",
      "\t Training loss (single batch): 1.5106558799743652\n",
      "\t Training loss (single batch): 1.2948899269104004\n",
      "\t Training loss (single batch): 0.8819789290428162\n",
      "\t Training loss (single batch): 1.009839415550232\n",
      "\t Training loss (single batch): 0.8402125239372253\n",
      "\t Training loss (single batch): 1.308699369430542\n",
      "\t Training loss (single batch): 1.2349783182144165\n",
      "\t Training loss (single batch): 1.202166199684143\n",
      "\t Training loss (single batch): 1.3078564405441284\n",
      "\t Training loss (single batch): 1.2306236028671265\n",
      "\t Training loss (single batch): 1.3365646600723267\n",
      "\t Training loss (single batch): 0.847328245639801\n",
      "\t Training loss (single batch): 1.323851227760315\n",
      "\t Training loss (single batch): 2.019251823425293\n",
      "\t Training loss (single batch): 0.9416332244873047\n",
      "\t Training loss (single batch): 0.7394890189170837\n",
      "\t Training loss (single batch): 1.0053002834320068\n",
      "\t Training loss (single batch): 1.0315407514572144\n",
      "\t Training loss (single batch): 1.2173908948898315\n",
      "\t Training loss (single batch): 1.9345858097076416\n",
      "\t Training loss (single batch): 0.9678184986114502\n",
      "\t Training loss (single batch): 1.2841715812683105\n",
      "\t Training loss (single batch): 1.3094416856765747\n",
      "\t Training loss (single batch): 1.2312297821044922\n",
      "\t Training loss (single batch): 0.9845607280731201\n",
      "\t Training loss (single batch): 1.404101014137268\n",
      "\t Training loss (single batch): 1.207265853881836\n",
      "\t Training loss (single batch): 1.0397940874099731\n",
      "\t Training loss (single batch): 1.1784669160842896\n",
      "\t Training loss (single batch): 1.2505680322647095\n",
      "\t Training loss (single batch): 1.1419090032577515\n",
      "\t Training loss (single batch): 1.4561192989349365\n",
      "\t Training loss (single batch): 0.6810536980628967\n",
      "\t Training loss (single batch): 1.3050576448440552\n",
      "\t Training loss (single batch): 1.450150966644287\n",
      "\t Training loss (single batch): 1.0083023309707642\n",
      "\t Training loss (single batch): 1.3381716012954712\n",
      "\t Training loss (single batch): 0.843078076839447\n",
      "\t Training loss (single batch): 1.0604058504104614\n",
      "\t Training loss (single batch): 1.0366936922073364\n",
      "\t Training loss (single batch): 1.2759827375411987\n",
      "\t Training loss (single batch): 0.7657010555267334\n",
      "\t Training loss (single batch): 0.8285300731658936\n",
      "\t Training loss (single batch): 1.47081458568573\n",
      "\t Training loss (single batch): 1.2129732370376587\n",
      "\t Training loss (single batch): 1.3729532957077026\n",
      "\t Training loss (single batch): 1.1819602251052856\n",
      "\t Training loss (single batch): 0.7125320434570312\n",
      "\t Training loss (single batch): 0.8375114798545837\n",
      "\t Training loss (single batch): 1.046736478805542\n",
      "\t Training loss (single batch): 1.5581035614013672\n",
      "\t Training loss (single batch): 0.9605767130851746\n",
      "\t Training loss (single batch): 1.088866949081421\n",
      "\t Training loss (single batch): 1.0240484476089478\n",
      "\t Training loss (single batch): 1.3568497896194458\n",
      "\t Training loss (single batch): 1.2595739364624023\n",
      "\t Training loss (single batch): 1.4289772510528564\n",
      "\t Training loss (single batch): 1.152163028717041\n",
      "\t Training loss (single batch): 1.5108932256698608\n",
      "\t Training loss (single batch): 1.2391510009765625\n",
      "\t Training loss (single batch): 1.3196299076080322\n",
      "\t Training loss (single batch): 1.3957014083862305\n",
      "\t Training loss (single batch): 1.0582908391952515\n",
      "\t Training loss (single batch): 0.9722735285758972\n",
      "\t Training loss (single batch): 1.1098966598510742\n",
      "\t Training loss (single batch): 1.6608437299728394\n",
      "\t Training loss (single batch): 1.026954174041748\n",
      "\t Training loss (single batch): 0.9134432077407837\n",
      "\t Training loss (single batch): 1.0304213762283325\n",
      "\t Training loss (single batch): 1.1057556867599487\n",
      "\t Training loss (single batch): 1.5355641841888428\n",
      "\t Training loss (single batch): 1.2833974361419678\n",
      "\t Training loss (single batch): 0.8453179001808167\n",
      "\t Training loss (single batch): 1.0294349193572998\n",
      "\t Training loss (single batch): 0.949516773223877\n",
      "\t Training loss (single batch): 1.5425667762756348\n",
      "\t Training loss (single batch): 1.0162450075149536\n",
      "\t Training loss (single batch): 1.5306552648544312\n",
      "\t Training loss (single batch): 1.1595594882965088\n",
      "\t Training loss (single batch): 1.0437580347061157\n",
      "\t Training loss (single batch): 1.3193215131759644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0617226362228394\n",
      "\t Training loss (single batch): 1.0327634811401367\n",
      "\t Training loss (single batch): 1.3009889125823975\n",
      "\t Training loss (single batch): 1.0842212438583374\n",
      "\t Training loss (single batch): 1.3834598064422607\n",
      "\t Training loss (single batch): 0.9737730026245117\n",
      "\t Training loss (single batch): 1.0160748958587646\n",
      "\t Training loss (single batch): 0.9446943998336792\n",
      "\t Training loss (single batch): 1.0913375616073608\n",
      "\t Training loss (single batch): 1.083715796470642\n",
      "\t Training loss (single batch): 1.131844162940979\n",
      "\t Training loss (single batch): 0.8266701698303223\n",
      "\t Training loss (single batch): 1.3661925792694092\n",
      "\t Training loss (single batch): 1.1415492296218872\n",
      "\t Training loss (single batch): 0.7540522217750549\n",
      "\t Training loss (single batch): 0.9501115679740906\n",
      "\t Training loss (single batch): 1.1604284048080444\n",
      "\t Training loss (single batch): 1.0912885665893555\n",
      "\t Training loss (single batch): 0.9439729452133179\n",
      "\t Training loss (single batch): 1.5521292686462402\n",
      "\t Training loss (single batch): 1.193016767501831\n",
      "\t Training loss (single batch): 1.4034419059753418\n",
      "\t Training loss (single batch): 1.0194405317306519\n",
      "\t Training loss (single batch): 1.299685001373291\n",
      "\t Training loss (single batch): 1.424450159072876\n",
      "\t Training loss (single batch): 1.2262122631072998\n",
      "\t Training loss (single batch): 1.542165994644165\n",
      "\t Training loss (single batch): 1.0245248079299927\n",
      "\t Training loss (single batch): 1.275836706161499\n",
      "\t Training loss (single batch): 0.8721210956573486\n",
      "\t Training loss (single batch): 1.2453895807266235\n",
      "\t Training loss (single batch): 0.8035998940467834\n",
      "\t Training loss (single batch): 1.2393587827682495\n",
      "\t Training loss (single batch): 1.5464892387390137\n",
      "\t Training loss (single batch): 1.2031302452087402\n",
      "\t Training loss (single batch): 1.2560384273529053\n",
      "\t Training loss (single batch): 1.0873024463653564\n",
      "\t Training loss (single batch): 1.4189403057098389\n",
      "\t Training loss (single batch): 1.4063814878463745\n",
      "\t Training loss (single batch): 1.1862764358520508\n",
      "\t Training loss (single batch): 0.8785752058029175\n",
      "\t Training loss (single batch): 1.1180895566940308\n",
      "\t Training loss (single batch): 1.5243468284606934\n",
      "\t Training loss (single batch): 1.2092424631118774\n",
      "\t Training loss (single batch): 1.169710636138916\n",
      "\t Training loss (single batch): 1.1813615560531616\n",
      "\t Training loss (single batch): 1.1539161205291748\n",
      "\t Training loss (single batch): 1.201066255569458\n",
      "\t Training loss (single batch): 1.0543687343597412\n",
      "\t Training loss (single batch): 0.9941205382347107\n",
      "\t Training loss (single batch): 1.1614117622375488\n",
      "\t Training loss (single batch): 1.8567553758621216\n",
      "\t Training loss (single batch): 1.2511333227157593\n",
      "\t Training loss (single batch): 1.2654086351394653\n",
      "\t Training loss (single batch): 1.1138778924942017\n",
      "\t Training loss (single batch): 1.4118211269378662\n",
      "\t Training loss (single batch): 1.4354724884033203\n",
      "\t Training loss (single batch): 1.380443811416626\n",
      "\t Training loss (single batch): 1.5029476881027222\n",
      "\t Training loss (single batch): 1.3984097242355347\n",
      "\t Training loss (single batch): 1.1068588495254517\n",
      "\t Training loss (single batch): 1.229718804359436\n",
      "\t Training loss (single batch): 1.1073445081710815\n",
      "\t Training loss (single batch): 1.2555323839187622\n",
      "\t Training loss (single batch): 0.818775475025177\n",
      "\t Training loss (single batch): 1.396736741065979\n",
      "\t Training loss (single batch): 1.0304358005523682\n",
      "\t Training loss (single batch): 1.24858820438385\n",
      "\t Training loss (single batch): 1.221062183380127\n",
      "\t Training loss (single batch): 1.0801734924316406\n",
      "\t Training loss (single batch): 1.7450244426727295\n",
      "\t Training loss (single batch): 1.6415423154830933\n",
      "\t Training loss (single batch): 1.0512093305587769\n",
      "\t Training loss (single batch): 1.0454949140548706\n",
      "\t Training loss (single batch): 1.5992472171783447\n",
      "\t Training loss (single batch): 1.2418354749679565\n",
      "\t Training loss (single batch): 1.0524120330810547\n",
      "\t Training loss (single batch): 1.0097682476043701\n",
      "\t Training loss (single batch): 0.8833552002906799\n",
      "\t Training loss (single batch): 1.4890320301055908\n",
      "\t Training loss (single batch): 0.9945093393325806\n",
      "\t Training loss (single batch): 1.0726951360702515\n",
      "\t Training loss (single batch): 0.8682731986045837\n",
      "\t Training loss (single batch): 0.8470216989517212\n",
      "\t Training loss (single batch): 1.0747100114822388\n",
      "\t Training loss (single batch): 1.3214296102523804\n",
      "\t Training loss (single batch): 0.9413445591926575\n",
      "\t Training loss (single batch): 1.324851393699646\n",
      "\t Training loss (single batch): 0.8229614496231079\n",
      "\t Training loss (single batch): 1.2442010641098022\n",
      "\t Training loss (single batch): 1.198226809501648\n",
      "\t Training loss (single batch): 1.2845370769500732\n",
      "\t Training loss (single batch): 1.0939651727676392\n",
      "\t Training loss (single batch): 1.4860042333602905\n",
      "\t Training loss (single batch): 0.9000048637390137\n",
      "\t Training loss (single batch): 1.9031270742416382\n",
      "\t Training loss (single batch): 1.097122073173523\n",
      "\t Training loss (single batch): 1.205700397491455\n",
      "\t Training loss (single batch): 1.6493809223175049\n",
      "\t Training loss (single batch): 1.2074676752090454\n",
      "\t Training loss (single batch): 1.0490134954452515\n",
      "\t Training loss (single batch): 1.539758563041687\n",
      "\t Training loss (single batch): 1.2372981309890747\n",
      "\t Training loss (single batch): 1.1666992902755737\n",
      "\t Training loss (single batch): 0.9762261509895325\n",
      "\t Training loss (single batch): 1.2475450038909912\n",
      "\t Training loss (single batch): 1.5683382749557495\n",
      "\t Training loss (single batch): 1.524275302886963\n",
      "\t Training loss (single batch): 1.1501308679580688\n",
      "\t Training loss (single batch): 1.3830512762069702\n",
      "\t Training loss (single batch): 0.9620744585990906\n",
      "\t Training loss (single batch): 0.9693025946617126\n",
      "\t Training loss (single batch): 1.2885195016860962\n",
      "\t Training loss (single batch): 1.3285620212554932\n",
      "\t Training loss (single batch): 1.1672513484954834\n",
      "\t Training loss (single batch): 1.1613211631774902\n",
      "\t Training loss (single batch): 1.3203229904174805\n",
      "\t Training loss (single batch): 1.2907252311706543\n",
      "\t Training loss (single batch): 0.9150036573410034\n",
      "\t Training loss (single batch): 1.289528250694275\n",
      "\t Training loss (single batch): 0.843493640422821\n",
      "\t Training loss (single batch): 1.1586836576461792\n",
      "\t Training loss (single batch): 1.3431483507156372\n",
      "\t Training loss (single batch): 1.605004072189331\n",
      "\t Training loss (single batch): 1.1061952114105225\n",
      "\t Training loss (single batch): 1.1015355587005615\n",
      "\t Training loss (single batch): 1.4434363842010498\n",
      "\t Training loss (single batch): 1.5623979568481445\n",
      "\t Training loss (single batch): 1.0145061016082764\n",
      "\t Training loss (single batch): 1.2574130296707153\n",
      "\t Training loss (single batch): 1.1755316257476807\n",
      "\t Training loss (single batch): 1.2496626377105713\n",
      "\t Training loss (single batch): 1.2870739698410034\n",
      "\t Training loss (single batch): 1.5171633958816528\n",
      "\t Training loss (single batch): 1.1048601865768433\n",
      "\t Training loss (single batch): 1.2765860557556152\n",
      "\t Training loss (single batch): 1.354572057723999\n",
      "\t Training loss (single batch): 1.5934832096099854\n",
      "\t Training loss (single batch): 1.1061378717422485\n",
      "\t Training loss (single batch): 1.1224186420440674\n",
      "\t Training loss (single batch): 0.9720505475997925\n",
      "\t Training loss (single batch): 1.3582332134246826\n",
      "\t Training loss (single batch): 1.182094693183899\n",
      "\t Training loss (single batch): 1.220668077468872\n",
      "\t Training loss (single batch): 1.1292314529418945\n",
      "\t Training loss (single batch): 1.1213109493255615\n",
      "\t Training loss (single batch): 1.160913109779358\n",
      "\t Training loss (single batch): 0.7624718546867371\n",
      "\t Training loss (single batch): 1.4850821495056152\n",
      "\t Training loss (single batch): 1.547555685043335\n",
      "\t Training loss (single batch): 0.9456275105476379\n",
      "\t Training loss (single batch): 1.2336899042129517\n",
      "\t Training loss (single batch): 0.88642817735672\n",
      "\t Training loss (single batch): 1.0369020700454712\n",
      "\t Training loss (single batch): 1.3493534326553345\n",
      "\t Training loss (single batch): 0.7665248513221741\n",
      "\t Training loss (single batch): 1.8228551149368286\n",
      "\t Training loss (single batch): 1.2108010053634644\n",
      "\t Training loss (single batch): 1.098541259765625\n",
      "\t Training loss (single batch): 1.1459089517593384\n",
      "\t Training loss (single batch): 1.423539161682129\n",
      "\t Training loss (single batch): 1.66468346118927\n",
      "\t Training loss (single batch): 0.8751344084739685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2471928596496582\n",
      "\t Training loss (single batch): 1.367133378982544\n",
      "\t Training loss (single batch): 1.3059415817260742\n",
      "\t Training loss (single batch): 1.2017916440963745\n",
      "\t Training loss (single batch): 1.1564253568649292\n",
      "\t Training loss (single batch): 1.2845196723937988\n",
      "\t Training loss (single batch): 1.315636157989502\n",
      "\t Training loss (single batch): 0.6448140144348145\n",
      "\t Training loss (single batch): 0.6096889972686768\n",
      "\t Training loss (single batch): 0.7856478095054626\n",
      "\t Training loss (single batch): 1.4583954811096191\n",
      "\t Training loss (single batch): 1.026942491531372\n",
      "\t Training loss (single batch): 1.4729641675949097\n",
      "\t Training loss (single batch): 1.2846105098724365\n",
      "\t Training loss (single batch): 1.2864651679992676\n",
      "\t Training loss (single batch): 1.0961871147155762\n",
      "\t Training loss (single batch): 1.3196487426757812\n",
      "\t Training loss (single batch): 1.0385462045669556\n",
      "\t Training loss (single batch): 1.2144109010696411\n",
      "\t Training loss (single batch): 0.7505294680595398\n",
      "\t Training loss (single batch): 1.334470510482788\n",
      "\t Training loss (single batch): 1.3366223573684692\n",
      "\t Training loss (single batch): 1.3402928113937378\n",
      "\t Training loss (single batch): 1.3887017965316772\n",
      "\t Training loss (single batch): 1.2030271291732788\n",
      "\t Training loss (single batch): 1.0890824794769287\n",
      "\t Training loss (single batch): 1.1576975584030151\n",
      "\t Training loss (single batch): 1.741286039352417\n",
      "\t Training loss (single batch): 1.1912000179290771\n",
      "\t Training loss (single batch): 1.295776128768921\n",
      "\t Training loss (single batch): 1.5307674407958984\n",
      "\t Training loss (single batch): 1.1638790369033813\n",
      "\t Training loss (single batch): 1.4048357009887695\n",
      "\t Training loss (single batch): 0.9787881374359131\n",
      "\t Training loss (single batch): 1.2517948150634766\n",
      "\t Training loss (single batch): 1.3240550756454468\n",
      "\t Training loss (single batch): 1.3458926677703857\n",
      "\t Training loss (single batch): 1.2551223039627075\n",
      "\t Training loss (single batch): 0.7301050424575806\n",
      "\t Training loss (single batch): 1.2375398874282837\n",
      "\t Training loss (single batch): 1.0311886072158813\n",
      "\t Training loss (single batch): 1.3945071697235107\n",
      "\t Training loss (single batch): 0.8095318078994751\n",
      "\t Training loss (single batch): 1.2912392616271973\n",
      "\t Training loss (single batch): 0.9478873610496521\n",
      "\t Training loss (single batch): 1.040618896484375\n",
      "\t Training loss (single batch): 1.3457063436508179\n",
      "\t Training loss (single batch): 1.4563186168670654\n",
      "\t Training loss (single batch): 0.8138984441757202\n",
      "\t Training loss (single batch): 0.9538938403129578\n",
      "\t Training loss (single batch): 1.2057995796203613\n",
      "\t Training loss (single batch): 1.2326762676239014\n",
      "\t Training loss (single batch): 0.7208946347236633\n",
      "\t Training loss (single batch): 1.086747169494629\n",
      "\t Training loss (single batch): 1.296943187713623\n",
      "\t Training loss (single batch): 1.1329424381256104\n",
      "\t Training loss (single batch): 1.1900626420974731\n",
      "\t Training loss (single batch): 0.9312877058982849\n",
      "\t Training loss (single batch): 0.8603191375732422\n",
      "\t Training loss (single batch): 1.0323222875595093\n",
      "\t Training loss (single batch): 1.4416468143463135\n",
      "\t Training loss (single batch): 1.2879868745803833\n",
      "\t Training loss (single batch): 1.4291390180587769\n",
      "##################################\n",
      "## EPOCH 71\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1947746276855469\n",
      "\t Training loss (single batch): 1.5658942461013794\n",
      "\t Training loss (single batch): 1.0474880933761597\n",
      "\t Training loss (single batch): 1.3055952787399292\n",
      "\t Training loss (single batch): 0.7734061479568481\n",
      "\t Training loss (single batch): 1.3259114027023315\n",
      "\t Training loss (single batch): 1.313483476638794\n",
      "\t Training loss (single batch): 1.1220279932022095\n",
      "\t Training loss (single batch): 0.9096189737319946\n",
      "\t Training loss (single batch): 1.4511557817459106\n",
      "\t Training loss (single batch): 1.1579813957214355\n",
      "\t Training loss (single batch): 1.1832808256149292\n",
      "\t Training loss (single batch): 1.622054100036621\n",
      "\t Training loss (single batch): 1.1526464223861694\n",
      "\t Training loss (single batch): 0.9726248979568481\n",
      "\t Training loss (single batch): 1.006500482559204\n",
      "\t Training loss (single batch): 1.1709580421447754\n",
      "\t Training loss (single batch): 0.9561694264411926\n",
      "\t Training loss (single batch): 1.4112255573272705\n",
      "\t Training loss (single batch): 1.4831517934799194\n",
      "\t Training loss (single batch): 1.1868776082992554\n",
      "\t Training loss (single batch): 1.2746676206588745\n",
      "\t Training loss (single batch): 1.3833322525024414\n",
      "\t Training loss (single batch): 1.049661636352539\n",
      "\t Training loss (single batch): 1.1008660793304443\n",
      "\t Training loss (single batch): 1.0545364618301392\n",
      "\t Training loss (single batch): 0.9330782890319824\n",
      "\t Training loss (single batch): 1.3739904165267944\n",
      "\t Training loss (single batch): 1.2388505935668945\n",
      "\t Training loss (single batch): 0.9928414225578308\n",
      "\t Training loss (single batch): 1.4179445505142212\n",
      "\t Training loss (single batch): 0.8238735198974609\n",
      "\t Training loss (single batch): 1.3398393392562866\n",
      "\t Training loss (single batch): 0.8507498502731323\n",
      "\t Training loss (single batch): 1.040975570678711\n",
      "\t Training loss (single batch): 1.2626352310180664\n",
      "\t Training loss (single batch): 1.3599299192428589\n",
      "\t Training loss (single batch): 1.0240378379821777\n",
      "\t Training loss (single batch): 1.121712565422058\n",
      "\t Training loss (single batch): 1.3107130527496338\n",
      "\t Training loss (single batch): 1.6629482507705688\n",
      "\t Training loss (single batch): 1.2391413450241089\n",
      "\t Training loss (single batch): 1.6551687717437744\n",
      "\t Training loss (single batch): 1.3576874732971191\n",
      "\t Training loss (single batch): 1.4293744564056396\n",
      "\t Training loss (single batch): 1.5851879119873047\n",
      "\t Training loss (single batch): 1.4589011669158936\n",
      "\t Training loss (single batch): 1.2021636962890625\n",
      "\t Training loss (single batch): 1.1183418035507202\n",
      "\t Training loss (single batch): 0.8916333317756653\n",
      "\t Training loss (single batch): 1.4534746408462524\n",
      "\t Training loss (single batch): 0.9911397695541382\n",
      "\t Training loss (single batch): 1.346299409866333\n",
      "\t Training loss (single batch): 1.6029506921768188\n",
      "\t Training loss (single batch): 1.1800355911254883\n",
      "\t Training loss (single batch): 0.8892872929573059\n",
      "\t Training loss (single batch): 0.9863790273666382\n",
      "\t Training loss (single batch): 1.541399598121643\n",
      "\t Training loss (single batch): 1.4744861125946045\n",
      "\t Training loss (single batch): 1.579047441482544\n",
      "\t Training loss (single batch): 1.0078620910644531\n",
      "\t Training loss (single batch): 1.5415239334106445\n",
      "\t Training loss (single batch): 1.6493092775344849\n",
      "\t Training loss (single batch): 1.2050083875656128\n",
      "\t Training loss (single batch): 1.1187666654586792\n",
      "\t Training loss (single batch): 1.157832145690918\n",
      "\t Training loss (single batch): 1.142484188079834\n",
      "\t Training loss (single batch): 1.1612799167633057\n",
      "\t Training loss (single batch): 1.124606966972351\n",
      "\t Training loss (single batch): 1.345519781112671\n",
      "\t Training loss (single batch): 1.4493910074234009\n",
      "\t Training loss (single batch): 1.002779483795166\n",
      "\t Training loss (single batch): 1.4416897296905518\n",
      "\t Training loss (single batch): 0.940401017665863\n",
      "\t Training loss (single batch): 1.4742568731307983\n",
      "\t Training loss (single batch): 1.3172587156295776\n",
      "\t Training loss (single batch): 1.5517767667770386\n",
      "\t Training loss (single batch): 1.0613375902175903\n",
      "\t Training loss (single batch): 1.2971633672714233\n",
      "\t Training loss (single batch): 1.0350418090820312\n",
      "\t Training loss (single batch): 1.0240973234176636\n",
      "\t Training loss (single batch): 0.9743388891220093\n",
      "\t Training loss (single batch): 1.4541398286819458\n",
      "\t Training loss (single batch): 1.3030972480773926\n",
      "\t Training loss (single batch): 1.2030916213989258\n",
      "\t Training loss (single batch): 1.490053653717041\n",
      "\t Training loss (single batch): 0.801601231098175\n",
      "\t Training loss (single batch): 1.2618167400360107\n",
      "\t Training loss (single batch): 1.493756651878357\n",
      "\t Training loss (single batch): 1.26143479347229\n",
      "\t Training loss (single batch): 1.3143348693847656\n",
      "\t Training loss (single batch): 1.4226317405700684\n",
      "\t Training loss (single batch): 1.2651841640472412\n",
      "\t Training loss (single batch): 1.0199249982833862\n",
      "\t Training loss (single batch): 1.5576127767562866\n",
      "\t Training loss (single batch): 1.4437397718429565\n",
      "\t Training loss (single batch): 1.2433834075927734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3224976062774658\n",
      "\t Training loss (single batch): 1.6556023359298706\n",
      "\t Training loss (single batch): 1.6982437372207642\n",
      "\t Training loss (single batch): 1.0569214820861816\n",
      "\t Training loss (single batch): 1.1315876245498657\n",
      "\t Training loss (single batch): 1.1650437116622925\n",
      "\t Training loss (single batch): 0.8211134076118469\n",
      "\t Training loss (single batch): 1.0254809856414795\n",
      "\t Training loss (single batch): 1.224257230758667\n",
      "\t Training loss (single batch): 1.081664800643921\n",
      "\t Training loss (single batch): 0.7189590334892273\n",
      "\t Training loss (single batch): 1.5887346267700195\n",
      "\t Training loss (single batch): 1.1385533809661865\n",
      "\t Training loss (single batch): 1.5558732748031616\n",
      "\t Training loss (single batch): 1.3398356437683105\n",
      "\t Training loss (single batch): 1.3303115367889404\n",
      "\t Training loss (single batch): 1.1918350458145142\n",
      "\t Training loss (single batch): 0.8220097422599792\n",
      "\t Training loss (single batch): 0.8109652400016785\n",
      "\t Training loss (single batch): 1.483605146408081\n",
      "\t Training loss (single batch): 1.1882991790771484\n",
      "\t Training loss (single batch): 1.1294753551483154\n",
      "\t Training loss (single batch): 1.2404742240905762\n",
      "\t Training loss (single batch): 1.4328001737594604\n",
      "\t Training loss (single batch): 1.2149783372879028\n",
      "\t Training loss (single batch): 1.6786795854568481\n",
      "\t Training loss (single batch): 1.2170090675354004\n",
      "\t Training loss (single batch): 1.1111222505569458\n",
      "\t Training loss (single batch): 1.0758557319641113\n",
      "\t Training loss (single batch): 0.80191969871521\n",
      "\t Training loss (single batch): 1.1970715522766113\n",
      "\t Training loss (single batch): 1.4026259183883667\n",
      "\t Training loss (single batch): 1.204681396484375\n",
      "\t Training loss (single batch): 0.9131377935409546\n",
      "\t Training loss (single batch): 0.9383362531661987\n",
      "\t Training loss (single batch): 1.1199064254760742\n",
      "\t Training loss (single batch): 1.1729422807693481\n",
      "\t Training loss (single batch): 1.8429772853851318\n",
      "\t Training loss (single batch): 1.77569580078125\n",
      "\t Training loss (single batch): 0.8489293456077576\n",
      "\t Training loss (single batch): 0.9856323599815369\n",
      "\t Training loss (single batch): 1.0756406784057617\n",
      "\t Training loss (single batch): 1.2900519371032715\n",
      "\t Training loss (single batch): 1.1777262687683105\n",
      "\t Training loss (single batch): 0.8238943219184875\n",
      "\t Training loss (single batch): 1.4075438976287842\n",
      "\t Training loss (single batch): 1.4676241874694824\n",
      "\t Training loss (single batch): 1.1893333196640015\n",
      "\t Training loss (single batch): 1.2503141164779663\n",
      "\t Training loss (single batch): 1.3066842555999756\n",
      "\t Training loss (single batch): 1.4857699871063232\n",
      "\t Training loss (single batch): 0.7499076724052429\n",
      "\t Training loss (single batch): 1.4675867557525635\n",
      "\t Training loss (single batch): 0.9856463074684143\n",
      "\t Training loss (single batch): 1.2338237762451172\n",
      "\t Training loss (single batch): 0.9792980551719666\n",
      "\t Training loss (single batch): 0.985878586769104\n",
      "\t Training loss (single batch): 1.5207514762878418\n",
      "\t Training loss (single batch): 1.360840916633606\n",
      "\t Training loss (single batch): 1.2391761541366577\n",
      "\t Training loss (single batch): 1.0379300117492676\n",
      "\t Training loss (single batch): 0.9962760210037231\n",
      "\t Training loss (single batch): 1.3270390033721924\n",
      "\t Training loss (single batch): 0.8269546031951904\n",
      "\t Training loss (single batch): 1.3093554973602295\n",
      "\t Training loss (single batch): 1.3939836025238037\n",
      "\t Training loss (single batch): 0.9961608648300171\n",
      "\t Training loss (single batch): 1.1237878799438477\n",
      "\t Training loss (single batch): 1.4076533317565918\n",
      "\t Training loss (single batch): 1.1212857961654663\n",
      "\t Training loss (single batch): 1.1977839469909668\n",
      "\t Training loss (single batch): 1.3888062238693237\n",
      "\t Training loss (single batch): 1.420448899269104\n",
      "\t Training loss (single batch): 1.7267262935638428\n",
      "\t Training loss (single batch): 0.6768675446510315\n",
      "\t Training loss (single batch): 1.189832329750061\n",
      "\t Training loss (single batch): 1.4193083047866821\n",
      "\t Training loss (single batch): 1.0664207935333252\n",
      "\t Training loss (single batch): 1.5190045833587646\n",
      "\t Training loss (single batch): 0.9222318530082703\n",
      "\t Training loss (single batch): 0.9395336508750916\n",
      "\t Training loss (single batch): 1.0763403177261353\n",
      "\t Training loss (single batch): 1.3626636266708374\n",
      "\t Training loss (single batch): 1.4874457120895386\n",
      "\t Training loss (single batch): 0.9446729421615601\n",
      "\t Training loss (single batch): 1.131309151649475\n",
      "\t Training loss (single batch): 1.2722841501235962\n",
      "\t Training loss (single batch): 1.8195425271987915\n",
      "\t Training loss (single batch): 1.146087646484375\n",
      "\t Training loss (single batch): 1.2143633365631104\n",
      "\t Training loss (single batch): 0.9775898456573486\n",
      "\t Training loss (single batch): 1.4098566770553589\n",
      "\t Training loss (single batch): 1.0931028127670288\n",
      "\t Training loss (single batch): 1.357643961906433\n",
      "\t Training loss (single batch): 1.373686671257019\n",
      "\t Training loss (single batch): 1.212995171546936\n",
      "\t Training loss (single batch): 1.1859674453735352\n",
      "\t Training loss (single batch): 0.9691731333732605\n",
      "\t Training loss (single batch): 1.067764163017273\n",
      "\t Training loss (single batch): 0.7396770715713501\n",
      "\t Training loss (single batch): 1.0588030815124512\n",
      "\t Training loss (single batch): 1.3145751953125\n",
      "\t Training loss (single batch): 1.4669513702392578\n",
      "\t Training loss (single batch): 1.209991693496704\n",
      "\t Training loss (single batch): 1.359269618988037\n",
      "\t Training loss (single batch): 1.2981231212615967\n",
      "\t Training loss (single batch): 1.0673853158950806\n",
      "\t Training loss (single batch): 1.6606910228729248\n",
      "\t Training loss (single batch): 1.0483283996582031\n",
      "\t Training loss (single batch): 1.4132888317108154\n",
      "\t Training loss (single batch): 1.6476637125015259\n",
      "\t Training loss (single batch): 1.20637845993042\n",
      "\t Training loss (single batch): 1.0108927488327026\n",
      "\t Training loss (single batch): 1.0143601894378662\n",
      "\t Training loss (single batch): 1.725117802619934\n",
      "\t Training loss (single batch): 1.0236871242523193\n",
      "\t Training loss (single batch): 1.329837679862976\n",
      "\t Training loss (single batch): 1.3346303701400757\n",
      "\t Training loss (single batch): 1.3423445224761963\n",
      "\t Training loss (single batch): 1.3406065702438354\n",
      "\t Training loss (single batch): 1.178644061088562\n",
      "\t Training loss (single batch): 0.9754571318626404\n",
      "\t Training loss (single batch): 1.0916928052902222\n",
      "\t Training loss (single batch): 1.9053943157196045\n",
      "\t Training loss (single batch): 1.4890720844268799\n",
      "\t Training loss (single batch): 0.8972633481025696\n",
      "\t Training loss (single batch): 1.0540581941604614\n",
      "\t Training loss (single batch): 1.2940592765808105\n",
      "\t Training loss (single batch): 1.3052887916564941\n",
      "\t Training loss (single batch): 0.8627024292945862\n",
      "\t Training loss (single batch): 1.1613222360610962\n",
      "\t Training loss (single batch): 0.7913304567337036\n",
      "\t Training loss (single batch): 1.4964159727096558\n",
      "\t Training loss (single batch): 1.2155303955078125\n",
      "\t Training loss (single batch): 1.075296401977539\n",
      "\t Training loss (single batch): 0.9660709500312805\n",
      "\t Training loss (single batch): 1.7937297821044922\n",
      "\t Training loss (single batch): 1.418687343597412\n",
      "\t Training loss (single batch): 1.5487602949142456\n",
      "\t Training loss (single batch): 0.8459724187850952\n",
      "\t Training loss (single batch): 1.0966475009918213\n",
      "\t Training loss (single batch): 0.8382552266120911\n",
      "\t Training loss (single batch): 1.0671069622039795\n",
      "\t Training loss (single batch): 1.4932941198349\n",
      "\t Training loss (single batch): 1.5708727836608887\n",
      "\t Training loss (single batch): 1.396915316581726\n",
      "\t Training loss (single batch): 1.0929409265518188\n",
      "\t Training loss (single batch): 1.0341711044311523\n",
      "\t Training loss (single batch): 1.5413192510604858\n",
      "\t Training loss (single batch): 0.9769065380096436\n",
      "\t Training loss (single batch): 1.2910983562469482\n",
      "\t Training loss (single batch): 0.8620341420173645\n",
      "\t Training loss (single batch): 1.5654950141906738\n",
      "\t Training loss (single batch): 0.9889671206474304\n",
      "\t Training loss (single batch): 1.1962246894836426\n",
      "\t Training loss (single batch): 0.9765600562095642\n",
      "\t Training loss (single batch): 1.63429856300354\n",
      "\t Training loss (single batch): 0.9994231462478638\n",
      "\t Training loss (single batch): 0.9929158687591553\n",
      "\t Training loss (single batch): 1.2317112684249878\n",
      "\t Training loss (single batch): 1.5253428220748901\n",
      "\t Training loss (single batch): 1.0281306505203247\n",
      "\t Training loss (single batch): 0.801089346408844\n",
      "\t Training loss (single batch): 0.7440381646156311\n",
      "\t Training loss (single batch): 1.5217018127441406\n",
      "\t Training loss (single batch): 1.4194111824035645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.7602779865264893\n",
      "\t Training loss (single batch): 0.9250000715255737\n",
      "\t Training loss (single batch): 0.6522961258888245\n",
      "\t Training loss (single batch): 1.170006275177002\n",
      "\t Training loss (single batch): 0.7466882467269897\n",
      "\t Training loss (single batch): 1.157021164894104\n",
      "\t Training loss (single batch): 1.217100739479065\n",
      "\t Training loss (single batch): 0.6944750547409058\n",
      "\t Training loss (single batch): 1.309264063835144\n",
      "\t Training loss (single batch): 0.9715827107429504\n",
      "\t Training loss (single batch): 1.0954562425613403\n",
      "\t Training loss (single batch): 1.1755549907684326\n",
      "\t Training loss (single batch): 0.8581966161727905\n",
      "\t Training loss (single batch): 0.9838907718658447\n",
      "\t Training loss (single batch): 1.2420307397842407\n",
      "\t Training loss (single batch): 1.2744020223617554\n",
      "\t Training loss (single batch): 1.1831680536270142\n",
      "\t Training loss (single batch): 0.9863413572311401\n",
      "\t Training loss (single batch): 1.247177243232727\n",
      "\t Training loss (single batch): 1.1567362546920776\n",
      "\t Training loss (single batch): 0.9537652730941772\n",
      "\t Training loss (single batch): 1.3221172094345093\n",
      "\t Training loss (single batch): 1.2690905332565308\n",
      "\t Training loss (single batch): 0.7292221188545227\n",
      "\t Training loss (single batch): 1.4114044904708862\n",
      "\t Training loss (single batch): 1.0260933637619019\n",
      "\t Training loss (single batch): 1.0856549739837646\n",
      "\t Training loss (single batch): 1.0424253940582275\n",
      "\t Training loss (single batch): 0.775008499622345\n",
      "\t Training loss (single batch): 1.0592647790908813\n",
      "\t Training loss (single batch): 0.9771326184272766\n",
      "\t Training loss (single batch): 1.151283860206604\n",
      "\t Training loss (single batch): 1.374652624130249\n",
      "\t Training loss (single batch): 1.3994868993759155\n",
      "\t Training loss (single batch): 1.3615258932113647\n",
      "\t Training loss (single batch): 1.0135743618011475\n",
      "\t Training loss (single batch): 1.2534152269363403\n",
      "\t Training loss (single batch): 1.2076939344406128\n",
      "\t Training loss (single batch): 1.6458592414855957\n",
      "\t Training loss (single batch): 0.7964348793029785\n",
      "\t Training loss (single batch): 1.2706447839736938\n",
      "\t Training loss (single batch): 0.8528861403465271\n",
      "\t Training loss (single batch): 1.0888932943344116\n",
      "\t Training loss (single batch): 1.2437556982040405\n",
      "\t Training loss (single batch): 1.5065914392471313\n",
      "\t Training loss (single batch): 1.237749457359314\n",
      "\t Training loss (single batch): 0.9431551694869995\n",
      "\t Training loss (single batch): 0.9278936982154846\n",
      "\t Training loss (single batch): 1.1563776731491089\n",
      "\t Training loss (single batch): 1.0177603960037231\n",
      "\t Training loss (single batch): 1.1455340385437012\n",
      "\t Training loss (single batch): 0.8947176337242126\n",
      "\t Training loss (single batch): 1.567250370979309\n",
      "\t Training loss (single batch): 1.1531305313110352\n",
      "##################################\n",
      "## EPOCH 72\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1424055099487305\n",
      "\t Training loss (single batch): 1.1647975444793701\n",
      "\t Training loss (single batch): 1.2506111860275269\n",
      "\t Training loss (single batch): 1.2699062824249268\n",
      "\t Training loss (single batch): 1.4049793481826782\n",
      "\t Training loss (single batch): 1.1445497274398804\n",
      "\t Training loss (single batch): 1.2901051044464111\n",
      "\t Training loss (single batch): 1.0563844442367554\n",
      "\t Training loss (single batch): 1.5379633903503418\n",
      "\t Training loss (single batch): 0.6315857172012329\n",
      "\t Training loss (single batch): 1.416931390762329\n",
      "\t Training loss (single batch): 1.2240662574768066\n",
      "\t Training loss (single batch): 1.4066029787063599\n",
      "\t Training loss (single batch): 1.350456714630127\n",
      "\t Training loss (single batch): 1.2204935550689697\n",
      "\t Training loss (single batch): 1.432887315750122\n",
      "\t Training loss (single batch): 1.2881883382797241\n",
      "\t Training loss (single batch): 1.0828876495361328\n",
      "\t Training loss (single batch): 1.062025785446167\n",
      "\t Training loss (single batch): 1.1736369132995605\n",
      "\t Training loss (single batch): 1.3523253202438354\n",
      "\t Training loss (single batch): 1.2352161407470703\n",
      "\t Training loss (single batch): 1.285448431968689\n",
      "\t Training loss (single batch): 1.0706908702850342\n",
      "\t Training loss (single batch): 0.8270318508148193\n",
      "\t Training loss (single batch): 1.234688401222229\n",
      "\t Training loss (single batch): 0.7172831892967224\n",
      "\t Training loss (single batch): 0.8346440196037292\n",
      "\t Training loss (single batch): 1.2103768587112427\n",
      "\t Training loss (single batch): 1.0131367444992065\n",
      "\t Training loss (single batch): 1.5079624652862549\n",
      "\t Training loss (single batch): 1.1749885082244873\n",
      "\t Training loss (single batch): 0.9397547841072083\n",
      "\t Training loss (single batch): 1.0981791019439697\n",
      "\t Training loss (single batch): 1.3820862770080566\n",
      "\t Training loss (single batch): 1.2251901626586914\n",
      "\t Training loss (single batch): 1.0046870708465576\n",
      "\t Training loss (single batch): 1.580338716506958\n",
      "\t Training loss (single batch): 1.0315861701965332\n",
      "\t Training loss (single batch): 1.003454327583313\n",
      "\t Training loss (single batch): 1.0799530744552612\n",
      "\t Training loss (single batch): 1.3628218173980713\n",
      "\t Training loss (single batch): 1.1094439029693604\n",
      "\t Training loss (single batch): 1.3087491989135742\n",
      "\t Training loss (single batch): 1.6310251951217651\n",
      "\t Training loss (single batch): 0.7452031970024109\n",
      "\t Training loss (single batch): 1.1421431303024292\n",
      "\t Training loss (single batch): 1.2564294338226318\n",
      "\t Training loss (single batch): 0.8501686453819275\n",
      "\t Training loss (single batch): 1.1764228343963623\n",
      "\t Training loss (single batch): 1.2636628150939941\n",
      "\t Training loss (single batch): 1.2367531061172485\n",
      "\t Training loss (single batch): 1.7023917436599731\n",
      "\t Training loss (single batch): 1.1551634073257446\n",
      "\t Training loss (single batch): 1.4990490674972534\n",
      "\t Training loss (single batch): 1.496394395828247\n",
      "\t Training loss (single batch): 1.0438531637191772\n",
      "\t Training loss (single batch): 1.0340818166732788\n",
      "\t Training loss (single batch): 1.158751130104065\n",
      "\t Training loss (single batch): 1.4716862440109253\n",
      "\t Training loss (single batch): 1.0416947603225708\n",
      "\t Training loss (single batch): 1.2921746969223022\n",
      "\t Training loss (single batch): 1.112805962562561\n",
      "\t Training loss (single batch): 0.8181631565093994\n",
      "\t Training loss (single batch): 0.9639449119567871\n",
      "\t Training loss (single batch): 1.3963409662246704\n",
      "\t Training loss (single batch): 1.3369109630584717\n",
      "\t Training loss (single batch): 1.027630090713501\n",
      "\t Training loss (single batch): 1.344707727432251\n",
      "\t Training loss (single batch): 1.2341583967208862\n",
      "\t Training loss (single batch): 0.9152483940124512\n",
      "\t Training loss (single batch): 1.3338993787765503\n",
      "\t Training loss (single batch): 1.0040574073791504\n",
      "\t Training loss (single batch): 1.1194478273391724\n",
      "\t Training loss (single batch): 1.220314383506775\n",
      "\t Training loss (single batch): 1.7904270887374878\n",
      "\t Training loss (single batch): 1.784440040588379\n",
      "\t Training loss (single batch): 1.5465482473373413\n",
      "\t Training loss (single batch): 0.629223644733429\n",
      "\t Training loss (single batch): 0.7699782252311707\n",
      "\t Training loss (single batch): 1.3432692289352417\n",
      "\t Training loss (single batch): 1.1947026252746582\n",
      "\t Training loss (single batch): 1.3747738599777222\n",
      "\t Training loss (single batch): 1.0592825412750244\n",
      "\t Training loss (single batch): 1.073119044303894\n",
      "\t Training loss (single batch): 1.7060582637786865\n",
      "\t Training loss (single batch): 1.55987548828125\n",
      "\t Training loss (single batch): 1.6112018823623657\n",
      "\t Training loss (single batch): 1.1582119464874268\n",
      "\t Training loss (single batch): 1.1152275800704956\n",
      "\t Training loss (single batch): 0.898435652256012\n",
      "\t Training loss (single batch): 1.3574093580245972\n",
      "\t Training loss (single batch): 1.9283424615859985\n",
      "\t Training loss (single batch): 1.1233755350112915\n",
      "\t Training loss (single batch): 1.161359190940857\n",
      "\t Training loss (single batch): 1.58950936794281\n",
      "\t Training loss (single batch): 1.4770965576171875\n",
      "\t Training loss (single batch): 0.9251554608345032\n",
      "\t Training loss (single batch): 0.8188203573226929\n",
      "\t Training loss (single batch): 1.1418490409851074\n",
      "\t Training loss (single batch): 1.3446635007858276\n",
      "\t Training loss (single batch): 1.2797293663024902\n",
      "\t Training loss (single batch): 1.2098664045333862\n",
      "\t Training loss (single batch): 1.447572112083435\n",
      "\t Training loss (single batch): 1.256553053855896\n",
      "\t Training loss (single batch): 0.9578537344932556\n",
      "\t Training loss (single batch): 1.0889257192611694\n",
      "\t Training loss (single batch): 0.938652753829956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8435274362564087\n",
      "\t Training loss (single batch): 1.288902997970581\n",
      "\t Training loss (single batch): 1.347269892692566\n",
      "\t Training loss (single batch): 1.1900519132614136\n",
      "\t Training loss (single batch): 1.2260652780532837\n",
      "\t Training loss (single batch): 1.0623562335968018\n",
      "\t Training loss (single batch): 0.8877931237220764\n",
      "\t Training loss (single batch): 1.105487585067749\n",
      "\t Training loss (single batch): 1.0891392230987549\n",
      "\t Training loss (single batch): 1.0995210409164429\n",
      "\t Training loss (single batch): 0.9507799744606018\n",
      "\t Training loss (single batch): 1.2711513042449951\n",
      "\t Training loss (single batch): 1.0230857133865356\n",
      "\t Training loss (single batch): 0.5248050093650818\n",
      "\t Training loss (single batch): 1.2195305824279785\n",
      "\t Training loss (single batch): 1.0847760438919067\n",
      "\t Training loss (single batch): 1.1296800374984741\n",
      "\t Training loss (single batch): 1.096779465675354\n",
      "\t Training loss (single batch): 1.5030124187469482\n",
      "\t Training loss (single batch): 1.6580513715744019\n",
      "\t Training loss (single batch): 1.3385446071624756\n",
      "\t Training loss (single batch): 0.8866388201713562\n",
      "\t Training loss (single batch): 1.0976277589797974\n",
      "\t Training loss (single batch): 0.7895138263702393\n",
      "\t Training loss (single batch): 1.2373604774475098\n",
      "\t Training loss (single batch): 1.3674827814102173\n",
      "\t Training loss (single batch): 1.354642629623413\n",
      "\t Training loss (single batch): 1.2454288005828857\n",
      "\t Training loss (single batch): 1.3843178749084473\n",
      "\t Training loss (single batch): 1.941102385520935\n",
      "\t Training loss (single batch): 0.8418315649032593\n",
      "\t Training loss (single batch): 1.2612724304199219\n",
      "\t Training loss (single batch): 1.1565364599227905\n",
      "\t Training loss (single batch): 1.0278053283691406\n",
      "\t Training loss (single batch): 1.5428078174591064\n",
      "\t Training loss (single batch): 1.3044633865356445\n",
      "\t Training loss (single batch): 0.793963611125946\n",
      "\t Training loss (single batch): 1.2505303621292114\n",
      "\t Training loss (single batch): 1.3453296422958374\n",
      "\t Training loss (single batch): 1.0892091989517212\n",
      "\t Training loss (single batch): 1.1187412738800049\n",
      "\t Training loss (single batch): 1.3362410068511963\n",
      "\t Training loss (single batch): 1.210134506225586\n",
      "\t Training loss (single batch): 1.0095806121826172\n",
      "\t Training loss (single batch): 0.9334859251976013\n",
      "\t Training loss (single batch): 0.8944522142410278\n",
      "\t Training loss (single batch): 1.0978140830993652\n",
      "\t Training loss (single batch): 1.5755788087844849\n",
      "\t Training loss (single batch): 1.3451852798461914\n",
      "\t Training loss (single batch): 0.8105728030204773\n",
      "\t Training loss (single batch): 1.108996033668518\n",
      "\t Training loss (single batch): 1.4473549127578735\n",
      "\t Training loss (single batch): 1.2963221073150635\n",
      "\t Training loss (single batch): 1.1568933725357056\n",
      "\t Training loss (single batch): 1.2276456356048584\n",
      "\t Training loss (single batch): 1.095119833946228\n",
      "\t Training loss (single batch): 0.9459256529808044\n",
      "\t Training loss (single batch): 1.0085865259170532\n",
      "\t Training loss (single batch): 0.923297107219696\n",
      "\t Training loss (single batch): 1.2172859907150269\n",
      "\t Training loss (single batch): 1.1129999160766602\n",
      "\t Training loss (single batch): 1.0065888166427612\n",
      "\t Training loss (single batch): 1.3630660772323608\n",
      "\t Training loss (single batch): 1.0027662515640259\n",
      "\t Training loss (single batch): 0.8397841453552246\n",
      "\t Training loss (single batch): 1.0235505104064941\n",
      "\t Training loss (single batch): 1.5970491170883179\n",
      "\t Training loss (single batch): 1.2817153930664062\n",
      "\t Training loss (single batch): 1.5391098260879517\n",
      "\t Training loss (single batch): 1.130581021308899\n",
      "\t Training loss (single batch): 1.2933216094970703\n",
      "\t Training loss (single batch): 1.3535691499710083\n",
      "\t Training loss (single batch): 0.8141698837280273\n",
      "\t Training loss (single batch): 1.0507725477218628\n",
      "\t Training loss (single batch): 0.9509415626525879\n",
      "\t Training loss (single batch): 0.9611077308654785\n",
      "\t Training loss (single batch): 1.2441288232803345\n",
      "\t Training loss (single batch): 0.8681376576423645\n",
      "\t Training loss (single batch): 1.6961978673934937\n",
      "\t Training loss (single batch): 0.7957661747932434\n",
      "\t Training loss (single batch): 1.737687587738037\n",
      "\t Training loss (single batch): 1.315287470817566\n",
      "\t Training loss (single batch): 0.9104896187782288\n",
      "\t Training loss (single batch): 1.074719786643982\n",
      "\t Training loss (single batch): 1.9180647134780884\n",
      "\t Training loss (single batch): 1.0705370903015137\n",
      "\t Training loss (single batch): 1.6343916654586792\n",
      "\t Training loss (single batch): 1.4201874732971191\n",
      "\t Training loss (single batch): 1.3958888053894043\n",
      "\t Training loss (single batch): 1.3572161197662354\n",
      "\t Training loss (single batch): 1.3468905687332153\n",
      "\t Training loss (single batch): 0.970099687576294\n",
      "\t Training loss (single batch): 1.0065317153930664\n",
      "\t Training loss (single batch): 0.9456220865249634\n",
      "\t Training loss (single batch): 0.7566676735877991\n",
      "\t Training loss (single batch): 0.9860748052597046\n",
      "\t Training loss (single batch): 0.9900475144386292\n",
      "\t Training loss (single batch): 1.4871835708618164\n",
      "\t Training loss (single batch): 0.9248678684234619\n",
      "\t Training loss (single batch): 1.3352919816970825\n",
      "\t Training loss (single batch): 0.9612192511558533\n",
      "\t Training loss (single batch): 1.2613977193832397\n",
      "\t Training loss (single batch): 0.7898085117340088\n",
      "\t Training loss (single batch): 1.3121707439422607\n",
      "\t Training loss (single batch): 1.1315193176269531\n",
      "\t Training loss (single batch): 1.5916537046432495\n",
      "\t Training loss (single batch): 0.8790661096572876\n",
      "\t Training loss (single batch): 1.4797019958496094\n",
      "\t Training loss (single batch): 1.134599208831787\n",
      "\t Training loss (single batch): 0.8664288520812988\n",
      "\t Training loss (single batch): 0.9280346035957336\n",
      "\t Training loss (single batch): 0.8877636194229126\n",
      "\t Training loss (single batch): 1.4276829957962036\n",
      "\t Training loss (single batch): 1.2023342847824097\n",
      "\t Training loss (single batch): 1.110426664352417\n",
      "\t Training loss (single batch): 1.5455719232559204\n",
      "\t Training loss (single batch): 1.2293674945831299\n",
      "\t Training loss (single batch): 1.0042550563812256\n",
      "\t Training loss (single batch): 1.364190697669983\n",
      "\t Training loss (single batch): 1.520500898361206\n",
      "\t Training loss (single batch): 0.9978716373443604\n",
      "\t Training loss (single batch): 0.898243248462677\n",
      "\t Training loss (single batch): 1.3369759321212769\n",
      "\t Training loss (single batch): 1.5041078329086304\n",
      "\t Training loss (single batch): 1.033900499343872\n",
      "\t Training loss (single batch): 0.8454022407531738\n",
      "\t Training loss (single batch): 1.0394409894943237\n",
      "\t Training loss (single batch): 1.0842491388320923\n",
      "\t Training loss (single batch): 1.104361891746521\n",
      "\t Training loss (single batch): 1.201810359954834\n",
      "\t Training loss (single batch): 1.036018967628479\n",
      "\t Training loss (single batch): 1.1295030117034912\n",
      "\t Training loss (single batch): 1.4721992015838623\n",
      "\t Training loss (single batch): 1.3089429140090942\n",
      "\t Training loss (single batch): 1.5068953037261963\n",
      "\t Training loss (single batch): 1.2464359998703003\n",
      "\t Training loss (single batch): 1.1320104598999023\n",
      "\t Training loss (single batch): 0.9419011473655701\n",
      "\t Training loss (single batch): 1.4914864301681519\n",
      "\t Training loss (single batch): 1.8193912506103516\n",
      "\t Training loss (single batch): 1.085474967956543\n",
      "\t Training loss (single batch): 1.0097603797912598\n",
      "\t Training loss (single batch): 1.2913700342178345\n",
      "\t Training loss (single batch): 1.2508901357650757\n",
      "\t Training loss (single batch): 1.467387318611145\n",
      "\t Training loss (single batch): 1.0691869258880615\n",
      "\t Training loss (single batch): 0.7444007992744446\n",
      "\t Training loss (single batch): 1.4780590534210205\n",
      "\t Training loss (single batch): 1.1959213018417358\n",
      "\t Training loss (single batch): 1.2559645175933838\n",
      "\t Training loss (single batch): 1.3313006162643433\n",
      "\t Training loss (single batch): 1.203046441078186\n",
      "\t Training loss (single batch): 1.178516149520874\n",
      "\t Training loss (single batch): 0.8814639449119568\n",
      "\t Training loss (single batch): 1.0512009859085083\n",
      "\t Training loss (single batch): 1.1092287302017212\n",
      "\t Training loss (single batch): 1.1994190216064453\n",
      "\t Training loss (single batch): 1.191954255104065\n",
      "\t Training loss (single batch): 0.8319413661956787\n",
      "\t Training loss (single batch): 1.1168469190597534\n",
      "\t Training loss (single batch): 0.9938578009605408\n",
      "\t Training loss (single batch): 1.3609886169433594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.7255856394767761\n",
      "\t Training loss (single batch): 1.7317698001861572\n",
      "\t Training loss (single batch): 1.2243455648422241\n",
      "\t Training loss (single batch): 1.3507091999053955\n",
      "\t Training loss (single batch): 2.0575761795043945\n",
      "\t Training loss (single batch): 1.0224345922470093\n",
      "\t Training loss (single batch): 0.9017421007156372\n",
      "\t Training loss (single batch): 1.150071144104004\n",
      "\t Training loss (single batch): 1.3091036081314087\n",
      "\t Training loss (single batch): 1.2935326099395752\n",
      "\t Training loss (single batch): 0.9537305235862732\n",
      "\t Training loss (single batch): 0.7308740019798279\n",
      "\t Training loss (single batch): 1.0550637245178223\n",
      "\t Training loss (single batch): 0.7733165621757507\n",
      "\t Training loss (single batch): 0.9819716811180115\n",
      "\t Training loss (single batch): 1.2628816366195679\n",
      "\t Training loss (single batch): 1.2335433959960938\n",
      "\t Training loss (single batch): 0.73980313539505\n",
      "\t Training loss (single batch): 0.7577775716781616\n",
      "\t Training loss (single batch): 0.8290919661521912\n",
      "\t Training loss (single batch): 0.8407919406890869\n",
      "\t Training loss (single batch): 0.8475304245948792\n",
      "\t Training loss (single batch): 1.3986766338348389\n",
      "\t Training loss (single batch): 1.1132487058639526\n",
      "\t Training loss (single batch): 1.0585055351257324\n",
      "\t Training loss (single batch): 1.4717917442321777\n",
      "\t Training loss (single batch): 1.630614995956421\n",
      "\t Training loss (single batch): 1.6135480403900146\n",
      "\t Training loss (single batch): 1.4929925203323364\n",
      "\t Training loss (single batch): 0.9608132243156433\n",
      "\t Training loss (single batch): 1.3769522905349731\n",
      "\t Training loss (single batch): 1.1084694862365723\n",
      "\t Training loss (single batch): 0.9227780699729919\n",
      "\t Training loss (single batch): 1.3008842468261719\n",
      "\t Training loss (single batch): 1.4439880847930908\n",
      "\t Training loss (single batch): 1.2999767065048218\n",
      "\t Training loss (single batch): 1.6136394739151\n",
      "\t Training loss (single batch): 1.0847688913345337\n",
      "\t Training loss (single batch): 1.0113393068313599\n",
      "\t Training loss (single batch): 1.3485654592514038\n",
      "\t Training loss (single batch): 1.2464935779571533\n",
      "\t Training loss (single batch): 1.2622147798538208\n",
      "\t Training loss (single batch): 0.9987029433250427\n",
      "\t Training loss (single batch): 1.171537160873413\n",
      "\t Training loss (single batch): 1.278870940208435\n",
      "\t Training loss (single batch): 1.3365626335144043\n",
      "\t Training loss (single batch): 0.5931008458137512\n",
      "##################################\n",
      "## EPOCH 73\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7794152498245239\n",
      "\t Training loss (single batch): 1.3569029569625854\n",
      "\t Training loss (single batch): 1.2305991649627686\n",
      "\t Training loss (single batch): 1.0614330768585205\n",
      "\t Training loss (single batch): 1.39472234249115\n",
      "\t Training loss (single batch): 1.009043574333191\n",
      "\t Training loss (single batch): 1.2434431314468384\n",
      "\t Training loss (single batch): 1.1909571886062622\n",
      "\t Training loss (single batch): 0.9889441132545471\n",
      "\t Training loss (single batch): 1.1075316667556763\n",
      "\t Training loss (single batch): 1.2296231985092163\n",
      "\t Training loss (single batch): 1.1463819742202759\n",
      "\t Training loss (single batch): 1.2167891263961792\n",
      "\t Training loss (single batch): 1.7959965467453003\n",
      "\t Training loss (single batch): 1.3049284219741821\n",
      "\t Training loss (single batch): 1.6019718647003174\n",
      "\t Training loss (single batch): 1.5356470346450806\n",
      "\t Training loss (single batch): 1.0532751083374023\n",
      "\t Training loss (single batch): 0.9771661162376404\n",
      "\t Training loss (single batch): 1.0920997858047485\n",
      "\t Training loss (single batch): 1.4655406475067139\n",
      "\t Training loss (single batch): 1.439755916595459\n",
      "\t Training loss (single batch): 0.9997672438621521\n",
      "\t Training loss (single batch): 1.4800610542297363\n",
      "\t Training loss (single batch): 1.292720913887024\n",
      "\t Training loss (single batch): 1.1946078538894653\n",
      "\t Training loss (single batch): 1.9106111526489258\n",
      "\t Training loss (single batch): 1.119614601135254\n",
      "\t Training loss (single batch): 1.4715332984924316\n",
      "\t Training loss (single batch): 1.3913542032241821\n",
      "\t Training loss (single batch): 1.2323163747787476\n",
      "\t Training loss (single batch): 0.9826459288597107\n",
      "\t Training loss (single batch): 0.8608452677726746\n",
      "\t Training loss (single batch): 1.3919878005981445\n",
      "\t Training loss (single batch): 1.2032488584518433\n",
      "\t Training loss (single batch): 1.4695709943771362\n",
      "\t Training loss (single batch): 0.935016930103302\n",
      "\t Training loss (single batch): 1.3424261808395386\n",
      "\t Training loss (single batch): 1.336291790008545\n",
      "\t Training loss (single batch): 1.3199677467346191\n",
      "\t Training loss (single batch): 1.0093475580215454\n",
      "\t Training loss (single batch): 1.1387741565704346\n",
      "\t Training loss (single batch): 0.7775518894195557\n",
      "\t Training loss (single batch): 0.922938346862793\n",
      "\t Training loss (single batch): 1.3316015005111694\n",
      "\t Training loss (single batch): 1.0125086307525635\n",
      "\t Training loss (single batch): 1.3523863554000854\n",
      "\t Training loss (single batch): 1.0092412233352661\n",
      "\t Training loss (single batch): 0.9249166250228882\n",
      "\t Training loss (single batch): 0.8473177552223206\n",
      "\t Training loss (single batch): 1.707706093788147\n",
      "\t Training loss (single batch): 1.1859159469604492\n",
      "\t Training loss (single batch): 1.2122834920883179\n",
      "\t Training loss (single batch): 1.3519978523254395\n",
      "\t Training loss (single batch): 1.3624851703643799\n",
      "\t Training loss (single batch): 1.108284592628479\n",
      "\t Training loss (single batch): 1.421638011932373\n",
      "\t Training loss (single batch): 1.0692685842514038\n",
      "\t Training loss (single batch): 1.0734645128250122\n",
      "\t Training loss (single batch): 1.30288827419281\n",
      "\t Training loss (single batch): 1.0711486339569092\n",
      "\t Training loss (single batch): 1.0707072019577026\n",
      "\t Training loss (single batch): 1.1892362833023071\n",
      "\t Training loss (single batch): 0.952455997467041\n",
      "\t Training loss (single batch): 1.1881356239318848\n",
      "\t Training loss (single batch): 0.9931738376617432\n",
      "\t Training loss (single batch): 0.8155285120010376\n",
      "\t Training loss (single batch): 0.6764814853668213\n",
      "\t Training loss (single batch): 1.0746837854385376\n",
      "\t Training loss (single batch): 0.7203865647315979\n",
      "\t Training loss (single batch): 0.9007442593574524\n",
      "\t Training loss (single batch): 1.0966317653656006\n",
      "\t Training loss (single batch): 1.2204573154449463\n",
      "\t Training loss (single batch): 1.3696246147155762\n",
      "\t Training loss (single batch): 1.5693938732147217\n",
      "\t Training loss (single batch): 0.8432312607765198\n",
      "\t Training loss (single batch): 1.1788090467453003\n",
      "\t Training loss (single batch): 1.098529577255249\n",
      "\t Training loss (single batch): 1.3281112909317017\n",
      "\t Training loss (single batch): 0.9278600811958313\n",
      "\t Training loss (single batch): 0.925474226474762\n",
      "\t Training loss (single batch): 1.20998215675354\n",
      "\t Training loss (single batch): 1.0218791961669922\n",
      "\t Training loss (single batch): 1.1109758615493774\n",
      "\t Training loss (single batch): 0.8601269721984863\n",
      "\t Training loss (single batch): 1.7343809604644775\n",
      "\t Training loss (single batch): 1.263509750366211\n",
      "\t Training loss (single batch): 1.237971305847168\n",
      "\t Training loss (single batch): 1.1005613803863525\n",
      "\t Training loss (single batch): 1.277146577835083\n",
      "\t Training loss (single batch): 1.0394811630249023\n",
      "\t Training loss (single batch): 1.4203670024871826\n",
      "\t Training loss (single batch): 1.2922793626785278\n",
      "\t Training loss (single batch): 1.1600348949432373\n",
      "\t Training loss (single batch): 1.1947218179702759\n",
      "\t Training loss (single batch): 1.0849838256835938\n",
      "\t Training loss (single batch): 1.0637931823730469\n",
      "\t Training loss (single batch): 1.1725051403045654\n",
      "\t Training loss (single batch): 0.985134482383728\n",
      "\t Training loss (single batch): 1.0685994625091553\n",
      "\t Training loss (single batch): 1.1630682945251465\n",
      "\t Training loss (single batch): 1.5088996887207031\n",
      "\t Training loss (single batch): 1.097265601158142\n",
      "\t Training loss (single batch): 0.8562971949577332\n",
      "\t Training loss (single batch): 1.038196086883545\n",
      "\t Training loss (single batch): 1.1764692068099976\n",
      "\t Training loss (single batch): 0.9968657493591309\n",
      "\t Training loss (single batch): 1.1132440567016602\n",
      "\t Training loss (single batch): 1.2524166107177734\n",
      "\t Training loss (single batch): 1.1363704204559326\n",
      "\t Training loss (single batch): 0.5483326315879822\n",
      "\t Training loss (single batch): 1.0619136095046997\n",
      "\t Training loss (single batch): 1.1449774503707886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.412895917892456\n",
      "\t Training loss (single batch): 1.2899190187454224\n",
      "\t Training loss (single batch): 1.0688477754592896\n",
      "\t Training loss (single batch): 1.6993508338928223\n",
      "\t Training loss (single batch): 1.3360955715179443\n",
      "\t Training loss (single batch): 1.224437952041626\n",
      "\t Training loss (single batch): 1.1668752431869507\n",
      "\t Training loss (single batch): 1.3292865753173828\n",
      "\t Training loss (single batch): 0.690422773361206\n",
      "\t Training loss (single batch): 0.7548569440841675\n",
      "\t Training loss (single batch): 1.2397032976150513\n",
      "\t Training loss (single batch): 1.1928480863571167\n",
      "\t Training loss (single batch): 1.3173177242279053\n",
      "\t Training loss (single batch): 1.304818868637085\n",
      "\t Training loss (single batch): 1.0116139650344849\n",
      "\t Training loss (single batch): 1.5833837985992432\n",
      "\t Training loss (single batch): 0.91581791639328\n",
      "\t Training loss (single batch): 0.9922786355018616\n",
      "\t Training loss (single batch): 1.0561566352844238\n",
      "\t Training loss (single batch): 1.1630500555038452\n",
      "\t Training loss (single batch): 1.2592576742172241\n",
      "\t Training loss (single batch): 1.2956608533859253\n",
      "\t Training loss (single batch): 0.9270359873771667\n",
      "\t Training loss (single batch): 0.6301366686820984\n",
      "\t Training loss (single batch): 0.8520303964614868\n",
      "\t Training loss (single batch): 0.9544646739959717\n",
      "\t Training loss (single batch): 1.0755280256271362\n",
      "\t Training loss (single batch): 1.0762869119644165\n",
      "\t Training loss (single batch): 1.0537395477294922\n",
      "\t Training loss (single batch): 1.3713189363479614\n",
      "\t Training loss (single batch): 1.7331801652908325\n",
      "\t Training loss (single batch): 1.019453525543213\n",
      "\t Training loss (single batch): 1.324479579925537\n",
      "\t Training loss (single batch): 1.730657935142517\n",
      "\t Training loss (single batch): 0.892381489276886\n",
      "\t Training loss (single batch): 1.4244545698165894\n",
      "\t Training loss (single batch): 1.2631454467773438\n",
      "\t Training loss (single batch): 1.0643279552459717\n",
      "\t Training loss (single batch): 1.2673407793045044\n",
      "\t Training loss (single batch): 0.9803354144096375\n",
      "\t Training loss (single batch): 1.7720930576324463\n",
      "\t Training loss (single batch): 0.967536211013794\n",
      "\t Training loss (single batch): 1.0031766891479492\n",
      "\t Training loss (single batch): 0.7987270355224609\n",
      "\t Training loss (single batch): 1.1183570623397827\n",
      "\t Training loss (single batch): 1.1619480848312378\n",
      "\t Training loss (single batch): 0.9832763671875\n",
      "\t Training loss (single batch): 0.9610652923583984\n",
      "\t Training loss (single batch): 1.3243993520736694\n",
      "\t Training loss (single batch): 1.1765443086624146\n",
      "\t Training loss (single batch): 1.039159893989563\n",
      "\t Training loss (single batch): 1.5336735248565674\n",
      "\t Training loss (single batch): 1.0498542785644531\n",
      "\t Training loss (single batch): 1.2425864934921265\n",
      "\t Training loss (single batch): 1.2086310386657715\n",
      "\t Training loss (single batch): 1.621433138847351\n",
      "\t Training loss (single batch): 1.0551509857177734\n",
      "\t Training loss (single batch): 1.0553767681121826\n",
      "\t Training loss (single batch): 1.2300448417663574\n",
      "\t Training loss (single batch): 1.1394721269607544\n",
      "\t Training loss (single batch): 1.141163945198059\n",
      "\t Training loss (single batch): 1.6529083251953125\n",
      "\t Training loss (single batch): 1.5377603769302368\n",
      "\t Training loss (single batch): 1.4177403450012207\n",
      "\t Training loss (single batch): 1.02775239944458\n",
      "\t Training loss (single batch): 1.267974853515625\n",
      "\t Training loss (single batch): 1.1246364116668701\n",
      "\t Training loss (single batch): 1.6304931640625\n",
      "\t Training loss (single batch): 1.3799428939819336\n",
      "\t Training loss (single batch): 1.688381552696228\n",
      "\t Training loss (single batch): 1.1599146127700806\n",
      "\t Training loss (single batch): 1.3469411134719849\n",
      "\t Training loss (single batch): 1.1095516681671143\n",
      "\t Training loss (single batch): 0.9258486032485962\n",
      "\t Training loss (single batch): 1.5073645114898682\n",
      "\t Training loss (single batch): 0.9292352199554443\n",
      "\t Training loss (single batch): 0.9188411831855774\n",
      "\t Training loss (single batch): 1.122117280960083\n",
      "\t Training loss (single batch): 0.9596642255783081\n",
      "\t Training loss (single batch): 1.1334033012390137\n",
      "\t Training loss (single batch): 1.5288363695144653\n",
      "\t Training loss (single batch): 1.6154887676239014\n",
      "\t Training loss (single batch): 1.0485975742340088\n",
      "\t Training loss (single batch): 1.2257837057113647\n",
      "\t Training loss (single batch): 1.191250205039978\n",
      "\t Training loss (single batch): 0.9877275228500366\n",
      "\t Training loss (single batch): 1.1217961311340332\n",
      "\t Training loss (single batch): 1.2589728832244873\n",
      "\t Training loss (single batch): 1.2174521684646606\n",
      "\t Training loss (single batch): 1.3102108240127563\n",
      "\t Training loss (single batch): 1.5045056343078613\n",
      "\t Training loss (single batch): 1.3220770359039307\n",
      "\t Training loss (single batch): 1.17460036277771\n",
      "\t Training loss (single batch): 0.9696098566055298\n",
      "\t Training loss (single batch): 1.1865290403366089\n",
      "\t Training loss (single batch): 1.2392895221710205\n",
      "\t Training loss (single batch): 1.126738429069519\n",
      "\t Training loss (single batch): 1.1984394788742065\n",
      "\t Training loss (single batch): 1.0957046747207642\n",
      "\t Training loss (single batch): 0.9434218406677246\n",
      "\t Training loss (single batch): 0.9753904342651367\n",
      "\t Training loss (single batch): 1.0203289985656738\n",
      "\t Training loss (single batch): 1.468928337097168\n",
      "\t Training loss (single batch): 1.0971726179122925\n",
      "\t Training loss (single batch): 1.2131402492523193\n",
      "\t Training loss (single batch): 1.0844324827194214\n",
      "\t Training loss (single batch): 1.3719559907913208\n",
      "\t Training loss (single batch): 1.0308940410614014\n",
      "\t Training loss (single batch): 0.9414231777191162\n",
      "\t Training loss (single batch): 0.8959473967552185\n",
      "\t Training loss (single batch): 1.28500235080719\n",
      "\t Training loss (single batch): 0.8490003943443298\n",
      "\t Training loss (single batch): 1.269869089126587\n",
      "\t Training loss (single batch): 1.219597339630127\n",
      "\t Training loss (single batch): 1.2315181493759155\n",
      "\t Training loss (single batch): 0.9693005681037903\n",
      "\t Training loss (single batch): 0.9969257712364197\n",
      "\t Training loss (single batch): 1.7286288738250732\n",
      "\t Training loss (single batch): 1.247109293937683\n",
      "\t Training loss (single batch): 1.0001535415649414\n",
      "\t Training loss (single batch): 1.742970585823059\n",
      "\t Training loss (single batch): 1.0059194564819336\n",
      "\t Training loss (single batch): 1.1412687301635742\n",
      "\t Training loss (single batch): 1.5900923013687134\n",
      "\t Training loss (single batch): 1.287750005722046\n",
      "\t Training loss (single batch): 0.9497212767601013\n",
      "\t Training loss (single batch): 1.8184866905212402\n",
      "\t Training loss (single batch): 1.4771698713302612\n",
      "\t Training loss (single batch): 1.336960792541504\n",
      "\t Training loss (single batch): 0.9400829076766968\n",
      "\t Training loss (single batch): 1.6979451179504395\n",
      "\t Training loss (single batch): 1.2338876724243164\n",
      "\t Training loss (single batch): 1.3604110479354858\n",
      "\t Training loss (single batch): 1.151221752166748\n",
      "\t Training loss (single batch): 1.3247525691986084\n",
      "\t Training loss (single batch): 1.3533124923706055\n",
      "\t Training loss (single batch): 1.2288389205932617\n",
      "\t Training loss (single batch): 1.3003032207489014\n",
      "\t Training loss (single batch): 1.2417452335357666\n",
      "\t Training loss (single batch): 1.014267086982727\n",
      "\t Training loss (single batch): 0.9527722001075745\n",
      "\t Training loss (single batch): 0.9072592258453369\n",
      "\t Training loss (single batch): 1.379775881767273\n",
      "\t Training loss (single batch): 1.7360749244689941\n",
      "\t Training loss (single batch): 1.097906470298767\n",
      "\t Training loss (single batch): 1.190266489982605\n",
      "\t Training loss (single batch): 1.377648115158081\n",
      "\t Training loss (single batch): 0.8943789601325989\n",
      "\t Training loss (single batch): 0.8043439388275146\n",
      "\t Training loss (single batch): 1.1354987621307373\n",
      "\t Training loss (single batch): 1.2731326818466187\n",
      "\t Training loss (single batch): 0.8626810908317566\n",
      "\t Training loss (single batch): 0.8173570036888123\n",
      "\t Training loss (single batch): 1.075090765953064\n",
      "\t Training loss (single batch): 2.196613311767578\n",
      "\t Training loss (single batch): 0.7743176817893982\n",
      "\t Training loss (single batch): 0.7325018644332886\n",
      "\t Training loss (single batch): 1.210412621498108\n",
      "\t Training loss (single batch): 1.570253849029541\n",
      "\t Training loss (single batch): 1.592897653579712\n",
      "\t Training loss (single batch): 1.1751676797866821\n",
      "\t Training loss (single batch): 1.1673336029052734\n",
      "\t Training loss (single batch): 0.9629178643226624\n",
      "\t Training loss (single batch): 1.0310618877410889\n",
      "\t Training loss (single batch): 0.9594478011131287\n",
      "\t Training loss (single batch): 1.505924105644226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.313515067100525\n",
      "\t Training loss (single batch): 1.344804048538208\n",
      "\t Training loss (single batch): 1.3884395360946655\n",
      "\t Training loss (single batch): 0.7880038022994995\n",
      "\t Training loss (single batch): 1.000389814376831\n",
      "\t Training loss (single batch): 1.2742611169815063\n",
      "\t Training loss (single batch): 1.0277535915374756\n",
      "\t Training loss (single batch): 1.0897483825683594\n",
      "\t Training loss (single batch): 1.1936681270599365\n",
      "\t Training loss (single batch): 1.4846099615097046\n",
      "\t Training loss (single batch): 1.7002856731414795\n",
      "\t Training loss (single batch): 0.9196946620941162\n",
      "\t Training loss (single batch): 1.4302349090576172\n",
      "\t Training loss (single batch): 0.7860147356987\n",
      "\t Training loss (single batch): 1.3841071128845215\n",
      "\t Training loss (single batch): 1.2675025463104248\n",
      "\t Training loss (single batch): 1.2700886726379395\n",
      "\t Training loss (single batch): 1.3450112342834473\n",
      "\t Training loss (single batch): 1.1596095561981201\n",
      "\t Training loss (single batch): 1.027026653289795\n",
      "\t Training loss (single batch): 1.2840218544006348\n",
      "\t Training loss (single batch): 1.6061099767684937\n",
      "\t Training loss (single batch): 1.1793246269226074\n",
      "\t Training loss (single batch): 0.7126212120056152\n",
      "\t Training loss (single batch): 1.3597369194030762\n",
      "\t Training loss (single batch): 1.2666749954223633\n",
      "\t Training loss (single batch): 1.3840755224227905\n",
      "\t Training loss (single batch): 1.8197214603424072\n",
      "\t Training loss (single batch): 0.8170396089553833\n",
      "\t Training loss (single batch): 1.5678507089614868\n",
      "\t Training loss (single batch): 1.1455833911895752\n",
      "\t Training loss (single batch): 1.2827814817428589\n",
      "\t Training loss (single batch): 1.2082217931747437\n",
      "\t Training loss (single batch): 1.2724612951278687\n",
      "\t Training loss (single batch): 1.1331430673599243\n",
      "\t Training loss (single batch): 1.411726713180542\n",
      "\t Training loss (single batch): 1.23723566532135\n",
      "\t Training loss (single batch): 1.323410153388977\n",
      "##################################\n",
      "## EPOCH 74\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0319327116012573\n",
      "\t Training loss (single batch): 1.3395720720291138\n",
      "\t Training loss (single batch): 1.1333495378494263\n",
      "\t Training loss (single batch): 1.4809856414794922\n",
      "\t Training loss (single batch): 1.2710192203521729\n",
      "\t Training loss (single batch): 1.37599778175354\n",
      "\t Training loss (single batch): 1.061608910560608\n",
      "\t Training loss (single batch): 1.5376280546188354\n",
      "\t Training loss (single batch): 1.5306047201156616\n",
      "\t Training loss (single batch): 1.580622911453247\n",
      "\t Training loss (single batch): 1.5910639762878418\n",
      "\t Training loss (single batch): 1.186594009399414\n",
      "\t Training loss (single batch): 1.1529834270477295\n",
      "\t Training loss (single batch): 1.1707717180252075\n",
      "\t Training loss (single batch): 1.2433570623397827\n",
      "\t Training loss (single batch): 1.0815465450286865\n",
      "\t Training loss (single batch): 1.24496328830719\n",
      "\t Training loss (single batch): 0.8237594366073608\n",
      "\t Training loss (single batch): 0.7375278472900391\n",
      "\t Training loss (single batch): 1.002150058746338\n",
      "\t Training loss (single batch): 1.3813514709472656\n",
      "\t Training loss (single batch): 1.2838196754455566\n",
      "\t Training loss (single batch): 1.3415805101394653\n",
      "\t Training loss (single batch): 1.657485008239746\n",
      "\t Training loss (single batch): 1.3524523973464966\n",
      "\t Training loss (single batch): 1.993855595588684\n",
      "\t Training loss (single batch): 0.7366181015968323\n",
      "\t Training loss (single batch): 1.34633207321167\n",
      "\t Training loss (single batch): 1.4290416240692139\n",
      "\t Training loss (single batch): 0.9221078157424927\n",
      "\t Training loss (single batch): 0.86328125\n",
      "\t Training loss (single batch): 0.954922616481781\n",
      "\t Training loss (single batch): 1.201537013053894\n",
      "\t Training loss (single batch): 1.3498648405075073\n",
      "\t Training loss (single batch): 1.1167035102844238\n",
      "\t Training loss (single batch): 1.222664713859558\n",
      "\t Training loss (single batch): 0.9379519820213318\n",
      "\t Training loss (single batch): 0.9041154384613037\n",
      "\t Training loss (single batch): 1.2177287340164185\n",
      "\t Training loss (single batch): 1.7159053087234497\n",
      "\t Training loss (single batch): 1.0761206150054932\n",
      "\t Training loss (single batch): 1.351345419883728\n",
      "\t Training loss (single batch): 1.5707286596298218\n",
      "\t Training loss (single batch): 0.8470043540000916\n",
      "\t Training loss (single batch): 0.8052979707717896\n",
      "\t Training loss (single batch): 1.0314359664916992\n",
      "\t Training loss (single batch): 1.4879627227783203\n",
      "\t Training loss (single batch): 1.288809061050415\n",
      "\t Training loss (single batch): 1.1800392866134644\n",
      "\t Training loss (single batch): 1.0656449794769287\n",
      "\t Training loss (single batch): 1.594835638999939\n",
      "\t Training loss (single batch): 0.8628917336463928\n",
      "\t Training loss (single batch): 0.8453525304794312\n",
      "\t Training loss (single batch): 1.1265966892242432\n",
      "\t Training loss (single batch): 0.9889096617698669\n",
      "\t Training loss (single batch): 1.7643117904663086\n",
      "\t Training loss (single batch): 1.5375386476516724\n",
      "\t Training loss (single batch): 1.0354925394058228\n",
      "\t Training loss (single batch): 1.1182119846343994\n",
      "\t Training loss (single batch): 1.0848054885864258\n",
      "\t Training loss (single batch): 1.5870178937911987\n",
      "\t Training loss (single batch): 1.2402677536010742\n",
      "\t Training loss (single batch): 0.9806700944900513\n",
      "\t Training loss (single batch): 1.060420036315918\n",
      "\t Training loss (single batch): 1.499748706817627\n",
      "\t Training loss (single batch): 1.313950538635254\n",
      "\t Training loss (single batch): 1.2975704669952393\n",
      "\t Training loss (single batch): 1.0347771644592285\n",
      "\t Training loss (single batch): 1.0668678283691406\n",
      "\t Training loss (single batch): 0.9472976326942444\n",
      "\t Training loss (single batch): 1.2025302648544312\n",
      "\t Training loss (single batch): 0.838591456413269\n",
      "\t Training loss (single batch): 1.3984421491622925\n",
      "\t Training loss (single batch): 1.0828630924224854\n",
      "\t Training loss (single batch): 0.8954975008964539\n",
      "\t Training loss (single batch): 1.2840609550476074\n",
      "\t Training loss (single batch): 1.174472689628601\n",
      "\t Training loss (single batch): 1.0555821657180786\n",
      "\t Training loss (single batch): 0.8002948760986328\n",
      "\t Training loss (single batch): 1.3939969539642334\n",
      "\t Training loss (single batch): 1.3199973106384277\n",
      "\t Training loss (single batch): 1.1169248819351196\n",
      "\t Training loss (single batch): 0.9730604887008667\n",
      "\t Training loss (single batch): 1.1025187969207764\n",
      "\t Training loss (single batch): 1.1480681896209717\n",
      "\t Training loss (single batch): 0.9568908214569092\n",
      "\t Training loss (single batch): 1.4737399816513062\n",
      "\t Training loss (single batch): 1.225140929222107\n",
      "\t Training loss (single batch): 1.1148180961608887\n",
      "\t Training loss (single batch): 1.0705854892730713\n",
      "\t Training loss (single batch): 1.059818148612976\n",
      "\t Training loss (single batch): 1.5020849704742432\n",
      "\t Training loss (single batch): 1.4064141511917114\n",
      "\t Training loss (single batch): 1.4820414781570435\n",
      "\t Training loss (single batch): 1.3756848573684692\n",
      "\t Training loss (single batch): 1.3944913148880005\n",
      "\t Training loss (single batch): 1.2272287607192993\n",
      "\t Training loss (single batch): 1.346743106842041\n",
      "\t Training loss (single batch): 1.4376028776168823\n",
      "\t Training loss (single batch): 1.153228759765625\n",
      "\t Training loss (single batch): 1.3553645610809326\n",
      "\t Training loss (single batch): 0.9165862798690796\n",
      "\t Training loss (single batch): 1.1222667694091797\n",
      "\t Training loss (single batch): 0.8200364708900452\n",
      "\t Training loss (single batch): 1.2142670154571533\n",
      "\t Training loss (single batch): 1.2896032333374023\n",
      "\t Training loss (single batch): 1.4095512628555298\n",
      "\t Training loss (single batch): 1.4533733129501343\n",
      "\t Training loss (single batch): 0.7879995107650757\n",
      "\t Training loss (single batch): 1.2702311277389526\n",
      "\t Training loss (single batch): 0.8472883701324463\n",
      "\t Training loss (single batch): 0.9633194804191589\n",
      "\t Training loss (single batch): 1.3253098726272583\n",
      "\t Training loss (single batch): 1.180111050605774\n",
      "\t Training loss (single batch): 1.3003287315368652\n",
      "\t Training loss (single batch): 1.156711459159851\n",
      "\t Training loss (single batch): 0.8233932852745056\n",
      "\t Training loss (single batch): 0.9020769000053406\n",
      "\t Training loss (single batch): 1.1157636642456055\n",
      "\t Training loss (single batch): 1.0264729261398315\n",
      "\t Training loss (single batch): 1.2465965747833252\n",
      "\t Training loss (single batch): 1.4210349321365356\n",
      "\t Training loss (single batch): 1.2363442182540894\n",
      "\t Training loss (single batch): 1.5854992866516113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2181813716888428\n",
      "\t Training loss (single batch): 1.2710968255996704\n",
      "\t Training loss (single batch): 0.9977808594703674\n",
      "\t Training loss (single batch): 0.7991262674331665\n",
      "\t Training loss (single batch): 1.3744754791259766\n",
      "\t Training loss (single batch): 1.5606886148452759\n",
      "\t Training loss (single batch): 1.2750071287155151\n",
      "\t Training loss (single batch): 0.8787500858306885\n",
      "\t Training loss (single batch): 0.9025845527648926\n",
      "\t Training loss (single batch): 1.165140151977539\n",
      "\t Training loss (single batch): 1.2487890720367432\n",
      "\t Training loss (single batch): 0.9886347055435181\n",
      "\t Training loss (single batch): 1.2680082321166992\n",
      "\t Training loss (single batch): 1.3880527019500732\n",
      "\t Training loss (single batch): 1.0352880954742432\n",
      "\t Training loss (single batch): 0.8846325874328613\n",
      "\t Training loss (single batch): 1.1467039585113525\n",
      "\t Training loss (single batch): 1.1208629608154297\n",
      "\t Training loss (single batch): 1.1080491542816162\n",
      "\t Training loss (single batch): 1.1684452295303345\n",
      "\t Training loss (single batch): 1.0580626726150513\n",
      "\t Training loss (single batch): 0.9071832299232483\n",
      "\t Training loss (single batch): 1.3291139602661133\n",
      "\t Training loss (single batch): 1.0185389518737793\n",
      "\t Training loss (single batch): 0.9642676711082458\n",
      "\t Training loss (single batch): 1.2336299419403076\n",
      "\t Training loss (single batch): 1.8202580213546753\n",
      "\t Training loss (single batch): 1.45371675491333\n",
      "\t Training loss (single batch): 1.0318546295166016\n",
      "\t Training loss (single batch): 1.122124433517456\n",
      "\t Training loss (single batch): 1.5659410953521729\n",
      "\t Training loss (single batch): 1.1213879585266113\n",
      "\t Training loss (single batch): 1.0232347249984741\n",
      "\t Training loss (single batch): 1.7202223539352417\n",
      "\t Training loss (single batch): 1.5844624042510986\n",
      "\t Training loss (single batch): 0.8703737258911133\n",
      "\t Training loss (single batch): 0.9823449850082397\n",
      "\t Training loss (single batch): 0.9571155905723572\n",
      "\t Training loss (single batch): 1.1090009212493896\n",
      "\t Training loss (single batch): 1.1108746528625488\n",
      "\t Training loss (single batch): 1.3196330070495605\n",
      "\t Training loss (single batch): 1.2398754358291626\n",
      "\t Training loss (single batch): 1.3815728425979614\n",
      "\t Training loss (single batch): 1.222563624382019\n",
      "\t Training loss (single batch): 1.2941442728042603\n",
      "\t Training loss (single batch): 0.7929680943489075\n",
      "\t Training loss (single batch): 1.2339482307434082\n",
      "\t Training loss (single batch): 1.4050618410110474\n",
      "\t Training loss (single batch): 1.2808319330215454\n",
      "\t Training loss (single batch): 1.1085411310195923\n",
      "\t Training loss (single batch): 1.4385980367660522\n",
      "\t Training loss (single batch): 1.223691463470459\n",
      "\t Training loss (single batch): 1.1660317182540894\n",
      "\t Training loss (single batch): 1.519921898841858\n",
      "\t Training loss (single batch): 1.0927428007125854\n",
      "\t Training loss (single batch): 1.3715776205062866\n",
      "\t Training loss (single batch): 1.034026026725769\n",
      "\t Training loss (single batch): 1.1669896841049194\n",
      "\t Training loss (single batch): 1.2279551029205322\n",
      "\t Training loss (single batch): 1.7070235013961792\n",
      "\t Training loss (single batch): 1.3550175428390503\n",
      "\t Training loss (single batch): 0.9190078377723694\n",
      "\t Training loss (single batch): 1.5226470232009888\n",
      "\t Training loss (single batch): 1.093031883239746\n",
      "\t Training loss (single batch): 1.1388212442398071\n",
      "\t Training loss (single batch): 1.2644517421722412\n",
      "\t Training loss (single batch): 1.0193617343902588\n",
      "\t Training loss (single batch): 1.4807510375976562\n",
      "\t Training loss (single batch): 1.4006483554840088\n",
      "\t Training loss (single batch): 1.3100749254226685\n",
      "\t Training loss (single batch): 1.1046240329742432\n",
      "\t Training loss (single batch): 1.145114541053772\n",
      "\t Training loss (single batch): 1.0938798189163208\n",
      "\t Training loss (single batch): 1.1914737224578857\n",
      "\t Training loss (single batch): 1.0165610313415527\n",
      "\t Training loss (single batch): 1.621029019355774\n",
      "\t Training loss (single batch): 1.0019766092300415\n",
      "\t Training loss (single batch): 1.0443320274353027\n",
      "\t Training loss (single batch): 1.1021181344985962\n",
      "\t Training loss (single batch): 1.0343948602676392\n",
      "\t Training loss (single batch): 0.8725788593292236\n",
      "\t Training loss (single batch): 1.1104649305343628\n",
      "\t Training loss (single batch): 1.0180680751800537\n",
      "\t Training loss (single batch): 0.8162286281585693\n",
      "\t Training loss (single batch): 1.1036198139190674\n",
      "\t Training loss (single batch): 1.143686294555664\n",
      "\t Training loss (single batch): 0.8931406140327454\n",
      "\t Training loss (single batch): 0.9416300654411316\n",
      "\t Training loss (single batch): 1.34208083152771\n",
      "\t Training loss (single batch): 1.283004641532898\n",
      "\t Training loss (single batch): 1.5497877597808838\n",
      "\t Training loss (single batch): 1.2226356267929077\n",
      "\t Training loss (single batch): 1.6581729650497437\n",
      "\t Training loss (single batch): 1.0470131635665894\n",
      "\t Training loss (single batch): 0.7876402139663696\n",
      "\t Training loss (single batch): 1.3656848669052124\n",
      "\t Training loss (single batch): 1.0279144048690796\n",
      "\t Training loss (single batch): 0.968876302242279\n",
      "\t Training loss (single batch): 1.0173457860946655\n",
      "\t Training loss (single batch): 1.3575975894927979\n",
      "\t Training loss (single batch): 1.1259536743164062\n",
      "\t Training loss (single batch): 0.8331998586654663\n",
      "\t Training loss (single batch): 0.911377489566803\n",
      "\t Training loss (single batch): 1.2002215385437012\n",
      "\t Training loss (single batch): 0.7813442349433899\n",
      "\t Training loss (single batch): 1.0537344217300415\n",
      "\t Training loss (single batch): 1.1169503927230835\n",
      "\t Training loss (single batch): 1.1583856344223022\n",
      "\t Training loss (single batch): 0.8546842932701111\n",
      "\t Training loss (single batch): 1.2098627090454102\n",
      "\t Training loss (single batch): 1.4404239654541016\n",
      "\t Training loss (single batch): 1.2609390020370483\n",
      "\t Training loss (single batch): 0.9972071647644043\n",
      "\t Training loss (single batch): 0.7758375406265259\n",
      "\t Training loss (single batch): 1.3344203233718872\n",
      "\t Training loss (single batch): 1.5477749109268188\n",
      "\t Training loss (single batch): 1.3191543817520142\n",
      "\t Training loss (single batch): 1.64823317527771\n",
      "\t Training loss (single batch): 0.8728793859481812\n",
      "\t Training loss (single batch): 0.7456943988800049\n",
      "\t Training loss (single batch): 1.8648178577423096\n",
      "\t Training loss (single batch): 1.0303221940994263\n",
      "\t Training loss (single batch): 1.094791293144226\n",
      "\t Training loss (single batch): 0.8288138508796692\n",
      "\t Training loss (single batch): 1.3901728391647339\n",
      "\t Training loss (single batch): 1.1244257688522339\n",
      "\t Training loss (single batch): 1.1284559965133667\n",
      "\t Training loss (single batch): 0.9880554676055908\n",
      "\t Training loss (single batch): 1.2605876922607422\n",
      "\t Training loss (single batch): 1.332247018814087\n",
      "\t Training loss (single batch): 1.182233452796936\n",
      "\t Training loss (single batch): 1.3293309211730957\n",
      "\t Training loss (single batch): 1.2305647134780884\n",
      "\t Training loss (single batch): 1.4975122213363647\n",
      "\t Training loss (single batch): 0.7382490038871765\n",
      "\t Training loss (single batch): 1.01752769947052\n",
      "\t Training loss (single batch): 1.2185548543930054\n",
      "\t Training loss (single batch): 1.2082585096359253\n",
      "\t Training loss (single batch): 0.7762212753295898\n",
      "\t Training loss (single batch): 0.8872490525245667\n",
      "\t Training loss (single batch): 1.143589735031128\n",
      "\t Training loss (single batch): 1.703035831451416\n",
      "\t Training loss (single batch): 1.1697752475738525\n",
      "\t Training loss (single batch): 2.1824049949645996\n",
      "\t Training loss (single batch): 1.8308218717575073\n",
      "\t Training loss (single batch): 1.2114839553833008\n",
      "\t Training loss (single batch): 1.1101219654083252\n",
      "\t Training loss (single batch): 0.9769585132598877\n",
      "\t Training loss (single batch): 1.1713647842407227\n",
      "\t Training loss (single batch): 0.7921872735023499\n",
      "\t Training loss (single batch): 1.180888056755066\n",
      "\t Training loss (single batch): 1.149542212486267\n",
      "\t Training loss (single batch): 1.2762370109558105\n",
      "\t Training loss (single batch): 1.08356511592865\n",
      "\t Training loss (single batch): 0.9625663161277771\n",
      "\t Training loss (single batch): 1.1470940113067627\n",
      "\t Training loss (single batch): 0.9289271235466003\n",
      "\t Training loss (single batch): 1.1390069723129272\n",
      "\t Training loss (single batch): 1.3798567056655884\n",
      "\t Training loss (single batch): 1.5420316457748413\n",
      "\t Training loss (single batch): 1.2754706144332886\n",
      "\t Training loss (single batch): 1.2488172054290771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1330386400222778\n",
      "\t Training loss (single batch): 1.2438040971755981\n",
      "\t Training loss (single batch): 1.234339714050293\n",
      "\t Training loss (single batch): 1.3263418674468994\n",
      "\t Training loss (single batch): 1.5514363050460815\n",
      "\t Training loss (single batch): 2.006157159805298\n",
      "\t Training loss (single batch): 0.9155408143997192\n",
      "\t Training loss (single batch): 1.3139986991882324\n",
      "\t Training loss (single batch): 1.5036218166351318\n",
      "\t Training loss (single batch): 1.0316325426101685\n",
      "\t Training loss (single batch): 0.9645199775695801\n",
      "\t Training loss (single batch): 1.3506165742874146\n",
      "\t Training loss (single batch): 1.2869194746017456\n",
      "\t Training loss (single batch): 1.0877920389175415\n",
      "\t Training loss (single batch): 1.0141143798828125\n",
      "\t Training loss (single batch): 0.9757744073867798\n",
      "\t Training loss (single batch): 1.2951546907424927\n",
      "\t Training loss (single batch): 1.024396538734436\n",
      "\t Training loss (single batch): 1.1617804765701294\n",
      "\t Training loss (single batch): 0.5757513046264648\n",
      "\t Training loss (single batch): 1.3178943395614624\n",
      "\t Training loss (single batch): 0.8904790282249451\n",
      "\t Training loss (single batch): 1.141405463218689\n",
      "\t Training loss (single batch): 1.4759021997451782\n",
      "\t Training loss (single batch): 1.2517799139022827\n",
      "\t Training loss (single batch): 0.9672786593437195\n",
      "\t Training loss (single batch): 1.5900144577026367\n",
      "\t Training loss (single batch): 1.1381185054779053\n",
      "\t Training loss (single batch): 1.3839819431304932\n",
      "\t Training loss (single batch): 1.5136113166809082\n",
      "\t Training loss (single batch): 0.6675929427146912\n",
      "##################################\n",
      "## EPOCH 75\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0410643815994263\n",
      "\t Training loss (single batch): 1.1414631605148315\n",
      "\t Training loss (single batch): 1.044547438621521\n",
      "\t Training loss (single batch): 1.2036336660385132\n",
      "\t Training loss (single batch): 1.2681267261505127\n",
      "\t Training loss (single batch): 1.1810659170150757\n",
      "\t Training loss (single batch): 1.0786676406860352\n",
      "\t Training loss (single batch): 0.983373761177063\n",
      "\t Training loss (single batch): 1.3369629383087158\n",
      "\t Training loss (single batch): 1.3706294298171997\n",
      "\t Training loss (single batch): 1.223267912864685\n",
      "\t Training loss (single batch): 0.8186613917350769\n",
      "\t Training loss (single batch): 0.9495788812637329\n",
      "\t Training loss (single batch): 1.3308838605880737\n",
      "\t Training loss (single batch): 1.284624457359314\n",
      "\t Training loss (single batch): 0.9765172600746155\n",
      "\t Training loss (single batch): 1.363768458366394\n",
      "\t Training loss (single batch): 1.5438841581344604\n",
      "\t Training loss (single batch): 0.7414131164550781\n",
      "\t Training loss (single batch): 1.3868565559387207\n",
      "\t Training loss (single batch): 1.4065659046173096\n",
      "\t Training loss (single batch): 1.4299577474594116\n",
      "\t Training loss (single batch): 1.1733753681182861\n",
      "\t Training loss (single batch): 1.1213057041168213\n",
      "\t Training loss (single batch): 0.908550500869751\n",
      "\t Training loss (single batch): 1.105629801750183\n",
      "\t Training loss (single batch): 1.0749255418777466\n",
      "\t Training loss (single batch): 1.77638840675354\n",
      "\t Training loss (single batch): 1.1471822261810303\n",
      "\t Training loss (single batch): 1.1710560321807861\n",
      "\t Training loss (single batch): 1.6584804058074951\n",
      "\t Training loss (single batch): 1.0568251609802246\n",
      "\t Training loss (single batch): 0.9730527400970459\n",
      "\t Training loss (single batch): 1.2752641439437866\n",
      "\t Training loss (single batch): 1.5664526224136353\n",
      "\t Training loss (single batch): 1.1908564567565918\n",
      "\t Training loss (single batch): 0.9839208126068115\n",
      "\t Training loss (single batch): 1.5356504917144775\n",
      "\t Training loss (single batch): 0.9680668711662292\n",
      "\t Training loss (single batch): 2.082873821258545\n",
      "\t Training loss (single batch): 1.7102839946746826\n",
      "\t Training loss (single batch): 1.0149686336517334\n",
      "\t Training loss (single batch): 1.1373603343963623\n",
      "\t Training loss (single batch): 1.1877152919769287\n",
      "\t Training loss (single batch): 1.0415589809417725\n",
      "\t Training loss (single batch): 1.0868185758590698\n",
      "\t Training loss (single batch): 1.6335440874099731\n",
      "\t Training loss (single batch): 0.931113600730896\n",
      "\t Training loss (single batch): 1.2100216150283813\n",
      "\t Training loss (single batch): 1.147830605506897\n",
      "\t Training loss (single batch): 1.2951388359069824\n",
      "\t Training loss (single batch): 1.0954310894012451\n",
      "\t Training loss (single batch): 0.8529586791992188\n",
      "\t Training loss (single batch): 1.4181132316589355\n",
      "\t Training loss (single batch): 0.9649023413658142\n",
      "\t Training loss (single batch): 1.0242520570755005\n",
      "\t Training loss (single batch): 1.041499137878418\n",
      "\t Training loss (single batch): 0.9999019503593445\n",
      "\t Training loss (single batch): 1.049709677696228\n",
      "\t Training loss (single batch): 1.4989733695983887\n",
      "\t Training loss (single batch): 1.878320336341858\n",
      "\t Training loss (single batch): 1.2207309007644653\n",
      "\t Training loss (single batch): 0.9564759731292725\n",
      "\t Training loss (single batch): 1.145530343055725\n",
      "\t Training loss (single batch): 0.9123929738998413\n",
      "\t Training loss (single batch): 1.0957444906234741\n",
      "\t Training loss (single batch): 1.0649933815002441\n",
      "\t Training loss (single batch): 1.2876064777374268\n",
      "\t Training loss (single batch): 1.119748592376709\n",
      "\t Training loss (single batch): 1.1697026491165161\n",
      "\t Training loss (single batch): 1.6024891138076782\n",
      "\t Training loss (single batch): 0.842543363571167\n",
      "\t Training loss (single batch): 1.0338037014007568\n",
      "\t Training loss (single batch): 1.1232210397720337\n",
      "\t Training loss (single batch): 1.1513420343399048\n",
      "\t Training loss (single batch): 0.8219930529594421\n",
      "\t Training loss (single batch): 1.1447937488555908\n",
      "\t Training loss (single batch): 1.4372875690460205\n",
      "\t Training loss (single batch): 1.0102043151855469\n",
      "\t Training loss (single batch): 0.896067202091217\n",
      "\t Training loss (single batch): 1.1649240255355835\n",
      "\t Training loss (single batch): 0.8376233577728271\n",
      "\t Training loss (single batch): 1.4660608768463135\n",
      "\t Training loss (single batch): 1.0144108533859253\n",
      "\t Training loss (single batch): 1.4429246187210083\n",
      "\t Training loss (single batch): 2.400378465652466\n",
      "\t Training loss (single batch): 1.1099140644073486\n",
      "\t Training loss (single batch): 1.3604182004928589\n",
      "\t Training loss (single batch): 1.601682424545288\n",
      "\t Training loss (single batch): 1.1939207315444946\n",
      "\t Training loss (single batch): 1.722177267074585\n",
      "\t Training loss (single batch): 0.7599867582321167\n",
      "\t Training loss (single batch): 0.8547033071517944\n",
      "\t Training loss (single batch): 1.376159429550171\n",
      "\t Training loss (single batch): 0.9440760612487793\n",
      "\t Training loss (single batch): 1.1117463111877441\n",
      "\t Training loss (single batch): 1.2235512733459473\n",
      "\t Training loss (single batch): 1.0214282274246216\n",
      "\t Training loss (single batch): 0.8371632099151611\n",
      "\t Training loss (single batch): 1.3181535005569458\n",
      "\t Training loss (single batch): 1.0302783250808716\n",
      "\t Training loss (single batch): 0.8692430853843689\n",
      "\t Training loss (single batch): 1.0179896354675293\n",
      "\t Training loss (single batch): 1.1917043924331665\n",
      "\t Training loss (single batch): 1.4698306322097778\n",
      "\t Training loss (single batch): 1.123881220817566\n",
      "\t Training loss (single batch): 0.9929455518722534\n",
      "\t Training loss (single batch): 0.8749120831489563\n",
      "\t Training loss (single batch): 0.8695946931838989\n",
      "\t Training loss (single batch): 0.9566908478736877\n",
      "\t Training loss (single batch): 1.1816242933273315\n",
      "\t Training loss (single batch): 2.1134417057037354\n",
      "\t Training loss (single batch): 1.1869503259658813\n",
      "\t Training loss (single batch): 1.1052968502044678\n",
      "\t Training loss (single batch): 0.7187579870223999\n",
      "\t Training loss (single batch): 0.9287571310997009\n",
      "\t Training loss (single batch): 1.1421363353729248\n",
      "\t Training loss (single batch): 1.9372544288635254\n",
      "\t Training loss (single batch): 1.204290509223938\n",
      "\t Training loss (single batch): 0.8795854449272156\n",
      "\t Training loss (single batch): 1.1034678220748901\n",
      "\t Training loss (single batch): 1.3769121170043945\n",
      "\t Training loss (single batch): 1.1342113018035889\n",
      "\t Training loss (single batch): 1.1796845197677612\n",
      "\t Training loss (single batch): 1.1173115968704224\n",
      "\t Training loss (single batch): 1.71750009059906\n",
      "\t Training loss (single batch): 1.5286993980407715\n",
      "\t Training loss (single batch): 1.5161689519882202\n",
      "\t Training loss (single batch): 1.171492338180542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0841950178146362\n",
      "\t Training loss (single batch): 1.9019412994384766\n",
      "\t Training loss (single batch): 0.9503748416900635\n",
      "\t Training loss (single batch): 0.9984540343284607\n",
      "\t Training loss (single batch): 0.9349396824836731\n",
      "\t Training loss (single batch): 1.6634278297424316\n",
      "\t Training loss (single batch): 1.1307246685028076\n",
      "\t Training loss (single batch): 1.2455235719680786\n",
      "\t Training loss (single batch): 1.1212661266326904\n",
      "\t Training loss (single batch): 1.5166172981262207\n",
      "\t Training loss (single batch): 1.0038963556289673\n",
      "\t Training loss (single batch): 1.1635048389434814\n",
      "\t Training loss (single batch): 1.2650268077850342\n",
      "\t Training loss (single batch): 1.1625434160232544\n",
      "\t Training loss (single batch): 1.190311074256897\n",
      "\t Training loss (single batch): 1.5826770067214966\n",
      "\t Training loss (single batch): 1.5243403911590576\n",
      "\t Training loss (single batch): 1.1408501863479614\n",
      "\t Training loss (single batch): 1.1832247972488403\n",
      "\t Training loss (single batch): 0.7867931723594666\n",
      "\t Training loss (single batch): 0.8412682414054871\n",
      "\t Training loss (single batch): 1.3015344142913818\n",
      "\t Training loss (single batch): 1.4503918886184692\n",
      "\t Training loss (single batch): 0.9119564294815063\n",
      "\t Training loss (single batch): 1.7586199045181274\n",
      "\t Training loss (single batch): 1.2752631902694702\n",
      "\t Training loss (single batch): 1.0258206129074097\n",
      "\t Training loss (single batch): 1.334912896156311\n",
      "\t Training loss (single batch): 1.1154472827911377\n",
      "\t Training loss (single batch): 1.6413483619689941\n",
      "\t Training loss (single batch): 1.1523284912109375\n",
      "\t Training loss (single batch): 0.996913731098175\n",
      "\t Training loss (single batch): 0.7646657824516296\n",
      "\t Training loss (single batch): 1.0739068984985352\n",
      "\t Training loss (single batch): 1.008377194404602\n",
      "\t Training loss (single batch): 1.0367252826690674\n",
      "\t Training loss (single batch): 0.8612819314002991\n",
      "\t Training loss (single batch): 1.4396822452545166\n",
      "\t Training loss (single batch): 0.9769070148468018\n",
      "\t Training loss (single batch): 1.2660887241363525\n",
      "\t Training loss (single batch): 1.3435864448547363\n",
      "\t Training loss (single batch): 1.2504042387008667\n",
      "\t Training loss (single batch): 0.991995632648468\n",
      "\t Training loss (single batch): 0.8652437925338745\n",
      "\t Training loss (single batch): 0.8147124648094177\n",
      "\t Training loss (single batch): 1.3232929706573486\n",
      "\t Training loss (single batch): 0.8520118594169617\n",
      "\t Training loss (single batch): 0.845159649848938\n",
      "\t Training loss (single batch): 1.1579242944717407\n",
      "\t Training loss (single batch): 1.7212516069412231\n",
      "\t Training loss (single batch): 1.2510117292404175\n",
      "\t Training loss (single batch): 1.288317322731018\n",
      "\t Training loss (single batch): 1.2652188539505005\n",
      "\t Training loss (single batch): 0.9622321128845215\n",
      "\t Training loss (single batch): 1.534096598625183\n",
      "\t Training loss (single batch): 1.415820598602295\n",
      "\t Training loss (single batch): 0.982903003692627\n",
      "\t Training loss (single batch): 1.5430467128753662\n",
      "\t Training loss (single batch): 1.4942803382873535\n",
      "\t Training loss (single batch): 1.4323451519012451\n",
      "\t Training loss (single batch): 1.3258665800094604\n",
      "\t Training loss (single batch): 0.9871949553489685\n",
      "\t Training loss (single batch): 1.2548129558563232\n",
      "\t Training loss (single batch): 0.6761332750320435\n",
      "\t Training loss (single batch): 1.4550342559814453\n",
      "\t Training loss (single batch): 1.0617109537124634\n",
      "\t Training loss (single batch): 0.9438428282737732\n",
      "\t Training loss (single batch): 1.0275218486785889\n",
      "\t Training loss (single batch): 1.1492480039596558\n",
      "\t Training loss (single batch): 0.8277801871299744\n",
      "\t Training loss (single batch): 0.8180626034736633\n",
      "\t Training loss (single batch): 1.2489458322525024\n",
      "\t Training loss (single batch): 0.8656731247901917\n",
      "\t Training loss (single batch): 1.63373601436615\n",
      "\t Training loss (single batch): 1.0811752080917358\n",
      "\t Training loss (single batch): 1.2094753980636597\n",
      "\t Training loss (single batch): 1.4858194589614868\n",
      "\t Training loss (single batch): 1.397106409072876\n",
      "\t Training loss (single batch): 1.2898204326629639\n",
      "\t Training loss (single batch): 0.868186891078949\n",
      "\t Training loss (single batch): 0.8976083397865295\n",
      "\t Training loss (single batch): 0.8748522400856018\n",
      "\t Training loss (single batch): 1.1129730939865112\n",
      "\t Training loss (single batch): 1.0710688829421997\n",
      "\t Training loss (single batch): 0.9938212037086487\n",
      "\t Training loss (single batch): 1.9472301006317139\n",
      "\t Training loss (single batch): 1.4873321056365967\n",
      "\t Training loss (single batch): 1.6912033557891846\n",
      "\t Training loss (single batch): 0.8714966177940369\n",
      "\t Training loss (single batch): 1.2799266576766968\n",
      "\t Training loss (single batch): 1.250742793083191\n",
      "\t Training loss (single batch): 1.3352630138397217\n",
      "\t Training loss (single batch): 1.294427752494812\n",
      "\t Training loss (single batch): 1.5960688591003418\n",
      "\t Training loss (single batch): 1.467685341835022\n",
      "\t Training loss (single batch): 1.8847129344940186\n",
      "\t Training loss (single batch): 1.0012683868408203\n",
      "\t Training loss (single batch): 0.5814886093139648\n",
      "\t Training loss (single batch): 0.8986419439315796\n",
      "\t Training loss (single batch): 0.9534021019935608\n",
      "\t Training loss (single batch): 1.151148796081543\n",
      "\t Training loss (single batch): 0.9351924061775208\n",
      "\t Training loss (single batch): 1.196282982826233\n",
      "\t Training loss (single batch): 0.9906559586524963\n",
      "\t Training loss (single batch): 0.9319208264350891\n",
      "\t Training loss (single batch): 1.2228158712387085\n",
      "\t Training loss (single batch): 1.5373106002807617\n",
      "\t Training loss (single batch): 0.9828177094459534\n",
      "\t Training loss (single batch): 1.0261845588684082\n",
      "\t Training loss (single batch): 1.0536913871765137\n",
      "\t Training loss (single batch): 0.8472861647605896\n",
      "\t Training loss (single batch): 1.1062055826187134\n",
      "\t Training loss (single batch): 1.035919427871704\n",
      "\t Training loss (single batch): 1.1786822080612183\n",
      "\t Training loss (single batch): 1.4572683572769165\n",
      "\t Training loss (single batch): 1.0529963970184326\n",
      "\t Training loss (single batch): 0.8297428488731384\n",
      "\t Training loss (single batch): 0.9929530024528503\n",
      "\t Training loss (single batch): 1.463914394378662\n",
      "\t Training loss (single batch): 0.9230756163597107\n",
      "\t Training loss (single batch): 1.251446008682251\n",
      "\t Training loss (single batch): 1.1259500980377197\n",
      "\t Training loss (single batch): 0.8921312093734741\n",
      "\t Training loss (single batch): 1.2005150318145752\n",
      "\t Training loss (single batch): 1.1405479907989502\n",
      "\t Training loss (single batch): 1.1449155807495117\n",
      "\t Training loss (single batch): 1.0522480010986328\n",
      "\t Training loss (single batch): 1.2246062755584717\n",
      "\t Training loss (single batch): 1.5295510292053223\n",
      "\t Training loss (single batch): 1.400103211402893\n",
      "\t Training loss (single batch): 1.1393311023712158\n",
      "\t Training loss (single batch): 1.1259294748306274\n",
      "\t Training loss (single batch): 1.4546751976013184\n",
      "\t Training loss (single batch): 1.4991060495376587\n",
      "\t Training loss (single batch): 1.1169769763946533\n",
      "\t Training loss (single batch): 1.0568082332611084\n",
      "\t Training loss (single batch): 1.3077784776687622\n",
      "\t Training loss (single batch): 1.204784870147705\n",
      "\t Training loss (single batch): 1.5335890054702759\n",
      "\t Training loss (single batch): 1.5020612478256226\n",
      "\t Training loss (single batch): 1.5105712413787842\n",
      "\t Training loss (single batch): 1.311691164970398\n",
      "\t Training loss (single batch): 1.0044169425964355\n",
      "\t Training loss (single batch): 0.9862203001976013\n",
      "\t Training loss (single batch): 1.3655290603637695\n",
      "\t Training loss (single batch): 0.8098985552787781\n",
      "\t Training loss (single batch): 1.1720361709594727\n",
      "\t Training loss (single batch): 0.9438076615333557\n",
      "\t Training loss (single batch): 1.2288764715194702\n",
      "\t Training loss (single batch): 0.9729478359222412\n",
      "\t Training loss (single batch): 0.7078301310539246\n",
      "\t Training loss (single batch): 0.9539896845817566\n",
      "\t Training loss (single batch): 1.045083999633789\n",
      "\t Training loss (single batch): 1.0828572511672974\n",
      "\t Training loss (single batch): 0.9309507608413696\n",
      "\t Training loss (single batch): 1.3012957572937012\n",
      "\t Training loss (single batch): 1.289036512374878\n",
      "\t Training loss (single batch): 0.9577839374542236\n",
      "\t Training loss (single batch): 0.7924548387527466\n",
      "\t Training loss (single batch): 1.0661842823028564\n",
      "\t Training loss (single batch): 1.1440942287445068\n",
      "\t Training loss (single batch): 1.2861639261245728\n",
      "\t Training loss (single batch): 0.7725067734718323\n",
      "\t Training loss (single batch): 1.2818257808685303\n",
      "\t Training loss (single batch): 1.4964879751205444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3683818578720093\n",
      "\t Training loss (single batch): 0.9625309705734253\n",
      "\t Training loss (single batch): 1.103926420211792\n",
      "\t Training loss (single batch): 1.2321062088012695\n",
      "\t Training loss (single batch): 1.2967426776885986\n",
      "\t Training loss (single batch): 1.0317599773406982\n",
      "\t Training loss (single batch): 1.0264819860458374\n",
      "\t Training loss (single batch): 0.9656289219856262\n",
      "\t Training loss (single batch): 1.552223801612854\n",
      "\t Training loss (single batch): 0.9533838629722595\n",
      "\t Training loss (single batch): 1.0641534328460693\n",
      "\t Training loss (single batch): 1.159131407737732\n",
      "\t Training loss (single batch): 1.1518417596817017\n",
      "\t Training loss (single batch): 1.2839537858963013\n",
      "\t Training loss (single batch): 1.281501293182373\n",
      "\t Training loss (single batch): 1.750246524810791\n",
      "\t Training loss (single batch): 1.0904375314712524\n",
      "\t Training loss (single batch): 1.631191372871399\n",
      "\t Training loss (single batch): 1.1818792819976807\n",
      "\t Training loss (single batch): 0.9757656455039978\n",
      "\t Training loss (single batch): 1.228288173675537\n",
      "\t Training loss (single batch): 1.267842411994934\n",
      "\t Training loss (single batch): 0.7767187356948853\n",
      "##################################\n",
      "## EPOCH 76\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8785368800163269\n",
      "\t Training loss (single batch): 0.9332512021064758\n",
      "\t Training loss (single batch): 1.0049821138381958\n",
      "\t Training loss (single batch): 1.453860878944397\n",
      "\t Training loss (single batch): 1.0220980644226074\n",
      "\t Training loss (single batch): 1.3183993101119995\n",
      "\t Training loss (single batch): 1.5113005638122559\n",
      "\t Training loss (single batch): 1.4209398031234741\n",
      "\t Training loss (single batch): 0.967541515827179\n",
      "\t Training loss (single batch): 1.0010870695114136\n",
      "\t Training loss (single batch): 1.081842303276062\n",
      "\t Training loss (single batch): 0.9892580509185791\n",
      "\t Training loss (single batch): 0.9529725313186646\n",
      "\t Training loss (single batch): 1.1367249488830566\n",
      "\t Training loss (single batch): 1.0532863140106201\n",
      "\t Training loss (single batch): 1.4022566080093384\n",
      "\t Training loss (single batch): 1.1613409519195557\n",
      "\t Training loss (single batch): 1.5680526494979858\n",
      "\t Training loss (single batch): 1.6122140884399414\n",
      "\t Training loss (single batch): 0.7863202691078186\n",
      "\t Training loss (single batch): 0.9854771494865417\n",
      "\t Training loss (single batch): 1.1505275964736938\n",
      "\t Training loss (single batch): 0.8327460289001465\n",
      "\t Training loss (single batch): 1.1858547925949097\n",
      "\t Training loss (single batch): 1.0982717275619507\n",
      "\t Training loss (single batch): 1.1487464904785156\n",
      "\t Training loss (single batch): 1.32864511013031\n",
      "\t Training loss (single batch): 1.1788660287857056\n",
      "\t Training loss (single batch): 1.731157660484314\n",
      "\t Training loss (single batch): 1.2436811923980713\n",
      "\t Training loss (single batch): 1.387094497680664\n",
      "\t Training loss (single batch): 1.2714887857437134\n",
      "\t Training loss (single batch): 0.8357786536216736\n",
      "\t Training loss (single batch): 1.1980992555618286\n",
      "\t Training loss (single batch): 1.1270196437835693\n",
      "\t Training loss (single batch): 0.7640056610107422\n",
      "\t Training loss (single batch): 0.8563203811645508\n",
      "\t Training loss (single batch): 0.9414371848106384\n",
      "\t Training loss (single batch): 1.2288740873336792\n",
      "\t Training loss (single batch): 1.2387934923171997\n",
      "\t Training loss (single batch): 1.3883893489837646\n",
      "\t Training loss (single batch): 1.3516699075698853\n",
      "\t Training loss (single batch): 1.1591259241104126\n",
      "\t Training loss (single batch): 0.9955670833587646\n",
      "\t Training loss (single batch): 1.2749420404434204\n",
      "\t Training loss (single batch): 1.326163649559021\n",
      "\t Training loss (single batch): 1.3820314407348633\n",
      "\t Training loss (single batch): 1.2235454320907593\n",
      "\t Training loss (single batch): 0.8869024515151978\n",
      "\t Training loss (single batch): 0.8667523860931396\n",
      "\t Training loss (single batch): 0.9582464098930359\n",
      "\t Training loss (single batch): 1.263392686843872\n",
      "\t Training loss (single batch): 1.3974218368530273\n",
      "\t Training loss (single batch): 1.0593136548995972\n",
      "\t Training loss (single batch): 1.303023099899292\n",
      "\t Training loss (single batch): 1.2551406621932983\n",
      "\t Training loss (single batch): 1.7590413093566895\n",
      "\t Training loss (single batch): 1.2557663917541504\n",
      "\t Training loss (single batch): 1.1688843965530396\n",
      "\t Training loss (single batch): 1.542595624923706\n",
      "\t Training loss (single batch): 1.3248467445373535\n",
      "\t Training loss (single batch): 1.7263810634613037\n",
      "\t Training loss (single batch): 1.2625789642333984\n",
      "\t Training loss (single batch): 1.021757960319519\n",
      "\t Training loss (single batch): 0.8388151526451111\n",
      "\t Training loss (single batch): 1.5115748643875122\n",
      "\t Training loss (single batch): 1.1848866939544678\n",
      "\t Training loss (single batch): 0.8491646647453308\n",
      "\t Training loss (single batch): 1.88861083984375\n",
      "\t Training loss (single batch): 1.1970754861831665\n",
      "\t Training loss (single batch): 1.4570391178131104\n",
      "\t Training loss (single batch): 0.5549933910369873\n",
      "\t Training loss (single batch): 1.0079491138458252\n",
      "\t Training loss (single batch): 0.9953882098197937\n",
      "\t Training loss (single batch): 1.514007568359375\n",
      "\t Training loss (single batch): 1.5500835180282593\n",
      "\t Training loss (single batch): 0.9566786289215088\n",
      "\t Training loss (single batch): 1.147332787513733\n",
      "\t Training loss (single batch): 0.5973355174064636\n",
      "\t Training loss (single batch): 1.2924158573150635\n",
      "\t Training loss (single batch): 1.2559915781021118\n",
      "\t Training loss (single batch): 0.8728004693984985\n",
      "\t Training loss (single batch): 0.8828758001327515\n",
      "\t Training loss (single batch): 0.8466136455535889\n",
      "\t Training loss (single batch): 0.9707760214805603\n",
      "\t Training loss (single batch): 0.9394875168800354\n",
      "\t Training loss (single batch): 1.5330926179885864\n",
      "\t Training loss (single batch): 1.1601415872573853\n",
      "\t Training loss (single batch): 0.936953604221344\n",
      "\t Training loss (single batch): 1.355172038078308\n",
      "\t Training loss (single batch): 1.2719460725784302\n",
      "\t Training loss (single batch): 1.3152508735656738\n",
      "\t Training loss (single batch): 0.7508379817008972\n",
      "\t Training loss (single batch): 1.4613306522369385\n",
      "\t Training loss (single batch): 1.8355082273483276\n",
      "\t Training loss (single batch): 0.8817952871322632\n",
      "\t Training loss (single batch): 1.2416231632232666\n",
      "\t Training loss (single batch): 0.9829047322273254\n",
      "\t Training loss (single batch): 1.6053822040557861\n",
      "\t Training loss (single batch): 1.6027847528457642\n",
      "\t Training loss (single batch): 0.9939653873443604\n",
      "\t Training loss (single batch): 1.4692959785461426\n",
      "\t Training loss (single batch): 0.8620765805244446\n",
      "\t Training loss (single batch): 1.076536774635315\n",
      "\t Training loss (single batch): 1.4166243076324463\n",
      "\t Training loss (single batch): 1.3421964645385742\n",
      "\t Training loss (single batch): 1.4037256240844727\n",
      "\t Training loss (single batch): 1.0953946113586426\n",
      "\t Training loss (single batch): 1.3120468854904175\n",
      "\t Training loss (single batch): 1.1317214965820312\n",
      "\t Training loss (single batch): 1.3845826387405396\n",
      "\t Training loss (single batch): 1.2455004453659058\n",
      "\t Training loss (single batch): 1.334182620048523\n",
      "\t Training loss (single batch): 1.2638522386550903\n",
      "\t Training loss (single batch): 1.6884585618972778\n",
      "\t Training loss (single batch): 1.1760587692260742\n",
      "\t Training loss (single batch): 0.9642139077186584\n",
      "\t Training loss (single batch): 1.1647984981536865\n",
      "\t Training loss (single batch): 1.0697475671768188\n",
      "\t Training loss (single batch): 1.1871058940887451\n",
      "\t Training loss (single batch): 1.2915256023406982\n",
      "\t Training loss (single batch): 1.2310627698898315\n",
      "\t Training loss (single batch): 1.3899712562561035\n",
      "\t Training loss (single batch): 1.0025136470794678\n",
      "\t Training loss (single batch): 1.0625652074813843\n",
      "\t Training loss (single batch): 1.25346839427948\n",
      "\t Training loss (single batch): 1.1980088949203491\n",
      "\t Training loss (single batch): 0.9626277089118958\n",
      "\t Training loss (single batch): 0.8558797240257263\n",
      "\t Training loss (single batch): 1.2176423072814941\n",
      "\t Training loss (single batch): 1.3840453624725342\n",
      "\t Training loss (single batch): 1.1853998899459839\n",
      "\t Training loss (single batch): 1.3747162818908691\n",
      "\t Training loss (single batch): 1.0329092741012573\n",
      "\t Training loss (single batch): 1.0315128564834595\n",
      "\t Training loss (single batch): 1.3799996376037598\n",
      "\t Training loss (single batch): 1.4397531747817993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1759778261184692\n",
      "\t Training loss (single batch): 1.3110347986221313\n",
      "\t Training loss (single batch): 1.061070442199707\n",
      "\t Training loss (single batch): 1.2376923561096191\n",
      "\t Training loss (single batch): 0.5812160968780518\n",
      "\t Training loss (single batch): 1.15662682056427\n",
      "\t Training loss (single batch): 0.9712751507759094\n",
      "\t Training loss (single batch): 1.3165533542633057\n",
      "\t Training loss (single batch): 1.2542186975479126\n",
      "\t Training loss (single batch): 0.9160221219062805\n",
      "\t Training loss (single batch): 0.7146094441413879\n",
      "\t Training loss (single batch): 1.2369685173034668\n",
      "\t Training loss (single batch): 1.0480698347091675\n",
      "\t Training loss (single batch): 1.268134593963623\n",
      "\t Training loss (single batch): 1.0248379707336426\n",
      "\t Training loss (single batch): 1.027286410331726\n",
      "\t Training loss (single batch): 1.4970735311508179\n",
      "\t Training loss (single batch): 1.057207465171814\n",
      "\t Training loss (single batch): 1.4234018325805664\n",
      "\t Training loss (single batch): 1.6012669801712036\n",
      "\t Training loss (single batch): 1.287410855293274\n",
      "\t Training loss (single batch): 1.3854409456253052\n",
      "\t Training loss (single batch): 1.783571720123291\n",
      "\t Training loss (single batch): 1.0615227222442627\n",
      "\t Training loss (single batch): 1.1786341667175293\n",
      "\t Training loss (single batch): 1.0253206491470337\n",
      "\t Training loss (single batch): 0.9837307929992676\n",
      "\t Training loss (single batch): 1.3758113384246826\n",
      "\t Training loss (single batch): 1.2075304985046387\n",
      "\t Training loss (single batch): 1.4695359468460083\n",
      "\t Training loss (single batch): 0.9613264203071594\n",
      "\t Training loss (single batch): 0.8121873736381531\n",
      "\t Training loss (single batch): 1.1713624000549316\n",
      "\t Training loss (single batch): 1.4215068817138672\n",
      "\t Training loss (single batch): 1.442758321762085\n",
      "\t Training loss (single batch): 1.1048660278320312\n",
      "\t Training loss (single batch): 1.0460511445999146\n",
      "\t Training loss (single batch): 1.2988616228103638\n",
      "\t Training loss (single batch): 1.5517371892929077\n",
      "\t Training loss (single batch): 1.3066633939743042\n",
      "\t Training loss (single batch): 1.1520612239837646\n",
      "\t Training loss (single batch): 1.8797601461410522\n",
      "\t Training loss (single batch): 1.1446820497512817\n",
      "\t Training loss (single batch): 1.6431024074554443\n",
      "\t Training loss (single batch): 1.1298993825912476\n",
      "\t Training loss (single batch): 0.9330535531044006\n",
      "\t Training loss (single batch): 1.1333715915679932\n",
      "\t Training loss (single batch): 1.9509752988815308\n",
      "\t Training loss (single batch): 1.439880132675171\n",
      "\t Training loss (single batch): 1.288743257522583\n",
      "\t Training loss (single batch): 1.233689785003662\n",
      "\t Training loss (single batch): 0.9761541485786438\n",
      "\t Training loss (single batch): 1.1161746978759766\n",
      "\t Training loss (single batch): 1.53347909450531\n",
      "\t Training loss (single batch): 1.0740529298782349\n",
      "\t Training loss (single batch): 1.161365032196045\n",
      "\t Training loss (single batch): 0.9832133650779724\n",
      "\t Training loss (single batch): 1.3039426803588867\n",
      "\t Training loss (single batch): 1.6692678928375244\n",
      "\t Training loss (single batch): 1.4493987560272217\n",
      "\t Training loss (single batch): 1.0461241006851196\n",
      "\t Training loss (single batch): 1.2860337495803833\n",
      "\t Training loss (single batch): 1.1635639667510986\n",
      "\t Training loss (single batch): 1.2150777578353882\n",
      "\t Training loss (single batch): 1.2787463665008545\n",
      "\t Training loss (single batch): 1.5985167026519775\n",
      "\t Training loss (single batch): 1.139102816581726\n",
      "\t Training loss (single batch): 1.036041498184204\n",
      "\t Training loss (single batch): 0.9748030304908752\n",
      "\t Training loss (single batch): 1.6607997417449951\n",
      "\t Training loss (single batch): 1.4627646207809448\n",
      "\t Training loss (single batch): 0.84125816822052\n",
      "\t Training loss (single batch): 1.3904259204864502\n",
      "\t Training loss (single batch): 0.9651392102241516\n",
      "\t Training loss (single batch): 1.2651395797729492\n",
      "\t Training loss (single batch): 1.4042813777923584\n",
      "\t Training loss (single batch): 1.4183894395828247\n",
      "\t Training loss (single batch): 1.4464244842529297\n",
      "\t Training loss (single batch): 1.0237373113632202\n",
      "\t Training loss (single batch): 1.702494502067566\n",
      "\t Training loss (single batch): 1.3428418636322021\n",
      "\t Training loss (single batch): 1.2064640522003174\n",
      "\t Training loss (single batch): 1.2212510108947754\n",
      "\t Training loss (single batch): 1.47952401638031\n",
      "\t Training loss (single batch): 1.0718634128570557\n",
      "\t Training loss (single batch): 0.9584736824035645\n",
      "\t Training loss (single batch): 1.0837292671203613\n",
      "\t Training loss (single batch): 0.7573322653770447\n",
      "\t Training loss (single batch): 1.474639654159546\n",
      "\t Training loss (single batch): 1.2741070985794067\n",
      "\t Training loss (single batch): 1.3466447591781616\n",
      "\t Training loss (single batch): 1.468042016029358\n",
      "\t Training loss (single batch): 1.325059175491333\n",
      "\t Training loss (single batch): 1.036076307296753\n",
      "\t Training loss (single batch): 0.7934145331382751\n",
      "\t Training loss (single batch): 0.966270387172699\n",
      "\t Training loss (single batch): 0.8639819622039795\n",
      "\t Training loss (single batch): 1.150384545326233\n",
      "\t Training loss (single batch): 1.198479175567627\n",
      "\t Training loss (single batch): 1.0717964172363281\n",
      "\t Training loss (single batch): 0.749438464641571\n",
      "\t Training loss (single batch): 0.7026426792144775\n",
      "\t Training loss (single batch): 1.295513391494751\n",
      "\t Training loss (single batch): 1.1860271692276\n",
      "\t Training loss (single batch): 1.8871803283691406\n",
      "\t Training loss (single batch): 1.0090972185134888\n",
      "\t Training loss (single batch): 0.7106981873512268\n",
      "\t Training loss (single batch): 1.045117974281311\n",
      "\t Training loss (single batch): 0.7906295657157898\n",
      "\t Training loss (single batch): 1.4764487743377686\n",
      "\t Training loss (single batch): 0.9621708989143372\n",
      "\t Training loss (single batch): 1.1957025527954102\n",
      "\t Training loss (single batch): 1.1478841304779053\n",
      "\t Training loss (single batch): 0.9839283227920532\n",
      "\t Training loss (single batch): 1.164531946182251\n",
      "\t Training loss (single batch): 0.949898898601532\n",
      "\t Training loss (single batch): 1.6414096355438232\n",
      "\t Training loss (single batch): 1.4237031936645508\n",
      "\t Training loss (single batch): 1.0718990564346313\n",
      "\t Training loss (single batch): 1.112317681312561\n",
      "\t Training loss (single batch): 1.0525373220443726\n",
      "\t Training loss (single batch): 0.93305504322052\n",
      "\t Training loss (single batch): 1.5781517028808594\n",
      "\t Training loss (single batch): 1.1522947549819946\n",
      "\t Training loss (single batch): 1.2615562677383423\n",
      "\t Training loss (single batch): 1.2519031763076782\n",
      "\t Training loss (single batch): 1.3661547899246216\n",
      "\t Training loss (single batch): 1.022769570350647\n",
      "\t Training loss (single batch): 1.1047693490982056\n",
      "\t Training loss (single batch): 1.335718035697937\n",
      "\t Training loss (single batch): 1.3653013706207275\n",
      "\t Training loss (single batch): 1.4555473327636719\n",
      "\t Training loss (single batch): 0.9912818074226379\n",
      "\t Training loss (single batch): 1.2847270965576172\n",
      "\t Training loss (single batch): 1.228915810585022\n",
      "\t Training loss (single batch): 1.515265703201294\n",
      "\t Training loss (single batch): 0.8878455758094788\n",
      "\t Training loss (single batch): 0.7936624884605408\n",
      "\t Training loss (single batch): 1.1400604248046875\n",
      "\t Training loss (single batch): 1.3774311542510986\n",
      "\t Training loss (single batch): 1.0265899896621704\n",
      "\t Training loss (single batch): 1.292563796043396\n",
      "\t Training loss (single batch): 1.2424575090408325\n",
      "\t Training loss (single batch): 0.7955300807952881\n",
      "\t Training loss (single batch): 1.4893639087677002\n",
      "\t Training loss (single batch): 1.3206355571746826\n",
      "\t Training loss (single batch): 1.2664848566055298\n",
      "\t Training loss (single batch): 1.3101425170898438\n",
      "\t Training loss (single batch): 0.8350798487663269\n",
      "\t Training loss (single batch): 1.34248685836792\n",
      "\t Training loss (single batch): 1.3466918468475342\n",
      "\t Training loss (single batch): 1.279599666595459\n",
      "\t Training loss (single batch): 1.5119619369506836\n",
      "\t Training loss (single batch): 0.9642531275749207\n",
      "\t Training loss (single batch): 1.5939207077026367\n",
      "\t Training loss (single batch): 1.4891290664672852\n",
      "\t Training loss (single batch): 1.3489463329315186\n",
      "\t Training loss (single batch): 1.2961455583572388\n",
      "\t Training loss (single batch): 0.8606083989143372\n",
      "\t Training loss (single batch): 1.527374267578125\n",
      "\t Training loss (single batch): 1.254821538925171\n",
      "\t Training loss (single batch): 1.1111811399459839\n",
      "\t Training loss (single batch): 0.7257531881332397\n",
      "\t Training loss (single batch): 1.5264184474945068\n",
      "\t Training loss (single batch): 1.2480154037475586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3808233737945557\n",
      "\t Training loss (single batch): 1.23708176612854\n",
      "\t Training loss (single batch): 1.3202438354492188\n",
      "\t Training loss (single batch): 1.1582906246185303\n",
      "\t Training loss (single batch): 1.1543933153152466\n",
      "\t Training loss (single batch): 1.0617715120315552\n",
      "\t Training loss (single batch): 0.9065858125686646\n",
      "\t Training loss (single batch): 0.9867940545082092\n",
      "\t Training loss (single batch): 1.1392332315444946\n",
      "\t Training loss (single batch): 1.4606153964996338\n",
      "\t Training loss (single batch): 1.0881192684173584\n",
      "\t Training loss (single batch): 1.1178717613220215\n",
      "\t Training loss (single batch): 1.0894908905029297\n",
      "\t Training loss (single batch): 0.8382585048675537\n",
      "\t Training loss (single batch): 0.5206473469734192\n",
      "##################################\n",
      "## EPOCH 77\n",
      "##################################\n",
      "\t Training loss (single batch): 1.254855751991272\n",
      "\t Training loss (single batch): 1.3546600341796875\n",
      "\t Training loss (single batch): 1.0222142934799194\n",
      "\t Training loss (single batch): 1.028029441833496\n",
      "\t Training loss (single batch): 1.3244078159332275\n",
      "\t Training loss (single batch): 1.6094244718551636\n",
      "\t Training loss (single batch): 1.2576549053192139\n",
      "\t Training loss (single batch): 0.7059476971626282\n",
      "\t Training loss (single batch): 1.2321593761444092\n",
      "\t Training loss (single batch): 1.3865995407104492\n",
      "\t Training loss (single batch): 0.8592252731323242\n",
      "\t Training loss (single batch): 1.6795209646224976\n",
      "\t Training loss (single batch): 1.0511716604232788\n",
      "\t Training loss (single batch): 1.4674296379089355\n",
      "\t Training loss (single batch): 0.8493708372116089\n",
      "\t Training loss (single batch): 1.4183186292648315\n",
      "\t Training loss (single batch): 1.47816002368927\n",
      "\t Training loss (single batch): 1.2235971689224243\n",
      "\t Training loss (single batch): 1.1991976499557495\n",
      "\t Training loss (single batch): 0.9293030500411987\n",
      "\t Training loss (single batch): 1.6502233743667603\n",
      "\t Training loss (single batch): 1.2029060125350952\n",
      "\t Training loss (single batch): 0.9859709143638611\n",
      "\t Training loss (single batch): 1.736474871635437\n",
      "\t Training loss (single batch): 1.1424261331558228\n",
      "\t Training loss (single batch): 1.5143760442733765\n",
      "\t Training loss (single batch): 1.4764549732208252\n",
      "\t Training loss (single batch): 1.3020257949829102\n",
      "\t Training loss (single batch): 1.0053093433380127\n",
      "\t Training loss (single batch): 1.1337465047836304\n",
      "\t Training loss (single batch): 1.62545645236969\n",
      "\t Training loss (single batch): 1.2669799327850342\n",
      "\t Training loss (single batch): 0.9843366146087646\n",
      "\t Training loss (single batch): 1.0123897790908813\n",
      "\t Training loss (single batch): 1.1782801151275635\n",
      "\t Training loss (single batch): 1.13819420337677\n",
      "\t Training loss (single batch): 0.9878929257392883\n",
      "\t Training loss (single batch): 1.3122049570083618\n",
      "\t Training loss (single batch): 1.2014728784561157\n",
      "\t Training loss (single batch): 1.2005555629730225\n",
      "\t Training loss (single batch): 1.1161563396453857\n",
      "\t Training loss (single batch): 1.1456531286239624\n",
      "\t Training loss (single batch): 1.0310190916061401\n",
      "\t Training loss (single batch): 1.0269935131072998\n",
      "\t Training loss (single batch): 1.1593451499938965\n",
      "\t Training loss (single batch): 0.6071515679359436\n",
      "\t Training loss (single batch): 1.4426606893539429\n",
      "\t Training loss (single batch): 1.3949323892593384\n",
      "\t Training loss (single batch): 1.340769648551941\n",
      "\t Training loss (single batch): 0.8960296511650085\n",
      "\t Training loss (single batch): 1.3247098922729492\n",
      "\t Training loss (single batch): 1.19990873336792\n",
      "\t Training loss (single batch): 1.7635282278060913\n",
      "\t Training loss (single batch): 0.8067358136177063\n",
      "\t Training loss (single batch): 1.2204409837722778\n",
      "\t Training loss (single batch): 1.411878228187561\n",
      "\t Training loss (single batch): 1.3203843832015991\n",
      "\t Training loss (single batch): 0.9941986203193665\n",
      "\t Training loss (single batch): 0.8580631017684937\n",
      "\t Training loss (single batch): 2.0006346702575684\n",
      "\t Training loss (single batch): 1.6323308944702148\n",
      "\t Training loss (single batch): 0.9736654758453369\n",
      "\t Training loss (single batch): 1.0128061771392822\n",
      "\t Training loss (single batch): 1.1658185720443726\n",
      "\t Training loss (single batch): 1.4441123008728027\n",
      "\t Training loss (single batch): 1.3907020092010498\n",
      "\t Training loss (single batch): 1.3685745000839233\n",
      "\t Training loss (single batch): 1.3073327541351318\n",
      "\t Training loss (single batch): 1.2152845859527588\n",
      "\t Training loss (single batch): 0.903057336807251\n",
      "\t Training loss (single batch): 0.791928231716156\n",
      "\t Training loss (single batch): 1.213460922241211\n",
      "\t Training loss (single batch): 1.2485142946243286\n",
      "\t Training loss (single batch): 1.2382785081863403\n",
      "\t Training loss (single batch): 1.183050513267517\n",
      "\t Training loss (single batch): 1.4654958248138428\n",
      "\t Training loss (single batch): 1.3626190423965454\n",
      "\t Training loss (single batch): 1.1316651105880737\n",
      "\t Training loss (single batch): 0.9630333781242371\n",
      "\t Training loss (single batch): 1.805732011795044\n",
      "\t Training loss (single batch): 1.3971772193908691\n",
      "\t Training loss (single batch): 1.1762652397155762\n",
      "\t Training loss (single batch): 1.440086841583252\n",
      "\t Training loss (single batch): 1.8161898851394653\n",
      "\t Training loss (single batch): 1.1910971403121948\n",
      "\t Training loss (single batch): 1.090270757675171\n",
      "\t Training loss (single batch): 1.8686702251434326\n",
      "\t Training loss (single batch): 1.28663969039917\n",
      "\t Training loss (single batch): 1.4301066398620605\n",
      "\t Training loss (single batch): 1.479849934577942\n",
      "\t Training loss (single batch): 1.0997674465179443\n",
      "\t Training loss (single batch): 1.2928889989852905\n",
      "\t Training loss (single batch): 1.0825676918029785\n",
      "\t Training loss (single batch): 1.1372573375701904\n",
      "\t Training loss (single batch): 1.0389600992202759\n",
      "\t Training loss (single batch): 1.15229332447052\n",
      "\t Training loss (single batch): 0.9725859761238098\n",
      "\t Training loss (single batch): 1.076366662979126\n",
      "\t Training loss (single batch): 1.1899309158325195\n",
      "\t Training loss (single batch): 0.9709237813949585\n",
      "\t Training loss (single batch): 0.8309988975524902\n",
      "\t Training loss (single batch): 0.9894103407859802\n",
      "\t Training loss (single batch): 0.7930834293365479\n",
      "\t Training loss (single batch): 1.6372971534729004\n",
      "\t Training loss (single batch): 1.5689319372177124\n",
      "\t Training loss (single batch): 1.0398098230361938\n",
      "\t Training loss (single batch): 1.2975660562515259\n",
      "\t Training loss (single batch): 1.1350187063217163\n",
      "\t Training loss (single batch): 1.3428714275360107\n",
      "\t Training loss (single batch): 0.8703061938285828\n",
      "\t Training loss (single batch): 1.251426100730896\n",
      "\t Training loss (single batch): 1.061458945274353\n",
      "\t Training loss (single batch): 1.6866308450698853\n",
      "\t Training loss (single batch): 0.980078935623169\n",
      "\t Training loss (single batch): 0.9813643097877502\n",
      "\t Training loss (single batch): 0.9817330837249756\n",
      "\t Training loss (single batch): 1.3553515672683716\n",
      "\t Training loss (single batch): 1.345597743988037\n",
      "\t Training loss (single batch): 1.1305917501449585\n",
      "\t Training loss (single batch): 1.0344328880310059\n",
      "\t Training loss (single batch): 1.176937460899353\n",
      "\t Training loss (single batch): 1.4655394554138184\n",
      "\t Training loss (single batch): 1.0186256170272827\n",
      "\t Training loss (single batch): 1.2399318218231201\n",
      "\t Training loss (single batch): 1.2270989418029785\n",
      "\t Training loss (single batch): 1.0057259798049927\n",
      "\t Training loss (single batch): 1.1325922012329102\n",
      "\t Training loss (single batch): 1.2816776037216187\n",
      "\t Training loss (single batch): 1.0612797737121582\n",
      "\t Training loss (single batch): 1.3483392000198364\n",
      "\t Training loss (single batch): 0.9007295966148376\n",
      "\t Training loss (single batch): 1.0924763679504395\n",
      "\t Training loss (single batch): 0.957362949848175\n",
      "\t Training loss (single batch): 1.0507258176803589\n",
      "\t Training loss (single batch): 0.925491213798523\n",
      "\t Training loss (single batch): 0.8470972776412964\n",
      "\t Training loss (single batch): 1.3852524757385254\n",
      "\t Training loss (single batch): 1.9522135257720947\n",
      "\t Training loss (single batch): 1.0025051832199097\n",
      "\t Training loss (single batch): 1.1447649002075195\n",
      "\t Training loss (single batch): 0.8601743578910828\n",
      "\t Training loss (single batch): 1.114216923713684\n",
      "\t Training loss (single batch): 1.2441096305847168\n",
      "\t Training loss (single batch): 1.3943370580673218\n",
      "\t Training loss (single batch): 0.9941652417182922\n",
      "\t Training loss (single batch): 1.2928847074508667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4233793020248413\n",
      "\t Training loss (single batch): 1.2814412117004395\n",
      "\t Training loss (single batch): 0.9149644374847412\n",
      "\t Training loss (single batch): 1.6127562522888184\n",
      "\t Training loss (single batch): 1.0062568187713623\n",
      "\t Training loss (single batch): 0.996218740940094\n",
      "\t Training loss (single batch): 1.0742359161376953\n",
      "\t Training loss (single batch): 1.2592262029647827\n",
      "\t Training loss (single batch): 1.299325942993164\n",
      "\t Training loss (single batch): 1.2077888250350952\n",
      "\t Training loss (single batch): 1.3966492414474487\n",
      "\t Training loss (single batch): 1.7138876914978027\n",
      "\t Training loss (single batch): 0.8024118542671204\n",
      "\t Training loss (single batch): 0.9559568762779236\n",
      "\t Training loss (single batch): 1.3137367963790894\n",
      "\t Training loss (single batch): 0.8116196393966675\n",
      "\t Training loss (single batch): 1.1353646516799927\n",
      "\t Training loss (single batch): 1.0047380924224854\n",
      "\t Training loss (single batch): 1.3226358890533447\n",
      "\t Training loss (single batch): 0.7489509582519531\n",
      "\t Training loss (single batch): 1.4984784126281738\n",
      "\t Training loss (single batch): 0.8199493288993835\n",
      "\t Training loss (single batch): 1.3307057619094849\n",
      "\t Training loss (single batch): 0.9842259287834167\n",
      "\t Training loss (single batch): 0.8544272184371948\n",
      "\t Training loss (single batch): 1.6505749225616455\n",
      "\t Training loss (single batch): 1.2435129880905151\n",
      "\t Training loss (single batch): 1.5333678722381592\n",
      "\t Training loss (single batch): 1.3805652856826782\n",
      "\t Training loss (single batch): 1.5162694454193115\n",
      "\t Training loss (single batch): 1.1219687461853027\n",
      "\t Training loss (single batch): 1.3621450662612915\n",
      "\t Training loss (single batch): 0.9230760931968689\n",
      "\t Training loss (single batch): 1.240714430809021\n",
      "\t Training loss (single batch): 1.3762634992599487\n",
      "\t Training loss (single batch): 1.5320769548416138\n",
      "\t Training loss (single batch): 0.9875653386116028\n",
      "\t Training loss (single batch): 1.3126776218414307\n",
      "\t Training loss (single batch): 1.4847089052200317\n",
      "\t Training loss (single batch): 1.5828001499176025\n",
      "\t Training loss (single batch): 1.4894282817840576\n",
      "\t Training loss (single batch): 1.0561084747314453\n",
      "\t Training loss (single batch): 1.0973011255264282\n",
      "\t Training loss (single batch): 1.2944309711456299\n",
      "\t Training loss (single batch): 1.6503360271453857\n",
      "\t Training loss (single batch): 1.1956788301467896\n",
      "\t Training loss (single batch): 0.9590344429016113\n",
      "\t Training loss (single batch): 1.3349230289459229\n",
      "\t Training loss (single batch): 1.065893292427063\n",
      "\t Training loss (single batch): 1.4099884033203125\n",
      "\t Training loss (single batch): 1.5429941415786743\n",
      "\t Training loss (single batch): 1.1315951347351074\n",
      "\t Training loss (single batch): 1.4020198583602905\n",
      "\t Training loss (single batch): 1.3643393516540527\n",
      "\t Training loss (single batch): 1.3496707677841187\n",
      "\t Training loss (single batch): 1.5431861877441406\n",
      "\t Training loss (single batch): 1.3399251699447632\n",
      "\t Training loss (single batch): 1.3362476825714111\n",
      "\t Training loss (single batch): 1.711166262626648\n",
      "\t Training loss (single batch): 0.9061596393585205\n",
      "\t Training loss (single batch): 0.9095644950866699\n",
      "\t Training loss (single batch): 1.3410649299621582\n",
      "\t Training loss (single batch): 1.1303989887237549\n",
      "\t Training loss (single batch): 1.4005839824676514\n",
      "\t Training loss (single batch): 1.6997168064117432\n",
      "\t Training loss (single batch): 1.1300619840621948\n",
      "\t Training loss (single batch): 1.0501857995986938\n",
      "\t Training loss (single batch): 1.4765502214431763\n",
      "\t Training loss (single batch): 1.2820861339569092\n",
      "\t Training loss (single batch): 1.0197056531906128\n",
      "\t Training loss (single batch): 1.1383458375930786\n",
      "\t Training loss (single batch): 0.8872538805007935\n",
      "\t Training loss (single batch): 1.3114360570907593\n",
      "\t Training loss (single batch): 1.1130291223526\n",
      "\t Training loss (single batch): 1.6883864402770996\n",
      "\t Training loss (single batch): 0.943530261516571\n",
      "\t Training loss (single batch): 1.1942685842514038\n",
      "\t Training loss (single batch): 1.1975916624069214\n",
      "\t Training loss (single batch): 1.3629350662231445\n",
      "\t Training loss (single batch): 0.8221853971481323\n",
      "\t Training loss (single batch): 1.5727607011795044\n",
      "\t Training loss (single batch): 0.9758323431015015\n",
      "\t Training loss (single batch): 1.5072323083877563\n",
      "\t Training loss (single batch): 1.190784215927124\n",
      "\t Training loss (single batch): 1.0405532121658325\n",
      "\t Training loss (single batch): 0.9016282558441162\n",
      "\t Training loss (single batch): 1.4223060607910156\n",
      "\t Training loss (single batch): 1.100827693939209\n",
      "\t Training loss (single batch): 0.950209379196167\n",
      "\t Training loss (single batch): 1.1265097856521606\n",
      "\t Training loss (single batch): 1.1248029470443726\n",
      "\t Training loss (single batch): 1.1728039979934692\n",
      "\t Training loss (single batch): 1.3968740701675415\n",
      "\t Training loss (single batch): 1.0195255279541016\n",
      "\t Training loss (single batch): 1.3402314186096191\n",
      "\t Training loss (single batch): 1.2658997774124146\n",
      "\t Training loss (single batch): 1.0496978759765625\n",
      "\t Training loss (single batch): 1.244311809539795\n",
      "\t Training loss (single batch): 0.9939890503883362\n",
      "\t Training loss (single batch): 1.32051682472229\n",
      "\t Training loss (single batch): 0.961286723613739\n",
      "\t Training loss (single batch): 1.4867359399795532\n",
      "\t Training loss (single batch): 1.577081561088562\n",
      "\t Training loss (single batch): 1.0332685708999634\n",
      "\t Training loss (single batch): 0.649870753288269\n",
      "\t Training loss (single batch): 1.1269924640655518\n",
      "\t Training loss (single batch): 1.0511881113052368\n",
      "\t Training loss (single batch): 1.396246314048767\n",
      "\t Training loss (single batch): 1.1412692070007324\n",
      "\t Training loss (single batch): 1.3974189758300781\n",
      "\t Training loss (single batch): 1.076028823852539\n",
      "\t Training loss (single batch): 1.465752363204956\n",
      "\t Training loss (single batch): 1.3080406188964844\n",
      "\t Training loss (single batch): 1.1157515048980713\n",
      "\t Training loss (single batch): 0.8220914602279663\n",
      "\t Training loss (single batch): 1.0912182331085205\n",
      "\t Training loss (single batch): 1.1976072788238525\n",
      "\t Training loss (single batch): 1.4000827074050903\n",
      "\t Training loss (single batch): 1.9298659563064575\n",
      "\t Training loss (single batch): 1.078277587890625\n",
      "\t Training loss (single batch): 0.7842151522636414\n",
      "\t Training loss (single batch): 1.2279479503631592\n",
      "\t Training loss (single batch): 1.1126621961593628\n",
      "\t Training loss (single batch): 1.6971187591552734\n",
      "\t Training loss (single batch): 1.7558047771453857\n",
      "\t Training loss (single batch): 0.8411116003990173\n",
      "\t Training loss (single batch): 1.155166506767273\n",
      "\t Training loss (single batch): 0.6705682873725891\n",
      "\t Training loss (single batch): 1.3442223072052002\n",
      "\t Training loss (single batch): 1.152446985244751\n",
      "\t Training loss (single batch): 0.8361176252365112\n",
      "\t Training loss (single batch): 1.406529188156128\n",
      "\t Training loss (single batch): 1.423705816268921\n",
      "\t Training loss (single batch): 1.2229840755462646\n",
      "\t Training loss (single batch): 1.5323479175567627\n",
      "\t Training loss (single batch): 0.9626438021659851\n",
      "\t Training loss (single batch): 0.8501359820365906\n",
      "\t Training loss (single batch): 1.4720664024353027\n",
      "\t Training loss (single batch): 0.9993390440940857\n",
      "\t Training loss (single batch): 1.223889946937561\n",
      "\t Training loss (single batch): 1.3915613889694214\n",
      "\t Training loss (single batch): 1.0131072998046875\n",
      "\t Training loss (single batch): 1.2527374029159546\n",
      "\t Training loss (single batch): 1.1796287298202515\n",
      "\t Training loss (single batch): 1.3057876825332642\n",
      "\t Training loss (single batch): 0.9043208956718445\n",
      "\t Training loss (single batch): 1.1642118692398071\n",
      "\t Training loss (single batch): 0.893543541431427\n",
      "\t Training loss (single batch): 1.0629138946533203\n",
      "\t Training loss (single batch): 1.1468108892440796\n",
      "\t Training loss (single batch): 1.092711091041565\n",
      "\t Training loss (single batch): 1.3729743957519531\n",
      "\t Training loss (single batch): 1.331281065940857\n",
      "\t Training loss (single batch): 0.9305197596549988\n",
      "\t Training loss (single batch): 1.6249839067459106\n",
      "\t Training loss (single batch): 1.45694100856781\n",
      "\t Training loss (single batch): 1.1455891132354736\n",
      "\t Training loss (single batch): 1.5621225833892822\n",
      "\t Training loss (single batch): 1.3980449438095093\n",
      "\t Training loss (single batch): 0.9427213072776794\n",
      "\t Training loss (single batch): 1.2000551223754883\n",
      "\t Training loss (single batch): 1.2681503295898438\n",
      "\t Training loss (single batch): 1.5561599731445312\n",
      "\t Training loss (single batch): 1.1800897121429443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1347935199737549\n",
      "\t Training loss (single batch): 1.0675543546676636\n",
      "\t Training loss (single batch): 1.1447768211364746\n",
      "\t Training loss (single batch): 1.385741114616394\n",
      "\t Training loss (single batch): 1.5372865200042725\n",
      "\t Training loss (single batch): 1.3844871520996094\n",
      "\t Training loss (single batch): 0.4492742419242859\n",
      "##################################\n",
      "## EPOCH 78\n",
      "##################################\n",
      "\t Training loss (single batch): 1.052937388420105\n",
      "\t Training loss (single batch): 1.5166983604431152\n",
      "\t Training loss (single batch): 0.8340044617652893\n",
      "\t Training loss (single batch): 1.2214308977127075\n",
      "\t Training loss (single batch): 1.4149551391601562\n",
      "\t Training loss (single batch): 1.4744374752044678\n",
      "\t Training loss (single batch): 0.9338172674179077\n",
      "\t Training loss (single batch): 1.37336003780365\n",
      "\t Training loss (single batch): 1.3302260637283325\n",
      "\t Training loss (single batch): 1.710362195968628\n",
      "\t Training loss (single batch): 1.6362299919128418\n",
      "\t Training loss (single batch): 0.9315173625946045\n",
      "\t Training loss (single batch): 1.0171566009521484\n",
      "\t Training loss (single batch): 1.3036290407180786\n",
      "\t Training loss (single batch): 0.9921435117721558\n",
      "\t Training loss (single batch): 0.7788171172142029\n",
      "\t Training loss (single batch): 1.516298532485962\n",
      "\t Training loss (single batch): 1.0883986949920654\n",
      "\t Training loss (single batch): 1.3981750011444092\n",
      "\t Training loss (single batch): 0.9632443785667419\n",
      "\t Training loss (single batch): 1.490100383758545\n",
      "\t Training loss (single batch): 0.9150726199150085\n",
      "\t Training loss (single batch): 1.2381447553634644\n",
      "\t Training loss (single batch): 1.4265401363372803\n",
      "\t Training loss (single batch): 1.1522822380065918\n",
      "\t Training loss (single batch): 1.519718050956726\n",
      "\t Training loss (single batch): 0.8241521120071411\n",
      "\t Training loss (single batch): 1.0863051414489746\n",
      "\t Training loss (single batch): 1.1493347883224487\n",
      "\t Training loss (single batch): 1.0081791877746582\n",
      "\t Training loss (single batch): 1.1963975429534912\n",
      "\t Training loss (single batch): 1.6345797777175903\n",
      "\t Training loss (single batch): 0.918461799621582\n",
      "\t Training loss (single batch): 1.2058383226394653\n",
      "\t Training loss (single batch): 1.0247728824615479\n",
      "\t Training loss (single batch): 1.2836495637893677\n",
      "\t Training loss (single batch): 1.2712445259094238\n",
      "\t Training loss (single batch): 0.7269659638404846\n",
      "\t Training loss (single batch): 1.0797953605651855\n",
      "\t Training loss (single batch): 1.1667028665542603\n",
      "\t Training loss (single batch): 1.0727299451828003\n",
      "\t Training loss (single batch): 1.4219543933868408\n",
      "\t Training loss (single batch): 1.6558139324188232\n",
      "\t Training loss (single batch): 1.4629496335983276\n",
      "\t Training loss (single batch): 0.7990473508834839\n",
      "\t Training loss (single batch): 1.190077543258667\n",
      "\t Training loss (single batch): 1.1008636951446533\n",
      "\t Training loss (single batch): 1.3562695980072021\n",
      "\t Training loss (single batch): 0.9148185849189758\n",
      "\t Training loss (single batch): 1.09542977809906\n",
      "\t Training loss (single batch): 1.6334601640701294\n",
      "\t Training loss (single batch): 1.270728349685669\n",
      "\t Training loss (single batch): 1.0008853673934937\n",
      "\t Training loss (single batch): 1.613207221031189\n",
      "\t Training loss (single batch): 1.8108676671981812\n",
      "\t Training loss (single batch): 0.8689060211181641\n",
      "\t Training loss (single batch): 1.559095859527588\n",
      "\t Training loss (single batch): 0.8705281615257263\n",
      "\t Training loss (single batch): 0.8900449275970459\n",
      "\t Training loss (single batch): 1.0586585998535156\n",
      "\t Training loss (single batch): 1.2751237154006958\n",
      "\t Training loss (single batch): 1.026820421218872\n",
      "\t Training loss (single batch): 1.004140019416809\n",
      "\t Training loss (single batch): 1.2251107692718506\n",
      "\t Training loss (single batch): 1.0827996730804443\n",
      "\t Training loss (single batch): 1.8060306310653687\n",
      "\t Training loss (single batch): 1.293188214302063\n",
      "\t Training loss (single batch): 1.5159177780151367\n",
      "\t Training loss (single batch): 1.9420900344848633\n",
      "\t Training loss (single batch): 0.8993297815322876\n",
      "\t Training loss (single batch): 0.7282618880271912\n",
      "\t Training loss (single batch): 0.9204172492027283\n",
      "\t Training loss (single batch): 1.2575159072875977\n",
      "\t Training loss (single batch): 1.3631466627120972\n",
      "\t Training loss (single batch): 0.7319759726524353\n",
      "\t Training loss (single batch): 0.8720800280570984\n",
      "\t Training loss (single batch): 0.6876196265220642\n",
      "\t Training loss (single batch): 1.353291630744934\n",
      "\t Training loss (single batch): 1.190310001373291\n",
      "\t Training loss (single batch): 1.0099589824676514\n",
      "\t Training loss (single batch): 1.3969378471374512\n",
      "\t Training loss (single batch): 1.037748098373413\n",
      "\t Training loss (single batch): 1.0497252941131592\n",
      "\t Training loss (single batch): 1.2654904127120972\n",
      "\t Training loss (single batch): 1.0514001846313477\n",
      "\t Training loss (single batch): 1.685569405555725\n",
      "\t Training loss (single batch): 0.9728071689605713\n",
      "\t Training loss (single batch): 2.1187524795532227\n",
      "\t Training loss (single batch): 1.196406364440918\n",
      "\t Training loss (single batch): 1.6739014387130737\n",
      "\t Training loss (single batch): 1.2787433862686157\n",
      "\t Training loss (single batch): 1.6323094367980957\n",
      "\t Training loss (single batch): 1.354783058166504\n",
      "\t Training loss (single batch): 1.6024503707885742\n",
      "\t Training loss (single batch): 1.7477082014083862\n",
      "\t Training loss (single batch): 0.6965564489364624\n",
      "\t Training loss (single batch): 1.467315673828125\n",
      "\t Training loss (single batch): 1.105437994003296\n",
      "\t Training loss (single batch): 1.1585948467254639\n",
      "\t Training loss (single batch): 1.1139237880706787\n",
      "\t Training loss (single batch): 0.8142753839492798\n",
      "\t Training loss (single batch): 1.074468731880188\n",
      "\t Training loss (single batch): 1.3298306465148926\n",
      "\t Training loss (single batch): 1.2054435014724731\n",
      "\t Training loss (single batch): 1.4171388149261475\n",
      "\t Training loss (single batch): 1.1837692260742188\n",
      "\t Training loss (single batch): 1.4507777690887451\n",
      "\t Training loss (single batch): 0.9842482805252075\n",
      "\t Training loss (single batch): 1.0058345794677734\n",
      "\t Training loss (single batch): 1.5966273546218872\n",
      "\t Training loss (single batch): 1.469868540763855\n",
      "\t Training loss (single batch): 0.8690304160118103\n",
      "\t Training loss (single batch): 0.9288607239723206\n",
      "\t Training loss (single batch): 1.3104572296142578\n",
      "\t Training loss (single batch): 0.8010589480400085\n",
      "\t Training loss (single batch): 1.1502571105957031\n",
      "\t Training loss (single batch): 0.9097527265548706\n",
      "\t Training loss (single batch): 1.2222568988800049\n",
      "\t Training loss (single batch): 0.849884033203125\n",
      "\t Training loss (single batch): 1.3896840810775757\n",
      "\t Training loss (single batch): 1.0754438638687134\n",
      "\t Training loss (single batch): 1.1828155517578125\n",
      "\t Training loss (single batch): 1.451378345489502\n",
      "\t Training loss (single batch): 0.9528207778930664\n",
      "\t Training loss (single batch): 1.1766437292099\n",
      "\t Training loss (single batch): 1.1061291694641113\n",
      "\t Training loss (single batch): 1.274531364440918\n",
      "\t Training loss (single batch): 1.060265302658081\n",
      "\t Training loss (single batch): 0.7949116826057434\n",
      "\t Training loss (single batch): 0.7496862411499023\n",
      "\t Training loss (single batch): 1.5940724611282349\n",
      "\t Training loss (single batch): 1.0654523372650146\n",
      "\t Training loss (single batch): 1.5669732093811035\n",
      "\t Training loss (single batch): 0.7366877198219299\n",
      "\t Training loss (single batch): 1.3689645528793335\n",
      "\t Training loss (single batch): 1.104960560798645\n",
      "\t Training loss (single batch): 1.4596095085144043\n",
      "\t Training loss (single batch): 1.1554474830627441\n",
      "\t Training loss (single batch): 1.6525285243988037\n",
      "\t Training loss (single batch): 1.020185947418213\n",
      "\t Training loss (single batch): 1.4606153964996338\n",
      "\t Training loss (single batch): 1.1212356090545654\n",
      "\t Training loss (single batch): 0.9878321886062622\n",
      "\t Training loss (single batch): 1.1126540899276733\n",
      "\t Training loss (single batch): 0.902250349521637\n",
      "\t Training loss (single batch): 1.1641566753387451\n",
      "\t Training loss (single batch): 1.1756000518798828\n",
      "\t Training loss (single batch): 1.4053431749343872\n",
      "\t Training loss (single batch): 1.1048049926757812\n",
      "\t Training loss (single batch): 1.0910112857818604\n",
      "\t Training loss (single batch): 1.5803982019424438\n",
      "\t Training loss (single batch): 1.8068320751190186\n",
      "\t Training loss (single batch): 0.9234005808830261\n",
      "\t Training loss (single batch): 1.173251748085022\n",
      "\t Training loss (single batch): 1.2338370084762573\n",
      "\t Training loss (single batch): 1.2773786783218384\n",
      "\t Training loss (single batch): 1.192232608795166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1171261072158813\n",
      "\t Training loss (single batch): 1.4049372673034668\n",
      "\t Training loss (single batch): 1.232984185218811\n",
      "\t Training loss (single batch): 1.3193655014038086\n",
      "\t Training loss (single batch): 1.1662368774414062\n",
      "\t Training loss (single batch): 1.1803090572357178\n",
      "\t Training loss (single batch): 1.1100225448608398\n",
      "\t Training loss (single batch): 0.7515767812728882\n",
      "\t Training loss (single batch): 1.72799813747406\n",
      "\t Training loss (single batch): 1.0684475898742676\n",
      "\t Training loss (single batch): 1.4600367546081543\n",
      "\t Training loss (single batch): 1.3494336605072021\n",
      "\t Training loss (single batch): 1.4530646800994873\n",
      "\t Training loss (single batch): 1.0634732246398926\n",
      "\t Training loss (single batch): 0.7375722527503967\n",
      "\t Training loss (single batch): 1.4860517978668213\n",
      "\t Training loss (single batch): 1.5055797100067139\n",
      "\t Training loss (single batch): 1.553463101387024\n",
      "\t Training loss (single batch): 1.3060096502304077\n",
      "\t Training loss (single batch): 1.122002124786377\n",
      "\t Training loss (single batch): 1.4972243309020996\n",
      "\t Training loss (single batch): 1.2709881067276\n",
      "\t Training loss (single batch): 1.4790401458740234\n",
      "\t Training loss (single batch): 1.3280081748962402\n",
      "\t Training loss (single batch): 0.9881517291069031\n",
      "\t Training loss (single batch): 1.1691282987594604\n",
      "\t Training loss (single batch): 1.4642072916030884\n",
      "\t Training loss (single batch): 1.2278703451156616\n",
      "\t Training loss (single batch): 1.1783480644226074\n",
      "\t Training loss (single batch): 1.3178046941757202\n",
      "\t Training loss (single batch): 1.1888501644134521\n",
      "\t Training loss (single batch): 1.1057448387145996\n",
      "\t Training loss (single batch): 1.1115893125534058\n",
      "\t Training loss (single batch): 1.0684558153152466\n",
      "\t Training loss (single batch): 1.3703874349594116\n",
      "\t Training loss (single batch): 1.3988789319992065\n",
      "\t Training loss (single batch): 1.293405294418335\n",
      "\t Training loss (single batch): 1.3646844625473022\n",
      "\t Training loss (single batch): 1.122348666191101\n",
      "\t Training loss (single batch): 1.0644506216049194\n",
      "\t Training loss (single batch): 1.024850845336914\n",
      "\t Training loss (single batch): 0.8012428879737854\n",
      "\t Training loss (single batch): 0.9279965162277222\n",
      "\t Training loss (single batch): 0.7013729214668274\n",
      "\t Training loss (single batch): 1.0901098251342773\n",
      "\t Training loss (single batch): 1.4071675539016724\n",
      "\t Training loss (single batch): 1.1176809072494507\n",
      "\t Training loss (single batch): 0.7517523169517517\n",
      "\t Training loss (single batch): 1.245937466621399\n",
      "\t Training loss (single batch): 0.8695436120033264\n",
      "\t Training loss (single batch): 1.1681379079818726\n",
      "\t Training loss (single batch): 1.3459069728851318\n",
      "\t Training loss (single batch): 0.944734513759613\n",
      "\t Training loss (single batch): 1.432965636253357\n",
      "\t Training loss (single batch): 1.1612274646759033\n",
      "\t Training loss (single batch): 1.5163520574569702\n",
      "\t Training loss (single batch): 1.0137335062026978\n",
      "\t Training loss (single batch): 1.0048699378967285\n",
      "\t Training loss (single batch): 1.2001233100891113\n",
      "\t Training loss (single batch): 0.8872116804122925\n",
      "\t Training loss (single batch): 1.5613842010498047\n",
      "\t Training loss (single batch): 1.0048644542694092\n",
      "\t Training loss (single batch): 1.5893794298171997\n",
      "\t Training loss (single batch): 1.3239797353744507\n",
      "\t Training loss (single batch): 1.0905710458755493\n",
      "\t Training loss (single batch): 0.8681976795196533\n",
      "\t Training loss (single batch): 1.4694023132324219\n",
      "\t Training loss (single batch): 1.0215914249420166\n",
      "\t Training loss (single batch): 1.186510443687439\n",
      "\t Training loss (single batch): 1.6029412746429443\n",
      "\t Training loss (single batch): 0.9524195790290833\n",
      "\t Training loss (single batch): 0.8339746594429016\n",
      "\t Training loss (single batch): 1.4399998188018799\n",
      "\t Training loss (single batch): 1.0733963251113892\n",
      "\t Training loss (single batch): 1.2566895484924316\n",
      "\t Training loss (single batch): 1.8160659074783325\n",
      "\t Training loss (single batch): 1.14762544631958\n",
      "\t Training loss (single batch): 1.145371675491333\n",
      "\t Training loss (single batch): 1.0784024000167847\n",
      "\t Training loss (single batch): 0.9126268029212952\n",
      "\t Training loss (single batch): 1.320629596710205\n",
      "\t Training loss (single batch): 0.8659833669662476\n",
      "\t Training loss (single batch): 1.2088571786880493\n",
      "\t Training loss (single batch): 1.435538411140442\n",
      "\t Training loss (single batch): 1.0163650512695312\n",
      "\t Training loss (single batch): 1.6020153760910034\n",
      "\t Training loss (single batch): 0.8760138750076294\n",
      "\t Training loss (single batch): 1.290202260017395\n",
      "\t Training loss (single batch): 0.9346681833267212\n",
      "\t Training loss (single batch): 1.22132408618927\n",
      "\t Training loss (single batch): 1.0965299606323242\n",
      "\t Training loss (single batch): 1.2444813251495361\n",
      "\t Training loss (single batch): 1.342218279838562\n",
      "\t Training loss (single batch): 0.7973669171333313\n",
      "\t Training loss (single batch): 0.7640085816383362\n",
      "\t Training loss (single batch): 1.0670548677444458\n",
      "\t Training loss (single batch): 0.8917595148086548\n",
      "\t Training loss (single batch): 1.034655213356018\n",
      "\t Training loss (single batch): 1.2664943933486938\n",
      "\t Training loss (single batch): 0.813960611820221\n",
      "\t Training loss (single batch): 1.1529396772384644\n",
      "\t Training loss (single batch): 1.3566104173660278\n",
      "\t Training loss (single batch): 1.4177806377410889\n",
      "\t Training loss (single batch): 1.2335774898529053\n",
      "\t Training loss (single batch): 0.9230566620826721\n",
      "\t Training loss (single batch): 0.7184005379676819\n",
      "\t Training loss (single batch): 1.426820993423462\n",
      "\t Training loss (single batch): 1.4181309938430786\n",
      "\t Training loss (single batch): 0.7322794795036316\n",
      "\t Training loss (single batch): 1.1328346729278564\n",
      "\t Training loss (single batch): 1.4499435424804688\n",
      "\t Training loss (single batch): 0.9176076054573059\n",
      "\t Training loss (single batch): 1.0116310119628906\n",
      "\t Training loss (single batch): 1.1153299808502197\n",
      "\t Training loss (single batch): 1.120976448059082\n",
      "\t Training loss (single batch): 1.157353401184082\n",
      "\t Training loss (single batch): 1.2464605569839478\n",
      "\t Training loss (single batch): 1.0060890913009644\n",
      "\t Training loss (single batch): 1.0206613540649414\n",
      "\t Training loss (single batch): 1.1020808219909668\n",
      "\t Training loss (single batch): 1.0872364044189453\n",
      "\t Training loss (single batch): 1.1939811706542969\n",
      "\t Training loss (single batch): 0.8295875787734985\n",
      "\t Training loss (single batch): 1.5035258531570435\n",
      "\t Training loss (single batch): 1.1636401414871216\n",
      "\t Training loss (single batch): 1.8361443281173706\n",
      "\t Training loss (single batch): 0.8737304210662842\n",
      "\t Training loss (single batch): 0.8898057341575623\n",
      "\t Training loss (single batch): 1.195783257484436\n",
      "\t Training loss (single batch): 1.0373904705047607\n",
      "\t Training loss (single batch): 1.495009422302246\n",
      "\t Training loss (single batch): 1.2639548778533936\n",
      "\t Training loss (single batch): 1.2162007093429565\n",
      "\t Training loss (single batch): 1.1190533638000488\n",
      "\t Training loss (single batch): 1.0530076026916504\n",
      "\t Training loss (single batch): 0.9355564713478088\n",
      "\t Training loss (single batch): 0.8615459203720093\n",
      "\t Training loss (single batch): 1.056984305381775\n",
      "\t Training loss (single batch): 0.770934522151947\n",
      "\t Training loss (single batch): 1.2212481498718262\n",
      "\t Training loss (single batch): 1.059936761856079\n",
      "\t Training loss (single batch): 1.0443543195724487\n",
      "\t Training loss (single batch): 1.3182363510131836\n",
      "\t Training loss (single batch): 1.5707777738571167\n",
      "\t Training loss (single batch): 1.5123488903045654\n",
      "\t Training loss (single batch): 0.9570391178131104\n",
      "\t Training loss (single batch): 0.7698402404785156\n",
      "\t Training loss (single batch): 0.9933991432189941\n",
      "\t Training loss (single batch): 0.8931843042373657\n",
      "\t Training loss (single batch): 0.9181623458862305\n",
      "\t Training loss (single batch): 0.8577216267585754\n",
      "\t Training loss (single batch): 1.0707201957702637\n",
      "\t Training loss (single batch): 1.4410407543182373\n",
      "\t Training loss (single batch): 0.9403043389320374\n",
      "\t Training loss (single batch): 1.0494502782821655\n",
      "\t Training loss (single batch): 0.9030407071113586\n",
      "\t Training loss (single batch): 1.243216633796692\n",
      "\t Training loss (single batch): 1.3042830228805542\n",
      "\t Training loss (single batch): 1.2072559595108032\n",
      "\t Training loss (single batch): 0.6282772421836853\n",
      "##################################\n",
      "## EPOCH 79\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2619818449020386\n",
      "\t Training loss (single batch): 1.337450385093689\n",
      "\t Training loss (single batch): 0.9074493646621704\n",
      "\t Training loss (single batch): 1.4665359258651733\n",
      "\t Training loss (single batch): 1.3857944011688232\n",
      "\t Training loss (single batch): 1.3543390035629272\n",
      "\t Training loss (single batch): 1.224375605583191\n",
      "\t Training loss (single batch): 1.1004884243011475\n",
      "\t Training loss (single batch): 1.1197324991226196\n",
      "\t Training loss (single batch): 1.08236825466156\n",
      "\t Training loss (single batch): 1.4860246181488037\n",
      "\t Training loss (single batch): 1.0480337142944336\n",
      "\t Training loss (single batch): 0.7176045775413513\n",
      "\t Training loss (single batch): 0.6184345483779907\n",
      "\t Training loss (single batch): 1.350938081741333\n",
      "\t Training loss (single batch): 1.2213525772094727\n",
      "\t Training loss (single batch): 1.443288803100586\n",
      "\t Training loss (single batch): 1.4323755502700806\n",
      "\t Training loss (single batch): 0.8811115622520447\n",
      "\t Training loss (single batch): 1.2920453548431396\n",
      "\t Training loss (single batch): 1.3410543203353882\n",
      "\t Training loss (single batch): 0.5926272869110107\n",
      "\t Training loss (single batch): 1.0688753128051758\n",
      "\t Training loss (single batch): 1.5238336324691772\n",
      "\t Training loss (single batch): 1.1898525953292847\n",
      "\t Training loss (single batch): 0.8558007478713989\n",
      "\t Training loss (single batch): 1.3252581357955933\n",
      "\t Training loss (single batch): 1.210044026374817\n",
      "\t Training loss (single batch): 1.2164478302001953\n",
      "\t Training loss (single batch): 1.0631617307662964\n",
      "\t Training loss (single batch): 0.9559344053268433\n",
      "\t Training loss (single batch): 1.6886564493179321\n",
      "\t Training loss (single batch): 1.2830936908721924\n",
      "\t Training loss (single batch): 1.155982494354248\n",
      "\t Training loss (single batch): 0.6685858368873596\n",
      "\t Training loss (single batch): 0.6330496668815613\n",
      "\t Training loss (single batch): 0.48416849970817566\n",
      "\t Training loss (single batch): 0.9081887006759644\n",
      "\t Training loss (single batch): 0.7930256724357605\n",
      "\t Training loss (single batch): 1.8164043426513672\n",
      "\t Training loss (single batch): 1.7618286609649658\n",
      "\t Training loss (single batch): 1.7829686403274536\n",
      "\t Training loss (single batch): 1.5733014345169067\n",
      "\t Training loss (single batch): 1.0067899227142334\n",
      "\t Training loss (single batch): 1.402074933052063\n",
      "\t Training loss (single batch): 0.9124922156333923\n",
      "\t Training loss (single batch): 1.2046735286712646\n",
      "\t Training loss (single batch): 1.1789324283599854\n",
      "\t Training loss (single batch): 1.3988380432128906\n",
      "\t Training loss (single batch): 1.3963590860366821\n",
      "\t Training loss (single batch): 1.206971287727356\n",
      "\t Training loss (single batch): 1.3004374504089355\n",
      "\t Training loss (single batch): 0.9652481079101562\n",
      "\t Training loss (single batch): 0.8162133693695068\n",
      "\t Training loss (single batch): 1.248052954673767\n",
      "\t Training loss (single batch): 0.964574933052063\n",
      "\t Training loss (single batch): 0.9635490775108337\n",
      "\t Training loss (single batch): 1.2469744682312012\n",
      "\t Training loss (single batch): 0.7695430517196655\n",
      "\t Training loss (single batch): 1.1260095834732056\n",
      "\t Training loss (single batch): 1.768842101097107\n",
      "\t Training loss (single batch): 0.7813265919685364\n",
      "\t Training loss (single batch): 0.8141375184059143\n",
      "\t Training loss (single batch): 1.24738347530365\n",
      "\t Training loss (single batch): 1.354770541191101\n",
      "\t Training loss (single batch): 0.8728227019309998\n",
      "\t Training loss (single batch): 0.9287047982215881\n",
      "\t Training loss (single batch): 1.445385456085205\n",
      "\t Training loss (single batch): 1.0593351125717163\n",
      "\t Training loss (single batch): 1.233375072479248\n",
      "\t Training loss (single batch): 0.9646898508071899\n",
      "\t Training loss (single batch): 1.105176568031311\n",
      "\t Training loss (single batch): 0.8172450065612793\n",
      "\t Training loss (single batch): 1.3579622507095337\n",
      "\t Training loss (single batch): 0.942361056804657\n",
      "\t Training loss (single batch): 1.0151108503341675\n",
      "\t Training loss (single batch): 1.2988444566726685\n",
      "\t Training loss (single batch): 1.453116536140442\n",
      "\t Training loss (single batch): 1.287522792816162\n",
      "\t Training loss (single batch): 0.7327668070793152\n",
      "\t Training loss (single batch): 1.0860010385513306\n",
      "\t Training loss (single batch): 0.8272461295127869\n",
      "\t Training loss (single batch): 1.1023447513580322\n",
      "\t Training loss (single batch): 1.0841608047485352\n",
      "\t Training loss (single batch): 1.2424527406692505\n",
      "\t Training loss (single batch): 1.250346064567566\n",
      "\t Training loss (single batch): 1.2089362144470215\n",
      "\t Training loss (single batch): 1.1987714767456055\n",
      "\t Training loss (single batch): 1.1543238162994385\n",
      "\t Training loss (single batch): 0.9983006715774536\n",
      "\t Training loss (single batch): 0.8838129043579102\n",
      "\t Training loss (single batch): 1.1075217723846436\n",
      "\t Training loss (single batch): 1.0563883781433105\n",
      "\t Training loss (single batch): 1.4561610221862793\n",
      "\t Training loss (single batch): 0.8598997592926025\n",
      "\t Training loss (single batch): 1.5028531551361084\n",
      "\t Training loss (single batch): 0.9817284941673279\n",
      "\t Training loss (single batch): 1.239516019821167\n",
      "\t Training loss (single batch): 0.8230794668197632\n",
      "\t Training loss (single batch): 1.2306466102600098\n",
      "\t Training loss (single batch): 1.2446975708007812\n",
      "\t Training loss (single batch): 1.2299456596374512\n",
      "\t Training loss (single batch): 1.5273628234863281\n",
      "\t Training loss (single batch): 1.1242808103561401\n",
      "\t Training loss (single batch): 1.025124192237854\n",
      "\t Training loss (single batch): 1.2025270462036133\n",
      "\t Training loss (single batch): 1.2148346900939941\n",
      "\t Training loss (single batch): 0.7894439697265625\n",
      "\t Training loss (single batch): 1.042899250984192\n",
      "\t Training loss (single batch): 1.0274895429611206\n",
      "\t Training loss (single batch): 0.8697895407676697\n",
      "\t Training loss (single batch): 0.6980429887771606\n",
      "\t Training loss (single batch): 1.2259210348129272\n",
      "\t Training loss (single batch): 1.049786925315857\n",
      "\t Training loss (single batch): 0.8644239902496338\n",
      "\t Training loss (single batch): 0.9983296394348145\n",
      "\t Training loss (single batch): 1.2185122966766357\n",
      "\t Training loss (single batch): 1.1775985956192017\n",
      "\t Training loss (single batch): 1.2279634475708008\n",
      "\t Training loss (single batch): 1.0780807733535767\n",
      "\t Training loss (single batch): 1.7432249784469604\n",
      "\t Training loss (single batch): 1.1268830299377441\n",
      "\t Training loss (single batch): 0.8758255243301392\n",
      "\t Training loss (single batch): 1.5705187320709229\n",
      "\t Training loss (single batch): 1.3379194736480713\n",
      "\t Training loss (single batch): 1.027626872062683\n",
      "\t Training loss (single batch): 1.2088518142700195\n",
      "\t Training loss (single batch): 0.8243826031684875\n",
      "\t Training loss (single batch): 1.251036286354065\n",
      "\t Training loss (single batch): 1.0097090005874634\n",
      "\t Training loss (single batch): 1.2665907144546509\n",
      "\t Training loss (single batch): 1.2083312273025513\n",
      "\t Training loss (single batch): 1.2949568033218384\n",
      "\t Training loss (single batch): 1.8581701517105103\n",
      "\t Training loss (single batch): 1.0938067436218262\n",
      "\t Training loss (single batch): 1.0302753448486328\n",
      "\t Training loss (single batch): 0.936298668384552\n",
      "\t Training loss (single batch): 1.0450068712234497\n",
      "\t Training loss (single batch): 0.9699419736862183\n",
      "\t Training loss (single batch): 1.5106265544891357\n",
      "\t Training loss (single batch): 1.15883469581604\n",
      "\t Training loss (single batch): 1.2614490985870361\n",
      "\t Training loss (single batch): 1.4436818361282349\n",
      "\t Training loss (single batch): 1.1942269802093506\n",
      "\t Training loss (single batch): 0.7230200171470642\n",
      "\t Training loss (single batch): 1.1703836917877197\n",
      "\t Training loss (single batch): 1.1194016933441162\n",
      "\t Training loss (single batch): 1.0951825380325317\n",
      "\t Training loss (single batch): 0.7776760458946228\n",
      "\t Training loss (single batch): 0.8415752649307251\n",
      "\t Training loss (single batch): 1.152031660079956\n",
      "\t Training loss (single batch): 1.0000104904174805\n",
      "\t Training loss (single batch): 0.7305277585983276\n",
      "\t Training loss (single batch): 1.2346378564834595\n",
      "\t Training loss (single batch): 0.9864988923072815\n",
      "\t Training loss (single batch): 0.9106401205062866\n",
      "\t Training loss (single batch): 1.2024718523025513\n",
      "\t Training loss (single batch): 1.0353447198867798\n",
      "\t Training loss (single batch): 1.2089251279830933\n",
      "\t Training loss (single batch): 0.9898738265037537\n",
      "\t Training loss (single batch): 1.0115488767623901\n",
      "\t Training loss (single batch): 0.9437790513038635\n",
      "\t Training loss (single batch): 1.1782026290893555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.258399248123169\n",
      "\t Training loss (single batch): 0.9427908062934875\n",
      "\t Training loss (single batch): 1.3267712593078613\n",
      "\t Training loss (single batch): 1.5837132930755615\n",
      "\t Training loss (single batch): 0.8920140266418457\n",
      "\t Training loss (single batch): 1.1228528022766113\n",
      "\t Training loss (single batch): 0.8782282471656799\n",
      "\t Training loss (single batch): 1.0002719163894653\n",
      "\t Training loss (single batch): 1.4852471351623535\n",
      "\t Training loss (single batch): 1.471845269203186\n",
      "\t Training loss (single batch): 1.1064821481704712\n",
      "\t Training loss (single batch): 1.2237582206726074\n",
      "\t Training loss (single batch): 1.1189968585968018\n",
      "\t Training loss (single batch): 1.321867823600769\n",
      "\t Training loss (single batch): 1.4994046688079834\n",
      "\t Training loss (single batch): 0.9691087603569031\n",
      "\t Training loss (single batch): 1.1431134939193726\n",
      "\t Training loss (single batch): 1.0620430707931519\n",
      "\t Training loss (single batch): 1.049239993095398\n",
      "\t Training loss (single batch): 1.165444254875183\n",
      "\t Training loss (single batch): 1.724783182144165\n",
      "\t Training loss (single batch): 1.3548297882080078\n",
      "\t Training loss (single batch): 1.2895169258117676\n",
      "\t Training loss (single batch): 1.4844759702682495\n",
      "\t Training loss (single batch): 0.600684404373169\n",
      "\t Training loss (single batch): 0.9798988699913025\n",
      "\t Training loss (single batch): 1.0156224966049194\n",
      "\t Training loss (single batch): 0.8436277508735657\n",
      "\t Training loss (single batch): 0.7928178906440735\n",
      "\t Training loss (single batch): 0.9233147501945496\n",
      "\t Training loss (single batch): 0.9878301024436951\n",
      "\t Training loss (single batch): 1.2262109518051147\n",
      "\t Training loss (single batch): 0.8491189479827881\n",
      "\t Training loss (single batch): 0.82117760181427\n",
      "\t Training loss (single batch): 1.326653242111206\n",
      "\t Training loss (single batch): 1.132102131843567\n",
      "\t Training loss (single batch): 1.1558791399002075\n",
      "\t Training loss (single batch): 1.2558655738830566\n",
      "\t Training loss (single batch): 0.8404113054275513\n",
      "\t Training loss (single batch): 1.0405017137527466\n",
      "\t Training loss (single batch): 1.5767943859100342\n",
      "\t Training loss (single batch): 0.9945446848869324\n",
      "\t Training loss (single batch): 0.9040994048118591\n",
      "\t Training loss (single batch): 1.2825299501419067\n",
      "\t Training loss (single batch): 1.0305465459823608\n",
      "\t Training loss (single batch): 1.3538259267807007\n",
      "\t Training loss (single batch): 1.308442234992981\n",
      "\t Training loss (single batch): 1.4571133852005005\n",
      "\t Training loss (single batch): 1.505125880241394\n",
      "\t Training loss (single batch): 1.0815825462341309\n",
      "\t Training loss (single batch): 1.3134582042694092\n",
      "\t Training loss (single batch): 0.9893732070922852\n",
      "\t Training loss (single batch): 1.2929866313934326\n",
      "\t Training loss (single batch): 1.371314287185669\n",
      "\t Training loss (single batch): 1.1142498254776\n",
      "\t Training loss (single batch): 1.4359928369522095\n",
      "\t Training loss (single batch): 1.307875633239746\n",
      "\t Training loss (single batch): 1.2305357456207275\n",
      "\t Training loss (single batch): 1.175390362739563\n",
      "\t Training loss (single batch): 1.2088602781295776\n",
      "\t Training loss (single batch): 0.7595018744468689\n",
      "\t Training loss (single batch): 0.9832943081855774\n",
      "\t Training loss (single batch): 1.2403889894485474\n",
      "\t Training loss (single batch): 1.2399535179138184\n",
      "\t Training loss (single batch): 1.1143392324447632\n",
      "\t Training loss (single batch): 0.9635195136070251\n",
      "\t Training loss (single batch): 0.896612823009491\n",
      "\t Training loss (single batch): 1.0829788446426392\n",
      "\t Training loss (single batch): 1.1999690532684326\n",
      "\t Training loss (single batch): 1.2098299264907837\n",
      "\t Training loss (single batch): 1.4205371141433716\n",
      "\t Training loss (single batch): 1.5709644556045532\n",
      "\t Training loss (single batch): 1.0368107557296753\n",
      "\t Training loss (single batch): 1.0156960487365723\n",
      "\t Training loss (single batch): 1.4482378959655762\n",
      "\t Training loss (single batch): 1.2748119831085205\n",
      "\t Training loss (single batch): 1.2556706666946411\n",
      "\t Training loss (single batch): 0.962049126625061\n",
      "\t Training loss (single batch): 0.8908990621566772\n",
      "\t Training loss (single batch): 1.0965824127197266\n",
      "\t Training loss (single batch): 0.9542021751403809\n",
      "\t Training loss (single batch): 0.9841954708099365\n",
      "\t Training loss (single batch): 1.5458309650421143\n",
      "\t Training loss (single batch): 1.8008568286895752\n",
      "\t Training loss (single batch): 1.0539017915725708\n",
      "\t Training loss (single batch): 1.53916597366333\n",
      "\t Training loss (single batch): 1.280225157737732\n",
      "\t Training loss (single batch): 1.4335414171218872\n",
      "\t Training loss (single batch): 0.8376148343086243\n",
      "\t Training loss (single batch): 1.3546340465545654\n",
      "\t Training loss (single batch): 1.1057723760604858\n",
      "\t Training loss (single batch): 1.3515719175338745\n",
      "\t Training loss (single batch): 1.7082173824310303\n",
      "\t Training loss (single batch): 1.08568274974823\n",
      "\t Training loss (single batch): 1.0747783184051514\n",
      "\t Training loss (single batch): 1.557280421257019\n",
      "\t Training loss (single batch): 1.417363166809082\n",
      "\t Training loss (single batch): 1.4709913730621338\n",
      "\t Training loss (single batch): 1.1809159517288208\n",
      "\t Training loss (single batch): 1.4408411979675293\n",
      "\t Training loss (single batch): 1.219680905342102\n",
      "\t Training loss (single batch): 1.338843822479248\n",
      "\t Training loss (single batch): 0.674265444278717\n",
      "\t Training loss (single batch): 1.196096658706665\n",
      "\t Training loss (single batch): 1.3706523180007935\n",
      "\t Training loss (single batch): 1.1280338764190674\n",
      "\t Training loss (single batch): 1.51474130153656\n",
      "\t Training loss (single batch): 1.2691216468811035\n",
      "\t Training loss (single batch): 1.5031319856643677\n",
      "\t Training loss (single batch): 1.2604681253433228\n",
      "\t Training loss (single batch): 0.7607297897338867\n",
      "\t Training loss (single batch): 1.6443132162094116\n",
      "\t Training loss (single batch): 1.1599851846694946\n",
      "\t Training loss (single batch): 1.1961344480514526\n",
      "\t Training loss (single batch): 1.25905442237854\n",
      "\t Training loss (single batch): 0.810555636882782\n",
      "\t Training loss (single batch): 1.0620661973953247\n",
      "\t Training loss (single batch): 1.3361939191818237\n",
      "\t Training loss (single batch): 1.2176513671875\n",
      "\t Training loss (single batch): 1.2909770011901855\n",
      "\t Training loss (single batch): 1.2316839694976807\n",
      "\t Training loss (single batch): 1.395763874053955\n",
      "\t Training loss (single batch): 1.0670413970947266\n",
      "\t Training loss (single batch): 1.0943118333816528\n",
      "\t Training loss (single batch): 0.8725950717926025\n",
      "\t Training loss (single batch): 0.9212027788162231\n",
      "\t Training loss (single batch): 1.3372910022735596\n",
      "\t Training loss (single batch): 1.2131952047348022\n",
      "\t Training loss (single batch): 1.0257676839828491\n",
      "\t Training loss (single batch): 1.295047402381897\n",
      "\t Training loss (single batch): 1.7931288480758667\n",
      "\t Training loss (single batch): 0.9718480706214905\n",
      "\t Training loss (single batch): 0.8399415016174316\n",
      "\t Training loss (single batch): 1.6203383207321167\n",
      "\t Training loss (single batch): 1.1986862421035767\n",
      "\t Training loss (single batch): 0.9974807500839233\n",
      "\t Training loss (single batch): 1.389159917831421\n",
      "\t Training loss (single batch): 1.1684415340423584\n",
      "\t Training loss (single batch): 1.2686790227890015\n",
      "\t Training loss (single batch): 1.2740262746810913\n",
      "\t Training loss (single batch): 1.122147798538208\n",
      "\t Training loss (single batch): 0.8164737820625305\n",
      "\t Training loss (single batch): 1.0814826488494873\n",
      "\t Training loss (single batch): 1.557279109954834\n",
      "\t Training loss (single batch): 1.2836986780166626\n",
      "\t Training loss (single batch): 1.1373733282089233\n",
      "\t Training loss (single batch): 0.8554537296295166\n",
      "\t Training loss (single batch): 1.0080068111419678\n",
      "\t Training loss (single batch): 0.7991030216217041\n",
      "\t Training loss (single batch): 1.5405694246292114\n",
      "\t Training loss (single batch): 1.171099305152893\n",
      "\t Training loss (single batch): 1.2102044820785522\n",
      "\t Training loss (single batch): 1.0950348377227783\n",
      "\t Training loss (single batch): 0.7898428440093994\n",
      "##################################\n",
      "## EPOCH 80\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2486311197280884\n",
      "\t Training loss (single batch): 1.1115449666976929\n",
      "\t Training loss (single batch): 1.4594939947128296\n",
      "\t Training loss (single batch): 1.450735092163086\n",
      "\t Training loss (single batch): 0.9361275434494019\n",
      "\t Training loss (single batch): 1.0852925777435303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0812575817108154\n",
      "\t Training loss (single batch): 0.9872981309890747\n",
      "\t Training loss (single batch): 1.4025323390960693\n",
      "\t Training loss (single batch): 0.9562347531318665\n",
      "\t Training loss (single batch): 1.1129447221755981\n",
      "\t Training loss (single batch): 0.9882900714874268\n",
      "\t Training loss (single batch): 0.6763325333595276\n",
      "\t Training loss (single batch): 0.9243420362472534\n",
      "\t Training loss (single batch): 1.2991725206375122\n",
      "\t Training loss (single batch): 0.9692375063896179\n",
      "\t Training loss (single batch): 1.103498935699463\n",
      "\t Training loss (single batch): 0.8680597543716431\n",
      "\t Training loss (single batch): 1.2518872022628784\n",
      "\t Training loss (single batch): 1.5792173147201538\n",
      "\t Training loss (single batch): 1.387442946434021\n",
      "\t Training loss (single batch): 0.8284533619880676\n",
      "\t Training loss (single batch): 1.3874720335006714\n",
      "\t Training loss (single batch): 0.8736631870269775\n",
      "\t Training loss (single batch): 1.3525655269622803\n",
      "\t Training loss (single batch): 1.239638090133667\n",
      "\t Training loss (single batch): 1.3144878149032593\n",
      "\t Training loss (single batch): 0.4985257685184479\n",
      "\t Training loss (single batch): 1.2731431722640991\n",
      "\t Training loss (single batch): 1.3618069887161255\n",
      "\t Training loss (single batch): 1.5511425733566284\n",
      "\t Training loss (single batch): 0.7165612578392029\n",
      "\t Training loss (single batch): 1.036826252937317\n",
      "\t Training loss (single batch): 1.119209885597229\n",
      "\t Training loss (single batch): 0.7486949563026428\n",
      "\t Training loss (single batch): 1.3043286800384521\n",
      "\t Training loss (single batch): 1.5275957584381104\n",
      "\t Training loss (single batch): 0.8533604741096497\n",
      "\t Training loss (single batch): 1.656178593635559\n",
      "\t Training loss (single batch): 1.289968490600586\n",
      "\t Training loss (single batch): 1.1847472190856934\n",
      "\t Training loss (single batch): 1.3188245296478271\n",
      "\t Training loss (single batch): 1.0735039710998535\n",
      "\t Training loss (single batch): 1.278471827507019\n",
      "\t Training loss (single batch): 1.2109216451644897\n",
      "\t Training loss (single batch): 1.0407428741455078\n",
      "\t Training loss (single batch): 1.1469941139221191\n",
      "\t Training loss (single batch): 0.934808075428009\n",
      "\t Training loss (single batch): 1.0293220281600952\n",
      "\t Training loss (single batch): 1.0071102380752563\n",
      "\t Training loss (single batch): 0.8842422962188721\n",
      "\t Training loss (single batch): 1.3519008159637451\n",
      "\t Training loss (single batch): 1.6922293901443481\n",
      "\t Training loss (single batch): 1.029407024383545\n",
      "\t Training loss (single batch): 1.1876212358474731\n",
      "\t Training loss (single batch): 1.3849800825119019\n",
      "\t Training loss (single batch): 1.503844976425171\n",
      "\t Training loss (single batch): 1.3365904092788696\n",
      "\t Training loss (single batch): 1.3327972888946533\n",
      "\t Training loss (single batch): 1.3976701498031616\n",
      "\t Training loss (single batch): 1.0578244924545288\n",
      "\t Training loss (single batch): 1.5774595737457275\n",
      "\t Training loss (single batch): 1.6866878271102905\n",
      "\t Training loss (single batch): 1.13166344165802\n",
      "\t Training loss (single batch): 1.4523757696151733\n",
      "\t Training loss (single batch): 1.1586484909057617\n",
      "\t Training loss (single batch): 1.0406360626220703\n",
      "\t Training loss (single batch): 0.8135764598846436\n",
      "\t Training loss (single batch): 1.0409350395202637\n",
      "\t Training loss (single batch): 1.368280053138733\n",
      "\t Training loss (single batch): 1.1058417558670044\n",
      "\t Training loss (single batch): 0.7962823510169983\n",
      "\t Training loss (single batch): 1.355006456375122\n",
      "\t Training loss (single batch): 0.9791651964187622\n",
      "\t Training loss (single batch): 1.27201247215271\n",
      "\t Training loss (single batch): 1.3376646041870117\n",
      "\t Training loss (single batch): 1.1873371601104736\n",
      "\t Training loss (single batch): 0.8957817554473877\n",
      "\t Training loss (single batch): 0.815250813961029\n",
      "\t Training loss (single batch): 2.107832431793213\n",
      "\t Training loss (single batch): 0.9637119770050049\n",
      "\t Training loss (single batch): 1.2435522079467773\n",
      "\t Training loss (single batch): 1.4642679691314697\n",
      "\t Training loss (single batch): 1.464361548423767\n",
      "\t Training loss (single batch): 1.1837266683578491\n",
      "\t Training loss (single batch): 1.0979421138763428\n",
      "\t Training loss (single batch): 1.5792427062988281\n",
      "\t Training loss (single batch): 1.1880369186401367\n",
      "\t Training loss (single batch): 1.1503241062164307\n",
      "\t Training loss (single batch): 1.0424957275390625\n",
      "\t Training loss (single batch): 1.8508058786392212\n",
      "\t Training loss (single batch): 1.2106544971466064\n",
      "\t Training loss (single batch): 1.6117475032806396\n",
      "\t Training loss (single batch): 1.4958852529525757\n",
      "\t Training loss (single batch): 0.7025017738342285\n",
      "\t Training loss (single batch): 0.9546211361885071\n",
      "\t Training loss (single batch): 1.3004761934280396\n",
      "\t Training loss (single batch): 1.517895221710205\n",
      "\t Training loss (single batch): 0.9530216455459595\n",
      "\t Training loss (single batch): 1.5128110647201538\n",
      "\t Training loss (single batch): 1.6182615756988525\n",
      "\t Training loss (single batch): 1.1540511846542358\n",
      "\t Training loss (single batch): 1.5142877101898193\n",
      "\t Training loss (single batch): 1.274643063545227\n",
      "\t Training loss (single batch): 1.2450181245803833\n",
      "\t Training loss (single batch): 1.3933359384536743\n",
      "\t Training loss (single batch): 0.9808570146560669\n",
      "\t Training loss (single batch): 1.298189640045166\n",
      "\t Training loss (single batch): 1.3757826089859009\n",
      "\t Training loss (single batch): 1.4208593368530273\n",
      "\t Training loss (single batch): 1.39133620262146\n",
      "\t Training loss (single batch): 1.0397741794586182\n",
      "\t Training loss (single batch): 1.290619969367981\n",
      "\t Training loss (single batch): 1.4502822160720825\n",
      "\t Training loss (single batch): 0.8897629380226135\n",
      "\t Training loss (single batch): 1.26334547996521\n",
      "\t Training loss (single batch): 1.081171989440918\n",
      "\t Training loss (single batch): 1.0760250091552734\n",
      "\t Training loss (single batch): 1.5924065113067627\n",
      "\t Training loss (single batch): 1.8404210805892944\n",
      "\t Training loss (single batch): 1.2607237100601196\n",
      "\t Training loss (single batch): 0.698345959186554\n",
      "\t Training loss (single batch): 1.3384898900985718\n",
      "\t Training loss (single batch): 1.4414082765579224\n",
      "\t Training loss (single batch): 1.029221534729004\n",
      "\t Training loss (single batch): 1.2631957530975342\n",
      "\t Training loss (single batch): 1.2453433275222778\n",
      "\t Training loss (single batch): 1.2220970392227173\n",
      "\t Training loss (single batch): 1.5331178903579712\n",
      "\t Training loss (single batch): 1.0797277688980103\n",
      "\t Training loss (single batch): 1.0241647958755493\n",
      "\t Training loss (single batch): 1.0278939008712769\n",
      "\t Training loss (single batch): 0.7575320601463318\n",
      "\t Training loss (single batch): 1.4769940376281738\n",
      "\t Training loss (single batch): 1.646664023399353\n",
      "\t Training loss (single batch): 1.043769359588623\n",
      "\t Training loss (single batch): 1.299181342124939\n",
      "\t Training loss (single batch): 1.3062814474105835\n",
      "\t Training loss (single batch): 1.2060388326644897\n",
      "\t Training loss (single batch): 0.8794919848442078\n",
      "\t Training loss (single batch): 1.1081883907318115\n",
      "\t Training loss (single batch): 0.54315185546875\n",
      "\t Training loss (single batch): 1.9591834545135498\n",
      "\t Training loss (single batch): 0.8108183145523071\n",
      "\t Training loss (single batch): 0.936122477054596\n",
      "\t Training loss (single batch): 1.182738184928894\n",
      "\t Training loss (single batch): 1.141527771949768\n",
      "\t Training loss (single batch): 1.32793128490448\n",
      "\t Training loss (single batch): 0.9138455986976624\n",
      "\t Training loss (single batch): 1.5478880405426025\n",
      "\t Training loss (single batch): 1.016545057296753\n",
      "\t Training loss (single batch): 0.9399039149284363\n",
      "\t Training loss (single batch): 1.0939538478851318\n",
      "\t Training loss (single batch): 1.6450910568237305\n",
      "\t Training loss (single batch): 0.9579703211784363\n",
      "\t Training loss (single batch): 1.3444035053253174\n",
      "\t Training loss (single batch): 1.4148013591766357\n",
      "\t Training loss (single batch): 0.873798131942749\n",
      "\t Training loss (single batch): 1.0867611169815063\n",
      "\t Training loss (single batch): 1.127591609954834\n",
      "\t Training loss (single batch): 1.006474494934082\n",
      "\t Training loss (single batch): 1.1871631145477295\n",
      "\t Training loss (single batch): 0.9940149188041687\n",
      "\t Training loss (single batch): 1.037786841392517\n",
      "\t Training loss (single batch): 1.1190361976623535\n",
      "\t Training loss (single batch): 0.9993878602981567\n",
      "\t Training loss (single batch): 1.1091543436050415\n",
      "\t Training loss (single batch): 0.8137692213058472\n",
      "\t Training loss (single batch): 1.0697351694107056\n",
      "\t Training loss (single batch): 1.4489902257919312\n",
      "\t Training loss (single batch): 1.3626552820205688\n",
      "\t Training loss (single batch): 0.9441716074943542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3397653102874756\n",
      "\t Training loss (single batch): 1.2410509586334229\n",
      "\t Training loss (single batch): 1.618857741355896\n",
      "\t Training loss (single batch): 1.241281509399414\n",
      "\t Training loss (single batch): 1.076650857925415\n",
      "\t Training loss (single batch): 1.1148053407669067\n",
      "\t Training loss (single batch): 1.175896167755127\n",
      "\t Training loss (single batch): 0.8704672455787659\n",
      "\t Training loss (single batch): 0.7877833247184753\n",
      "\t Training loss (single batch): 1.1550382375717163\n",
      "\t Training loss (single batch): 1.206858515739441\n",
      "\t Training loss (single batch): 1.1801165342330933\n",
      "\t Training loss (single batch): 1.350602388381958\n",
      "\t Training loss (single batch): 1.07659912109375\n",
      "\t Training loss (single batch): 1.1223018169403076\n",
      "\t Training loss (single batch): 1.137250304222107\n",
      "\t Training loss (single batch): 1.1076658964157104\n",
      "\t Training loss (single batch): 0.9479774832725525\n",
      "\t Training loss (single batch): 1.4216951131820679\n",
      "\t Training loss (single batch): 1.292969822883606\n",
      "\t Training loss (single batch): 1.4124501943588257\n",
      "\t Training loss (single batch): 1.133709192276001\n",
      "\t Training loss (single batch): 1.5487289428710938\n",
      "\t Training loss (single batch): 1.275235652923584\n",
      "\t Training loss (single batch): 1.597383975982666\n",
      "\t Training loss (single batch): 0.946492075920105\n",
      "\t Training loss (single batch): 0.8402464985847473\n",
      "\t Training loss (single batch): 0.9216040968894958\n",
      "\t Training loss (single batch): 0.9500460028648376\n",
      "\t Training loss (single batch): 1.2606682777404785\n",
      "\t Training loss (single batch): 0.921184778213501\n",
      "\t Training loss (single batch): 1.1155290603637695\n",
      "\t Training loss (single batch): 1.2290863990783691\n",
      "\t Training loss (single batch): 0.8159530758857727\n",
      "\t Training loss (single batch): 1.7671937942504883\n",
      "\t Training loss (single batch): 0.9161180257797241\n",
      "\t Training loss (single batch): 1.2053430080413818\n",
      "\t Training loss (single batch): 0.8786507844924927\n",
      "\t Training loss (single batch): 1.1836613416671753\n",
      "\t Training loss (single batch): 1.4680044651031494\n",
      "\t Training loss (single batch): 1.4596315622329712\n",
      "\t Training loss (single batch): 1.4396257400512695\n",
      "\t Training loss (single batch): 1.628737449645996\n",
      "\t Training loss (single batch): 1.4400596618652344\n",
      "\t Training loss (single batch): 0.8140447735786438\n",
      "\t Training loss (single batch): 1.2441232204437256\n",
      "\t Training loss (single batch): 1.7652045488357544\n",
      "\t Training loss (single batch): 1.302808403968811\n",
      "\t Training loss (single batch): 1.1351611614227295\n",
      "\t Training loss (single batch): 1.3728537559509277\n",
      "\t Training loss (single batch): 1.5986963510513306\n",
      "\t Training loss (single batch): 1.5851203203201294\n",
      "\t Training loss (single batch): 0.9062329530715942\n",
      "\t Training loss (single batch): 0.8974370360374451\n",
      "\t Training loss (single batch): 0.9664710164070129\n",
      "\t Training loss (single batch): 0.9879006147384644\n",
      "\t Training loss (single batch): 0.9354374408721924\n",
      "\t Training loss (single batch): 1.0743186473846436\n",
      "\t Training loss (single batch): 1.2117329835891724\n",
      "\t Training loss (single batch): 1.4877363443374634\n",
      "\t Training loss (single batch): 0.9017415642738342\n",
      "\t Training loss (single batch): 0.9258310198783875\n",
      "\t Training loss (single batch): 0.8931187391281128\n",
      "\t Training loss (single batch): 1.3660975694656372\n",
      "\t Training loss (single batch): 0.8389937281608582\n",
      "\t Training loss (single batch): 1.106472373008728\n",
      "\t Training loss (single batch): 1.3378627300262451\n",
      "\t Training loss (single batch): 1.2503058910369873\n",
      "\t Training loss (single batch): 1.0963060855865479\n",
      "\t Training loss (single batch): 1.254768967628479\n",
      "\t Training loss (single batch): 1.3540005683898926\n",
      "\t Training loss (single batch): 0.9903483390808105\n",
      "\t Training loss (single batch): 1.4752750396728516\n",
      "\t Training loss (single batch): 1.4085785150527954\n",
      "\t Training loss (single batch): 1.3834441900253296\n",
      "\t Training loss (single batch): 1.5111232995986938\n",
      "\t Training loss (single batch): 1.437180519104004\n",
      "\t Training loss (single batch): 0.9953502416610718\n",
      "\t Training loss (single batch): 1.0581896305084229\n",
      "\t Training loss (single batch): 0.7101895213127136\n",
      "\t Training loss (single batch): 1.1761198043823242\n",
      "\t Training loss (single batch): 1.4510668516159058\n",
      "\t Training loss (single batch): 0.7213224172592163\n",
      "\t Training loss (single batch): 1.0672377347946167\n",
      "\t Training loss (single batch): 1.4112005233764648\n",
      "\t Training loss (single batch): 1.0580530166625977\n",
      "\t Training loss (single batch): 1.3167232275009155\n",
      "\t Training loss (single batch): 1.1756340265274048\n",
      "\t Training loss (single batch): 1.153350591659546\n",
      "\t Training loss (single batch): 0.9699286818504333\n",
      "\t Training loss (single batch): 1.3846664428710938\n",
      "\t Training loss (single batch): 1.594506025314331\n",
      "\t Training loss (single batch): 0.9870527386665344\n",
      "\t Training loss (single batch): 1.0306087732315063\n",
      "\t Training loss (single batch): 1.167219638824463\n",
      "\t Training loss (single batch): 1.0700780153274536\n",
      "\t Training loss (single batch): 1.5248838663101196\n",
      "\t Training loss (single batch): 1.1761291027069092\n",
      "\t Training loss (single batch): 1.312780737876892\n",
      "\t Training loss (single batch): 1.3156801462173462\n",
      "\t Training loss (single batch): 1.2333602905273438\n",
      "\t Training loss (single batch): 1.1156892776489258\n",
      "\t Training loss (single batch): 1.3644510507583618\n",
      "\t Training loss (single batch): 1.3415322303771973\n",
      "\t Training loss (single batch): 1.4472886323928833\n",
      "\t Training loss (single batch): 1.5486124753952026\n",
      "\t Training loss (single batch): 1.7558748722076416\n",
      "\t Training loss (single batch): 1.0928319692611694\n",
      "\t Training loss (single batch): 1.3472599983215332\n",
      "\t Training loss (single batch): 1.3196839094161987\n",
      "\t Training loss (single batch): 1.1880587339401245\n",
      "\t Training loss (single batch): 1.6584476232528687\n",
      "\t Training loss (single batch): 1.4560680389404297\n",
      "\t Training loss (single batch): 0.8754781484603882\n",
      "\t Training loss (single batch): 1.1152817010879517\n",
      "\t Training loss (single batch): 1.5795127153396606\n",
      "\t Training loss (single batch): 1.0934261083602905\n",
      "\t Training loss (single batch): 1.170189619064331\n",
      "\t Training loss (single batch): 1.186047911643982\n",
      "\t Training loss (single batch): 1.0907258987426758\n",
      "\t Training loss (single batch): 0.9903521537780762\n",
      "\t Training loss (single batch): 1.1353509426116943\n",
      "\t Training loss (single batch): 1.0741909742355347\n",
      "\t Training loss (single batch): 1.5943130254745483\n",
      "\t Training loss (single batch): 1.1197373867034912\n",
      "\t Training loss (single batch): 1.2026816606521606\n",
      "\t Training loss (single batch): 1.3132182359695435\n",
      "\t Training loss (single batch): 1.3460980653762817\n",
      "\t Training loss (single batch): 1.0265703201293945\n",
      "\t Training loss (single batch): 0.990676999092102\n",
      "\t Training loss (single batch): 1.0734087228775024\n",
      "\t Training loss (single batch): 0.9796270728111267\n",
      "\t Training loss (single batch): 0.7089810967445374\n",
      "\t Training loss (single batch): 1.0944308042526245\n",
      "\t Training loss (single batch): 1.0714967250823975\n",
      "\t Training loss (single batch): 0.7356919646263123\n",
      "\t Training loss (single batch): 1.0562739372253418\n",
      "\t Training loss (single batch): 0.9784865379333496\n",
      "\t Training loss (single batch): 1.434095025062561\n",
      "\t Training loss (single batch): 1.6389052867889404\n",
      "\t Training loss (single batch): 1.0462942123413086\n",
      "\t Training loss (single batch): 1.4294531345367432\n",
      "\t Training loss (single batch): 0.9810466170310974\n",
      "\t Training loss (single batch): 1.3606631755828857\n",
      "\t Training loss (single batch): 1.9402227401733398\n",
      "##################################\n",
      "## EPOCH 81\n",
      "##################################\n",
      "\t Training loss (single batch): 1.335654377937317\n",
      "\t Training loss (single batch): 0.7010800242424011\n",
      "\t Training loss (single batch): 1.301224708557129\n",
      "\t Training loss (single batch): 1.1577954292297363\n",
      "\t Training loss (single batch): 1.216115951538086\n",
      "\t Training loss (single batch): 1.2324038743972778\n",
      "\t Training loss (single batch): 1.4072291851043701\n",
      "\t Training loss (single batch): 1.2093299627304077\n",
      "\t Training loss (single batch): 1.3855252265930176\n",
      "\t Training loss (single batch): 1.2411173582077026\n",
      "\t Training loss (single batch): 1.3422799110412598\n",
      "\t Training loss (single batch): 1.249302625656128\n",
      "\t Training loss (single batch): 1.483445167541504\n",
      "\t Training loss (single batch): 1.2889809608459473\n",
      "\t Training loss (single batch): 1.3749887943267822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.355684757232666\n",
      "\t Training loss (single batch): 0.9808365702629089\n",
      "\t Training loss (single batch): 1.3770456314086914\n",
      "\t Training loss (single batch): 1.1004594564437866\n",
      "\t Training loss (single batch): 1.2121913433074951\n",
      "\t Training loss (single batch): 0.780590295791626\n",
      "\t Training loss (single batch): 1.7759796380996704\n",
      "\t Training loss (single batch): 1.0807541608810425\n",
      "\t Training loss (single batch): 0.8058748841285706\n",
      "\t Training loss (single batch): 1.4304100275039673\n",
      "\t Training loss (single batch): 1.4242579936981201\n",
      "\t Training loss (single batch): 1.1835814714431763\n",
      "\t Training loss (single batch): 0.8560013771057129\n",
      "\t Training loss (single batch): 1.1005586385726929\n",
      "\t Training loss (single batch): 1.327823519706726\n",
      "\t Training loss (single batch): 1.405555248260498\n",
      "\t Training loss (single batch): 1.2817424535751343\n",
      "\t Training loss (single batch): 1.3617219924926758\n",
      "\t Training loss (single batch): 1.4198808670043945\n",
      "\t Training loss (single batch): 1.336418867111206\n",
      "\t Training loss (single batch): 0.936651349067688\n",
      "\t Training loss (single batch): 1.4592430591583252\n",
      "\t Training loss (single batch): 1.20989990234375\n",
      "\t Training loss (single batch): 1.219377875328064\n",
      "\t Training loss (single batch): 1.2870299816131592\n",
      "\t Training loss (single batch): 1.2456668615341187\n",
      "\t Training loss (single batch): 1.514231562614441\n",
      "\t Training loss (single batch): 1.3050034046173096\n",
      "\t Training loss (single batch): 1.415120244026184\n",
      "\t Training loss (single batch): 1.2410145998001099\n",
      "\t Training loss (single batch): 0.950823187828064\n",
      "\t Training loss (single batch): 1.8995999097824097\n",
      "\t Training loss (single batch): 1.0700212717056274\n",
      "\t Training loss (single batch): 1.0931166410446167\n",
      "\t Training loss (single batch): 1.3235036134719849\n",
      "\t Training loss (single batch): 1.2562415599822998\n",
      "\t Training loss (single batch): 1.0361486673355103\n",
      "\t Training loss (single batch): 0.8749328851699829\n",
      "\t Training loss (single batch): 1.792242169380188\n",
      "\t Training loss (single batch): 1.3102753162384033\n",
      "\t Training loss (single batch): 0.8919091820716858\n",
      "\t Training loss (single batch): 1.2324905395507812\n",
      "\t Training loss (single batch): 0.906497061252594\n",
      "\t Training loss (single batch): 0.9038653373718262\n",
      "\t Training loss (single batch): 1.7454055547714233\n",
      "\t Training loss (single batch): 0.9640045166015625\n",
      "\t Training loss (single batch): 1.1102169752120972\n",
      "\t Training loss (single batch): 1.2150083780288696\n",
      "\t Training loss (single batch): 1.2840498685836792\n",
      "\t Training loss (single batch): 1.6847172975540161\n",
      "\t Training loss (single batch): 1.403617024421692\n",
      "\t Training loss (single batch): 1.1930296421051025\n",
      "\t Training loss (single batch): 1.208354115486145\n",
      "\t Training loss (single batch): 1.5511447191238403\n",
      "\t Training loss (single batch): 1.2388197183609009\n",
      "\t Training loss (single batch): 1.1787550449371338\n",
      "\t Training loss (single batch): 1.1386710405349731\n",
      "\t Training loss (single batch): 0.812421441078186\n",
      "\t Training loss (single batch): 0.9552334547042847\n",
      "\t Training loss (single batch): 1.0831280946731567\n",
      "\t Training loss (single batch): 0.823170006275177\n",
      "\t Training loss (single batch): 1.1356946229934692\n",
      "\t Training loss (single batch): 1.0317765474319458\n",
      "\t Training loss (single batch): 1.113759994506836\n",
      "\t Training loss (single batch): 1.209309458732605\n",
      "\t Training loss (single batch): 0.9912816882133484\n",
      "\t Training loss (single batch): 1.3509521484375\n",
      "\t Training loss (single batch): 0.8391923308372498\n",
      "\t Training loss (single batch): 0.9923701882362366\n",
      "\t Training loss (single batch): 1.2892558574676514\n",
      "\t Training loss (single batch): 0.9698915481567383\n",
      "\t Training loss (single batch): 1.37263023853302\n",
      "\t Training loss (single batch): 1.0519566535949707\n",
      "\t Training loss (single batch): 1.0686439275741577\n",
      "\t Training loss (single batch): 1.2624536752700806\n",
      "\t Training loss (single batch): 1.121827483177185\n",
      "\t Training loss (single batch): 0.8831785321235657\n",
      "\t Training loss (single batch): 1.3731709718704224\n",
      "\t Training loss (single batch): 1.249352216720581\n",
      "\t Training loss (single batch): 1.508141040802002\n",
      "\t Training loss (single batch): 1.6628936529159546\n",
      "\t Training loss (single batch): 1.0223097801208496\n",
      "\t Training loss (single batch): 1.5419725179672241\n",
      "\t Training loss (single batch): 1.2968682050704956\n",
      "\t Training loss (single batch): 1.125605583190918\n",
      "\t Training loss (single batch): 1.0503414869308472\n",
      "\t Training loss (single batch): 1.337101697921753\n",
      "\t Training loss (single batch): 0.9074132442474365\n",
      "\t Training loss (single batch): 0.9731425046920776\n",
      "\t Training loss (single batch): 1.3822555541992188\n",
      "\t Training loss (single batch): 0.9618211984634399\n",
      "\t Training loss (single batch): 1.1916394233703613\n",
      "\t Training loss (single batch): 1.3247777223587036\n",
      "\t Training loss (single batch): 1.711495041847229\n",
      "\t Training loss (single batch): 0.9913734197616577\n",
      "\t Training loss (single batch): 1.0203717947006226\n",
      "\t Training loss (single batch): 0.8297849297523499\n",
      "\t Training loss (single batch): 1.128322958946228\n",
      "\t Training loss (single batch): 1.2740118503570557\n",
      "\t Training loss (single batch): 1.2105528116226196\n",
      "\t Training loss (single batch): 1.0999614000320435\n",
      "\t Training loss (single batch): 1.3098971843719482\n",
      "\t Training loss (single batch): 1.122889757156372\n",
      "\t Training loss (single batch): 1.4337081909179688\n",
      "\t Training loss (single batch): 1.803221583366394\n",
      "\t Training loss (single batch): 1.1344449520111084\n",
      "\t Training loss (single batch): 0.8582091331481934\n",
      "\t Training loss (single batch): 0.9488638639450073\n",
      "\t Training loss (single batch): 1.2186017036437988\n",
      "\t Training loss (single batch): 0.9302319884300232\n",
      "\t Training loss (single batch): 1.253261923789978\n",
      "\t Training loss (single batch): 1.1561306715011597\n",
      "\t Training loss (single batch): 1.3219774961471558\n",
      "\t Training loss (single batch): 1.0456799268722534\n",
      "\t Training loss (single batch): 1.203068733215332\n",
      "\t Training loss (single batch): 0.8717938661575317\n",
      "\t Training loss (single batch): 1.2780492305755615\n",
      "\t Training loss (single batch): 1.0093177556991577\n",
      "\t Training loss (single batch): 0.8888003826141357\n",
      "\t Training loss (single batch): 1.4409838914871216\n",
      "\t Training loss (single batch): 1.2242542505264282\n",
      "\t Training loss (single batch): 1.2968857288360596\n",
      "\t Training loss (single batch): 1.0122036933898926\n",
      "\t Training loss (single batch): 1.4750109910964966\n",
      "\t Training loss (single batch): 0.9453966021537781\n",
      "\t Training loss (single batch): 1.3952304124832153\n",
      "\t Training loss (single batch): 1.2357964515686035\n",
      "\t Training loss (single batch): 1.1494165658950806\n",
      "\t Training loss (single batch): 1.4167720079421997\n",
      "\t Training loss (single batch): 0.9484227299690247\n",
      "\t Training loss (single batch): 1.2086405754089355\n",
      "\t Training loss (single batch): 1.2069408893585205\n",
      "\t Training loss (single batch): 1.4359245300292969\n",
      "\t Training loss (single batch): 1.2502678632736206\n",
      "\t Training loss (single batch): 1.3998874425888062\n",
      "\t Training loss (single batch): 1.8665729761123657\n",
      "\t Training loss (single batch): 0.8908743262290955\n",
      "\t Training loss (single batch): 1.002844214439392\n",
      "\t Training loss (single batch): 1.45833158493042\n",
      "\t Training loss (single batch): 0.9113920331001282\n",
      "\t Training loss (single batch): 1.6970694065093994\n",
      "\t Training loss (single batch): 1.271490454673767\n",
      "\t Training loss (single batch): 1.1264927387237549\n",
      "\t Training loss (single batch): 1.0759403705596924\n",
      "\t Training loss (single batch): 0.8825117349624634\n",
      "\t Training loss (single batch): 1.1234989166259766\n",
      "\t Training loss (single batch): 1.195119023323059\n",
      "\t Training loss (single batch): 1.1806455850601196\n",
      "\t Training loss (single batch): 0.7214152216911316\n",
      "\t Training loss (single batch): 1.4715216159820557\n",
      "\t Training loss (single batch): 1.26499605178833\n",
      "\t Training loss (single batch): 1.2343294620513916\n",
      "\t Training loss (single batch): 1.1899170875549316\n",
      "\t Training loss (single batch): 1.0914406776428223\n",
      "\t Training loss (single batch): 1.028196930885315\n",
      "\t Training loss (single batch): 1.680710792541504\n",
      "\t Training loss (single batch): 1.609034776687622\n",
      "\t Training loss (single batch): 1.3769819736480713\n",
      "\t Training loss (single batch): 1.0924454927444458\n",
      "\t Training loss (single batch): 0.892802894115448\n",
      "\t Training loss (single batch): 1.3904434442520142\n",
      "\t Training loss (single batch): 1.818621039390564\n",
      "\t Training loss (single batch): 1.1232490539550781\n",
      "\t Training loss (single batch): 1.0636132955551147\n",
      "\t Training loss (single batch): 1.2413647174835205\n",
      "\t Training loss (single batch): 1.2050575017929077\n",
      "\t Training loss (single batch): 1.0226024389266968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.364546537399292\n",
      "\t Training loss (single batch): 1.3049415349960327\n",
      "\t Training loss (single batch): 0.8258346319198608\n",
      "\t Training loss (single batch): 1.444224238395691\n",
      "\t Training loss (single batch): 1.4052679538726807\n",
      "\t Training loss (single batch): 0.9473993182182312\n",
      "\t Training loss (single batch): 1.6638833284378052\n",
      "\t Training loss (single batch): 1.5659595727920532\n",
      "\t Training loss (single batch): 1.1376221179962158\n",
      "\t Training loss (single batch): 1.0057435035705566\n",
      "\t Training loss (single batch): 1.1362844705581665\n",
      "\t Training loss (single batch): 1.030908465385437\n",
      "\t Training loss (single batch): 1.4162334203720093\n",
      "\t Training loss (single batch): 1.2320576906204224\n",
      "\t Training loss (single batch): 1.2483124732971191\n",
      "\t Training loss (single batch): 0.9845560193061829\n",
      "\t Training loss (single batch): 1.2074081897735596\n",
      "\t Training loss (single batch): 1.548002004623413\n",
      "\t Training loss (single batch): 1.0618860721588135\n",
      "\t Training loss (single batch): 0.9546793699264526\n",
      "\t Training loss (single batch): 0.8245952129364014\n",
      "\t Training loss (single batch): 1.7297868728637695\n",
      "\t Training loss (single batch): 1.012390375137329\n",
      "\t Training loss (single batch): 0.8355339765548706\n",
      "\t Training loss (single batch): 1.4216585159301758\n",
      "\t Training loss (single batch): 1.2803266048431396\n",
      "\t Training loss (single batch): 1.3144118785858154\n",
      "\t Training loss (single batch): 1.394914150238037\n",
      "\t Training loss (single batch): 1.199141263961792\n",
      "\t Training loss (single batch): 1.5184983015060425\n",
      "\t Training loss (single batch): 1.0160541534423828\n",
      "\t Training loss (single batch): 1.2164034843444824\n",
      "\t Training loss (single batch): 1.1213579177856445\n",
      "\t Training loss (single batch): 1.930401086807251\n",
      "\t Training loss (single batch): 1.1273857355117798\n",
      "\t Training loss (single batch): 1.0399032831192017\n",
      "\t Training loss (single batch): 1.2506701946258545\n",
      "\t Training loss (single batch): 1.2033661603927612\n",
      "\t Training loss (single batch): 1.5047897100448608\n",
      "\t Training loss (single batch): 1.080001711845398\n",
      "\t Training loss (single batch): 1.3571746349334717\n",
      "\t Training loss (single batch): 1.3846720457077026\n",
      "\t Training loss (single batch): 1.4520543813705444\n",
      "\t Training loss (single batch): 1.2862430810928345\n",
      "\t Training loss (single batch): 0.7168781161308289\n",
      "\t Training loss (single batch): 1.2970688343048096\n",
      "\t Training loss (single batch): 0.9834599494934082\n",
      "\t Training loss (single batch): 0.7851925492286682\n",
      "\t Training loss (single batch): 1.0241352319717407\n",
      "\t Training loss (single batch): 1.04443359375\n",
      "\t Training loss (single batch): 1.4072775840759277\n",
      "\t Training loss (single batch): 0.796729326248169\n",
      "\t Training loss (single batch): 1.4186160564422607\n",
      "\t Training loss (single batch): 1.4762800931930542\n",
      "\t Training loss (single batch): 0.4810403287410736\n",
      "\t Training loss (single batch): 1.1002682447433472\n",
      "\t Training loss (single batch): 1.295882225036621\n",
      "\t Training loss (single batch): 0.9916806221008301\n",
      "\t Training loss (single batch): 1.174902319908142\n",
      "\t Training loss (single batch): 0.983753502368927\n",
      "\t Training loss (single batch): 1.2310020923614502\n",
      "\t Training loss (single batch): 1.189810872077942\n",
      "\t Training loss (single batch): 1.8371753692626953\n",
      "\t Training loss (single batch): 1.6784777641296387\n",
      "\t Training loss (single batch): 0.6718465089797974\n",
      "\t Training loss (single batch): 1.5376089811325073\n",
      "\t Training loss (single batch): 1.0747833251953125\n",
      "\t Training loss (single batch): 1.3034359216690063\n",
      "\t Training loss (single batch): 1.5495091676712036\n",
      "\t Training loss (single batch): 1.21217679977417\n",
      "\t Training loss (single batch): 1.0752092599868774\n",
      "\t Training loss (single batch): 1.3147907257080078\n",
      "\t Training loss (single batch): 1.1790404319763184\n",
      "\t Training loss (single batch): 0.9681384563446045\n",
      "\t Training loss (single batch): 1.1312601566314697\n",
      "\t Training loss (single batch): 1.1751689910888672\n",
      "\t Training loss (single batch): 1.0059587955474854\n",
      "\t Training loss (single batch): 0.832660973072052\n",
      "\t Training loss (single batch): 1.3054453134536743\n",
      "\t Training loss (single batch): 1.459534764289856\n",
      "\t Training loss (single batch): 1.2169097661972046\n",
      "\t Training loss (single batch): 1.035988211631775\n",
      "\t Training loss (single batch): 1.2830184698104858\n",
      "\t Training loss (single batch): 1.1305204629898071\n",
      "\t Training loss (single batch): 1.247577428817749\n",
      "\t Training loss (single batch): 1.2290269136428833\n",
      "\t Training loss (single batch): 1.3795429468154907\n",
      "\t Training loss (single batch): 0.922804594039917\n",
      "\t Training loss (single batch): 0.9357259273529053\n",
      "\t Training loss (single batch): 1.7306089401245117\n",
      "\t Training loss (single batch): 1.3263633251190186\n",
      "\t Training loss (single batch): 1.5247210264205933\n",
      "\t Training loss (single batch): 1.2370116710662842\n",
      "\t Training loss (single batch): 0.8508404493331909\n",
      "\t Training loss (single batch): 1.1520562171936035\n",
      "\t Training loss (single batch): 1.3440512418746948\n",
      "\t Training loss (single batch): 1.2859265804290771\n",
      "\t Training loss (single batch): 0.7892129421234131\n",
      "\t Training loss (single batch): 1.2166274785995483\n",
      "\t Training loss (single batch): 1.0106123685836792\n",
      "\t Training loss (single batch): 1.4267247915267944\n",
      "\t Training loss (single batch): 1.3496705293655396\n",
      "\t Training loss (single batch): 1.439642071723938\n",
      "\t Training loss (single batch): 0.9514462351799011\n",
      "\t Training loss (single batch): 1.0415602922439575\n",
      "\t Training loss (single batch): 0.9200673699378967\n",
      "\t Training loss (single batch): 1.5026901960372925\n",
      "\t Training loss (single batch): 0.7828889489173889\n",
      "\t Training loss (single batch): 0.9628862738609314\n",
      "\t Training loss (single batch): 1.5146331787109375\n",
      "\t Training loss (single batch): 1.0773646831512451\n",
      "\t Training loss (single batch): 1.1138403415679932\n",
      "\t Training loss (single batch): 1.3692489862442017\n",
      "\t Training loss (single batch): 1.224288821220398\n",
      "\t Training loss (single batch): 1.0056967735290527\n",
      "\t Training loss (single batch): 0.9226950407028198\n",
      "\t Training loss (single batch): 0.9966525435447693\n",
      "\t Training loss (single batch): 1.5234876871109009\n",
      "\t Training loss (single batch): 1.2300556898117065\n",
      "\t Training loss (single batch): 1.2194249629974365\n",
      "\t Training loss (single batch): 1.3565980195999146\n",
      "\t Training loss (single batch): 1.2799023389816284\n",
      "\t Training loss (single batch): 1.2190021276474\n",
      "\t Training loss (single batch): 0.635051965713501\n",
      "\t Training loss (single batch): 0.9739001393318176\n",
      "\t Training loss (single batch): 1.048637866973877\n",
      "\t Training loss (single batch): 1.0598056316375732\n",
      "\t Training loss (single batch): 1.2052452564239502\n",
      "\t Training loss (single batch): 0.9155524969100952\n",
      "\t Training loss (single batch): 1.162377119064331\n",
      "\t Training loss (single batch): 0.7956247925758362\n",
      "\t Training loss (single batch): 1.259747862815857\n",
      "\t Training loss (single batch): 1.5511069297790527\n",
      "\t Training loss (single batch): 1.569525957107544\n",
      "\t Training loss (single batch): 0.6039294004440308\n",
      "##################################\n",
      "## EPOCH 82\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3714410066604614\n",
      "\t Training loss (single batch): 1.0791635513305664\n",
      "\t Training loss (single batch): 1.4364713430404663\n",
      "\t Training loss (single batch): 0.9569233655929565\n",
      "\t Training loss (single batch): 1.2374988794326782\n",
      "\t Training loss (single batch): 1.9346423149108887\n",
      "\t Training loss (single batch): 1.2322535514831543\n",
      "\t Training loss (single batch): 1.5190985202789307\n",
      "\t Training loss (single batch): 1.6260937452316284\n",
      "\t Training loss (single batch): 1.398146390914917\n",
      "\t Training loss (single batch): 1.3558980226516724\n",
      "\t Training loss (single batch): 1.3942651748657227\n",
      "\t Training loss (single batch): 0.850983738899231\n",
      "\t Training loss (single batch): 1.5455642938613892\n",
      "\t Training loss (single batch): 1.3303451538085938\n",
      "\t Training loss (single batch): 1.3319439888000488\n",
      "\t Training loss (single batch): 1.2011560201644897\n",
      "\t Training loss (single batch): 0.7916392087936401\n",
      "\t Training loss (single batch): 1.1829354763031006\n",
      "\t Training loss (single batch): 1.4588475227355957\n",
      "\t Training loss (single batch): 1.1418497562408447\n",
      "\t Training loss (single batch): 1.0043275356292725\n",
      "\t Training loss (single batch): 0.89768385887146\n",
      "\t Training loss (single batch): 1.1106674671173096\n",
      "\t Training loss (single batch): 0.8774414658546448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.6245124340057373\n",
      "\t Training loss (single batch): 1.5798834562301636\n",
      "\t Training loss (single batch): 1.4182544946670532\n",
      "\t Training loss (single batch): 0.9946963787078857\n",
      "\t Training loss (single batch): 1.4841920137405396\n",
      "\t Training loss (single batch): 0.9015976190567017\n",
      "\t Training loss (single batch): 1.356058955192566\n",
      "\t Training loss (single batch): 1.0691726207733154\n",
      "\t Training loss (single batch): 1.5489140748977661\n",
      "\t Training loss (single batch): 1.2342040538787842\n",
      "\t Training loss (single batch): 0.9835473895072937\n",
      "\t Training loss (single batch): 0.8300017714500427\n",
      "\t Training loss (single batch): 1.1527496576309204\n",
      "\t Training loss (single batch): 1.1690369844436646\n",
      "\t Training loss (single batch): 0.9263426661491394\n",
      "\t Training loss (single batch): 1.5218807458877563\n",
      "\t Training loss (single batch): 1.5768165588378906\n",
      "\t Training loss (single batch): 0.8647463917732239\n",
      "\t Training loss (single batch): 1.350853681564331\n",
      "\t Training loss (single batch): 1.4337626695632935\n",
      "\t Training loss (single batch): 1.5919907093048096\n",
      "\t Training loss (single batch): 1.587970495223999\n",
      "\t Training loss (single batch): 1.251437783241272\n",
      "\t Training loss (single batch): 0.942258358001709\n",
      "\t Training loss (single batch): 1.0854052305221558\n",
      "\t Training loss (single batch): 1.1256076097488403\n",
      "\t Training loss (single batch): 1.523916482925415\n",
      "\t Training loss (single batch): 0.8659217357635498\n",
      "\t Training loss (single batch): 1.143088459968567\n",
      "\t Training loss (single batch): 1.3794056177139282\n",
      "\t Training loss (single batch): 1.2148388624191284\n",
      "\t Training loss (single batch): 1.1864452362060547\n",
      "\t Training loss (single batch): 1.8177237510681152\n",
      "\t Training loss (single batch): 1.1000922918319702\n",
      "\t Training loss (single batch): 1.17423415184021\n",
      "\t Training loss (single batch): 1.1884572505950928\n",
      "\t Training loss (single batch): 1.0144315958023071\n",
      "\t Training loss (single batch): 1.0605499744415283\n",
      "\t Training loss (single batch): 1.7934188842773438\n",
      "\t Training loss (single batch): 1.1118016242980957\n",
      "\t Training loss (single batch): 0.7456995844841003\n",
      "\t Training loss (single batch): 0.9808943867683411\n",
      "\t Training loss (single batch): 1.0198724269866943\n",
      "\t Training loss (single batch): 1.3648009300231934\n",
      "\t Training loss (single batch): 1.2497131824493408\n",
      "\t Training loss (single batch): 0.9820071458816528\n",
      "\t Training loss (single batch): 0.9187208414077759\n",
      "\t Training loss (single batch): 1.0782045125961304\n",
      "\t Training loss (single batch): 1.2555519342422485\n",
      "\t Training loss (single batch): 1.2510466575622559\n",
      "\t Training loss (single batch): 1.073512315750122\n",
      "\t Training loss (single batch): 1.3856563568115234\n",
      "\t Training loss (single batch): 1.1290582418441772\n",
      "\t Training loss (single batch): 0.8607228398323059\n",
      "\t Training loss (single batch): 0.8874602913856506\n",
      "\t Training loss (single batch): 0.9678077697753906\n",
      "\t Training loss (single batch): 1.0179706811904907\n",
      "\t Training loss (single batch): 1.3757246732711792\n",
      "\t Training loss (single batch): 1.3247931003570557\n",
      "\t Training loss (single batch): 0.7342435121536255\n",
      "\t Training loss (single batch): 1.3494932651519775\n",
      "\t Training loss (single batch): 0.9323055744171143\n",
      "\t Training loss (single batch): 1.215374231338501\n",
      "\t Training loss (single batch): 1.3751720190048218\n",
      "\t Training loss (single batch): 1.3708900213241577\n",
      "\t Training loss (single batch): 1.249811053276062\n",
      "\t Training loss (single batch): 0.7294290065765381\n",
      "\t Training loss (single batch): 1.1883280277252197\n",
      "\t Training loss (single batch): 0.9389411807060242\n",
      "\t Training loss (single batch): 1.1018215417861938\n",
      "\t Training loss (single batch): 1.093571662902832\n",
      "\t Training loss (single batch): 0.9339803457260132\n",
      "\t Training loss (single batch): 1.145371437072754\n",
      "\t Training loss (single batch): 1.1345787048339844\n",
      "\t Training loss (single batch): 1.1375244855880737\n",
      "\t Training loss (single batch): 1.009236216545105\n",
      "\t Training loss (single batch): 1.0230071544647217\n",
      "\t Training loss (single batch): 1.1608877182006836\n",
      "\t Training loss (single batch): 1.4469470977783203\n",
      "\t Training loss (single batch): 1.3643348217010498\n",
      "\t Training loss (single batch): 1.1988277435302734\n",
      "\t Training loss (single batch): 1.831575632095337\n",
      "\t Training loss (single batch): 0.9357846975326538\n",
      "\t Training loss (single batch): 1.230249285697937\n",
      "\t Training loss (single batch): 1.4441592693328857\n",
      "\t Training loss (single batch): 1.5136220455169678\n",
      "\t Training loss (single batch): 0.9069790244102478\n",
      "\t Training loss (single batch): 1.6151593923568726\n",
      "\t Training loss (single batch): 1.2417782545089722\n",
      "\t Training loss (single batch): 0.9054023623466492\n",
      "\t Training loss (single batch): 1.2955968379974365\n",
      "\t Training loss (single batch): 1.1890045404434204\n",
      "\t Training loss (single batch): 1.2660924196243286\n",
      "\t Training loss (single batch): 0.9722444415092468\n",
      "\t Training loss (single batch): 1.2331873178482056\n",
      "\t Training loss (single batch): 1.2405568361282349\n",
      "\t Training loss (single batch): 1.314126968383789\n",
      "\t Training loss (single batch): 1.2287797927856445\n",
      "\t Training loss (single batch): 1.414339303970337\n",
      "\t Training loss (single batch): 1.0425649881362915\n",
      "\t Training loss (single batch): 1.125046968460083\n",
      "\t Training loss (single batch): 1.2836596965789795\n",
      "\t Training loss (single batch): 1.1307973861694336\n",
      "\t Training loss (single batch): 0.9571337699890137\n",
      "\t Training loss (single batch): 1.1163278818130493\n",
      "\t Training loss (single batch): 1.09170401096344\n",
      "\t Training loss (single batch): 1.3521182537078857\n",
      "\t Training loss (single batch): 1.1442625522613525\n",
      "\t Training loss (single batch): 1.2944865226745605\n",
      "\t Training loss (single batch): 1.3215429782867432\n",
      "\t Training loss (single batch): 1.0090707540512085\n",
      "\t Training loss (single batch): 1.3365095853805542\n",
      "\t Training loss (single batch): 0.9813318252563477\n",
      "\t Training loss (single batch): 1.1486175060272217\n",
      "\t Training loss (single batch): 0.9990993738174438\n",
      "\t Training loss (single batch): 1.2520064115524292\n",
      "\t Training loss (single batch): 0.6190658807754517\n",
      "\t Training loss (single batch): 1.3804426193237305\n",
      "\t Training loss (single batch): 0.9993426203727722\n",
      "\t Training loss (single batch): 1.1792491674423218\n",
      "\t Training loss (single batch): 1.444920301437378\n",
      "\t Training loss (single batch): 1.1191041469573975\n",
      "\t Training loss (single batch): 0.8387997150421143\n",
      "\t Training loss (single batch): 1.2478634119033813\n",
      "\t Training loss (single batch): 1.3408124446868896\n",
      "\t Training loss (single batch): 0.9255674481391907\n",
      "\t Training loss (single batch): 1.3493356704711914\n",
      "\t Training loss (single batch): 0.8797546625137329\n",
      "\t Training loss (single batch): 1.0510605573654175\n",
      "\t Training loss (single batch): 1.2811444997787476\n",
      "\t Training loss (single batch): 1.3084213733673096\n",
      "\t Training loss (single batch): 0.8106200098991394\n",
      "\t Training loss (single batch): 0.8553891181945801\n",
      "\t Training loss (single batch): 1.4082969427108765\n",
      "\t Training loss (single batch): 1.101752758026123\n",
      "\t Training loss (single batch): 1.1518003940582275\n",
      "\t Training loss (single batch): 1.239310383796692\n",
      "\t Training loss (single batch): 0.7935303449630737\n",
      "\t Training loss (single batch): 0.9723557829856873\n",
      "\t Training loss (single batch): 1.090903878211975\n",
      "\t Training loss (single batch): 1.665789246559143\n",
      "\t Training loss (single batch): 0.9391435384750366\n",
      "\t Training loss (single batch): 1.0637332201004028\n",
      "\t Training loss (single batch): 1.2873425483703613\n",
      "\t Training loss (single batch): 1.2501951456069946\n",
      "\t Training loss (single batch): 1.1247117519378662\n",
      "\t Training loss (single batch): 1.5408904552459717\n",
      "\t Training loss (single batch): 1.2748618125915527\n",
      "\t Training loss (single batch): 1.0483120679855347\n",
      "\t Training loss (single batch): 1.3247684240341187\n",
      "\t Training loss (single batch): 1.5986058712005615\n",
      "\t Training loss (single batch): 1.3053712844848633\n",
      "\t Training loss (single batch): 1.2179468870162964\n",
      "\t Training loss (single batch): 1.4884729385375977\n",
      "\t Training loss (single batch): 1.2736972570419312\n",
      "\t Training loss (single batch): 1.338712215423584\n",
      "\t Training loss (single batch): 1.0610361099243164\n",
      "\t Training loss (single batch): 1.3210517168045044\n",
      "\t Training loss (single batch): 1.2146707773208618\n",
      "\t Training loss (single batch): 1.0576022863388062\n",
      "\t Training loss (single batch): 1.235632061958313\n",
      "\t Training loss (single batch): 1.0825027227401733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8480767011642456\n",
      "\t Training loss (single batch): 1.3945611715316772\n",
      "\t Training loss (single batch): 0.9304959774017334\n",
      "\t Training loss (single batch): 0.9634325504302979\n",
      "\t Training loss (single batch): 1.572430968284607\n",
      "\t Training loss (single batch): 1.1424907445907593\n",
      "\t Training loss (single batch): 1.0541049242019653\n",
      "\t Training loss (single batch): 1.249544620513916\n",
      "\t Training loss (single batch): 1.1360008716583252\n",
      "\t Training loss (single batch): 1.2116670608520508\n",
      "\t Training loss (single batch): 1.2229070663452148\n",
      "\t Training loss (single batch): 1.1205265522003174\n",
      "\t Training loss (single batch): 1.2305593490600586\n",
      "\t Training loss (single batch): 1.4509390592575073\n",
      "\t Training loss (single batch): 0.8599365949630737\n",
      "\t Training loss (single batch): 0.8696552515029907\n",
      "\t Training loss (single batch): 1.059436559677124\n",
      "\t Training loss (single batch): 1.3319536447525024\n",
      "\t Training loss (single batch): 1.2062875032424927\n",
      "\t Training loss (single batch): 1.4283572435379028\n",
      "\t Training loss (single batch): 1.6112737655639648\n",
      "\t Training loss (single batch): 0.8847190737724304\n",
      "\t Training loss (single batch): 0.8600134253501892\n",
      "\t Training loss (single batch): 1.3577536344528198\n",
      "\t Training loss (single batch): 1.2072006464004517\n",
      "\t Training loss (single batch): 1.2031679153442383\n",
      "\t Training loss (single batch): 1.4067175388336182\n",
      "\t Training loss (single batch): 1.2548567056655884\n",
      "\t Training loss (single batch): 1.051265001296997\n",
      "\t Training loss (single batch): 0.920978844165802\n",
      "\t Training loss (single batch): 0.7417070865631104\n",
      "\t Training loss (single batch): 0.9828455448150635\n",
      "\t Training loss (single batch): 1.274749755859375\n",
      "\t Training loss (single batch): 1.1519432067871094\n",
      "\t Training loss (single batch): 1.4177584648132324\n",
      "\t Training loss (single batch): 1.2669117450714111\n",
      "\t Training loss (single batch): 1.3617438077926636\n",
      "\t Training loss (single batch): 1.5847958326339722\n",
      "\t Training loss (single batch): 1.4299465417861938\n",
      "\t Training loss (single batch): 1.3642091751098633\n",
      "\t Training loss (single batch): 1.5049386024475098\n",
      "\t Training loss (single batch): 0.7907679677009583\n",
      "\t Training loss (single batch): 0.7995049357414246\n",
      "\t Training loss (single batch): 1.0289580821990967\n",
      "\t Training loss (single batch): 0.7491875886917114\n",
      "\t Training loss (single batch): 1.3479024171829224\n",
      "\t Training loss (single batch): 0.9484300017356873\n",
      "\t Training loss (single batch): 1.1719328165054321\n",
      "\t Training loss (single batch): 1.2081573009490967\n",
      "\t Training loss (single batch): 1.353376865386963\n",
      "\t Training loss (single batch): 1.4302945137023926\n",
      "\t Training loss (single batch): 0.8930996060371399\n",
      "\t Training loss (single batch): 0.9633917212486267\n",
      "\t Training loss (single batch): 1.2591986656188965\n",
      "\t Training loss (single batch): 1.3972129821777344\n",
      "\t Training loss (single batch): 1.3290843963623047\n",
      "\t Training loss (single batch): 1.0349953174591064\n",
      "\t Training loss (single batch): 1.1837410926818848\n",
      "\t Training loss (single batch): 0.9807892441749573\n",
      "\t Training loss (single batch): 1.2062405347824097\n",
      "\t Training loss (single batch): 0.9606240391731262\n",
      "\t Training loss (single batch): 1.0811965465545654\n",
      "\t Training loss (single batch): 0.9731484055519104\n",
      "\t Training loss (single batch): 0.6154834628105164\n",
      "\t Training loss (single batch): 0.8624482750892639\n",
      "\t Training loss (single batch): 1.1147780418395996\n",
      "\t Training loss (single batch): 1.0471532344818115\n",
      "\t Training loss (single batch): 1.3393843173980713\n",
      "\t Training loss (single batch): 1.1913834810256958\n",
      "\t Training loss (single batch): 1.503342866897583\n",
      "\t Training loss (single batch): 1.9303289651870728\n",
      "\t Training loss (single batch): 1.0115565061569214\n",
      "\t Training loss (single batch): 1.3517905473709106\n",
      "\t Training loss (single batch): 1.3607784509658813\n",
      "\t Training loss (single batch): 1.2961524724960327\n",
      "\t Training loss (single batch): 1.6861586570739746\n",
      "\t Training loss (single batch): 1.064853549003601\n",
      "\t Training loss (single batch): 0.9742711782455444\n",
      "\t Training loss (single batch): 0.9866087436676025\n",
      "\t Training loss (single batch): 0.9727737307548523\n",
      "\t Training loss (single batch): 1.5186386108398438\n",
      "\t Training loss (single batch): 1.1723616123199463\n",
      "\t Training loss (single batch): 1.2982405424118042\n",
      "\t Training loss (single batch): 1.5450210571289062\n",
      "\t Training loss (single batch): 1.5123287439346313\n",
      "\t Training loss (single batch): 1.669192910194397\n",
      "\t Training loss (single batch): 1.0307378768920898\n",
      "\t Training loss (single batch): 1.1577154397964478\n",
      "\t Training loss (single batch): 1.5085647106170654\n",
      "\t Training loss (single batch): 0.8070191144943237\n",
      "\t Training loss (single batch): 0.7718629837036133\n",
      "\t Training loss (single batch): 0.9975767135620117\n",
      "\t Training loss (single batch): 1.590772271156311\n",
      "\t Training loss (single batch): 1.2304142713546753\n",
      "\t Training loss (single batch): 1.1121749877929688\n",
      "\t Training loss (single batch): 1.3395370244979858\n",
      "\t Training loss (single batch): 1.252734899520874\n",
      "\t Training loss (single batch): 0.8429675102233887\n",
      "\t Training loss (single batch): 1.359466791152954\n",
      "\t Training loss (single batch): 1.351643681526184\n",
      "\t Training loss (single batch): 2.1553902626037598\n",
      "\t Training loss (single batch): 1.223239541053772\n",
      "\t Training loss (single batch): 1.3829277753829956\n",
      "\t Training loss (single batch): 1.861099362373352\n",
      "\t Training loss (single batch): 1.228195071220398\n",
      "\t Training loss (single batch): 1.078101396560669\n",
      "\t Training loss (single batch): 1.09929621219635\n",
      "\t Training loss (single batch): 1.3318462371826172\n",
      "\t Training loss (single batch): 1.3705016374588013\n",
      "\t Training loss (single batch): 1.3479121923446655\n",
      "\t Training loss (single batch): 1.218639612197876\n",
      "\t Training loss (single batch): 0.9060869216918945\n",
      "\t Training loss (single batch): 1.3070814609527588\n",
      "\t Training loss (single batch): 1.2459588050842285\n",
      "\t Training loss (single batch): 1.1468662023544312\n",
      "\t Training loss (single batch): 1.1980235576629639\n",
      "\t Training loss (single batch): 1.0171278715133667\n",
      "\t Training loss (single batch): 1.2992546558380127\n",
      "\t Training loss (single batch): 1.4179319143295288\n",
      "\t Training loss (single batch): 1.1823288202285767\n",
      "\t Training loss (single batch): 0.9244251847267151\n",
      "\t Training loss (single batch): 0.9877707362174988\n",
      "\t Training loss (single batch): 1.076680064201355\n",
      "\t Training loss (single batch): 1.7658915519714355\n",
      "\t Training loss (single batch): 1.1122666597366333\n",
      "\t Training loss (single batch): 1.7895961999893188\n",
      "\t Training loss (single batch): 0.9801239967346191\n",
      "\t Training loss (single batch): 0.8294631838798523\n",
      "\t Training loss (single batch): 1.297346591949463\n",
      "\t Training loss (single batch): 0.5477010011672974\n",
      "##################################\n",
      "## EPOCH 83\n",
      "##################################\n",
      "\t Training loss (single batch): 1.350928783416748\n",
      "\t Training loss (single batch): 0.7852056622505188\n",
      "\t Training loss (single batch): 0.8435797095298767\n",
      "\t Training loss (single batch): 1.0469201803207397\n",
      "\t Training loss (single batch): 1.2669315338134766\n",
      "\t Training loss (single batch): 1.3927563428878784\n",
      "\t Training loss (single batch): 1.2341135740280151\n",
      "\t Training loss (single batch): 0.8740346431732178\n",
      "\t Training loss (single batch): 0.9567684531211853\n",
      "\t Training loss (single batch): 1.2030500173568726\n",
      "\t Training loss (single batch): 0.8042777180671692\n",
      "\t Training loss (single batch): 1.0304678678512573\n",
      "\t Training loss (single batch): 1.1353495121002197\n",
      "\t Training loss (single batch): 0.9723371863365173\n",
      "\t Training loss (single batch): 1.211879014968872\n",
      "\t Training loss (single batch): 1.3152154684066772\n",
      "\t Training loss (single batch): 1.0728617906570435\n",
      "\t Training loss (single batch): 1.6404980421066284\n",
      "\t Training loss (single batch): 1.420376181602478\n",
      "\t Training loss (single batch): 1.418263554573059\n",
      "\t Training loss (single batch): 1.1782675981521606\n",
      "\t Training loss (single batch): 0.8539850115776062\n",
      "\t Training loss (single batch): 1.1188735961914062\n",
      "\t Training loss (single batch): 1.121505618095398\n",
      "\t Training loss (single batch): 1.3820477724075317\n",
      "\t Training loss (single batch): 1.2984145879745483\n",
      "\t Training loss (single batch): 1.036142110824585\n",
      "\t Training loss (single batch): 0.757021427154541\n",
      "\t Training loss (single batch): 1.0081111192703247\n",
      "\t Training loss (single batch): 1.3887563943862915\n",
      "\t Training loss (single batch): 1.4991525411605835\n",
      "\t Training loss (single batch): 1.1221181154251099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.488043189048767\n",
      "\t Training loss (single batch): 1.1250964403152466\n",
      "\t Training loss (single batch): 1.6357853412628174\n",
      "\t Training loss (single batch): 1.373165249824524\n",
      "\t Training loss (single batch): 0.9474842548370361\n",
      "\t Training loss (single batch): 0.9890257120132446\n",
      "\t Training loss (single batch): 1.3331252336502075\n",
      "\t Training loss (single batch): 1.1062124967575073\n",
      "\t Training loss (single batch): 1.0018304586410522\n",
      "\t Training loss (single batch): 0.8252725005149841\n",
      "\t Training loss (single batch): 0.9512131810188293\n",
      "\t Training loss (single batch): 0.805553674697876\n",
      "\t Training loss (single batch): 0.7832067608833313\n",
      "\t Training loss (single batch): 0.8857880234718323\n",
      "\t Training loss (single batch): 1.2523741722106934\n",
      "\t Training loss (single batch): 1.0233826637268066\n",
      "\t Training loss (single batch): 1.1799181699752808\n",
      "\t Training loss (single batch): 1.237947702407837\n",
      "\t Training loss (single batch): 0.718453586101532\n",
      "\t Training loss (single batch): 0.7128376960754395\n",
      "\t Training loss (single batch): 1.4377559423446655\n",
      "\t Training loss (single batch): 0.9479156136512756\n",
      "\t Training loss (single batch): 0.9267675876617432\n",
      "\t Training loss (single batch): 1.3234355449676514\n",
      "\t Training loss (single batch): 1.237716794013977\n",
      "\t Training loss (single batch): 0.7951754331588745\n",
      "\t Training loss (single batch): 1.3014919757843018\n",
      "\t Training loss (single batch): 1.2669224739074707\n",
      "\t Training loss (single batch): 0.8399524688720703\n",
      "\t Training loss (single batch): 0.8550089597702026\n",
      "\t Training loss (single batch): 1.0110725164413452\n",
      "\t Training loss (single batch): 0.8287587761878967\n",
      "\t Training loss (single batch): 0.7790440320968628\n",
      "\t Training loss (single batch): 1.4744826555252075\n",
      "\t Training loss (single batch): 1.0114518404006958\n",
      "\t Training loss (single batch): 1.5766441822052002\n",
      "\t Training loss (single batch): 1.2883222103118896\n",
      "\t Training loss (single batch): 1.3888338804244995\n",
      "\t Training loss (single batch): 1.3151161670684814\n",
      "\t Training loss (single batch): 0.9873067736625671\n",
      "\t Training loss (single batch): 1.1875358819961548\n",
      "\t Training loss (single batch): 1.2527066469192505\n",
      "\t Training loss (single batch): 1.449600338935852\n",
      "\t Training loss (single batch): 1.0117547512054443\n",
      "\t Training loss (single batch): 1.234116554260254\n",
      "\t Training loss (single batch): 0.8231196999549866\n",
      "\t Training loss (single batch): 1.2253919839859009\n",
      "\t Training loss (single batch): 2.467971086502075\n",
      "\t Training loss (single batch): 0.9567351937294006\n",
      "\t Training loss (single batch): 1.590645670890808\n",
      "\t Training loss (single batch): 1.56757652759552\n",
      "\t Training loss (single batch): 1.0352941751480103\n",
      "\t Training loss (single batch): 0.9479970335960388\n",
      "\t Training loss (single batch): 1.0631732940673828\n",
      "\t Training loss (single batch): 0.7498742341995239\n",
      "\t Training loss (single batch): 1.2476009130477905\n",
      "\t Training loss (single batch): 0.7357977032661438\n",
      "\t Training loss (single batch): 1.5856724977493286\n",
      "\t Training loss (single batch): 0.8646587133407593\n",
      "\t Training loss (single batch): 0.9147288799285889\n",
      "\t Training loss (single batch): 0.9671018719673157\n",
      "\t Training loss (single batch): 1.2178044319152832\n",
      "\t Training loss (single batch): 1.4781246185302734\n",
      "\t Training loss (single batch): 1.8251012563705444\n",
      "\t Training loss (single batch): 1.289226770401001\n",
      "\t Training loss (single batch): 1.09872567653656\n",
      "\t Training loss (single batch): 0.9582876563072205\n",
      "\t Training loss (single batch): 1.5306798219680786\n",
      "\t Training loss (single batch): 1.640285849571228\n",
      "\t Training loss (single batch): 1.0337963104248047\n",
      "\t Training loss (single batch): 1.3526562452316284\n",
      "\t Training loss (single batch): 1.1574209928512573\n",
      "\t Training loss (single batch): 0.9236103296279907\n",
      "\t Training loss (single batch): 0.954814076423645\n",
      "\t Training loss (single batch): 0.9884660840034485\n",
      "\t Training loss (single batch): 0.9441435933113098\n",
      "\t Training loss (single batch): 1.1758558750152588\n",
      "\t Training loss (single batch): 1.2744024991989136\n",
      "\t Training loss (single batch): 1.0922945737838745\n",
      "\t Training loss (single batch): 1.683933138847351\n",
      "\t Training loss (single batch): 0.7730452418327332\n",
      "\t Training loss (single batch): 1.0769439935684204\n",
      "\t Training loss (single batch): 1.6877613067626953\n",
      "\t Training loss (single batch): 1.0228087902069092\n",
      "\t Training loss (single batch): 0.967851459980011\n",
      "\t Training loss (single batch): 1.1279937028884888\n",
      "\t Training loss (single batch): 1.4466203451156616\n",
      "\t Training loss (single batch): 1.8195607662200928\n",
      "\t Training loss (single batch): 1.04092538356781\n",
      "\t Training loss (single batch): 1.1503461599349976\n",
      "\t Training loss (single batch): 1.390054702758789\n",
      "\t Training loss (single batch): 1.1525264978408813\n",
      "\t Training loss (single batch): 1.3355075120925903\n",
      "\t Training loss (single batch): 1.3886234760284424\n",
      "\t Training loss (single batch): 1.197325348854065\n",
      "\t Training loss (single batch): 0.9638549089431763\n",
      "\t Training loss (single batch): 1.2466886043548584\n",
      "\t Training loss (single batch): 0.9789398908615112\n",
      "\t Training loss (single batch): 1.3888250589370728\n",
      "\t Training loss (single batch): 0.7555975317955017\n",
      "\t Training loss (single batch): 1.2976044416427612\n",
      "\t Training loss (single batch): 1.2175194025039673\n",
      "\t Training loss (single batch): 0.919680655002594\n",
      "\t Training loss (single batch): 1.1350491046905518\n",
      "\t Training loss (single batch): 1.0171475410461426\n",
      "\t Training loss (single batch): 1.1120210886001587\n",
      "\t Training loss (single batch): 1.2849684953689575\n",
      "\t Training loss (single batch): 1.2713640928268433\n",
      "\t Training loss (single batch): 1.1371877193450928\n",
      "\t Training loss (single batch): 1.327867865562439\n",
      "\t Training loss (single batch): 1.0863183736801147\n",
      "\t Training loss (single batch): 1.178560733795166\n",
      "\t Training loss (single batch): 1.3926432132720947\n",
      "\t Training loss (single batch): 1.1465195417404175\n",
      "\t Training loss (single batch): 1.1436772346496582\n",
      "\t Training loss (single batch): 0.9273953437805176\n",
      "\t Training loss (single batch): 1.2111629247665405\n",
      "\t Training loss (single batch): 0.949911892414093\n",
      "\t Training loss (single batch): 0.9548491835594177\n",
      "\t Training loss (single batch): 1.2840189933776855\n",
      "\t Training loss (single batch): 1.0298657417297363\n",
      "\t Training loss (single batch): 1.5044677257537842\n",
      "\t Training loss (single batch): 0.810661792755127\n",
      "\t Training loss (single batch): 1.5653117895126343\n",
      "\t Training loss (single batch): 0.995539128780365\n",
      "\t Training loss (single batch): 1.3405593633651733\n",
      "\t Training loss (single batch): 1.234187364578247\n",
      "\t Training loss (single batch): 1.1023287773132324\n",
      "\t Training loss (single batch): 1.643652319908142\n",
      "\t Training loss (single batch): 1.0892202854156494\n",
      "\t Training loss (single batch): 0.9089804291725159\n",
      "\t Training loss (single batch): 0.8111698627471924\n",
      "\t Training loss (single batch): 0.9417450428009033\n",
      "\t Training loss (single batch): 1.2351094484329224\n",
      "\t Training loss (single batch): 1.409799337387085\n",
      "\t Training loss (single batch): 1.0724356174468994\n",
      "\t Training loss (single batch): 1.1335617303848267\n",
      "\t Training loss (single batch): 0.8327755331993103\n",
      "\t Training loss (single batch): 1.4379371404647827\n",
      "\t Training loss (single batch): 1.2232060432434082\n",
      "\t Training loss (single batch): 1.3297826051712036\n",
      "\t Training loss (single batch): 1.2915449142456055\n",
      "\t Training loss (single batch): 1.4408913850784302\n",
      "\t Training loss (single batch): 1.2179290056228638\n",
      "\t Training loss (single batch): 0.8661029934883118\n",
      "\t Training loss (single batch): 1.2636197805404663\n",
      "\t Training loss (single batch): 1.3181487321853638\n",
      "\t Training loss (single batch): 1.23792564868927\n",
      "\t Training loss (single batch): 0.9072266221046448\n",
      "\t Training loss (single batch): 1.1025587320327759\n",
      "\t Training loss (single batch): 0.7952126264572144\n",
      "\t Training loss (single batch): 0.9988399147987366\n",
      "\t Training loss (single batch): 1.7157601118087769\n",
      "\t Training loss (single batch): 1.4077707529067993\n",
      "\t Training loss (single batch): 1.1141897439956665\n",
      "\t Training loss (single batch): 0.9854534864425659\n",
      "\t Training loss (single batch): 1.3303052186965942\n",
      "\t Training loss (single batch): 1.4027975797653198\n",
      "\t Training loss (single batch): 1.017750859260559\n",
      "\t Training loss (single batch): 1.6295286417007446\n",
      "\t Training loss (single batch): 0.8505149483680725\n",
      "\t Training loss (single batch): 0.9100474119186401\n",
      "\t Training loss (single batch): 0.8847704529762268\n",
      "\t Training loss (single batch): 1.0067063570022583\n",
      "\t Training loss (single batch): 1.2881072759628296\n",
      "\t Training loss (single batch): 1.0906054973602295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7324185371398926\n",
      "\t Training loss (single batch): 1.1353814601898193\n",
      "\t Training loss (single batch): 0.9447152614593506\n",
      "\t Training loss (single batch): 1.2107083797454834\n",
      "\t Training loss (single batch): 1.3092347383499146\n",
      "\t Training loss (single batch): 1.490077018737793\n",
      "\t Training loss (single batch): 0.9749545454978943\n",
      "\t Training loss (single batch): 0.9808775782585144\n",
      "\t Training loss (single batch): 1.35299551486969\n",
      "\t Training loss (single batch): 1.0021589994430542\n",
      "\t Training loss (single batch): 1.192507266998291\n",
      "\t Training loss (single batch): 1.3115041255950928\n",
      "\t Training loss (single batch): 1.4480317831039429\n",
      "\t Training loss (single batch): 1.4257203340530396\n",
      "\t Training loss (single batch): 0.9173895716667175\n",
      "\t Training loss (single batch): 1.432952642440796\n",
      "\t Training loss (single batch): 1.2355669736862183\n",
      "\t Training loss (single batch): 1.090506911277771\n",
      "\t Training loss (single batch): 0.8067957162857056\n",
      "\t Training loss (single batch): 1.310079574584961\n",
      "\t Training loss (single batch): 1.0818781852722168\n",
      "\t Training loss (single batch): 1.132767677307129\n",
      "\t Training loss (single batch): 1.195194959640503\n",
      "\t Training loss (single batch): 1.1544722318649292\n",
      "\t Training loss (single batch): 1.1825445890426636\n",
      "\t Training loss (single batch): 1.4796175956726074\n",
      "\t Training loss (single batch): 1.2947336435317993\n",
      "\t Training loss (single batch): 1.1823853254318237\n",
      "\t Training loss (single batch): 1.0976709127426147\n",
      "\t Training loss (single batch): 1.3469995260238647\n",
      "\t Training loss (single batch): 0.7797260284423828\n",
      "\t Training loss (single batch): 1.2194972038269043\n",
      "\t Training loss (single batch): 0.9822662472724915\n",
      "\t Training loss (single batch): 0.9096124172210693\n",
      "\t Training loss (single batch): 2.215825319290161\n",
      "\t Training loss (single batch): 1.0387717485427856\n",
      "\t Training loss (single batch): 1.553770661354065\n",
      "\t Training loss (single batch): 1.1395533084869385\n",
      "\t Training loss (single batch): 1.2431691884994507\n",
      "\t Training loss (single batch): 1.2009830474853516\n",
      "\t Training loss (single batch): 1.2396612167358398\n",
      "\t Training loss (single batch): 1.8524378538131714\n",
      "\t Training loss (single batch): 1.0865724086761475\n",
      "\t Training loss (single batch): 1.3699936866760254\n",
      "\t Training loss (single batch): 1.122963547706604\n",
      "\t Training loss (single batch): 0.9403719305992126\n",
      "\t Training loss (single batch): 0.97011798620224\n",
      "\t Training loss (single batch): 1.5366138219833374\n",
      "\t Training loss (single batch): 1.2627787590026855\n",
      "\t Training loss (single batch): 1.377041220664978\n",
      "\t Training loss (single batch): 1.1575884819030762\n",
      "\t Training loss (single batch): 1.0411354303359985\n",
      "\t Training loss (single batch): 1.3661128282546997\n",
      "\t Training loss (single batch): 1.219368577003479\n",
      "\t Training loss (single batch): 1.5843812227249146\n",
      "\t Training loss (single batch): 0.9798688888549805\n",
      "\t Training loss (single batch): 0.8797039985656738\n",
      "\t Training loss (single batch): 0.9981805682182312\n",
      "\t Training loss (single batch): 1.1594990491867065\n",
      "\t Training loss (single batch): 1.3676486015319824\n",
      "\t Training loss (single batch): 1.0608772039413452\n",
      "\t Training loss (single batch): 0.9907096028327942\n",
      "\t Training loss (single batch): 0.9531176686286926\n",
      "\t Training loss (single batch): 1.2602750062942505\n",
      "\t Training loss (single batch): 1.654173493385315\n",
      "\t Training loss (single batch): 1.286572813987732\n",
      "\t Training loss (single batch): 0.9033355116844177\n",
      "\t Training loss (single batch): 1.4293644428253174\n",
      "\t Training loss (single batch): 0.8685354590415955\n",
      "\t Training loss (single batch): 1.306210994720459\n",
      "\t Training loss (single batch): 0.8498543500900269\n",
      "\t Training loss (single batch): 1.529640793800354\n",
      "\t Training loss (single batch): 1.6830016374588013\n",
      "\t Training loss (single batch): 0.9282083511352539\n",
      "\t Training loss (single batch): 1.1854585409164429\n",
      "\t Training loss (single batch): 0.9988685250282288\n",
      "\t Training loss (single batch): 1.2573952674865723\n",
      "\t Training loss (single batch): 0.6599553227424622\n",
      "\t Training loss (single batch): 1.1640188694000244\n",
      "\t Training loss (single batch): 1.241884469985962\n",
      "\t Training loss (single batch): 0.9488800764083862\n",
      "\t Training loss (single batch): 1.3230637311935425\n",
      "\t Training loss (single batch): 1.155624508857727\n",
      "\t Training loss (single batch): 1.315342664718628\n",
      "\t Training loss (single batch): 1.0868556499481201\n",
      "\t Training loss (single batch): 1.6628865003585815\n",
      "\t Training loss (single batch): 1.6253026723861694\n",
      "\t Training loss (single batch): 1.377877116203308\n",
      "\t Training loss (single batch): 1.7642298936843872\n",
      "\t Training loss (single batch): 1.2706130743026733\n",
      "\t Training loss (single batch): 1.6553555727005005\n",
      "\t Training loss (single batch): 1.2680623531341553\n",
      "\t Training loss (single batch): 0.9683912396430969\n",
      "\t Training loss (single batch): 1.1645549535751343\n",
      "\t Training loss (single batch): 0.9719040393829346\n",
      "\t Training loss (single batch): 1.175078272819519\n",
      "\t Training loss (single batch): 1.363323450088501\n",
      "\t Training loss (single batch): 0.9942139983177185\n",
      "\t Training loss (single batch): 1.0867761373519897\n",
      "\t Training loss (single batch): 1.200186014175415\n",
      "\t Training loss (single batch): 0.9389405846595764\n",
      "\t Training loss (single batch): 1.1025272607803345\n",
      "\t Training loss (single batch): 1.1869449615478516\n",
      "\t Training loss (single batch): 0.6414259076118469\n",
      "\t Training loss (single batch): 1.2449331283569336\n",
      "\t Training loss (single batch): 1.0843130350112915\n",
      "\t Training loss (single batch): 1.273917317390442\n",
      "\t Training loss (single batch): 1.138406753540039\n",
      "\t Training loss (single batch): 1.0004396438598633\n",
      "\t Training loss (single batch): 0.8613691926002502\n",
      "\t Training loss (single batch): 1.0408155918121338\n",
      "\t Training loss (single batch): 1.0251491069793701\n",
      "\t Training loss (single batch): 1.079209327697754\n",
      "\t Training loss (single batch): 1.272613525390625\n",
      "\t Training loss (single batch): 1.2984954118728638\n",
      "\t Training loss (single batch): 0.9067521095275879\n",
      "\t Training loss (single batch): 0.9566683769226074\n",
      "\t Training loss (single batch): 0.8818848133087158\n",
      "\t Training loss (single batch): 1.4348881244659424\n",
      "##################################\n",
      "## EPOCH 84\n",
      "##################################\n",
      "\t Training loss (single batch): 1.390335202217102\n",
      "\t Training loss (single batch): 1.3201391696929932\n",
      "\t Training loss (single batch): 1.4184471368789673\n",
      "\t Training loss (single batch): 0.8238133192062378\n",
      "\t Training loss (single batch): 0.8468257188796997\n",
      "\t Training loss (single batch): 1.3111217021942139\n",
      "\t Training loss (single batch): 1.2192442417144775\n",
      "\t Training loss (single batch): 0.9524443745613098\n",
      "\t Training loss (single batch): 1.2745530605316162\n",
      "\t Training loss (single batch): 1.2769445180892944\n",
      "\t Training loss (single batch): 1.0030235052108765\n",
      "\t Training loss (single batch): 1.1085591316223145\n",
      "\t Training loss (single batch): 0.7875528931617737\n",
      "\t Training loss (single batch): 1.2144584655761719\n",
      "\t Training loss (single batch): 1.449090600013733\n",
      "\t Training loss (single batch): 1.255926251411438\n",
      "\t Training loss (single batch): 1.4007993936538696\n",
      "\t Training loss (single batch): 0.6993168592453003\n",
      "\t Training loss (single batch): 1.415871024131775\n",
      "\t Training loss (single batch): 1.117428183555603\n",
      "\t Training loss (single batch): 1.0759111642837524\n",
      "\t Training loss (single batch): 1.2530829906463623\n",
      "\t Training loss (single batch): 1.633935570716858\n",
      "\t Training loss (single batch): 1.2387868165969849\n",
      "\t Training loss (single batch): 0.9960468411445618\n",
      "\t Training loss (single batch): 1.1648863554000854\n",
      "\t Training loss (single batch): 1.0884267091751099\n",
      "\t Training loss (single batch): 1.058260202407837\n",
      "\t Training loss (single batch): 1.5611928701400757\n",
      "\t Training loss (single batch): 0.7065234780311584\n",
      "\t Training loss (single batch): 1.3638684749603271\n",
      "\t Training loss (single batch): 1.346030354499817\n",
      "\t Training loss (single batch): 1.1370168924331665\n",
      "\t Training loss (single batch): 1.1574618816375732\n",
      "\t Training loss (single batch): 1.2964552640914917\n",
      "\t Training loss (single batch): 1.4407285451889038\n",
      "\t Training loss (single batch): 1.115369200706482\n",
      "\t Training loss (single batch): 0.9584360122680664\n",
      "\t Training loss (single batch): 0.871202290058136\n",
      "\t Training loss (single batch): 1.2362185716629028\n",
      "\t Training loss (single batch): 1.2526662349700928\n",
      "\t Training loss (single batch): 1.194414734840393\n",
      "\t Training loss (single batch): 1.1570967435836792\n",
      "\t Training loss (single batch): 1.6432054042816162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2294628620147705\n",
      "\t Training loss (single batch): 1.2817929983139038\n",
      "\t Training loss (single batch): 1.4593713283538818\n",
      "\t Training loss (single batch): 1.250557780265808\n",
      "\t Training loss (single batch): 1.6469603776931763\n",
      "\t Training loss (single batch): 1.2804133892059326\n",
      "\t Training loss (single batch): 1.2642323970794678\n",
      "\t Training loss (single batch): 1.1002609729766846\n",
      "\t Training loss (single batch): 1.0266716480255127\n",
      "\t Training loss (single batch): 0.9386600255966187\n",
      "\t Training loss (single batch): 0.9592787623405457\n",
      "\t Training loss (single batch): 1.1631507873535156\n",
      "\t Training loss (single batch): 1.1495124101638794\n",
      "\t Training loss (single batch): 0.9663144946098328\n",
      "\t Training loss (single batch): 1.4968454837799072\n",
      "\t Training loss (single batch): 2.064218759536743\n",
      "\t Training loss (single batch): 1.348854660987854\n",
      "\t Training loss (single batch): 0.7081195116043091\n",
      "\t Training loss (single batch): 1.1819076538085938\n",
      "\t Training loss (single batch): 1.5945864915847778\n",
      "\t Training loss (single batch): 0.6761689186096191\n",
      "\t Training loss (single batch): 1.2018603086471558\n",
      "\t Training loss (single batch): 0.9414172172546387\n",
      "\t Training loss (single batch): 1.279371738433838\n",
      "\t Training loss (single batch): 1.3633856773376465\n",
      "\t Training loss (single batch): 1.4534807205200195\n",
      "\t Training loss (single batch): 1.1861833333969116\n",
      "\t Training loss (single batch): 1.2582714557647705\n",
      "\t Training loss (single batch): 1.1031529903411865\n",
      "\t Training loss (single batch): 1.0233622789382935\n",
      "\t Training loss (single batch): 1.435890793800354\n",
      "\t Training loss (single batch): 1.4783711433410645\n",
      "\t Training loss (single batch): 1.342721700668335\n",
      "\t Training loss (single batch): 1.3876070976257324\n",
      "\t Training loss (single batch): 1.3100708723068237\n",
      "\t Training loss (single batch): 1.4515773057937622\n",
      "\t Training loss (single batch): 1.2368298768997192\n",
      "\t Training loss (single batch): 1.3200721740722656\n",
      "\t Training loss (single batch): 1.0282847881317139\n",
      "\t Training loss (single batch): 1.123730182647705\n",
      "\t Training loss (single batch): 1.703096628189087\n",
      "\t Training loss (single batch): 1.1886075735092163\n",
      "\t Training loss (single batch): 1.2234150171279907\n",
      "\t Training loss (single batch): 1.1366232633590698\n",
      "\t Training loss (single batch): 1.1534833908081055\n",
      "\t Training loss (single batch): 1.3571974039077759\n",
      "\t Training loss (single batch): 1.2025916576385498\n",
      "\t Training loss (single batch): 0.9668900370597839\n",
      "\t Training loss (single batch): 0.9899418354034424\n",
      "\t Training loss (single batch): 1.3420614004135132\n",
      "\t Training loss (single batch): 0.771148681640625\n",
      "\t Training loss (single batch): 0.8738639950752258\n",
      "\t Training loss (single batch): 1.082503318786621\n",
      "\t Training loss (single batch): 1.2225546836853027\n",
      "\t Training loss (single batch): 1.3642616271972656\n",
      "\t Training loss (single batch): 1.3996291160583496\n",
      "\t Training loss (single batch): 0.9473462700843811\n",
      "\t Training loss (single batch): 1.0437877178192139\n",
      "\t Training loss (single batch): 1.167479157447815\n",
      "\t Training loss (single batch): 0.9818326830863953\n",
      "\t Training loss (single batch): 1.0771243572235107\n",
      "\t Training loss (single batch): 1.0013997554779053\n",
      "\t Training loss (single batch): 1.4235786199569702\n",
      "\t Training loss (single batch): 0.8508362770080566\n",
      "\t Training loss (single batch): 1.3006482124328613\n",
      "\t Training loss (single batch): 1.2428592443466187\n",
      "\t Training loss (single batch): 1.0255645513534546\n",
      "\t Training loss (single batch): 0.8654182553291321\n",
      "\t Training loss (single batch): 2.1313655376434326\n",
      "\t Training loss (single batch): 1.106323003768921\n",
      "\t Training loss (single batch): 1.694887638092041\n",
      "\t Training loss (single batch): 1.1094563007354736\n",
      "\t Training loss (single batch): 1.001964807510376\n",
      "\t Training loss (single batch): 1.5285837650299072\n",
      "\t Training loss (single batch): 1.133142352104187\n",
      "\t Training loss (single batch): 1.311393141746521\n",
      "\t Training loss (single batch): 0.9345710873603821\n",
      "\t Training loss (single batch): 1.1931359767913818\n",
      "\t Training loss (single batch): 1.377290964126587\n",
      "\t Training loss (single batch): 0.8609657287597656\n",
      "\t Training loss (single batch): 1.5028434991836548\n",
      "\t Training loss (single batch): 1.0427089929580688\n",
      "\t Training loss (single batch): 1.2469995021820068\n",
      "\t Training loss (single batch): 1.1287298202514648\n",
      "\t Training loss (single batch): 1.3767669200897217\n",
      "\t Training loss (single batch): 0.9996058344841003\n",
      "\t Training loss (single batch): 1.144795536994934\n",
      "\t Training loss (single batch): 1.1064900159835815\n",
      "\t Training loss (single batch): 0.9549814462661743\n",
      "\t Training loss (single batch): 1.1337661743164062\n",
      "\t Training loss (single batch): 1.688130259513855\n",
      "\t Training loss (single batch): 0.885319173336029\n",
      "\t Training loss (single batch): 1.2799431085586548\n",
      "\t Training loss (single batch): 1.0933477878570557\n",
      "\t Training loss (single batch): 1.0389548540115356\n",
      "\t Training loss (single batch): 0.6639007925987244\n",
      "\t Training loss (single batch): 1.1909472942352295\n",
      "\t Training loss (single batch): 1.0502551794052124\n",
      "\t Training loss (single batch): 1.3926899433135986\n",
      "\t Training loss (single batch): 1.0893787145614624\n",
      "\t Training loss (single batch): 1.18156898021698\n",
      "\t Training loss (single batch): 1.6516121625900269\n",
      "\t Training loss (single batch): 1.2231289148330688\n",
      "\t Training loss (single batch): 1.3416690826416016\n",
      "\t Training loss (single batch): 1.712196946144104\n",
      "\t Training loss (single batch): 1.4721803665161133\n",
      "\t Training loss (single batch): 1.0497808456420898\n",
      "\t Training loss (single batch): 1.1482700109481812\n",
      "\t Training loss (single batch): 1.3158955574035645\n",
      "\t Training loss (single batch): 0.9127209186553955\n",
      "\t Training loss (single batch): 1.4826619625091553\n",
      "\t Training loss (single batch): 1.584938645362854\n",
      "\t Training loss (single batch): 1.4661195278167725\n",
      "\t Training loss (single batch): 1.5268220901489258\n",
      "\t Training loss (single batch): 1.2790030241012573\n",
      "\t Training loss (single batch): 0.9526467323303223\n",
      "\t Training loss (single batch): 1.1667853593826294\n",
      "\t Training loss (single batch): 1.362744688987732\n",
      "\t Training loss (single batch): 1.5260592699050903\n",
      "\t Training loss (single batch): 0.9132344722747803\n",
      "\t Training loss (single batch): 1.5892417430877686\n",
      "\t Training loss (single batch): 1.063993215560913\n",
      "\t Training loss (single batch): 1.1251962184906006\n",
      "\t Training loss (single batch): 0.9688880443572998\n",
      "\t Training loss (single batch): 1.312801718711853\n",
      "\t Training loss (single batch): 1.1102497577667236\n",
      "\t Training loss (single batch): 1.6331418752670288\n",
      "\t Training loss (single batch): 0.6929308176040649\n",
      "\t Training loss (single batch): 1.1251697540283203\n",
      "\t Training loss (single batch): 1.3160134553909302\n",
      "\t Training loss (single batch): 0.9673235416412354\n",
      "\t Training loss (single batch): 0.9640002250671387\n",
      "\t Training loss (single batch): 1.3201212882995605\n",
      "\t Training loss (single batch): 1.141492486000061\n",
      "\t Training loss (single batch): 1.0811803340911865\n",
      "\t Training loss (single batch): 1.468760371208191\n",
      "\t Training loss (single batch): 0.9696160554885864\n",
      "\t Training loss (single batch): 1.338680624961853\n",
      "\t Training loss (single batch): 0.9960153698921204\n",
      "\t Training loss (single batch): 1.1373658180236816\n",
      "\t Training loss (single batch): 1.4051908254623413\n",
      "\t Training loss (single batch): 1.1671861410140991\n",
      "\t Training loss (single batch): 1.198530912399292\n",
      "\t Training loss (single batch): 0.9536908864974976\n",
      "\t Training loss (single batch): 1.2199937105178833\n",
      "\t Training loss (single batch): 0.918755054473877\n",
      "\t Training loss (single batch): 0.7844768166542053\n",
      "\t Training loss (single batch): 1.5370476245880127\n",
      "\t Training loss (single batch): 1.5015416145324707\n",
      "\t Training loss (single batch): 1.2500207424163818\n",
      "\t Training loss (single batch): 1.1652814149856567\n",
      "\t Training loss (single batch): 1.1355903148651123\n",
      "\t Training loss (single batch): 0.8313531279563904\n",
      "\t Training loss (single batch): 1.2445417642593384\n",
      "\t Training loss (single batch): 1.2173571586608887\n",
      "\t Training loss (single batch): 0.9841746091842651\n",
      "\t Training loss (single batch): 1.22140634059906\n",
      "\t Training loss (single batch): 0.8865432143211365\n",
      "\t Training loss (single batch): 1.3270277976989746\n",
      "\t Training loss (single batch): 0.7286550998687744\n",
      "\t Training loss (single batch): 0.9919050335884094\n",
      "\t Training loss (single batch): 0.8943755626678467\n",
      "\t Training loss (single batch): 1.2216898202896118\n",
      "\t Training loss (single batch): 1.2386019229888916\n",
      "\t Training loss (single batch): 1.6890969276428223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1013133525848389\n",
      "\t Training loss (single batch): 1.3550318479537964\n",
      "\t Training loss (single batch): 0.9595341682434082\n",
      "\t Training loss (single batch): 1.6738035678863525\n",
      "\t Training loss (single batch): 1.2114962339401245\n",
      "\t Training loss (single batch): 0.9028913378715515\n",
      "\t Training loss (single batch): 1.4782557487487793\n",
      "\t Training loss (single batch): 0.5408071875572205\n",
      "\t Training loss (single batch): 0.9872519373893738\n",
      "\t Training loss (single batch): 1.2484952211380005\n",
      "\t Training loss (single batch): 1.429684042930603\n",
      "\t Training loss (single batch): 1.2385599613189697\n",
      "\t Training loss (single batch): 1.4197111129760742\n",
      "\t Training loss (single batch): 1.775194525718689\n",
      "\t Training loss (single batch): 1.3389511108398438\n",
      "\t Training loss (single batch): 1.5782923698425293\n",
      "\t Training loss (single batch): 1.6113260984420776\n",
      "\t Training loss (single batch): 2.537257432937622\n",
      "\t Training loss (single batch): 1.1097304821014404\n",
      "\t Training loss (single batch): 1.0513006448745728\n",
      "\t Training loss (single batch): 1.058521032333374\n",
      "\t Training loss (single batch): 0.984082818031311\n",
      "\t Training loss (single batch): 1.217962384223938\n",
      "\t Training loss (single batch): 0.9552993774414062\n",
      "\t Training loss (single batch): 1.5421807765960693\n",
      "\t Training loss (single batch): 1.1722995042800903\n",
      "\t Training loss (single batch): 1.3305269479751587\n",
      "\t Training loss (single batch): 0.9845304489135742\n",
      "\t Training loss (single batch): 1.6323455572128296\n",
      "\t Training loss (single batch): 0.9282899498939514\n",
      "\t Training loss (single batch): 1.64138662815094\n",
      "\t Training loss (single batch): 1.17816162109375\n",
      "\t Training loss (single batch): 0.9249593615531921\n",
      "\t Training loss (single batch): 1.439771056175232\n",
      "\t Training loss (single batch): 1.1005407571792603\n",
      "\t Training loss (single batch): 0.9324080944061279\n",
      "\t Training loss (single batch): 1.0948740243911743\n",
      "\t Training loss (single batch): 0.9520703554153442\n",
      "\t Training loss (single batch): 1.3929945230484009\n",
      "\t Training loss (single batch): 1.166959524154663\n",
      "\t Training loss (single batch): 0.888221800327301\n",
      "\t Training loss (single batch): 1.403416395187378\n",
      "\t Training loss (single batch): 1.0768710374832153\n",
      "\t Training loss (single batch): 1.5186209678649902\n",
      "\t Training loss (single batch): 1.304969310760498\n",
      "\t Training loss (single batch): 0.9561381936073303\n",
      "\t Training loss (single batch): 1.1605063676834106\n",
      "\t Training loss (single batch): 0.9558833241462708\n",
      "\t Training loss (single batch): 1.2736631631851196\n",
      "\t Training loss (single batch): 0.995802104473114\n",
      "\t Training loss (single batch): 1.032683253288269\n",
      "\t Training loss (single batch): 1.0704448223114014\n",
      "\t Training loss (single batch): 1.018213152885437\n",
      "\t Training loss (single batch): 1.36831533908844\n",
      "\t Training loss (single batch): 1.183851718902588\n",
      "\t Training loss (single batch): 1.3377125263214111\n",
      "\t Training loss (single batch): 1.2435365915298462\n",
      "\t Training loss (single batch): 0.8490184545516968\n",
      "\t Training loss (single batch): 0.9853390455245972\n",
      "\t Training loss (single batch): 1.2897495031356812\n",
      "\t Training loss (single batch): 1.1248195171356201\n",
      "\t Training loss (single batch): 1.1805493831634521\n",
      "\t Training loss (single batch): 1.3334102630615234\n",
      "\t Training loss (single batch): 1.1502307653427124\n",
      "\t Training loss (single batch): 1.4734783172607422\n",
      "\t Training loss (single batch): 1.0477442741394043\n",
      "\t Training loss (single batch): 1.4512219429016113\n",
      "\t Training loss (single batch): 0.7918967604637146\n",
      "\t Training loss (single batch): 1.0235199928283691\n",
      "\t Training loss (single batch): 1.4321918487548828\n",
      "\t Training loss (single batch): 1.5829694271087646\n",
      "\t Training loss (single batch): 1.2603919506072998\n",
      "\t Training loss (single batch): 1.1124889850616455\n",
      "\t Training loss (single batch): 0.8479558825492859\n",
      "\t Training loss (single batch): 1.4966719150543213\n",
      "\t Training loss (single batch): 1.2741204500198364\n",
      "\t Training loss (single batch): 1.064150333404541\n",
      "\t Training loss (single batch): 0.9485018253326416\n",
      "\t Training loss (single batch): 1.0177730321884155\n",
      "\t Training loss (single batch): 1.0725717544555664\n",
      "\t Training loss (single batch): 1.3424346446990967\n",
      "\t Training loss (single batch): 1.6449888944625854\n",
      "\t Training loss (single batch): 1.1192716360092163\n",
      "\t Training loss (single batch): 0.9822924137115479\n",
      "\t Training loss (single batch): 0.9878761172294617\n",
      "\t Training loss (single batch): 1.3703055381774902\n",
      "\t Training loss (single batch): 1.4114660024642944\n",
      "\t Training loss (single batch): 1.689284324645996\n",
      "\t Training loss (single batch): 0.8360947370529175\n",
      "\t Training loss (single batch): 1.4733648300170898\n",
      "\t Training loss (single batch): 0.9220418930053711\n",
      "\t Training loss (single batch): 1.1803816556930542\n",
      "\t Training loss (single batch): 0.9924898743629456\n",
      "\t Training loss (single batch): 1.3497061729431152\n",
      "\t Training loss (single batch): 1.1483393907546997\n",
      "\t Training loss (single batch): 0.9109659790992737\n",
      "\t Training loss (single batch): 0.9416258335113525\n",
      "\t Training loss (single batch): 0.722727358341217\n",
      "\t Training loss (single batch): 1.1089587211608887\n",
      "\t Training loss (single batch): 1.4031981229782104\n",
      "\t Training loss (single batch): 0.8690738081932068\n",
      "\t Training loss (single batch): 1.3614193201065063\n",
      "\t Training loss (single batch): 1.643964409828186\n",
      "\t Training loss (single batch): 1.024519681930542\n",
      "\t Training loss (single batch): 1.287628412246704\n",
      "\t Training loss (single batch): 0.8139574527740479\n",
      "\t Training loss (single batch): 1.378588080406189\n",
      "\t Training loss (single batch): 3.127570629119873\n",
      "##################################\n",
      "## EPOCH 85\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9362714290618896\n",
      "\t Training loss (single batch): 0.9649568200111389\n",
      "\t Training loss (single batch): 1.0584651231765747\n",
      "\t Training loss (single batch): 1.3257633447647095\n",
      "\t Training loss (single batch): 0.9417172074317932\n",
      "\t Training loss (single batch): 1.6084764003753662\n",
      "\t Training loss (single batch): 1.577094316482544\n",
      "\t Training loss (single batch): 1.2063456773757935\n",
      "\t Training loss (single batch): 1.5895863771438599\n",
      "\t Training loss (single batch): 1.1403565406799316\n",
      "\t Training loss (single batch): 1.1053452491760254\n",
      "\t Training loss (single batch): 1.22359299659729\n",
      "\t Training loss (single batch): 2.4399709701538086\n",
      "\t Training loss (single batch): 0.968171238899231\n",
      "\t Training loss (single batch): 0.9074010848999023\n",
      "\t Training loss (single batch): 1.00151526927948\n",
      "\t Training loss (single batch): 1.2707809209823608\n",
      "\t Training loss (single batch): 1.09480881690979\n",
      "\t Training loss (single batch): 1.5061838626861572\n",
      "\t Training loss (single batch): 1.0275139808654785\n",
      "\t Training loss (single batch): 1.3429874181747437\n",
      "\t Training loss (single batch): 0.8463690280914307\n",
      "\t Training loss (single batch): 1.2407513856887817\n",
      "\t Training loss (single batch): 1.5060762166976929\n",
      "\t Training loss (single batch): 0.9421406984329224\n",
      "\t Training loss (single batch): 1.2998894453048706\n",
      "\t Training loss (single batch): 1.0617629289627075\n",
      "\t Training loss (single batch): 0.8893616199493408\n",
      "\t Training loss (single batch): 1.0545687675476074\n",
      "\t Training loss (single batch): 1.1429492235183716\n",
      "\t Training loss (single batch): 1.2562062740325928\n",
      "\t Training loss (single batch): 0.9357706308364868\n",
      "\t Training loss (single batch): 1.3584364652633667\n",
      "\t Training loss (single batch): 1.0147794485092163\n",
      "\t Training loss (single batch): 1.4898556470870972\n",
      "\t Training loss (single batch): 0.9725891351699829\n",
      "\t Training loss (single batch): 1.1807793378829956\n",
      "\t Training loss (single batch): 1.0280905961990356\n",
      "\t Training loss (single batch): 1.2527230978012085\n",
      "\t Training loss (single batch): 1.0825806856155396\n",
      "\t Training loss (single batch): 1.6952080726623535\n",
      "\t Training loss (single batch): 1.1313543319702148\n",
      "\t Training loss (single batch): 1.2336751222610474\n",
      "\t Training loss (single batch): 1.1951508522033691\n",
      "\t Training loss (single batch): 1.3309440612792969\n",
      "\t Training loss (single batch): 1.0463300943374634\n",
      "\t Training loss (single batch): 0.8222400546073914\n",
      "\t Training loss (single batch): 1.601570725440979\n",
      "\t Training loss (single batch): 1.2489296197891235\n",
      "\t Training loss (single batch): 0.5810549855232239\n",
      "\t Training loss (single batch): 1.1564738750457764\n",
      "\t Training loss (single batch): 1.2093243598937988\n",
      "\t Training loss (single batch): 1.1263395547866821\n",
      "\t Training loss (single batch): 1.3166857957839966\n",
      "\t Training loss (single batch): 1.0745247602462769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9412604570388794\n",
      "\t Training loss (single batch): 1.5642513036727905\n",
      "\t Training loss (single batch): 1.487412452697754\n",
      "\t Training loss (single batch): 1.2974454164505005\n",
      "\t Training loss (single batch): 0.9008752703666687\n",
      "\t Training loss (single batch): 1.3575279712677002\n",
      "\t Training loss (single batch): 1.0113805532455444\n",
      "\t Training loss (single batch): 1.4147471189498901\n",
      "\t Training loss (single batch): 1.302897572517395\n",
      "\t Training loss (single batch): 1.4684275388717651\n",
      "\t Training loss (single batch): 1.1967825889587402\n",
      "\t Training loss (single batch): 1.2991787195205688\n",
      "\t Training loss (single batch): 1.2160500288009644\n",
      "\t Training loss (single batch): 0.6631599068641663\n",
      "\t Training loss (single batch): 0.9488959908485413\n",
      "\t Training loss (single batch): 0.981776237487793\n",
      "\t Training loss (single batch): 0.9163679480552673\n",
      "\t Training loss (single batch): 0.9644834399223328\n",
      "\t Training loss (single batch): 1.004338264465332\n",
      "\t Training loss (single batch): 0.9071056246757507\n",
      "\t Training loss (single batch): 0.974685549736023\n",
      "\t Training loss (single batch): 1.175103783607483\n",
      "\t Training loss (single batch): 1.350895881652832\n",
      "\t Training loss (single batch): 0.9890921711921692\n",
      "\t Training loss (single batch): 1.2632945775985718\n",
      "\t Training loss (single batch): 1.0649043321609497\n",
      "\t Training loss (single batch): 0.7838250398635864\n",
      "\t Training loss (single batch): 1.1621065139770508\n",
      "\t Training loss (single batch): 0.838880717754364\n",
      "\t Training loss (single batch): 1.081392765045166\n",
      "\t Training loss (single batch): 0.7766639590263367\n",
      "\t Training loss (single batch): 1.0027856826782227\n",
      "\t Training loss (single batch): 1.985298752784729\n",
      "\t Training loss (single batch): 0.7614575624465942\n",
      "\t Training loss (single batch): 1.4178825616836548\n",
      "\t Training loss (single batch): 1.0138949155807495\n",
      "\t Training loss (single batch): 1.2897247076034546\n",
      "\t Training loss (single batch): 1.625497817993164\n",
      "\t Training loss (single batch): 1.013817548751831\n",
      "\t Training loss (single batch): 1.350678563117981\n",
      "\t Training loss (single batch): 1.2438796758651733\n",
      "\t Training loss (single batch): 1.5557514429092407\n",
      "\t Training loss (single batch): 1.4912127256393433\n",
      "\t Training loss (single batch): 1.3393285274505615\n",
      "\t Training loss (single batch): 1.1133307218551636\n",
      "\t Training loss (single batch): 0.9297420978546143\n",
      "\t Training loss (single batch): 0.880558967590332\n",
      "\t Training loss (single batch): 0.9077008366584778\n",
      "\t Training loss (single batch): 1.0771957635879517\n",
      "\t Training loss (single batch): 1.4769234657287598\n",
      "\t Training loss (single batch): 1.2937196493148804\n",
      "\t Training loss (single batch): 1.1475886106491089\n",
      "\t Training loss (single batch): 1.1476831436157227\n",
      "\t Training loss (single batch): 0.9270482659339905\n",
      "\t Training loss (single batch): 0.7797830700874329\n",
      "\t Training loss (single batch): 0.8810406923294067\n",
      "\t Training loss (single batch): 1.3015965223312378\n",
      "\t Training loss (single batch): 0.8458482027053833\n",
      "\t Training loss (single batch): 2.0167136192321777\n",
      "\t Training loss (single batch): 1.314366340637207\n",
      "\t Training loss (single batch): 1.7267217636108398\n",
      "\t Training loss (single batch): 1.2349714040756226\n",
      "\t Training loss (single batch): 1.0594874620437622\n",
      "\t Training loss (single batch): 1.0819907188415527\n",
      "\t Training loss (single batch): 0.9083219766616821\n",
      "\t Training loss (single batch): 1.145590901374817\n",
      "\t Training loss (single batch): 1.072850227355957\n",
      "\t Training loss (single batch): 1.4251443147659302\n",
      "\t Training loss (single batch): 1.3197228908538818\n",
      "\t Training loss (single batch): 0.6622650027275085\n",
      "\t Training loss (single batch): 1.1654784679412842\n",
      "\t Training loss (single batch): 1.0961298942565918\n",
      "\t Training loss (single batch): 1.6309583187103271\n",
      "\t Training loss (single batch): 0.9060050249099731\n",
      "\t Training loss (single batch): 0.7350061535835266\n",
      "\t Training loss (single batch): 0.9250484108924866\n",
      "\t Training loss (single batch): 1.084191918373108\n",
      "\t Training loss (single batch): 1.3801703453063965\n",
      "\t Training loss (single batch): 1.0246691703796387\n",
      "\t Training loss (single batch): 0.7591091394424438\n",
      "\t Training loss (single batch): 1.455186128616333\n",
      "\t Training loss (single batch): 0.838523268699646\n",
      "\t Training loss (single batch): 1.026611328125\n",
      "\t Training loss (single batch): 0.8127303123474121\n",
      "\t Training loss (single batch): 0.7943073511123657\n",
      "\t Training loss (single batch): 0.9222613573074341\n",
      "\t Training loss (single batch): 1.5453388690948486\n",
      "\t Training loss (single batch): 1.4682722091674805\n",
      "\t Training loss (single batch): 0.6790007948875427\n",
      "\t Training loss (single batch): 0.9648949503898621\n",
      "\t Training loss (single batch): 1.2721458673477173\n",
      "\t Training loss (single batch): 1.9789098501205444\n",
      "\t Training loss (single batch): 1.1969600915908813\n",
      "\t Training loss (single batch): 1.2184330224990845\n",
      "\t Training loss (single batch): 1.048696756362915\n",
      "\t Training loss (single batch): 1.3002842664718628\n",
      "\t Training loss (single batch): 1.3927572965621948\n",
      "\t Training loss (single batch): 1.5898789167404175\n",
      "\t Training loss (single batch): 1.5670379400253296\n",
      "\t Training loss (single batch): 0.9328797459602356\n",
      "\t Training loss (single batch): 1.2662910223007202\n",
      "\t Training loss (single batch): 0.8524979948997498\n",
      "\t Training loss (single batch): 1.3263053894042969\n",
      "\t Training loss (single batch): 1.1867711544036865\n",
      "\t Training loss (single batch): 1.1157584190368652\n",
      "\t Training loss (single batch): 0.791204035282135\n",
      "\t Training loss (single batch): 1.1742675304412842\n",
      "\t Training loss (single batch): 1.6113778352737427\n",
      "\t Training loss (single batch): 1.2579855918884277\n",
      "\t Training loss (single batch): 1.538576364517212\n",
      "\t Training loss (single batch): 1.1510887145996094\n",
      "\t Training loss (single batch): 1.6508393287658691\n",
      "\t Training loss (single batch): 1.0218837261199951\n",
      "\t Training loss (single batch): 0.942644476890564\n",
      "\t Training loss (single batch): 0.9381269216537476\n",
      "\t Training loss (single batch): 1.3250774145126343\n",
      "\t Training loss (single batch): 1.1828702688217163\n",
      "\t Training loss (single batch): 1.35832941532135\n",
      "\t Training loss (single batch): 1.2512378692626953\n",
      "\t Training loss (single batch): 1.4475027322769165\n",
      "\t Training loss (single batch): 1.6561874151229858\n",
      "\t Training loss (single batch): 0.8357739448547363\n",
      "\t Training loss (single batch): 1.3498822450637817\n",
      "\t Training loss (single batch): 1.1625099182128906\n",
      "\t Training loss (single batch): 1.0908862352371216\n",
      "\t Training loss (single batch): 1.0376136302947998\n",
      "\t Training loss (single batch): 1.6804155111312866\n",
      "\t Training loss (single batch): 1.1338791847229004\n",
      "\t Training loss (single batch): 1.4520350694656372\n",
      "\t Training loss (single batch): 0.7209952473640442\n",
      "\t Training loss (single batch): 1.7713367938995361\n",
      "\t Training loss (single batch): 1.2497013807296753\n",
      "\t Training loss (single batch): 1.2819770574569702\n",
      "\t Training loss (single batch): 1.7072159051895142\n",
      "\t Training loss (single batch): 1.2256838083267212\n",
      "\t Training loss (single batch): 0.8654358983039856\n",
      "\t Training loss (single batch): 1.5378867387771606\n",
      "\t Training loss (single batch): 0.9366295337677002\n",
      "\t Training loss (single batch): 1.071831464767456\n",
      "\t Training loss (single batch): 1.0468013286590576\n",
      "\t Training loss (single batch): 0.8910819292068481\n",
      "\t Training loss (single batch): 1.4109421968460083\n",
      "\t Training loss (single batch): 1.164982795715332\n",
      "\t Training loss (single batch): 0.8963157534599304\n",
      "\t Training loss (single batch): 0.8537040948867798\n",
      "\t Training loss (single batch): 1.2464195489883423\n",
      "\t Training loss (single batch): 1.2056856155395508\n",
      "\t Training loss (single batch): 1.025278925895691\n",
      "\t Training loss (single batch): 1.1335322856903076\n",
      "\t Training loss (single batch): 0.7883890867233276\n",
      "\t Training loss (single batch): 0.9126112461090088\n",
      "\t Training loss (single batch): 1.1223734617233276\n",
      "\t Training loss (single batch): 1.2928415536880493\n",
      "\t Training loss (single batch): 1.6468149423599243\n",
      "\t Training loss (single batch): 1.7469452619552612\n",
      "\t Training loss (single batch): 1.7452090978622437\n",
      "\t Training loss (single batch): 1.4584637880325317\n",
      "\t Training loss (single batch): 0.9675177335739136\n",
      "\t Training loss (single batch): 1.1785112619400024\n",
      "\t Training loss (single batch): 1.28074312210083\n",
      "\t Training loss (single batch): 1.1822264194488525\n",
      "\t Training loss (single batch): 1.224279761314392\n",
      "\t Training loss (single batch): 1.2701271772384644\n",
      "\t Training loss (single batch): 1.2423396110534668\n",
      "\t Training loss (single batch): 1.1970891952514648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4987773895263672\n",
      "\t Training loss (single batch): 1.1363111734390259\n",
      "\t Training loss (single batch): 1.6229571104049683\n",
      "\t Training loss (single batch): 1.4007326364517212\n",
      "\t Training loss (single batch): 1.1970466375350952\n",
      "\t Training loss (single batch): 0.9271865487098694\n",
      "\t Training loss (single batch): 1.0569305419921875\n",
      "\t Training loss (single batch): 1.1397583484649658\n",
      "\t Training loss (single batch): 1.0034196376800537\n",
      "\t Training loss (single batch): 0.565471351146698\n",
      "\t Training loss (single batch): 0.8509727120399475\n",
      "\t Training loss (single batch): 1.302846908569336\n",
      "\t Training loss (single batch): 1.2701079845428467\n",
      "\t Training loss (single batch): 0.7244237065315247\n",
      "\t Training loss (single batch): 0.9602153301239014\n",
      "\t Training loss (single batch): 1.3581289052963257\n",
      "\t Training loss (single batch): 1.1637670993804932\n",
      "\t Training loss (single batch): 1.0186480283737183\n",
      "\t Training loss (single batch): 0.7546979784965515\n",
      "\t Training loss (single batch): 1.099050760269165\n",
      "\t Training loss (single batch): 1.2132948637008667\n",
      "\t Training loss (single batch): 0.8962303400039673\n",
      "\t Training loss (single batch): 1.0240871906280518\n",
      "\t Training loss (single batch): 1.1846023797988892\n",
      "\t Training loss (single batch): 1.0569384098052979\n",
      "\t Training loss (single batch): 1.2654101848602295\n",
      "\t Training loss (single batch): 0.9467028975486755\n",
      "\t Training loss (single batch): 0.9781599044799805\n",
      "\t Training loss (single batch): 0.8325597047805786\n",
      "\t Training loss (single batch): 1.1636532545089722\n",
      "\t Training loss (single batch): 1.450005054473877\n",
      "\t Training loss (single batch): 1.2392252683639526\n",
      "\t Training loss (single batch): 1.0100964307785034\n",
      "\t Training loss (single batch): 1.2695683240890503\n",
      "\t Training loss (single batch): 0.6649792194366455\n",
      "\t Training loss (single batch): 1.3070915937423706\n",
      "\t Training loss (single batch): 1.3146840333938599\n",
      "\t Training loss (single batch): 1.1313773393630981\n",
      "\t Training loss (single batch): 1.2002475261688232\n",
      "\t Training loss (single batch): 1.0791139602661133\n",
      "\t Training loss (single batch): 1.2611916065216064\n",
      "\t Training loss (single batch): 1.3337115049362183\n",
      "\t Training loss (single batch): 1.1278300285339355\n",
      "\t Training loss (single batch): 1.0172936916351318\n",
      "\t Training loss (single batch): 1.2101060152053833\n",
      "\t Training loss (single batch): 1.6349925994873047\n",
      "\t Training loss (single batch): 1.3868850469589233\n",
      "\t Training loss (single batch): 0.5911879539489746\n",
      "\t Training loss (single batch): 1.9441968202590942\n",
      "\t Training loss (single batch): 1.4416358470916748\n",
      "\t Training loss (single batch): 0.7915587425231934\n",
      "\t Training loss (single batch): 1.130049705505371\n",
      "\t Training loss (single batch): 1.0800449848175049\n",
      "\t Training loss (single batch): 1.0919042825698853\n",
      "\t Training loss (single batch): 0.9971507787704468\n",
      "\t Training loss (single batch): 1.239598274230957\n",
      "\t Training loss (single batch): 1.0464948415756226\n",
      "\t Training loss (single batch): 1.3638043403625488\n",
      "\t Training loss (single batch): 1.2676304578781128\n",
      "\t Training loss (single batch): 0.8008531928062439\n",
      "\t Training loss (single batch): 1.3624616861343384\n",
      "\t Training loss (single batch): 1.0342364311218262\n",
      "\t Training loss (single batch): 0.8639454245567322\n",
      "\t Training loss (single batch): 1.0248124599456787\n",
      "\t Training loss (single batch): 0.9380567669868469\n",
      "\t Training loss (single batch): 1.1760749816894531\n",
      "\t Training loss (single batch): 1.0208277702331543\n",
      "\t Training loss (single batch): 1.312441349029541\n",
      "\t Training loss (single batch): 1.4620583057403564\n",
      "\t Training loss (single batch): 1.227674961090088\n",
      "\t Training loss (single batch): 1.4587081670761108\n",
      "\t Training loss (single batch): 1.1977169513702393\n",
      "\t Training loss (single batch): 1.3388651609420776\n",
      "\t Training loss (single batch): 0.9684520959854126\n",
      "\t Training loss (single batch): 1.4205713272094727\n",
      "\t Training loss (single batch): 1.517005205154419\n",
      "\t Training loss (single batch): 0.9830531477928162\n",
      "\t Training loss (single batch): 1.2795145511627197\n",
      "\t Training loss (single batch): 1.0954443216323853\n",
      "\t Training loss (single batch): 0.9196147918701172\n",
      "\t Training loss (single batch): 1.2155475616455078\n",
      "\t Training loss (single batch): 1.3621604442596436\n",
      "\t Training loss (single batch): 1.3603219985961914\n",
      "\t Training loss (single batch): 1.0316370725631714\n",
      "\t Training loss (single batch): 0.9907712936401367\n",
      "\t Training loss (single batch): 0.9674175977706909\n",
      "\t Training loss (single batch): 1.3161592483520508\n",
      "\t Training loss (single batch): 0.9486612677574158\n",
      "\t Training loss (single batch): 1.2371325492858887\n",
      "\t Training loss (single batch): 1.5435680150985718\n",
      "\t Training loss (single batch): 0.9694203734397888\n",
      "\t Training loss (single batch): 1.3647115230560303\n",
      "\t Training loss (single batch): 0.9422115087509155\n",
      "\t Training loss (single batch): 1.1608525514602661\n",
      "\t Training loss (single batch): 0.6451665759086609\n",
      "\t Training loss (single batch): 1.1717644929885864\n",
      "\t Training loss (single batch): 2.788346529006958\n",
      "##################################\n",
      "## EPOCH 86\n",
      "##################################\n",
      "\t Training loss (single batch): 0.996954619884491\n",
      "\t Training loss (single batch): 1.0607961416244507\n",
      "\t Training loss (single batch): 1.1114252805709839\n",
      "\t Training loss (single batch): 1.0859707593917847\n",
      "\t Training loss (single batch): 0.7899932861328125\n",
      "\t Training loss (single batch): 0.9724603295326233\n",
      "\t Training loss (single batch): 1.7555724382400513\n",
      "\t Training loss (single batch): 1.5657696723937988\n",
      "\t Training loss (single batch): 1.0559574365615845\n",
      "\t Training loss (single batch): 1.6054935455322266\n",
      "\t Training loss (single batch): 1.0889629125595093\n",
      "\t Training loss (single batch): 1.1524306535720825\n",
      "\t Training loss (single batch): 1.358839511871338\n",
      "\t Training loss (single batch): 1.1845942735671997\n",
      "\t Training loss (single batch): 1.256991982460022\n",
      "\t Training loss (single batch): 1.1069157123565674\n",
      "\t Training loss (single batch): 1.050384759902954\n",
      "\t Training loss (single batch): 1.7555949687957764\n",
      "\t Training loss (single batch): 1.1502984762191772\n",
      "\t Training loss (single batch): 1.301169753074646\n",
      "\t Training loss (single batch): 0.9369499087333679\n",
      "\t Training loss (single batch): 1.1623361110687256\n",
      "\t Training loss (single batch): 1.8243998289108276\n",
      "\t Training loss (single batch): 0.7679228782653809\n",
      "\t Training loss (single batch): 1.0916374921798706\n",
      "\t Training loss (single batch): 1.4312375783920288\n",
      "\t Training loss (single batch): 0.9506720900535583\n",
      "\t Training loss (single batch): 0.9510552287101746\n",
      "\t Training loss (single batch): 1.2179512977600098\n",
      "\t Training loss (single batch): 1.0670934915542603\n",
      "\t Training loss (single batch): 1.0658273696899414\n",
      "\t Training loss (single batch): 0.7967807054519653\n",
      "\t Training loss (single batch): 1.138000726699829\n",
      "\t Training loss (single batch): 1.2062647342681885\n",
      "\t Training loss (single batch): 1.0986297130584717\n",
      "\t Training loss (single batch): 1.3004226684570312\n",
      "\t Training loss (single batch): 0.9722579717636108\n",
      "\t Training loss (single batch): 1.2212938070297241\n",
      "\t Training loss (single batch): 0.9562761187553406\n",
      "\t Training loss (single batch): 1.3238372802734375\n",
      "\t Training loss (single batch): 1.0947717428207397\n",
      "\t Training loss (single batch): 1.2848135232925415\n",
      "\t Training loss (single batch): 1.2378020286560059\n",
      "\t Training loss (single batch): 1.3905470371246338\n",
      "\t Training loss (single batch): 1.4092644453048706\n",
      "\t Training loss (single batch): 1.192622423171997\n",
      "\t Training loss (single batch): 0.9255101084709167\n",
      "\t Training loss (single batch): 1.0378245115280151\n",
      "\t Training loss (single batch): 1.0337018966674805\n",
      "\t Training loss (single batch): 1.2392470836639404\n",
      "\t Training loss (single batch): 1.1849311590194702\n",
      "\t Training loss (single batch): 1.0635677576065063\n",
      "\t Training loss (single batch): 1.1109273433685303\n",
      "\t Training loss (single batch): 1.1486401557922363\n",
      "\t Training loss (single batch): 1.5428730249404907\n",
      "\t Training loss (single batch): 1.0467091798782349\n",
      "\t Training loss (single batch): 0.9745180606842041\n",
      "\t Training loss (single batch): 0.9119623899459839\n",
      "\t Training loss (single batch): 0.8421085476875305\n",
      "\t Training loss (single batch): 0.9799156188964844\n",
      "\t Training loss (single batch): 1.4453840255737305\n",
      "\t Training loss (single batch): 1.5249567031860352\n",
      "\t Training loss (single batch): 1.021323561668396\n",
      "\t Training loss (single batch): 1.0566000938415527\n",
      "\t Training loss (single batch): 1.1821147203445435\n",
      "\t Training loss (single batch): 1.163779854774475\n",
      "\t Training loss (single batch): 0.9620548486709595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.4341658353805542\n",
      "\t Training loss (single batch): 1.138409972190857\n",
      "\t Training loss (single batch): 1.212594985961914\n",
      "\t Training loss (single batch): 1.110075831413269\n",
      "\t Training loss (single batch): 1.3883026838302612\n",
      "\t Training loss (single batch): 0.7316277623176575\n",
      "\t Training loss (single batch): 1.4256762266159058\n",
      "\t Training loss (single batch): 1.775801420211792\n",
      "\t Training loss (single batch): 1.0492035150527954\n",
      "\t Training loss (single batch): 1.1324772834777832\n",
      "\t Training loss (single batch): 1.3686271905899048\n",
      "\t Training loss (single batch): 1.0555996894836426\n",
      "\t Training loss (single batch): 1.1227033138275146\n",
      "\t Training loss (single batch): 1.1056393384933472\n",
      "\t Training loss (single batch): 1.284424066543579\n",
      "\t Training loss (single batch): 1.4928503036499023\n",
      "\t Training loss (single batch): 1.629145860671997\n",
      "\t Training loss (single batch): 0.9846813678741455\n",
      "\t Training loss (single batch): 1.2540867328643799\n",
      "\t Training loss (single batch): 0.8996061682701111\n",
      "\t Training loss (single batch): 0.7417756915092468\n",
      "\t Training loss (single batch): 1.25247323513031\n",
      "\t Training loss (single batch): 1.5102894306182861\n",
      "\t Training loss (single batch): 1.149017095565796\n",
      "\t Training loss (single batch): 0.943075954914093\n",
      "\t Training loss (single batch): 1.170035481452942\n",
      "\t Training loss (single batch): 0.8201486468315125\n",
      "\t Training loss (single batch): 1.1824455261230469\n",
      "\t Training loss (single batch): 1.2702112197875977\n",
      "\t Training loss (single batch): 1.2153828144073486\n",
      "\t Training loss (single batch): 0.8375946283340454\n",
      "\t Training loss (single batch): 1.331982135772705\n",
      "\t Training loss (single batch): 1.3959234952926636\n",
      "\t Training loss (single batch): 1.54268217086792\n",
      "\t Training loss (single batch): 1.2406448125839233\n",
      "\t Training loss (single batch): 1.3102023601531982\n",
      "\t Training loss (single batch): 1.4161956310272217\n",
      "\t Training loss (single batch): 1.7391282320022583\n",
      "\t Training loss (single batch): 1.2258456945419312\n",
      "\t Training loss (single batch): 0.8606936931610107\n",
      "\t Training loss (single batch): 1.3314893245697021\n",
      "\t Training loss (single batch): 1.3167724609375\n",
      "\t Training loss (single batch): 0.9721351861953735\n",
      "\t Training loss (single batch): 0.9042956233024597\n",
      "\t Training loss (single batch): 0.9280892610549927\n",
      "\t Training loss (single batch): 1.036730408668518\n",
      "\t Training loss (single batch): 0.9120464324951172\n",
      "\t Training loss (single batch): 1.2856667041778564\n",
      "\t Training loss (single batch): 0.753095269203186\n",
      "\t Training loss (single batch): 1.3919894695281982\n",
      "\t Training loss (single batch): 0.8870126008987427\n",
      "\t Training loss (single batch): 0.7825091481208801\n",
      "\t Training loss (single batch): 1.1982516050338745\n",
      "\t Training loss (single batch): 1.311159372329712\n",
      "\t Training loss (single batch): 1.6299208402633667\n",
      "\t Training loss (single batch): 1.0688555240631104\n",
      "\t Training loss (single batch): 1.4843175411224365\n",
      "\t Training loss (single batch): 1.2444255352020264\n",
      "\t Training loss (single batch): 1.3417987823486328\n",
      "\t Training loss (single batch): 1.1682963371276855\n",
      "\t Training loss (single batch): 1.2844566106796265\n",
      "\t Training loss (single batch): 1.4761439561843872\n",
      "\t Training loss (single batch): 1.300782322883606\n",
      "\t Training loss (single batch): 1.1127735376358032\n",
      "\t Training loss (single batch): 1.2994191646575928\n",
      "\t Training loss (single batch): 1.1358692646026611\n",
      "\t Training loss (single batch): 1.1828075647354126\n",
      "\t Training loss (single batch): 1.3561147451400757\n",
      "\t Training loss (single batch): 1.1775058507919312\n",
      "\t Training loss (single batch): 1.1359893083572388\n",
      "\t Training loss (single batch): 1.4854601621627808\n",
      "\t Training loss (single batch): 1.3285750150680542\n",
      "\t Training loss (single batch): 1.0835825204849243\n",
      "\t Training loss (single batch): 0.6372812390327454\n",
      "\t Training loss (single batch): 1.2073376178741455\n",
      "\t Training loss (single batch): 1.8051319122314453\n",
      "\t Training loss (single batch): 1.2466213703155518\n",
      "\t Training loss (single batch): 1.0371291637420654\n",
      "\t Training loss (single batch): 1.1058602333068848\n",
      "\t Training loss (single batch): 1.7184128761291504\n",
      "\t Training loss (single batch): 1.068155288696289\n",
      "\t Training loss (single batch): 1.4099193811416626\n",
      "\t Training loss (single batch): 0.9515779614448547\n",
      "\t Training loss (single batch): 0.9357984662055969\n",
      "\t Training loss (single batch): 1.7271897792816162\n",
      "\t Training loss (single batch): 1.429864525794983\n",
      "\t Training loss (single batch): 1.0956841707229614\n",
      "\t Training loss (single batch): 1.782537579536438\n",
      "\t Training loss (single batch): 0.9932571649551392\n",
      "\t Training loss (single batch): 1.37442147731781\n",
      "\t Training loss (single batch): 1.0234744548797607\n",
      "\t Training loss (single batch): 1.3288363218307495\n",
      "\t Training loss (single batch): 1.5863484144210815\n",
      "\t Training loss (single batch): 0.9499649405479431\n",
      "\t Training loss (single batch): 1.0875400304794312\n",
      "\t Training loss (single batch): 0.9397538304328918\n",
      "\t Training loss (single batch): 1.4332365989685059\n",
      "\t Training loss (single batch): 1.2452408075332642\n",
      "\t Training loss (single batch): 1.3172798156738281\n",
      "\t Training loss (single batch): 1.7062335014343262\n",
      "\t Training loss (single batch): 1.322898507118225\n",
      "\t Training loss (single batch): 1.5735714435577393\n",
      "\t Training loss (single batch): 1.161216139793396\n",
      "\t Training loss (single batch): 1.1956729888916016\n",
      "\t Training loss (single batch): 1.0831186771392822\n",
      "\t Training loss (single batch): 1.2170697450637817\n",
      "\t Training loss (single batch): 1.033850908279419\n",
      "\t Training loss (single batch): 1.0120432376861572\n",
      "\t Training loss (single batch): 1.4400054216384888\n",
      "\t Training loss (single batch): 1.481352686882019\n",
      "\t Training loss (single batch): 1.1217375993728638\n",
      "\t Training loss (single batch): 1.2664448022842407\n",
      "\t Training loss (single batch): 0.7025964260101318\n",
      "\t Training loss (single batch): 1.0043970346450806\n",
      "\t Training loss (single batch): 1.186024785041809\n",
      "\t Training loss (single batch): 1.3507155179977417\n",
      "\t Training loss (single batch): 0.8231084942817688\n",
      "\t Training loss (single batch): 1.1793678998947144\n",
      "\t Training loss (single batch): 1.0565434694290161\n",
      "\t Training loss (single batch): 1.3046562671661377\n",
      "\t Training loss (single batch): 1.4556605815887451\n",
      "\t Training loss (single batch): 1.028762936592102\n",
      "\t Training loss (single batch): 1.042728066444397\n",
      "\t Training loss (single batch): 1.632660150527954\n",
      "\t Training loss (single batch): 1.252009391784668\n",
      "\t Training loss (single batch): 0.822230339050293\n",
      "\t Training loss (single batch): 1.2366806268692017\n",
      "\t Training loss (single batch): 1.2682530879974365\n",
      "\t Training loss (single batch): 0.7546208500862122\n",
      "\t Training loss (single batch): 1.2402148246765137\n",
      "\t Training loss (single batch): 1.6851565837860107\n",
      "\t Training loss (single batch): 1.127906084060669\n",
      "\t Training loss (single batch): 0.9040464162826538\n",
      "\t Training loss (single batch): 1.3507107496261597\n",
      "\t Training loss (single batch): 1.1164031028747559\n",
      "\t Training loss (single batch): 1.5684821605682373\n",
      "\t Training loss (single batch): 0.8648011088371277\n",
      "\t Training loss (single batch): 0.9095084071159363\n",
      "\t Training loss (single batch): 0.8729850053787231\n",
      "\t Training loss (single batch): 1.1192774772644043\n",
      "\t Training loss (single batch): 0.955830454826355\n",
      "\t Training loss (single batch): 1.0491830110549927\n",
      "\t Training loss (single batch): 1.4137831926345825\n",
      "\t Training loss (single batch): 1.7668617963790894\n",
      "\t Training loss (single batch): 1.1709213256835938\n",
      "\t Training loss (single batch): 1.0654644966125488\n",
      "\t Training loss (single batch): 0.8755001425743103\n",
      "\t Training loss (single batch): 1.6700538396835327\n",
      "\t Training loss (single batch): 1.214709997177124\n",
      "\t Training loss (single batch): 1.0057140588760376\n",
      "\t Training loss (single batch): 1.3185782432556152\n",
      "\t Training loss (single batch): 1.2879105806350708\n",
      "\t Training loss (single batch): 0.8665168285369873\n",
      "\t Training loss (single batch): 0.9405999183654785\n",
      "\t Training loss (single batch): 1.236992597579956\n",
      "\t Training loss (single batch): 0.8793205618858337\n",
      "\t Training loss (single batch): 0.7769097685813904\n",
      "\t Training loss (single batch): 1.214524507522583\n",
      "\t Training loss (single batch): 1.174063801765442\n",
      "\t Training loss (single batch): 1.1494697332382202\n",
      "\t Training loss (single batch): 1.7617608308792114\n",
      "\t Training loss (single batch): 1.1549865007400513\n",
      "\t Training loss (single batch): 1.2217626571655273\n",
      "\t Training loss (single batch): 0.8914926052093506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9033918976783752\n",
      "\t Training loss (single batch): 1.102126121520996\n",
      "\t Training loss (single batch): 1.4662714004516602\n",
      "\t Training loss (single batch): 1.0095700025558472\n",
      "\t Training loss (single batch): 1.102484107017517\n",
      "\t Training loss (single batch): 1.3485738039016724\n",
      "\t Training loss (single batch): 1.2537263631820679\n",
      "\t Training loss (single batch): 1.1799542903900146\n",
      "\t Training loss (single batch): 0.9348834156990051\n",
      "\t Training loss (single batch): 1.2322779893875122\n",
      "\t Training loss (single batch): 1.053109049797058\n",
      "\t Training loss (single batch): 0.8619053363800049\n",
      "\t Training loss (single batch): 1.4168578386306763\n",
      "\t Training loss (single batch): 0.9731342196464539\n",
      "\t Training loss (single batch): 1.275867223739624\n",
      "\t Training loss (single batch): 1.578965663909912\n",
      "\t Training loss (single batch): 1.0964701175689697\n",
      "\t Training loss (single batch): 1.172163963317871\n",
      "\t Training loss (single batch): 1.4575693607330322\n",
      "\t Training loss (single batch): 1.1970748901367188\n",
      "\t Training loss (single batch): 0.9576916098594666\n",
      "\t Training loss (single batch): 0.9284438490867615\n",
      "\t Training loss (single batch): 0.5773617029190063\n",
      "\t Training loss (single batch): 1.5396569967269897\n",
      "\t Training loss (single batch): 0.9834068417549133\n",
      "\t Training loss (single batch): 1.113742470741272\n",
      "\t Training loss (single batch): 1.1412813663482666\n",
      "\t Training loss (single batch): 1.097988486289978\n",
      "\t Training loss (single batch): 0.8929450511932373\n",
      "\t Training loss (single batch): 1.5271308422088623\n",
      "\t Training loss (single batch): 1.7595465183258057\n",
      "\t Training loss (single batch): 1.3444290161132812\n",
      "\t Training loss (single batch): 1.154646396636963\n",
      "\t Training loss (single batch): 0.8944754004478455\n",
      "\t Training loss (single batch): 0.8239049315452576\n",
      "\t Training loss (single batch): 0.8895255327224731\n",
      "\t Training loss (single batch): 1.2773035764694214\n",
      "\t Training loss (single batch): 0.9732237458229065\n",
      "\t Training loss (single batch): 1.035936951637268\n",
      "\t Training loss (single batch): 1.335009217262268\n",
      "\t Training loss (single batch): 1.109652042388916\n",
      "\t Training loss (single batch): 1.1883702278137207\n",
      "\t Training loss (single batch): 0.9008166193962097\n",
      "\t Training loss (single batch): 0.9692909717559814\n",
      "\t Training loss (single batch): 0.8967301845550537\n",
      "\t Training loss (single batch): 1.0390504598617554\n",
      "\t Training loss (single batch): 0.968396782875061\n",
      "\t Training loss (single batch): 1.3160467147827148\n",
      "\t Training loss (single batch): 1.5576279163360596\n",
      "\t Training loss (single batch): 1.0221463441848755\n",
      "\t Training loss (single batch): 1.4732109308242798\n",
      "\t Training loss (single batch): 1.3850276470184326\n",
      "\t Training loss (single batch): 1.3683885335922241\n",
      "\t Training loss (single batch): 1.0474586486816406\n",
      "\t Training loss (single batch): 0.9235629439353943\n",
      "\t Training loss (single batch): 1.2920018434524536\n",
      "\t Training loss (single batch): 0.9435440301895142\n",
      "\t Training loss (single batch): 0.800885796546936\n",
      "\t Training loss (single batch): 0.9071921110153198\n",
      "\t Training loss (single batch): 0.8823875784873962\n",
      "\t Training loss (single batch): 0.7895179986953735\n",
      "\t Training loss (single batch): 1.303231954574585\n",
      "\t Training loss (single batch): 0.7637751698493958\n",
      "\t Training loss (single batch): 1.4001169204711914\n",
      "\t Training loss (single batch): 1.1836779117584229\n",
      "\t Training loss (single batch): 1.247510552406311\n",
      "\t Training loss (single batch): 1.0671186447143555\n",
      "\t Training loss (single batch): 0.8102640509605408\n",
      "\t Training loss (single batch): 1.7821600437164307\n",
      "\t Training loss (single batch): 1.4448271989822388\n",
      "\t Training loss (single batch): 1.0752277374267578\n",
      "\t Training loss (single batch): 1.1287033557891846\n",
      "\t Training loss (single batch): 1.2947440147399902\n",
      "\t Training loss (single batch): 0.9997795820236206\n",
      "\t Training loss (single batch): 1.4248342514038086\n",
      "\t Training loss (single batch): 1.3913441896438599\n",
      "\t Training loss (single batch): 1.2269235849380493\n",
      "\t Training loss (single batch): 1.1498302221298218\n",
      "\t Training loss (single batch): 1.2843303680419922\n",
      "\t Training loss (single batch): 0.9954965114593506\n",
      "\t Training loss (single batch): 1.772445559501648\n",
      "\t Training loss (single batch): 1.2781407833099365\n",
      "\t Training loss (single batch): 0.82938551902771\n",
      "\t Training loss (single batch): 1.4882278442382812\n",
      "\t Training loss (single batch): 1.2632101774215698\n",
      "\t Training loss (single batch): 0.4303589463233948\n",
      "##################################\n",
      "## EPOCH 87\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8854968547821045\n",
      "\t Training loss (single batch): 1.7584519386291504\n",
      "\t Training loss (single batch): 1.1337887048721313\n",
      "\t Training loss (single batch): 1.0154415369033813\n",
      "\t Training loss (single batch): 1.1740802526474\n",
      "\t Training loss (single batch): 1.0899124145507812\n",
      "\t Training loss (single batch): 1.3646210432052612\n",
      "\t Training loss (single batch): 1.570022702217102\n",
      "\t Training loss (single batch): 1.190451741218567\n",
      "\t Training loss (single batch): 1.3145588636398315\n",
      "\t Training loss (single batch): 0.8602695465087891\n",
      "\t Training loss (single batch): 1.109101414680481\n",
      "\t Training loss (single batch): 1.2480946779251099\n",
      "\t Training loss (single batch): 1.0478718280792236\n",
      "\t Training loss (single batch): 1.3015682697296143\n",
      "\t Training loss (single batch): 1.2002159357070923\n",
      "\t Training loss (single batch): 0.8043786883354187\n",
      "\t Training loss (single batch): 1.1644325256347656\n",
      "\t Training loss (single batch): 0.9436988234519958\n",
      "\t Training loss (single batch): 0.9748291969299316\n",
      "\t Training loss (single batch): 1.2745115756988525\n",
      "\t Training loss (single batch): 1.304057240486145\n",
      "\t Training loss (single batch): 1.1250934600830078\n",
      "\t Training loss (single batch): 0.6541482210159302\n",
      "\t Training loss (single batch): 0.8010865449905396\n",
      "\t Training loss (single batch): 1.0213416814804077\n",
      "\t Training loss (single batch): 0.8181869983673096\n",
      "\t Training loss (single batch): 2.074110984802246\n",
      "\t Training loss (single batch): 1.0639638900756836\n",
      "\t Training loss (single batch): 1.680052399635315\n",
      "\t Training loss (single batch): 0.8120105862617493\n",
      "\t Training loss (single batch): 1.1782491207122803\n",
      "\t Training loss (single batch): 1.8696540594100952\n",
      "\t Training loss (single batch): 1.7104331254959106\n",
      "\t Training loss (single batch): 0.7996242642402649\n",
      "\t Training loss (single batch): 1.4190956354141235\n",
      "\t Training loss (single batch): 1.553794503211975\n",
      "\t Training loss (single batch): 1.0046911239624023\n",
      "\t Training loss (single batch): 1.5763130187988281\n",
      "\t Training loss (single batch): 1.138622522354126\n",
      "\t Training loss (single batch): 0.9794518351554871\n",
      "\t Training loss (single batch): 1.1434061527252197\n",
      "\t Training loss (single batch): 0.8145284056663513\n",
      "\t Training loss (single batch): 0.6647385358810425\n",
      "\t Training loss (single batch): 1.2989146709442139\n",
      "\t Training loss (single batch): 1.3963383436203003\n",
      "\t Training loss (single batch): 1.19451904296875\n",
      "\t Training loss (single batch): 1.2164250612258911\n",
      "\t Training loss (single batch): 0.7490600943565369\n",
      "\t Training loss (single batch): 1.774420976638794\n",
      "\t Training loss (single batch): 1.116478443145752\n",
      "\t Training loss (single batch): 1.449497103691101\n",
      "\t Training loss (single batch): 0.9970484375953674\n",
      "\t Training loss (single batch): 1.3217599391937256\n",
      "\t Training loss (single batch): 1.1481472253799438\n",
      "\t Training loss (single batch): 1.3989166021347046\n",
      "\t Training loss (single batch): 1.2088698148727417\n",
      "\t Training loss (single batch): 1.0056700706481934\n",
      "\t Training loss (single batch): 0.7124905586242676\n",
      "\t Training loss (single batch): 1.5639047622680664\n",
      "\t Training loss (single batch): 0.9099276661872864\n",
      "\t Training loss (single batch): 1.0994749069213867\n",
      "\t Training loss (single batch): 1.3884577751159668\n",
      "\t Training loss (single batch): 0.9393358826637268\n",
      "\t Training loss (single batch): 0.7491322159767151\n",
      "\t Training loss (single batch): 0.5980024337768555\n",
      "\t Training loss (single batch): 1.766215443611145\n",
      "\t Training loss (single batch): 1.4311909675598145\n",
      "\t Training loss (single batch): 1.374436616897583\n",
      "\t Training loss (single batch): 0.9337202310562134\n",
      "\t Training loss (single batch): 1.897325038909912\n",
      "\t Training loss (single batch): 0.9079484939575195\n",
      "\t Training loss (single batch): 1.2170805931091309\n",
      "\t Training loss (single batch): 0.9352599382400513\n",
      "\t Training loss (single batch): 1.6077890396118164\n",
      "\t Training loss (single batch): 1.059241771697998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.250459909439087\n",
      "\t Training loss (single batch): 1.2322715520858765\n",
      "\t Training loss (single batch): 1.278615951538086\n",
      "\t Training loss (single batch): 1.1014999151229858\n",
      "\t Training loss (single batch): 1.0077085494995117\n",
      "\t Training loss (single batch): 1.7420322895050049\n",
      "\t Training loss (single batch): 1.0671136379241943\n",
      "\t Training loss (single batch): 1.1355633735656738\n",
      "\t Training loss (single batch): 1.0199534893035889\n",
      "\t Training loss (single batch): 1.1004343032836914\n",
      "\t Training loss (single batch): 1.2446014881134033\n",
      "\t Training loss (single batch): 0.7518185973167419\n",
      "\t Training loss (single batch): 1.1153228282928467\n",
      "\t Training loss (single batch): 1.1620687246322632\n",
      "\t Training loss (single batch): 1.0762295722961426\n",
      "\t Training loss (single batch): 1.2857991456985474\n",
      "\t Training loss (single batch): 1.3765069246292114\n",
      "\t Training loss (single batch): 1.0745652914047241\n",
      "\t Training loss (single batch): 1.031235933303833\n",
      "\t Training loss (single batch): 0.9496567249298096\n",
      "\t Training loss (single batch): 1.100347876548767\n",
      "\t Training loss (single batch): 1.0386813879013062\n",
      "\t Training loss (single batch): 0.8286734819412231\n",
      "\t Training loss (single batch): 1.59209144115448\n",
      "\t Training loss (single batch): 0.9607158899307251\n",
      "\t Training loss (single batch): 1.1497688293457031\n",
      "\t Training loss (single batch): 1.11432945728302\n",
      "\t Training loss (single batch): 1.4524836540222168\n",
      "\t Training loss (single batch): 0.9878180623054504\n",
      "\t Training loss (single batch): 1.0601645708084106\n",
      "\t Training loss (single batch): 1.4728059768676758\n",
      "\t Training loss (single batch): 1.5003912448883057\n",
      "\t Training loss (single batch): 1.3637630939483643\n",
      "\t Training loss (single batch): 1.1843472719192505\n",
      "\t Training loss (single batch): 1.1892198324203491\n",
      "\t Training loss (single batch): 1.3866173028945923\n",
      "\t Training loss (single batch): 0.7691554427146912\n",
      "\t Training loss (single batch): 0.9002466797828674\n",
      "\t Training loss (single batch): 1.2678947448730469\n",
      "\t Training loss (single batch): 0.9936439990997314\n",
      "\t Training loss (single batch): 1.3707594871520996\n",
      "\t Training loss (single batch): 0.9806865453720093\n",
      "\t Training loss (single batch): 1.2639482021331787\n",
      "\t Training loss (single batch): 1.2310314178466797\n",
      "\t Training loss (single batch): 0.9067109227180481\n",
      "\t Training loss (single batch): 1.0435742139816284\n",
      "\t Training loss (single batch): 1.1507084369659424\n",
      "\t Training loss (single batch): 1.057456135749817\n",
      "\t Training loss (single batch): 1.178460717201233\n",
      "\t Training loss (single batch): 1.377511739730835\n",
      "\t Training loss (single batch): 1.0486119985580444\n",
      "\t Training loss (single batch): 0.9182655215263367\n",
      "\t Training loss (single batch): 1.0720449686050415\n",
      "\t Training loss (single batch): 1.1825125217437744\n",
      "\t Training loss (single batch): 0.8984554409980774\n",
      "\t Training loss (single batch): 0.7389470338821411\n",
      "\t Training loss (single batch): 1.6845054626464844\n",
      "\t Training loss (single batch): 1.178830862045288\n",
      "\t Training loss (single batch): 1.3227763175964355\n",
      "\t Training loss (single batch): 0.9463028311729431\n",
      "\t Training loss (single batch): 1.1479414701461792\n",
      "\t Training loss (single batch): 1.0594162940979004\n",
      "\t Training loss (single batch): 0.9673181772232056\n",
      "\t Training loss (single batch): 1.473076581954956\n",
      "\t Training loss (single batch): 1.5866386890411377\n",
      "\t Training loss (single batch): 0.9500942230224609\n",
      "\t Training loss (single batch): 1.5219733715057373\n",
      "\t Training loss (single batch): 1.0066803693771362\n",
      "\t Training loss (single batch): 0.968569278717041\n",
      "\t Training loss (single batch): 1.1477751731872559\n",
      "\t Training loss (single batch): 1.3800549507141113\n",
      "\t Training loss (single batch): 1.8728278875350952\n",
      "\t Training loss (single batch): 1.7797188758850098\n",
      "\t Training loss (single batch): 0.7753261923789978\n",
      "\t Training loss (single batch): 1.7099477052688599\n",
      "\t Training loss (single batch): 1.0866461992263794\n",
      "\t Training loss (single batch): 1.8461825847625732\n",
      "\t Training loss (single batch): 1.044743537902832\n",
      "\t Training loss (single batch): 1.7400541305541992\n",
      "\t Training loss (single batch): 1.1058619022369385\n",
      "\t Training loss (single batch): 1.5525506734848022\n",
      "\t Training loss (single batch): 2.059537172317505\n",
      "\t Training loss (single batch): 1.059418797492981\n",
      "\t Training loss (single batch): 1.660377860069275\n",
      "\t Training loss (single batch): 1.5864098072052002\n",
      "\t Training loss (single batch): 0.9204505085945129\n",
      "\t Training loss (single batch): 1.242721438407898\n",
      "\t Training loss (single batch): 1.3463151454925537\n",
      "\t Training loss (single batch): 0.8395751118659973\n",
      "\t Training loss (single batch): 1.1986615657806396\n",
      "\t Training loss (single batch): 0.8975064158439636\n",
      "\t Training loss (single batch): 0.8954280018806458\n",
      "\t Training loss (single batch): 1.371063232421875\n",
      "\t Training loss (single batch): 1.2213892936706543\n",
      "\t Training loss (single batch): 1.3399039506912231\n",
      "\t Training loss (single batch): 1.399997353553772\n",
      "\t Training loss (single batch): 1.2656949758529663\n",
      "\t Training loss (single batch): 1.4762375354766846\n",
      "\t Training loss (single batch): 0.9979442954063416\n",
      "\t Training loss (single batch): 1.3690688610076904\n",
      "\t Training loss (single batch): 1.4599272012710571\n",
      "\t Training loss (single batch): 1.0941152572631836\n",
      "\t Training loss (single batch): 0.8281271457672119\n",
      "\t Training loss (single batch): 1.0716626644134521\n",
      "\t Training loss (single batch): 1.2689751386642456\n",
      "\t Training loss (single batch): 1.0715194940567017\n",
      "\t Training loss (single batch): 1.396852731704712\n",
      "\t Training loss (single batch): 1.0179885625839233\n",
      "\t Training loss (single batch): 1.3644393682479858\n",
      "\t Training loss (single batch): 1.281022310256958\n",
      "\t Training loss (single batch): 1.2660531997680664\n",
      "\t Training loss (single batch): 1.1536357402801514\n",
      "\t Training loss (single batch): 0.7689996957778931\n",
      "\t Training loss (single batch): 1.2996455430984497\n",
      "\t Training loss (single batch): 1.367830514907837\n",
      "\t Training loss (single batch): 1.2514064311981201\n",
      "\t Training loss (single batch): 1.4066579341888428\n",
      "\t Training loss (single batch): 0.7628989815711975\n",
      "\t Training loss (single batch): 1.098028540611267\n",
      "\t Training loss (single batch): 1.1562473773956299\n",
      "\t Training loss (single batch): 0.8021525144577026\n",
      "\t Training loss (single batch): 1.1271950006484985\n",
      "\t Training loss (single batch): 1.1140538454055786\n",
      "\t Training loss (single batch): 1.1121771335601807\n",
      "\t Training loss (single batch): 1.5773266553878784\n",
      "\t Training loss (single batch): 0.8129801154136658\n",
      "\t Training loss (single batch): 0.9476354718208313\n",
      "\t Training loss (single batch): 1.1924008131027222\n",
      "\t Training loss (single batch): 1.1302257776260376\n",
      "\t Training loss (single batch): 1.0943189859390259\n",
      "\t Training loss (single batch): 1.1877943277359009\n",
      "\t Training loss (single batch): 1.0970314741134644\n",
      "\t Training loss (single batch): 1.2695139646530151\n",
      "\t Training loss (single batch): 1.235011100769043\n",
      "\t Training loss (single batch): 1.2837337255477905\n",
      "\t Training loss (single batch): 1.2216613292694092\n",
      "\t Training loss (single batch): 1.3644473552703857\n",
      "\t Training loss (single batch): 0.8978144526481628\n",
      "\t Training loss (single batch): 1.3266388177871704\n",
      "\t Training loss (single batch): 0.9712273478507996\n",
      "\t Training loss (single batch): 0.9731411337852478\n",
      "\t Training loss (single batch): 1.0882148742675781\n",
      "\t Training loss (single batch): 0.8683330416679382\n",
      "\t Training loss (single batch): 1.137434720993042\n",
      "\t Training loss (single batch): 0.9448962807655334\n",
      "\t Training loss (single batch): 1.3925070762634277\n",
      "\t Training loss (single batch): 0.8102518320083618\n",
      "\t Training loss (single batch): 0.9928566217422485\n",
      "\t Training loss (single batch): 2.1185474395751953\n",
      "\t Training loss (single batch): 1.1360888481140137\n",
      "\t Training loss (single batch): 1.2749736309051514\n",
      "\t Training loss (single batch): 0.8323801159858704\n",
      "\t Training loss (single batch): 0.8311142325401306\n",
      "\t Training loss (single batch): 1.5468095541000366\n",
      "\t Training loss (single batch): 1.5558868646621704\n",
      "\t Training loss (single batch): 1.0663787126541138\n",
      "\t Training loss (single batch): 0.8441874384880066\n",
      "\t Training loss (single batch): 1.0860391855239868\n",
      "\t Training loss (single batch): 1.0415140390396118\n",
      "\t Training loss (single batch): 1.2652353048324585\n",
      "\t Training loss (single batch): 1.4636811017990112\n",
      "\t Training loss (single batch): 2.0824100971221924\n",
      "\t Training loss (single batch): 1.4412132501602173\n",
      "\t Training loss (single batch): 1.9062697887420654\n",
      "\t Training loss (single batch): 1.2498186826705933\n",
      "\t Training loss (single batch): 0.9358172416687012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.8804419040679932\n",
      "\t Training loss (single batch): 1.2069185972213745\n",
      "\t Training loss (single batch): 1.2347911596298218\n",
      "\t Training loss (single batch): 1.269845962524414\n",
      "\t Training loss (single batch): 1.6505446434020996\n",
      "\t Training loss (single batch): 1.1759051084518433\n",
      "\t Training loss (single batch): 1.105313777923584\n",
      "\t Training loss (single batch): 1.1084359884262085\n",
      "\t Training loss (single batch): 0.8475542664527893\n",
      "\t Training loss (single batch): 1.0477557182312012\n",
      "\t Training loss (single batch): 0.889227569103241\n",
      "\t Training loss (single batch): 1.1192008256912231\n",
      "\t Training loss (single batch): 1.6166731119155884\n",
      "\t Training loss (single batch): 1.0190794467926025\n",
      "\t Training loss (single batch): 1.0491067171096802\n",
      "\t Training loss (single batch): 1.0330580472946167\n",
      "\t Training loss (single batch): 1.3468704223632812\n",
      "\t Training loss (single batch): 1.4942594766616821\n",
      "\t Training loss (single batch): 1.6889841556549072\n",
      "\t Training loss (single batch): 1.083925485610962\n",
      "\t Training loss (single batch): 1.8791745901107788\n",
      "\t Training loss (single batch): 1.4015504121780396\n",
      "\t Training loss (single batch): 1.0419193506240845\n",
      "\t Training loss (single batch): 1.2827706336975098\n",
      "\t Training loss (single batch): 1.2260626554489136\n",
      "\t Training loss (single batch): 1.0703219175338745\n",
      "\t Training loss (single batch): 1.5739116668701172\n",
      "\t Training loss (single batch): 1.1282012462615967\n",
      "\t Training loss (single batch): 1.0180033445358276\n",
      "\t Training loss (single batch): 1.06741464138031\n",
      "\t Training loss (single batch): 1.2183736562728882\n",
      "\t Training loss (single batch): 1.1088080406188965\n",
      "\t Training loss (single batch): 1.6748586893081665\n",
      "\t Training loss (single batch): 1.495251178741455\n",
      "\t Training loss (single batch): 1.8039964437484741\n",
      "\t Training loss (single batch): 1.2533531188964844\n",
      "\t Training loss (single batch): 1.8446801900863647\n",
      "\t Training loss (single batch): 1.4834775924682617\n",
      "\t Training loss (single batch): 1.0227234363555908\n",
      "\t Training loss (single batch): 1.273379921913147\n",
      "\t Training loss (single batch): 1.11902916431427\n",
      "\t Training loss (single batch): 1.1032450199127197\n",
      "\t Training loss (single batch): 1.226651668548584\n",
      "\t Training loss (single batch): 1.4618419408798218\n",
      "\t Training loss (single batch): 1.2178629636764526\n",
      "\t Training loss (single batch): 1.3675435781478882\n",
      "\t Training loss (single batch): 1.063116431236267\n",
      "\t Training loss (single batch): 0.606852650642395\n",
      "\t Training loss (single batch): 1.0855945348739624\n",
      "\t Training loss (single batch): 1.1060054302215576\n",
      "\t Training loss (single batch): 0.885404109954834\n",
      "\t Training loss (single batch): 1.6012513637542725\n",
      "\t Training loss (single batch): 0.8400648236274719\n",
      "\t Training loss (single batch): 1.3995633125305176\n",
      "\t Training loss (single batch): 1.2063133716583252\n",
      "\t Training loss (single batch): 0.831322431564331\n",
      "\t Training loss (single batch): 1.0837904214859009\n",
      "\t Training loss (single batch): 0.8787584900856018\n",
      "\t Training loss (single batch): 1.4464250802993774\n",
      "\t Training loss (single batch): 1.0756826400756836\n",
      "\t Training loss (single batch): 1.5670620203018188\n",
      "\t Training loss (single batch): 1.1958180665969849\n",
      "\t Training loss (single batch): 0.8161029815673828\n",
      "\t Training loss (single batch): 1.064599633216858\n",
      "\t Training loss (single batch): 1.0996887683868408\n",
      "\t Training loss (single batch): 1.2967361211776733\n",
      "\t Training loss (single batch): 1.0842812061309814\n",
      "\t Training loss (single batch): 0.83568274974823\n",
      "\t Training loss (single batch): 1.1647555828094482\n",
      "\t Training loss (single batch): 1.0545302629470825\n",
      "\t Training loss (single batch): 1.239363193511963\n",
      "\t Training loss (single batch): 1.0071014165878296\n",
      "\t Training loss (single batch): 0.9851405024528503\n",
      "\t Training loss (single batch): 1.590876817703247\n",
      "\t Training loss (single batch): 0.9382991194725037\n",
      "##################################\n",
      "## EPOCH 88\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8499062657356262\n",
      "\t Training loss (single batch): 1.246883749961853\n",
      "\t Training loss (single batch): 1.0524542331695557\n",
      "\t Training loss (single batch): 1.026993751525879\n",
      "\t Training loss (single batch): 1.0081579685211182\n",
      "\t Training loss (single batch): 1.2111786603927612\n",
      "\t Training loss (single batch): 1.2176111936569214\n",
      "\t Training loss (single batch): 1.1419280767440796\n",
      "\t Training loss (single batch): 1.3413935899734497\n",
      "\t Training loss (single batch): 1.2017111778259277\n",
      "\t Training loss (single batch): 1.4574612379074097\n",
      "\t Training loss (single batch): 1.2989246845245361\n",
      "\t Training loss (single batch): 1.1276130676269531\n",
      "\t Training loss (single batch): 1.0633922815322876\n",
      "\t Training loss (single batch): 1.2472811937332153\n",
      "\t Training loss (single batch): 1.0284019708633423\n",
      "\t Training loss (single batch): 1.3331557512283325\n",
      "\t Training loss (single batch): 0.8373731374740601\n",
      "\t Training loss (single batch): 1.1941792964935303\n",
      "\t Training loss (single batch): 2.063142776489258\n",
      "\t Training loss (single batch): 1.102428913116455\n",
      "\t Training loss (single batch): 0.7644670605659485\n",
      "\t Training loss (single batch): 1.3711276054382324\n",
      "\t Training loss (single batch): 1.0280016660690308\n",
      "\t Training loss (single batch): 1.131576657295227\n",
      "\t Training loss (single batch): 1.4040714502334595\n",
      "\t Training loss (single batch): 1.5398433208465576\n",
      "\t Training loss (single batch): 1.152755856513977\n",
      "\t Training loss (single batch): 1.003646969795227\n",
      "\t Training loss (single batch): 0.7201171517372131\n",
      "\t Training loss (single batch): 1.1875064373016357\n",
      "\t Training loss (single batch): 0.9671167731285095\n",
      "\t Training loss (single batch): 1.352973222732544\n",
      "\t Training loss (single batch): 0.7808507680892944\n",
      "\t Training loss (single batch): 1.344730019569397\n",
      "\t Training loss (single batch): 1.3234484195709229\n",
      "\t Training loss (single batch): 1.6617921590805054\n",
      "\t Training loss (single batch): 0.9310790300369263\n",
      "\t Training loss (single batch): 1.3406754732131958\n",
      "\t Training loss (single batch): 0.9835174083709717\n",
      "\t Training loss (single batch): 1.6677987575531006\n",
      "\t Training loss (single batch): 1.0587562322616577\n",
      "\t Training loss (single batch): 1.4885793924331665\n",
      "\t Training loss (single batch): 1.1149905920028687\n",
      "\t Training loss (single batch): 1.1384557485580444\n",
      "\t Training loss (single batch): 1.080793857574463\n",
      "\t Training loss (single batch): 0.895906925201416\n",
      "\t Training loss (single batch): 0.8088096976280212\n",
      "\t Training loss (single batch): 1.2917968034744263\n",
      "\t Training loss (single batch): 1.3293166160583496\n",
      "\t Training loss (single batch): 1.025473713874817\n",
      "\t Training loss (single batch): 1.3474526405334473\n",
      "\t Training loss (single batch): 0.9263132214546204\n",
      "\t Training loss (single batch): 1.1281259059906006\n",
      "\t Training loss (single batch): 0.8079444169998169\n",
      "\t Training loss (single batch): 0.8077966570854187\n",
      "\t Training loss (single batch): 1.136002779006958\n",
      "\t Training loss (single batch): 0.9302322268486023\n",
      "\t Training loss (single batch): 0.9479231238365173\n",
      "\t Training loss (single batch): 1.3408769369125366\n",
      "\t Training loss (single batch): 1.8812415599822998\n",
      "\t Training loss (single batch): 1.133859634399414\n",
      "\t Training loss (single batch): 1.1114451885223389\n",
      "\t Training loss (single batch): 1.2564469575881958\n",
      "\t Training loss (single batch): 1.2164642810821533\n",
      "\t Training loss (single batch): 0.6522037386894226\n",
      "\t Training loss (single batch): 1.378707766532898\n",
      "\t Training loss (single batch): 1.0825309753417969\n",
      "\t Training loss (single batch): 1.3055639266967773\n",
      "\t Training loss (single batch): 1.1973315477371216\n",
      "\t Training loss (single batch): 0.8246816992759705\n",
      "\t Training loss (single batch): 1.499162197113037\n",
      "\t Training loss (single batch): 0.9026532173156738\n",
      "\t Training loss (single batch): 1.242068886756897\n",
      "\t Training loss (single batch): 1.4936498403549194\n",
      "\t Training loss (single batch): 1.187474012374878\n",
      "\t Training loss (single batch): 0.832539439201355\n",
      "\t Training loss (single batch): 1.136725664138794\n",
      "\t Training loss (single batch): 0.8964123129844666\n",
      "\t Training loss (single batch): 1.1545466184616089\n",
      "\t Training loss (single batch): 1.3820977210998535\n",
      "\t Training loss (single batch): 1.2496284246444702\n",
      "\t Training loss (single batch): 1.4576457738876343\n",
      "\t Training loss (single batch): 0.7908692359924316\n",
      "\t Training loss (single batch): 1.3506592512130737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9542071223258972\n",
      "\t Training loss (single batch): 1.076934814453125\n",
      "\t Training loss (single batch): 1.3792318105697632\n",
      "\t Training loss (single batch): 1.0124598741531372\n",
      "\t Training loss (single batch): 1.030310869216919\n",
      "\t Training loss (single batch): 0.7296182513237\n",
      "\t Training loss (single batch): 1.3348606824874878\n",
      "\t Training loss (single batch): 1.327425479888916\n",
      "\t Training loss (single batch): 1.4510903358459473\n",
      "\t Training loss (single batch): 0.8240170478820801\n",
      "\t Training loss (single batch): 1.2029269933700562\n",
      "\t Training loss (single batch): 1.1325106620788574\n",
      "\t Training loss (single batch): 0.9971453547477722\n",
      "\t Training loss (single batch): 1.1100486516952515\n",
      "\t Training loss (single batch): 0.8026038408279419\n",
      "\t Training loss (single batch): 1.252854347229004\n",
      "\t Training loss (single batch): 1.0481690168380737\n",
      "\t Training loss (single batch): 1.2750498056411743\n",
      "\t Training loss (single batch): 1.0152194499969482\n",
      "\t Training loss (single batch): 1.1606491804122925\n",
      "\t Training loss (single batch): 0.7960906028747559\n",
      "\t Training loss (single batch): 0.846893310546875\n",
      "\t Training loss (single batch): 1.210903525352478\n",
      "\t Training loss (single batch): 1.6166822910308838\n",
      "\t Training loss (single batch): 1.0642296075820923\n",
      "\t Training loss (single batch): 1.1691004037857056\n",
      "\t Training loss (single batch): 1.0119234323501587\n",
      "\t Training loss (single batch): 0.7202246189117432\n",
      "\t Training loss (single batch): 1.2902870178222656\n",
      "\t Training loss (single batch): 1.071116328239441\n",
      "\t Training loss (single batch): 0.8026927709579468\n",
      "\t Training loss (single batch): 0.8708935379981995\n",
      "\t Training loss (single batch): 1.417747139930725\n",
      "\t Training loss (single batch): 0.8894827961921692\n",
      "\t Training loss (single batch): 0.9072723984718323\n",
      "\t Training loss (single batch): 0.8876574635505676\n",
      "\t Training loss (single batch): 0.9245362281799316\n",
      "\t Training loss (single batch): 1.1302170753479004\n",
      "\t Training loss (single batch): 1.321579933166504\n",
      "\t Training loss (single batch): 1.1564021110534668\n",
      "\t Training loss (single batch): 1.3578892946243286\n",
      "\t Training loss (single batch): 1.21035635471344\n",
      "\t Training loss (single batch): 0.9800859689712524\n",
      "\t Training loss (single batch): 1.2875114679336548\n",
      "\t Training loss (single batch): 1.1716238260269165\n",
      "\t Training loss (single batch): 1.1982613801956177\n",
      "\t Training loss (single batch): 1.046600341796875\n",
      "\t Training loss (single batch): 1.0858924388885498\n",
      "\t Training loss (single batch): 1.2558329105377197\n",
      "\t Training loss (single batch): 1.709936261177063\n",
      "\t Training loss (single batch): 1.5396021604537964\n",
      "\t Training loss (single batch): 1.2795393466949463\n",
      "\t Training loss (single batch): 1.5863858461380005\n",
      "\t Training loss (single batch): 0.9089860320091248\n",
      "\t Training loss (single batch): 1.1433097124099731\n",
      "\t Training loss (single batch): 1.2891085147857666\n",
      "\t Training loss (single batch): 1.356412649154663\n",
      "\t Training loss (single batch): 1.2830512523651123\n",
      "\t Training loss (single batch): 1.1974115371704102\n",
      "\t Training loss (single batch): 1.1517012119293213\n",
      "\t Training loss (single batch): 1.1209741830825806\n",
      "\t Training loss (single batch): 1.437975525856018\n",
      "\t Training loss (single batch): 1.103305459022522\n",
      "\t Training loss (single batch): 1.053420901298523\n",
      "\t Training loss (single batch): 1.2773045301437378\n",
      "\t Training loss (single batch): 1.4020098447799683\n",
      "\t Training loss (single batch): 1.24405837059021\n",
      "\t Training loss (single batch): 1.231964349746704\n",
      "\t Training loss (single batch): 1.3840032815933228\n",
      "\t Training loss (single batch): 1.0561752319335938\n",
      "\t Training loss (single batch): 1.2896536588668823\n",
      "\t Training loss (single batch): 1.6311817169189453\n",
      "\t Training loss (single batch): 1.216931700706482\n",
      "\t Training loss (single batch): 1.6048038005828857\n",
      "\t Training loss (single batch): 0.8495882153511047\n",
      "\t Training loss (single batch): 1.0949678421020508\n",
      "\t Training loss (single batch): 0.9509122371673584\n",
      "\t Training loss (single batch): 1.048269271850586\n",
      "\t Training loss (single batch): 0.8332452178001404\n",
      "\t Training loss (single batch): 1.1447768211364746\n",
      "\t Training loss (single batch): 1.052579641342163\n",
      "\t Training loss (single batch): 1.3926342725753784\n",
      "\t Training loss (single batch): 1.7844260931015015\n",
      "\t Training loss (single batch): 1.9012349843978882\n",
      "\t Training loss (single batch): 0.8647902607917786\n",
      "\t Training loss (single batch): 1.0236296653747559\n",
      "\t Training loss (single batch): 1.2816672325134277\n",
      "\t Training loss (single batch): 1.1973956823349\n",
      "\t Training loss (single batch): 0.9887742400169373\n",
      "\t Training loss (single batch): 0.8086057305335999\n",
      "\t Training loss (single batch): 1.0794743299484253\n",
      "\t Training loss (single batch): 1.0346804857254028\n",
      "\t Training loss (single batch): 1.4728127717971802\n",
      "\t Training loss (single batch): 0.7491019368171692\n",
      "\t Training loss (single batch): 1.2942306995391846\n",
      "\t Training loss (single batch): 1.6226803064346313\n",
      "\t Training loss (single batch): 1.092758297920227\n",
      "\t Training loss (single batch): 0.9734027981758118\n",
      "\t Training loss (single batch): 1.1142345666885376\n",
      "\t Training loss (single batch): 1.0637351274490356\n",
      "\t Training loss (single batch): 1.714308738708496\n",
      "\t Training loss (single batch): 1.401522159576416\n",
      "\t Training loss (single batch): 1.7424824237823486\n",
      "\t Training loss (single batch): 1.0981743335723877\n",
      "\t Training loss (single batch): 0.8959609270095825\n",
      "\t Training loss (single batch): 1.3580102920532227\n",
      "\t Training loss (single batch): 1.5911198854446411\n",
      "\t Training loss (single batch): 1.3393138647079468\n",
      "\t Training loss (single batch): 1.5631860494613647\n",
      "\t Training loss (single batch): 1.1242496967315674\n",
      "\t Training loss (single batch): 1.9033186435699463\n",
      "\t Training loss (single batch): 0.9277108311653137\n",
      "\t Training loss (single batch): 1.3932009935379028\n",
      "\t Training loss (single batch): 0.9177240133285522\n",
      "\t Training loss (single batch): 1.1586363315582275\n",
      "\t Training loss (single batch): 1.04140305519104\n",
      "\t Training loss (single batch): 1.3133071660995483\n",
      "\t Training loss (single batch): 1.6243878602981567\n",
      "\t Training loss (single batch): 1.1651558876037598\n",
      "\t Training loss (single batch): 0.9459407925605774\n",
      "\t Training loss (single batch): 1.0831170082092285\n",
      "\t Training loss (single batch): 0.9134145975112915\n",
      "\t Training loss (single batch): 1.393483281135559\n",
      "\t Training loss (single batch): 0.9408043622970581\n",
      "\t Training loss (single batch): 1.2181094884872437\n",
      "\t Training loss (single batch): 1.2096582651138306\n",
      "\t Training loss (single batch): 1.3978208303451538\n",
      "\t Training loss (single batch): 0.9531297087669373\n",
      "\t Training loss (single batch): 1.058755874633789\n",
      "\t Training loss (single batch): 1.2442388534545898\n",
      "\t Training loss (single batch): 0.7735118269920349\n",
      "\t Training loss (single batch): 1.054469347000122\n",
      "\t Training loss (single batch): 0.7439473867416382\n",
      "\t Training loss (single batch): 1.4047598838806152\n",
      "\t Training loss (single batch): 1.073687195777893\n",
      "\t Training loss (single batch): 1.205579400062561\n",
      "\t Training loss (single batch): 2.0602827072143555\n",
      "\t Training loss (single batch): 1.4051491022109985\n",
      "\t Training loss (single batch): 0.85686194896698\n",
      "\t Training loss (single batch): 1.1220446825027466\n",
      "\t Training loss (single batch): 0.9098362922668457\n",
      "\t Training loss (single batch): 1.4002180099487305\n",
      "\t Training loss (single batch): 1.3602594137191772\n",
      "\t Training loss (single batch): 1.0587780475616455\n",
      "\t Training loss (single batch): 1.1275850534439087\n",
      "\t Training loss (single batch): 1.327812671661377\n",
      "\t Training loss (single batch): 0.935472309589386\n",
      "\t Training loss (single batch): 1.0245176553726196\n",
      "\t Training loss (single batch): 1.4246188402175903\n",
      "\t Training loss (single batch): 0.876134991645813\n",
      "\t Training loss (single batch): 1.108471155166626\n",
      "\t Training loss (single batch): 0.9847468733787537\n",
      "\t Training loss (single batch): 1.1554616689682007\n",
      "\t Training loss (single batch): 1.346043586730957\n",
      "\t Training loss (single batch): 2.108699083328247\n",
      "\t Training loss (single batch): 1.3995227813720703\n",
      "\t Training loss (single batch): 1.855221152305603\n",
      "\t Training loss (single batch): 1.357394814491272\n",
      "\t Training loss (single batch): 1.3537689447402954\n",
      "\t Training loss (single batch): 1.2012300491333008\n",
      "\t Training loss (single batch): 1.0808335542678833\n",
      "\t Training loss (single batch): 1.0328428745269775\n",
      "\t Training loss (single batch): 1.0792734622955322\n",
      "\t Training loss (single batch): 0.955437958240509\n",
      "\t Training loss (single batch): 1.3716809749603271\n",
      "\t Training loss (single batch): 1.1012505292892456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.409859299659729\n",
      "\t Training loss (single batch): 0.9187657833099365\n",
      "\t Training loss (single batch): 1.2342056035995483\n",
      "\t Training loss (single batch): 1.078007698059082\n",
      "\t Training loss (single batch): 1.3513667583465576\n",
      "\t Training loss (single batch): 0.7684648633003235\n",
      "\t Training loss (single batch): 1.0848313570022583\n",
      "\t Training loss (single batch): 1.2168477773666382\n",
      "\t Training loss (single batch): 1.023125410079956\n",
      "\t Training loss (single batch): 0.6684855818748474\n",
      "\t Training loss (single batch): 1.178481936454773\n",
      "\t Training loss (single batch): 1.1121876239776611\n",
      "\t Training loss (single batch): 0.7740020751953125\n",
      "\t Training loss (single batch): 1.1930007934570312\n",
      "\t Training loss (single batch): 1.2081632614135742\n",
      "\t Training loss (single batch): 0.8864773511886597\n",
      "\t Training loss (single batch): 1.4735649824142456\n",
      "\t Training loss (single batch): 1.0508043766021729\n",
      "\t Training loss (single batch): 1.187956690788269\n",
      "\t Training loss (single batch): 0.9888030886650085\n",
      "\t Training loss (single batch): 1.2710556983947754\n",
      "\t Training loss (single batch): 0.8815193176269531\n",
      "\t Training loss (single batch): 1.118868350982666\n",
      "\t Training loss (single batch): 1.0203700065612793\n",
      "\t Training loss (single batch): 1.519002914428711\n",
      "\t Training loss (single batch): 1.396400809288025\n",
      "\t Training loss (single batch): 1.4064046144485474\n",
      "\t Training loss (single batch): 1.7705668210983276\n",
      "\t Training loss (single batch): 1.154358148574829\n",
      "\t Training loss (single batch): 0.790752112865448\n",
      "\t Training loss (single batch): 0.8692964315414429\n",
      "\t Training loss (single batch): 0.9326571822166443\n",
      "\t Training loss (single batch): 1.2183219194412231\n",
      "\t Training loss (single batch): 1.091729998588562\n",
      "\t Training loss (single batch): 1.0483700037002563\n",
      "\t Training loss (single batch): 0.7693634629249573\n",
      "\t Training loss (single batch): 1.0803992748260498\n",
      "\t Training loss (single batch): 1.4244751930236816\n",
      "\t Training loss (single batch): 1.1222069263458252\n",
      "\t Training loss (single batch): 1.3055552244186401\n",
      "\t Training loss (single batch): 1.21799898147583\n",
      "\t Training loss (single batch): 1.2949830293655396\n",
      "\t Training loss (single batch): 0.8227371573448181\n",
      "\t Training loss (single batch): 1.4415191411972046\n",
      "\t Training loss (single batch): 1.4354721307754517\n",
      "\t Training loss (single batch): 0.8747479915618896\n",
      "\t Training loss (single batch): 1.2857478857040405\n",
      "\t Training loss (single batch): 0.5380358695983887\n",
      "\t Training loss (single batch): 1.4778547286987305\n",
      "\t Training loss (single batch): 1.120888113975525\n",
      "\t Training loss (single batch): 0.9593684077262878\n",
      "\t Training loss (single batch): 0.7584829330444336\n",
      "\t Training loss (single batch): 0.9509506821632385\n",
      "\t Training loss (single batch): 0.9716433882713318\n",
      "\t Training loss (single batch): 1.3038794994354248\n",
      "\t Training loss (single batch): 1.249368667602539\n",
      "\t Training loss (single batch): 0.9826099872589111\n",
      "\t Training loss (single batch): 1.0324642658233643\n",
      "\t Training loss (single batch): 1.1664162874221802\n",
      "\t Training loss (single batch): 1.521586298942566\n",
      "\t Training loss (single batch): 1.8032652139663696\n",
      "\t Training loss (single batch): 1.1313387155532837\n",
      "\t Training loss (single batch): 1.2255816459655762\n",
      "\t Training loss (single batch): 1.1606296300888062\n",
      "\t Training loss (single batch): 1.7728856801986694\n",
      "\t Training loss (single batch): 1.1088838577270508\n",
      "##################################\n",
      "## EPOCH 89\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1155809164047241\n",
      "\t Training loss (single batch): 0.8238714337348938\n",
      "\t Training loss (single batch): 1.2423168420791626\n",
      "\t Training loss (single batch): 1.4845117330551147\n",
      "\t Training loss (single batch): 1.1591874361038208\n",
      "\t Training loss (single batch): 1.2950516939163208\n",
      "\t Training loss (single batch): 1.10323965549469\n",
      "\t Training loss (single batch): 1.2844048738479614\n",
      "\t Training loss (single batch): 1.161322832107544\n",
      "\t Training loss (single batch): 1.1916186809539795\n",
      "\t Training loss (single batch): 1.6224839687347412\n",
      "\t Training loss (single batch): 1.0625663995742798\n",
      "\t Training loss (single batch): 1.4669041633605957\n",
      "\t Training loss (single batch): 1.2110891342163086\n",
      "\t Training loss (single batch): 0.9153422117233276\n",
      "\t Training loss (single batch): 0.8617075085639954\n",
      "\t Training loss (single batch): 1.0998576879501343\n",
      "\t Training loss (single batch): 1.2681275606155396\n",
      "\t Training loss (single batch): 1.515589714050293\n",
      "\t Training loss (single batch): 1.13442862033844\n",
      "\t Training loss (single batch): 1.2170859575271606\n",
      "\t Training loss (single batch): 1.5487092733383179\n",
      "\t Training loss (single batch): 0.9930258989334106\n",
      "\t Training loss (single batch): 1.201668381690979\n",
      "\t Training loss (single batch): 1.4623181819915771\n",
      "\t Training loss (single batch): 1.1346323490142822\n",
      "\t Training loss (single batch): 1.1487295627593994\n",
      "\t Training loss (single batch): 0.9892349243164062\n",
      "\t Training loss (single batch): 1.1436281204223633\n",
      "\t Training loss (single batch): 0.8588625192642212\n",
      "\t Training loss (single batch): 1.097795009613037\n",
      "\t Training loss (single batch): 0.6883939504623413\n",
      "\t Training loss (single batch): 1.2856706380844116\n",
      "\t Training loss (single batch): 0.9530308246612549\n",
      "\t Training loss (single batch): 1.081445336341858\n",
      "\t Training loss (single batch): 2.0753469467163086\n",
      "\t Training loss (single batch): 0.9199774861335754\n",
      "\t Training loss (single batch): 1.0049993991851807\n",
      "\t Training loss (single batch): 1.0724186897277832\n",
      "\t Training loss (single batch): 0.9908357262611389\n",
      "\t Training loss (single batch): 1.5541541576385498\n",
      "\t Training loss (single batch): 0.8448637127876282\n",
      "\t Training loss (single batch): 0.9210999608039856\n",
      "\t Training loss (single batch): 0.9434303641319275\n",
      "\t Training loss (single batch): 1.204322099685669\n",
      "\t Training loss (single batch): 1.0469640493392944\n",
      "\t Training loss (single batch): 1.0034092664718628\n",
      "\t Training loss (single batch): 0.8452764749526978\n",
      "\t Training loss (single batch): 1.2517883777618408\n",
      "\t Training loss (single batch): 2.415220260620117\n",
      "\t Training loss (single batch): 1.2838433980941772\n",
      "\t Training loss (single batch): 1.2316230535507202\n",
      "\t Training loss (single batch): 0.6931253671646118\n",
      "\t Training loss (single batch): 1.5236188173294067\n",
      "\t Training loss (single batch): 1.4019368886947632\n",
      "\t Training loss (single batch): 1.0404266119003296\n",
      "\t Training loss (single batch): 1.268005609512329\n",
      "\t Training loss (single batch): 1.5124348402023315\n",
      "\t Training loss (single batch): 1.2219282388687134\n",
      "\t Training loss (single batch): 1.1055588722229004\n",
      "\t Training loss (single batch): 0.8036020398139954\n",
      "\t Training loss (single batch): 1.2577756643295288\n",
      "\t Training loss (single batch): 1.2813973426818848\n",
      "\t Training loss (single batch): 0.702163577079773\n",
      "\t Training loss (single batch): 1.121348261833191\n",
      "\t Training loss (single batch): 0.8447272777557373\n",
      "\t Training loss (single batch): 1.169072151184082\n",
      "\t Training loss (single batch): 1.366221308708191\n",
      "\t Training loss (single batch): 1.1151337623596191\n",
      "\t Training loss (single batch): 1.4199419021606445\n",
      "\t Training loss (single batch): 1.2956300973892212\n",
      "\t Training loss (single batch): 1.3408817052841187\n",
      "\t Training loss (single batch): 1.502426028251648\n",
      "\t Training loss (single batch): 0.7721738815307617\n",
      "\t Training loss (single batch): 1.4389781951904297\n",
      "\t Training loss (single batch): 1.2724496126174927\n",
      "\t Training loss (single batch): 1.198947787284851\n",
      "\t Training loss (single batch): 0.9358809590339661\n",
      "\t Training loss (single batch): 0.9572048783302307\n",
      "\t Training loss (single batch): 1.1840776205062866\n",
      "\t Training loss (single batch): 0.9787525534629822\n",
      "\t Training loss (single batch): 1.3837110996246338\n",
      "\t Training loss (single batch): 1.2422828674316406\n",
      "\t Training loss (single batch): 1.7666748762130737\n",
      "\t Training loss (single batch): 1.115442156791687\n",
      "\t Training loss (single batch): 1.3405699729919434\n",
      "\t Training loss (single batch): 1.3231217861175537\n",
      "\t Training loss (single batch): 1.217693567276001\n",
      "\t Training loss (single batch): 1.3776075839996338\n",
      "\t Training loss (single batch): 1.0963051319122314\n",
      "\t Training loss (single batch): 1.2810770273208618\n",
      "\t Training loss (single batch): 0.9931733012199402\n",
      "\t Training loss (single batch): 1.1104254722595215\n",
      "\t Training loss (single batch): 1.1087104082107544\n",
      "\t Training loss (single batch): 1.6698330640792847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1660963296890259\n",
      "\t Training loss (single batch): 1.465815782546997\n",
      "\t Training loss (single batch): 1.2187116146087646\n",
      "\t Training loss (single batch): 1.306681513786316\n",
      "\t Training loss (single batch): 0.7974139451980591\n",
      "\t Training loss (single batch): 0.8673667311668396\n",
      "\t Training loss (single batch): 1.1254202127456665\n",
      "\t Training loss (single batch): 1.3723642826080322\n",
      "\t Training loss (single batch): 1.2152304649353027\n",
      "\t Training loss (single batch): 1.0696829557418823\n",
      "\t Training loss (single batch): 1.0070831775665283\n",
      "\t Training loss (single batch): 0.8621808290481567\n",
      "\t Training loss (single batch): 1.1988550424575806\n",
      "\t Training loss (single batch): 1.0141956806182861\n",
      "\t Training loss (single batch): 0.9153745770454407\n",
      "\t Training loss (single batch): 1.734504222869873\n",
      "\t Training loss (single batch): 0.9057338237762451\n",
      "\t Training loss (single batch): 1.132238745689392\n",
      "\t Training loss (single batch): 1.1052021980285645\n",
      "\t Training loss (single batch): 0.9415867924690247\n",
      "\t Training loss (single batch): 1.265304684638977\n",
      "\t Training loss (single batch): 0.8695604205131531\n",
      "\t Training loss (single batch): 1.1096737384796143\n",
      "\t Training loss (single batch): 1.1196328401565552\n",
      "\t Training loss (single batch): 1.2208952903747559\n",
      "\t Training loss (single batch): 1.017561674118042\n",
      "\t Training loss (single batch): 1.5617048740386963\n",
      "\t Training loss (single batch): 1.255306363105774\n",
      "\t Training loss (single batch): 0.9494591355323792\n",
      "\t Training loss (single batch): 1.0756127834320068\n",
      "\t Training loss (single batch): 1.1855019330978394\n",
      "\t Training loss (single batch): 1.1880146265029907\n",
      "\t Training loss (single batch): 0.9287233352661133\n",
      "\t Training loss (single batch): 1.3396393060684204\n",
      "\t Training loss (single batch): 1.4704854488372803\n",
      "\t Training loss (single batch): 0.6233217120170593\n",
      "\t Training loss (single batch): 0.9990622997283936\n",
      "\t Training loss (single batch): 1.288107991218567\n",
      "\t Training loss (single batch): 1.35785973072052\n",
      "\t Training loss (single batch): 1.3199790716171265\n",
      "\t Training loss (single batch): 1.3456679582595825\n",
      "\t Training loss (single batch): 1.5490444898605347\n",
      "\t Training loss (single batch): 1.1810458898544312\n",
      "\t Training loss (single batch): 0.961017906665802\n",
      "\t Training loss (single batch): 1.2125118970870972\n",
      "\t Training loss (single batch): 1.3838473558425903\n",
      "\t Training loss (single batch): 0.6616902947425842\n",
      "\t Training loss (single batch): 1.0499387979507446\n",
      "\t Training loss (single batch): 1.543927550315857\n",
      "\t Training loss (single batch): 1.0748507976531982\n",
      "\t Training loss (single batch): 1.1154005527496338\n",
      "\t Training loss (single batch): 1.0873417854309082\n",
      "\t Training loss (single batch): 1.799232006072998\n",
      "\t Training loss (single batch): 1.481528401374817\n",
      "\t Training loss (single batch): 1.1795759201049805\n",
      "\t Training loss (single batch): 1.3577195405960083\n",
      "\t Training loss (single batch): 1.3008646965026855\n",
      "\t Training loss (single batch): 1.1148756742477417\n",
      "\t Training loss (single batch): 1.332022786140442\n",
      "\t Training loss (single batch): 1.0747692584991455\n",
      "\t Training loss (single batch): 0.8468431830406189\n",
      "\t Training loss (single batch): 1.0996211767196655\n",
      "\t Training loss (single batch): 0.9008180499076843\n",
      "\t Training loss (single batch): 1.2247920036315918\n",
      "\t Training loss (single batch): 1.1089731454849243\n",
      "\t Training loss (single batch): 1.2400702238082886\n",
      "\t Training loss (single batch): 1.0242058038711548\n",
      "\t Training loss (single batch): 1.407192349433899\n",
      "\t Training loss (single batch): 0.8048397898674011\n",
      "\t Training loss (single batch): 0.9992455244064331\n",
      "\t Training loss (single batch): 1.1668835878372192\n",
      "\t Training loss (single batch): 1.2475250959396362\n",
      "\t Training loss (single batch): 1.6255686283111572\n",
      "\t Training loss (single batch): 1.3410569429397583\n",
      "\t Training loss (single batch): 0.9724252820014954\n",
      "\t Training loss (single batch): 1.0525496006011963\n",
      "\t Training loss (single batch): 1.4738553762435913\n",
      "\t Training loss (single batch): 1.2810261249542236\n",
      "\t Training loss (single batch): 1.1890250444412231\n",
      "\t Training loss (single batch): 1.1002085208892822\n",
      "\t Training loss (single batch): 1.708332896232605\n",
      "\t Training loss (single batch): 1.397966980934143\n",
      "\t Training loss (single batch): 0.8198487758636475\n",
      "\t Training loss (single batch): 1.4006109237670898\n",
      "\t Training loss (single batch): 0.8770105242729187\n",
      "\t Training loss (single batch): 1.266425371170044\n",
      "\t Training loss (single batch): 1.0668121576309204\n",
      "\t Training loss (single batch): 1.333605408668518\n",
      "\t Training loss (single batch): 1.215980052947998\n",
      "\t Training loss (single batch): 0.8664274215698242\n",
      "\t Training loss (single batch): 0.9753363132476807\n",
      "\t Training loss (single batch): 0.9112856984138489\n",
      "\t Training loss (single batch): 1.0349476337432861\n",
      "\t Training loss (single batch): 1.457774043083191\n",
      "\t Training loss (single batch): 1.2664910554885864\n",
      "\t Training loss (single batch): 1.4623520374298096\n",
      "\t Training loss (single batch): 1.461162805557251\n",
      "\t Training loss (single batch): 1.311160683631897\n",
      "\t Training loss (single batch): 1.0013355016708374\n",
      "\t Training loss (single batch): 1.132002592086792\n",
      "\t Training loss (single batch): 1.7269556522369385\n",
      "\t Training loss (single batch): 1.1303215026855469\n",
      "\t Training loss (single batch): 1.2919868230819702\n",
      "\t Training loss (single batch): 1.2934590578079224\n",
      "\t Training loss (single batch): 1.3609710931777954\n",
      "\t Training loss (single batch): 1.2750529050827026\n",
      "\t Training loss (single batch): 1.1439793109893799\n",
      "\t Training loss (single batch): 0.9878459572792053\n",
      "\t Training loss (single batch): 1.3039133548736572\n",
      "\t Training loss (single batch): 1.28459632396698\n",
      "\t Training loss (single batch): 1.0238105058670044\n",
      "\t Training loss (single batch): 1.1951488256454468\n",
      "\t Training loss (single batch): 0.8708353638648987\n",
      "\t Training loss (single batch): 1.143323302268982\n",
      "\t Training loss (single batch): 1.3048081398010254\n",
      "\t Training loss (single batch): 1.2246732711791992\n",
      "\t Training loss (single batch): 1.0245366096496582\n",
      "\t Training loss (single batch): 0.8949235081672668\n",
      "\t Training loss (single batch): 1.278074860572815\n",
      "\t Training loss (single batch): 1.1717314720153809\n",
      "\t Training loss (single batch): 1.2010234594345093\n",
      "\t Training loss (single batch): 0.9960611462593079\n",
      "\t Training loss (single batch): 1.4995216131210327\n",
      "\t Training loss (single batch): 1.051757574081421\n",
      "\t Training loss (single batch): 0.9045839309692383\n",
      "\t Training loss (single batch): 0.7091860771179199\n",
      "\t Training loss (single batch): 1.2623405456542969\n",
      "\t Training loss (single batch): 1.205565094947815\n",
      "\t Training loss (single batch): 1.569474458694458\n",
      "\t Training loss (single batch): 1.2637864351272583\n",
      "\t Training loss (single batch): 1.079161524772644\n",
      "\t Training loss (single batch): 1.4411453008651733\n",
      "\t Training loss (single batch): 1.396260142326355\n",
      "\t Training loss (single batch): 1.3332234621047974\n",
      "\t Training loss (single batch): 1.4194426536560059\n",
      "\t Training loss (single batch): 1.5497632026672363\n",
      "\t Training loss (single batch): 1.3273489475250244\n",
      "\t Training loss (single batch): 0.9130546450614929\n",
      "\t Training loss (single batch): 0.9540231823921204\n",
      "\t Training loss (single batch): 1.209734320640564\n",
      "\t Training loss (single batch): 1.115615725517273\n",
      "\t Training loss (single batch): 0.9662098288536072\n",
      "\t Training loss (single batch): 1.4369595050811768\n",
      "\t Training loss (single batch): 1.2391068935394287\n",
      "\t Training loss (single batch): 0.9728012681007385\n",
      "\t Training loss (single batch): 1.1935889720916748\n",
      "\t Training loss (single batch): 1.1647944450378418\n",
      "\t Training loss (single batch): 1.3743150234222412\n",
      "\t Training loss (single batch): 1.4066184759140015\n",
      "\t Training loss (single batch): 1.0476939678192139\n",
      "\t Training loss (single batch): 0.8853585124015808\n",
      "\t Training loss (single batch): 1.1451541185379028\n",
      "\t Training loss (single batch): 0.7221004366874695\n",
      "\t Training loss (single batch): 1.1297457218170166\n",
      "\t Training loss (single batch): 1.2520778179168701\n",
      "\t Training loss (single batch): 1.2196298837661743\n",
      "\t Training loss (single batch): 1.4830138683319092\n",
      "\t Training loss (single batch): 2.082505702972412\n",
      "\t Training loss (single batch): 1.093460202217102\n",
      "\t Training loss (single batch): 1.5594528913497925\n",
      "\t Training loss (single batch): 1.355185866355896\n",
      "\t Training loss (single batch): 1.3953619003295898\n",
      "\t Training loss (single batch): 1.5779019594192505\n",
      "\t Training loss (single batch): 1.2658412456512451\n",
      "\t Training loss (single batch): 1.0121862888336182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3916183710098267\n",
      "\t Training loss (single batch): 0.5275797843933105\n",
      "\t Training loss (single batch): 0.9227423667907715\n",
      "\t Training loss (single batch): 1.8900244235992432\n",
      "\t Training loss (single batch): 1.4082047939300537\n",
      "\t Training loss (single batch): 1.4166535139083862\n",
      "\t Training loss (single batch): 1.163924217224121\n",
      "\t Training loss (single batch): 1.0936459302902222\n",
      "\t Training loss (single batch): 1.8790068626403809\n",
      "\t Training loss (single batch): 0.8899608254432678\n",
      "\t Training loss (single batch): 1.1337372064590454\n",
      "\t Training loss (single batch): 1.0966719388961792\n",
      "\t Training loss (single batch): 1.065276861190796\n",
      "\t Training loss (single batch): 1.2978180646896362\n",
      "\t Training loss (single batch): 1.3699572086334229\n",
      "\t Training loss (single batch): 0.7077285051345825\n",
      "\t Training loss (single batch): 0.9017319083213806\n",
      "\t Training loss (single batch): 0.7137190699577332\n",
      "\t Training loss (single batch): 1.1664201021194458\n",
      "\t Training loss (single batch): 1.501377820968628\n",
      "\t Training loss (single batch): 1.0695700645446777\n",
      "\t Training loss (single batch): 1.3415201902389526\n",
      "\t Training loss (single batch): 0.8994383811950684\n",
      "\t Training loss (single batch): 1.2061971426010132\n",
      "\t Training loss (single batch): 1.293155312538147\n",
      "\t Training loss (single batch): 1.4470744132995605\n",
      "\t Training loss (single batch): 0.9200156331062317\n",
      "\t Training loss (single batch): 1.26212477684021\n",
      "\t Training loss (single batch): 0.8945236802101135\n",
      "\t Training loss (single batch): 1.0626046657562256\n",
      "\t Training loss (single batch): 0.9527862071990967\n",
      "\t Training loss (single batch): 1.7187711000442505\n",
      "\t Training loss (single batch): 1.4821784496307373\n",
      "\t Training loss (single batch): 0.7184014916419983\n",
      "\t Training loss (single batch): 1.0776846408843994\n",
      "\t Training loss (single batch): 1.2148411273956299\n",
      "\t Training loss (single batch): 1.2628663778305054\n",
      "\t Training loss (single batch): 1.3502295017242432\n",
      "\t Training loss (single batch): 0.9500898122787476\n",
      "\t Training loss (single batch): 0.8543996214866638\n",
      "\t Training loss (single batch): 1.0985695123672485\n",
      "\t Training loss (single batch): 1.0918381214141846\n",
      "\t Training loss (single batch): 1.2992162704467773\n",
      "\t Training loss (single batch): 0.7789560556411743\n",
      "\t Training loss (single batch): 1.001893401145935\n",
      "\t Training loss (single batch): 0.7528398036956787\n",
      "\t Training loss (single batch): 1.0668227672576904\n",
      "\t Training loss (single batch): 1.4769518375396729\n",
      "\t Training loss (single batch): 1.3696564435958862\n",
      "\t Training loss (single batch): 0.85280442237854\n",
      "\t Training loss (single batch): 1.078993558883667\n",
      "\t Training loss (single batch): 0.9761180281639099\n",
      "\t Training loss (single batch): 2.0012497901916504\n",
      "\t Training loss (single batch): 1.4324471950531006\n",
      "\t Training loss (single batch): 1.4585716724395752\n",
      "\t Training loss (single batch): 1.7956966161727905\n",
      "\t Training loss (single batch): 2.1738152503967285\n",
      "##################################\n",
      "## EPOCH 90\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8689988851547241\n",
      "\t Training loss (single batch): 1.1036949157714844\n",
      "\t Training loss (single batch): 1.0997025966644287\n",
      "\t Training loss (single batch): 0.8164706230163574\n",
      "\t Training loss (single batch): 1.2230697870254517\n",
      "\t Training loss (single batch): 1.1766293048858643\n",
      "\t Training loss (single batch): 1.204941749572754\n",
      "\t Training loss (single batch): 0.913013756275177\n",
      "\t Training loss (single batch): 1.082874059677124\n",
      "\t Training loss (single batch): 0.9023877382278442\n",
      "\t Training loss (single batch): 1.6951406002044678\n",
      "\t Training loss (single batch): 1.0862258672714233\n",
      "\t Training loss (single batch): 1.0229181051254272\n",
      "\t Training loss (single batch): 1.1253936290740967\n",
      "\t Training loss (single batch): 1.0086854696273804\n",
      "\t Training loss (single batch): 1.3972928524017334\n",
      "\t Training loss (single batch): 0.838300347328186\n",
      "\t Training loss (single batch): 0.8026872873306274\n",
      "\t Training loss (single batch): 1.5109952688217163\n",
      "\t Training loss (single batch): 1.304444432258606\n",
      "\t Training loss (single batch): 0.9513669013977051\n",
      "\t Training loss (single batch): 1.3041921854019165\n",
      "\t Training loss (single batch): 1.5691660642623901\n",
      "\t Training loss (single batch): 0.9290662407875061\n",
      "\t Training loss (single batch): 0.817814290523529\n",
      "\t Training loss (single batch): 1.4054985046386719\n",
      "\t Training loss (single batch): 1.1861416101455688\n",
      "\t Training loss (single batch): 1.125105619430542\n",
      "\t Training loss (single batch): 0.9497112035751343\n",
      "\t Training loss (single batch): 1.2702172994613647\n",
      "\t Training loss (single batch): 1.2406827211380005\n",
      "\t Training loss (single batch): 1.0175762176513672\n",
      "\t Training loss (single batch): 1.1134306192398071\n",
      "\t Training loss (single batch): 1.724778413772583\n",
      "\t Training loss (single batch): 1.255622386932373\n",
      "\t Training loss (single batch): 1.165114164352417\n",
      "\t Training loss (single batch): 1.0014969110488892\n",
      "\t Training loss (single batch): 1.5836116075515747\n",
      "\t Training loss (single batch): 0.8925768733024597\n",
      "\t Training loss (single batch): 1.3199505805969238\n",
      "\t Training loss (single batch): 1.198080062866211\n",
      "\t Training loss (single batch): 0.9422435164451599\n",
      "\t Training loss (single batch): 0.7149589657783508\n",
      "\t Training loss (single batch): 1.2342592477798462\n",
      "\t Training loss (single batch): 1.4285582304000854\n",
      "\t Training loss (single batch): 0.9666243195533752\n",
      "\t Training loss (single batch): 1.104862093925476\n",
      "\t Training loss (single batch): 1.5480587482452393\n",
      "\t Training loss (single batch): 0.9494929313659668\n",
      "\t Training loss (single batch): 1.3637394905090332\n",
      "\t Training loss (single batch): 1.1523991823196411\n",
      "\t Training loss (single batch): 1.1880805492401123\n",
      "\t Training loss (single batch): 1.3635057210922241\n",
      "\t Training loss (single batch): 0.9177669286727905\n",
      "\t Training loss (single batch): 1.333412528038025\n",
      "\t Training loss (single batch): 0.932983934879303\n",
      "\t Training loss (single batch): 1.1307625770568848\n",
      "\t Training loss (single batch): 0.8134744167327881\n",
      "\t Training loss (single batch): 1.2629865407943726\n",
      "\t Training loss (single batch): 1.155346155166626\n",
      "\t Training loss (single batch): 1.074550747871399\n",
      "\t Training loss (single batch): 0.8477078080177307\n",
      "\t Training loss (single batch): 1.2474726438522339\n",
      "\t Training loss (single batch): 1.5754902362823486\n",
      "\t Training loss (single batch): 1.1769664287567139\n",
      "\t Training loss (single batch): 0.9533270001411438\n",
      "\t Training loss (single batch): 1.2450917959213257\n",
      "\t Training loss (single batch): 1.3273992538452148\n",
      "\t Training loss (single batch): 1.175327181816101\n",
      "\t Training loss (single batch): 1.0331867933273315\n",
      "\t Training loss (single batch): 0.8837464451789856\n",
      "\t Training loss (single batch): 1.4635825157165527\n",
      "\t Training loss (single batch): 1.3323262929916382\n",
      "\t Training loss (single batch): 0.8595389723777771\n",
      "\t Training loss (single batch): 0.7895254492759705\n",
      "\t Training loss (single batch): 1.5177620649337769\n",
      "\t Training loss (single batch): 1.2561707496643066\n",
      "\t Training loss (single batch): 1.0658931732177734\n",
      "\t Training loss (single batch): 1.4813899993896484\n",
      "\t Training loss (single batch): 1.277157187461853\n",
      "\t Training loss (single batch): 1.1329585313796997\n",
      "\t Training loss (single batch): 1.3949016332626343\n",
      "\t Training loss (single batch): 1.4455556869506836\n",
      "\t Training loss (single batch): 1.0859884023666382\n",
      "\t Training loss (single batch): 1.2541944980621338\n",
      "\t Training loss (single batch): 1.1900173425674438\n",
      "\t Training loss (single batch): 1.5266318321228027\n",
      "\t Training loss (single batch): 1.2240360975265503\n",
      "\t Training loss (single batch): 1.040830135345459\n",
      "\t Training loss (single batch): 1.2521432638168335\n",
      "\t Training loss (single batch): 1.3838610649108887\n",
      "\t Training loss (single batch): 1.3066617250442505\n",
      "\t Training loss (single batch): 0.9214612245559692\n",
      "\t Training loss (single batch): 1.0712591409683228\n",
      "\t Training loss (single batch): 1.1618510484695435\n",
      "\t Training loss (single batch): 0.8732696771621704\n",
      "\t Training loss (single batch): 1.5470454692840576\n",
      "\t Training loss (single batch): 0.8846643567085266\n",
      "\t Training loss (single batch): 1.007817268371582\n",
      "\t Training loss (single batch): 1.0195025205612183\n",
      "\t Training loss (single batch): 1.0088719129562378\n",
      "\t Training loss (single batch): 0.995187520980835\n",
      "\t Training loss (single batch): 1.3245383501052856\n",
      "\t Training loss (single batch): 1.3528145551681519\n",
      "\t Training loss (single batch): 0.8394705653190613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.169906735420227\n",
      "\t Training loss (single batch): 0.9865212440490723\n",
      "\t Training loss (single batch): 1.4261316061019897\n",
      "\t Training loss (single batch): 1.5346378087997437\n",
      "\t Training loss (single batch): 1.1475051641464233\n",
      "\t Training loss (single batch): 1.2631666660308838\n",
      "\t Training loss (single batch): 0.9333282709121704\n",
      "\t Training loss (single batch): 1.2465494871139526\n",
      "\t Training loss (single batch): 1.1517219543457031\n",
      "\t Training loss (single batch): 1.4524786472320557\n",
      "\t Training loss (single batch): 1.372832179069519\n",
      "\t Training loss (single batch): 1.1447330713272095\n",
      "\t Training loss (single batch): 1.3346792459487915\n",
      "\t Training loss (single batch): 1.0473287105560303\n",
      "\t Training loss (single batch): 1.1435133218765259\n",
      "\t Training loss (single batch): 0.6509005427360535\n",
      "\t Training loss (single batch): 0.775202751159668\n",
      "\t Training loss (single batch): 1.3021513223648071\n",
      "\t Training loss (single batch): 0.93003910779953\n",
      "\t Training loss (single batch): 1.300794005393982\n",
      "\t Training loss (single batch): 0.8341777920722961\n",
      "\t Training loss (single batch): 1.0406337976455688\n",
      "\t Training loss (single batch): 1.908403992652893\n",
      "\t Training loss (single batch): 0.9656737446784973\n",
      "\t Training loss (single batch): 0.9379681944847107\n",
      "\t Training loss (single batch): 0.991358757019043\n",
      "\t Training loss (single batch): 0.893785297870636\n",
      "\t Training loss (single batch): 1.3760886192321777\n",
      "\t Training loss (single batch): 1.5727201700210571\n",
      "\t Training loss (single batch): 1.3837884664535522\n",
      "\t Training loss (single batch): 1.3498187065124512\n",
      "\t Training loss (single batch): 1.5517363548278809\n",
      "\t Training loss (single batch): 0.9729568958282471\n",
      "\t Training loss (single batch): 1.034807801246643\n",
      "\t Training loss (single batch): 1.3824636936187744\n",
      "\t Training loss (single batch): 1.834047794342041\n",
      "\t Training loss (single batch): 1.0739269256591797\n",
      "\t Training loss (single batch): 1.3753050565719604\n",
      "\t Training loss (single batch): 0.9203214645385742\n",
      "\t Training loss (single batch): 1.419220209121704\n",
      "\t Training loss (single batch): 1.1215413808822632\n",
      "\t Training loss (single batch): 1.2519673109054565\n",
      "\t Training loss (single batch): 1.5318350791931152\n",
      "\t Training loss (single batch): 1.1943893432617188\n",
      "\t Training loss (single batch): 1.2179689407348633\n",
      "\t Training loss (single batch): 0.9433844685554504\n",
      "\t Training loss (single batch): 0.8208388686180115\n",
      "\t Training loss (single batch): 1.7198748588562012\n",
      "\t Training loss (single batch): 1.5569348335266113\n",
      "\t Training loss (single batch): 1.2932045459747314\n",
      "\t Training loss (single batch): 1.2696980237960815\n",
      "\t Training loss (single batch): 1.3832292556762695\n",
      "\t Training loss (single batch): 1.4789035320281982\n",
      "\t Training loss (single batch): 1.0279500484466553\n",
      "\t Training loss (single batch): 1.186262607574463\n",
      "\t Training loss (single batch): 0.7069562077522278\n",
      "\t Training loss (single batch): 1.0081582069396973\n",
      "\t Training loss (single batch): 1.2978523969650269\n",
      "\t Training loss (single batch): 1.0473471879959106\n",
      "\t Training loss (single batch): 0.7884377241134644\n",
      "\t Training loss (single batch): 1.0757747888565063\n",
      "\t Training loss (single batch): 1.2007030248641968\n",
      "\t Training loss (single batch): 1.0196757316589355\n",
      "\t Training loss (single batch): 0.8729138374328613\n",
      "\t Training loss (single batch): 1.4450948238372803\n",
      "\t Training loss (single batch): 1.5799086093902588\n",
      "\t Training loss (single batch): 1.260007381439209\n",
      "\t Training loss (single batch): 1.0704936981201172\n",
      "\t Training loss (single batch): 1.38706374168396\n",
      "\t Training loss (single batch): 1.2367345094680786\n",
      "\t Training loss (single batch): 1.168898582458496\n",
      "\t Training loss (single batch): 1.1289989948272705\n",
      "\t Training loss (single batch): 0.9321280717849731\n",
      "\t Training loss (single batch): 1.1283760070800781\n",
      "\t Training loss (single batch): 0.8261494636535645\n",
      "\t Training loss (single batch): 1.0697815418243408\n",
      "\t Training loss (single batch): 1.2478742599487305\n",
      "\t Training loss (single batch): 0.8312807679176331\n",
      "\t Training loss (single batch): 1.0093929767608643\n",
      "\t Training loss (single batch): 1.233688235282898\n",
      "\t Training loss (single batch): 1.3985111713409424\n",
      "\t Training loss (single batch): 0.8254525661468506\n",
      "\t Training loss (single batch): 1.5860862731933594\n",
      "\t Training loss (single batch): 1.4374560117721558\n",
      "\t Training loss (single batch): 1.114963412284851\n",
      "\t Training loss (single batch): 1.179076910018921\n",
      "\t Training loss (single batch): 1.0698190927505493\n",
      "\t Training loss (single batch): 1.0068578720092773\n",
      "\t Training loss (single batch): 1.3689194917678833\n",
      "\t Training loss (single batch): 0.8761869072914124\n",
      "\t Training loss (single batch): 0.9675722718238831\n",
      "\t Training loss (single batch): 1.0037254095077515\n",
      "\t Training loss (single batch): 0.9222572445869446\n",
      "\t Training loss (single batch): 1.2360811233520508\n",
      "\t Training loss (single batch): 1.3773971796035767\n",
      "\t Training loss (single batch): 1.4524825811386108\n",
      "\t Training loss (single batch): 0.9909412860870361\n",
      "\t Training loss (single batch): 1.0313310623168945\n",
      "\t Training loss (single batch): 0.8406832814216614\n",
      "\t Training loss (single batch): 1.0121397972106934\n",
      "\t Training loss (single batch): 1.0738614797592163\n",
      "\t Training loss (single batch): 1.24699068069458\n",
      "\t Training loss (single batch): 0.9997201561927795\n",
      "\t Training loss (single batch): 1.2045307159423828\n",
      "\t Training loss (single batch): 1.0916669368743896\n",
      "\t Training loss (single batch): 1.4534659385681152\n",
      "\t Training loss (single batch): 1.1305961608886719\n",
      "\t Training loss (single batch): 1.4521031379699707\n",
      "\t Training loss (single batch): 1.7579172849655151\n",
      "\t Training loss (single batch): 1.0109457969665527\n",
      "\t Training loss (single batch): 1.362974762916565\n",
      "\t Training loss (single batch): 1.1355972290039062\n",
      "\t Training loss (single batch): 1.2298572063446045\n",
      "\t Training loss (single batch): 1.3023446798324585\n",
      "\t Training loss (single batch): 1.1150016784667969\n",
      "\t Training loss (single batch): 1.119368553161621\n",
      "\t Training loss (single batch): 0.8910647630691528\n",
      "\t Training loss (single batch): 0.8000316619873047\n",
      "\t Training loss (single batch): 1.546962022781372\n",
      "\t Training loss (single batch): 1.4066616296768188\n",
      "\t Training loss (single batch): 0.6318915486335754\n",
      "\t Training loss (single batch): 0.9405991435050964\n",
      "\t Training loss (single batch): 0.9814397692680359\n",
      "\t Training loss (single batch): 1.2407629489898682\n",
      "\t Training loss (single batch): 1.8342679738998413\n",
      "\t Training loss (single batch): 1.3146494626998901\n",
      "\t Training loss (single batch): 0.6426562666893005\n",
      "\t Training loss (single batch): 0.7041389346122742\n",
      "\t Training loss (single batch): 0.9877424836158752\n",
      "\t Training loss (single batch): 1.2620371580123901\n",
      "\t Training loss (single batch): 0.8920711278915405\n",
      "\t Training loss (single batch): 1.00643789768219\n",
      "\t Training loss (single batch): 1.146157145500183\n",
      "\t Training loss (single batch): 1.0603337287902832\n",
      "\t Training loss (single batch): 1.2944047451019287\n",
      "\t Training loss (single batch): 0.9347507357597351\n",
      "\t Training loss (single batch): 0.8893811106681824\n",
      "\t Training loss (single batch): 0.9733250141143799\n",
      "\t Training loss (single batch): 0.9406627416610718\n",
      "\t Training loss (single batch): 1.268316626548767\n",
      "\t Training loss (single batch): 0.9079433083534241\n",
      "\t Training loss (single batch): 1.6428302526474\n",
      "\t Training loss (single batch): 1.3990044593811035\n",
      "\t Training loss (single batch): 0.8219356536865234\n",
      "\t Training loss (single batch): 1.2407634258270264\n",
      "\t Training loss (single batch): 1.2265782356262207\n",
      "\t Training loss (single batch): 2.0386314392089844\n",
      "\t Training loss (single batch): 1.4400312900543213\n",
      "\t Training loss (single batch): 1.1795494556427002\n",
      "\t Training loss (single batch): 1.3253544569015503\n",
      "\t Training loss (single batch): 1.411798119544983\n",
      "\t Training loss (single batch): 1.328145980834961\n",
      "\t Training loss (single batch): 1.2555512189865112\n",
      "\t Training loss (single batch): 1.4877837896347046\n",
      "\t Training loss (single batch): 0.9361494779586792\n",
      "\t Training loss (single batch): 1.2838419675827026\n",
      "\t Training loss (single batch): 0.8181934356689453\n",
      "\t Training loss (single batch): 0.8115386366844177\n",
      "\t Training loss (single batch): 1.1294841766357422\n",
      "\t Training loss (single batch): 1.1860582828521729\n",
      "\t Training loss (single batch): 1.4534392356872559\n",
      "\t Training loss (single batch): 1.3655858039855957\n",
      "\t Training loss (single batch): 1.186431884765625\n",
      "\t Training loss (single batch): 0.8467774987220764\n",
      "\t Training loss (single batch): 1.1834450960159302\n",
      "\t Training loss (single batch): 1.6802327632904053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.347092628479004\n",
      "\t Training loss (single batch): 0.865970253944397\n",
      "\t Training loss (single batch): 1.0479462146759033\n",
      "\t Training loss (single batch): 0.8741661906242371\n",
      "\t Training loss (single batch): 1.2433770895004272\n",
      "\t Training loss (single batch): 1.1058008670806885\n",
      "\t Training loss (single batch): 1.3079097270965576\n",
      "\t Training loss (single batch): 1.1135039329528809\n",
      "\t Training loss (single batch): 0.8053085207939148\n",
      "\t Training loss (single batch): 1.3094756603240967\n",
      "\t Training loss (single batch): 1.73195219039917\n",
      "\t Training loss (single batch): 1.5031439065933228\n",
      "\t Training loss (single batch): 0.894516110420227\n",
      "\t Training loss (single batch): 1.2686882019042969\n",
      "\t Training loss (single batch): 0.8908903002738953\n",
      "\t Training loss (single batch): 1.2074551582336426\n",
      "\t Training loss (single batch): 1.1126295328140259\n",
      "\t Training loss (single batch): 2.0231382846832275\n",
      "\t Training loss (single batch): 1.4473716020584106\n",
      "\t Training loss (single batch): 1.0655465126037598\n",
      "\t Training loss (single batch): 0.8247509002685547\n",
      "\t Training loss (single batch): 1.1352150440216064\n",
      "\t Training loss (single batch): 1.3524675369262695\n",
      "\t Training loss (single batch): 0.8211439251899719\n",
      "\t Training loss (single batch): 0.8473157286643982\n",
      "\t Training loss (single batch): 1.0904879570007324\n",
      "\t Training loss (single batch): 1.183120846748352\n",
      "\t Training loss (single batch): 1.353370189666748\n",
      "\t Training loss (single batch): 0.8153499364852905\n",
      "\t Training loss (single batch): 1.546669840812683\n",
      "\t Training loss (single batch): 1.041878581047058\n",
      "\t Training loss (single batch): 1.1305773258209229\n",
      "\t Training loss (single batch): 1.5276297330856323\n",
      "\t Training loss (single batch): 1.0497807264328003\n",
      "\t Training loss (single batch): 1.2613781690597534\n",
      "\t Training loss (single batch): 1.5819770097732544\n",
      "\t Training loss (single batch): 1.4507023096084595\n",
      "\t Training loss (single batch): 1.2125197649002075\n",
      "\t Training loss (single batch): 0.8053354620933533\n",
      "\t Training loss (single batch): 1.3124573230743408\n",
      "\t Training loss (single batch): 0.8419604897499084\n",
      "\t Training loss (single batch): 1.3769220113754272\n",
      "\t Training loss (single batch): 1.509437918663025\n",
      "\t Training loss (single batch): 1.3490145206451416\n",
      "\t Training loss (single batch): 1.2425512075424194\n",
      "\t Training loss (single batch): 3.297334671020508\n",
      "##################################\n",
      "## EPOCH 91\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9275716543197632\n",
      "\t Training loss (single batch): 1.2788965702056885\n",
      "\t Training loss (single batch): 1.5841304063796997\n",
      "\t Training loss (single batch): 1.227738380432129\n",
      "\t Training loss (single batch): 0.9215760231018066\n",
      "\t Training loss (single batch): 1.117891788482666\n",
      "\t Training loss (single batch): 1.3876397609710693\n",
      "\t Training loss (single batch): 1.347610354423523\n",
      "\t Training loss (single batch): 1.6798524856567383\n",
      "\t Training loss (single batch): 1.2625919580459595\n",
      "\t Training loss (single batch): 1.2942720651626587\n",
      "\t Training loss (single batch): 0.9156445264816284\n",
      "\t Training loss (single batch): 1.5520256757736206\n",
      "\t Training loss (single batch): 0.9393113851547241\n",
      "\t Training loss (single batch): 0.804179847240448\n",
      "\t Training loss (single batch): 1.031559705734253\n",
      "\t Training loss (single batch): 1.1645607948303223\n",
      "\t Training loss (single batch): 1.1076445579528809\n",
      "\t Training loss (single batch): 1.6184195280075073\n",
      "\t Training loss (single batch): 0.7951906323432922\n",
      "\t Training loss (single batch): 1.3896528482437134\n",
      "\t Training loss (single batch): 0.695576012134552\n",
      "\t Training loss (single batch): 1.381921410560608\n",
      "\t Training loss (single batch): 1.6489585638046265\n",
      "\t Training loss (single batch): 0.8593857288360596\n",
      "\t Training loss (single batch): 1.6232796907424927\n",
      "\t Training loss (single batch): 1.218843698501587\n",
      "\t Training loss (single batch): 1.1948881149291992\n",
      "\t Training loss (single batch): 1.1730459928512573\n",
      "\t Training loss (single batch): 1.2042080163955688\n",
      "\t Training loss (single batch): 1.4190260171890259\n",
      "\t Training loss (single batch): 1.021450400352478\n",
      "\t Training loss (single batch): 1.6869361400604248\n",
      "\t Training loss (single batch): 1.163120150566101\n",
      "\t Training loss (single batch): 1.02627694606781\n",
      "\t Training loss (single batch): 1.4882837533950806\n",
      "\t Training loss (single batch): 1.0596855878829956\n",
      "\t Training loss (single batch): 0.788365364074707\n",
      "\t Training loss (single batch): 1.037521243095398\n",
      "\t Training loss (single batch): 1.6102375984191895\n",
      "\t Training loss (single batch): 1.0918186902999878\n",
      "\t Training loss (single batch): 1.0016075372695923\n",
      "\t Training loss (single batch): 1.2525932788848877\n",
      "\t Training loss (single batch): 1.1523326635360718\n",
      "\t Training loss (single batch): 1.2806156873703003\n",
      "\t Training loss (single batch): 1.7416374683380127\n",
      "\t Training loss (single batch): 1.565474271774292\n",
      "\t Training loss (single batch): 0.918247640132904\n",
      "\t Training loss (single batch): 1.0006307363510132\n",
      "\t Training loss (single batch): 1.1137681007385254\n",
      "\t Training loss (single batch): 0.6617221236228943\n",
      "\t Training loss (single batch): 1.4835504293441772\n",
      "\t Training loss (single batch): 1.7367043495178223\n",
      "\t Training loss (single batch): 0.9944284558296204\n",
      "\t Training loss (single batch): 1.4115608930587769\n",
      "\t Training loss (single batch): 1.5993691682815552\n",
      "\t Training loss (single batch): 1.2577441930770874\n",
      "\t Training loss (single batch): 1.42799973487854\n",
      "\t Training loss (single batch): 1.0278751850128174\n",
      "\t Training loss (single batch): 1.0363571643829346\n",
      "\t Training loss (single batch): 1.2962710857391357\n",
      "\t Training loss (single batch): 0.8404542207717896\n",
      "\t Training loss (single batch): 2.0170068740844727\n",
      "\t Training loss (single batch): 1.1826798915863037\n",
      "\t Training loss (single batch): 0.9885478019714355\n",
      "\t Training loss (single batch): 1.0303359031677246\n",
      "\t Training loss (single batch): 0.9293199777603149\n",
      "\t Training loss (single batch): 1.0612555742263794\n",
      "\t Training loss (single batch): 1.546576738357544\n",
      "\t Training loss (single batch): 1.2035998106002808\n",
      "\t Training loss (single batch): 1.3132734298706055\n",
      "\t Training loss (single batch): 1.0298720598220825\n",
      "\t Training loss (single batch): 0.8706859946250916\n",
      "\t Training loss (single batch): 1.3096022605895996\n",
      "\t Training loss (single batch): 1.3259309530258179\n",
      "\t Training loss (single batch): 1.8397194147109985\n",
      "\t Training loss (single batch): 1.5778568983078003\n",
      "\t Training loss (single batch): 1.3296633958816528\n",
      "\t Training loss (single batch): 1.744391679763794\n",
      "\t Training loss (single batch): 0.9393469095230103\n",
      "\t Training loss (single batch): 0.8535342812538147\n",
      "\t Training loss (single batch): 0.747961163520813\n",
      "\t Training loss (single batch): 0.85161292552948\n",
      "\t Training loss (single batch): 1.3617947101593018\n",
      "\t Training loss (single batch): 1.349618673324585\n",
      "\t Training loss (single batch): 1.1346919536590576\n",
      "\t Training loss (single batch): 0.6311984062194824\n",
      "\t Training loss (single batch): 0.677401602268219\n",
      "\t Training loss (single batch): 1.3087306022644043\n",
      "\t Training loss (single batch): 0.9163427352905273\n",
      "\t Training loss (single batch): 0.7916558384895325\n",
      "\t Training loss (single batch): 1.6618417501449585\n",
      "\t Training loss (single batch): 1.436736822128296\n",
      "\t Training loss (single batch): 0.9410113096237183\n",
      "\t Training loss (single batch): 1.242080569267273\n",
      "\t Training loss (single batch): 1.1525017023086548\n",
      "\t Training loss (single batch): 0.7464392781257629\n",
      "\t Training loss (single batch): 1.4840911626815796\n",
      "\t Training loss (single batch): 1.0013359785079956\n",
      "\t Training loss (single batch): 1.316356897354126\n",
      "\t Training loss (single batch): 1.2078853845596313\n",
      "\t Training loss (single batch): 1.0607378482818604\n",
      "\t Training loss (single batch): 1.231904149055481\n",
      "\t Training loss (single batch): 0.9135700464248657\n",
      "\t Training loss (single batch): 1.1516368389129639\n",
      "\t Training loss (single batch): 0.6962202787399292\n",
      "\t Training loss (single batch): 1.221511960029602\n",
      "\t Training loss (single batch): 1.0194193124771118\n",
      "\t Training loss (single batch): 0.936768651008606\n",
      "\t Training loss (single batch): 1.1917082071304321\n",
      "\t Training loss (single batch): 1.5724397897720337\n",
      "\t Training loss (single batch): 1.2601072788238525\n",
      "\t Training loss (single batch): 0.8266339898109436\n",
      "\t Training loss (single batch): 0.9556097984313965\n",
      "\t Training loss (single batch): 0.9362155199050903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0850660800933838\n",
      "\t Training loss (single batch): 1.0931885242462158\n",
      "\t Training loss (single batch): 1.3197652101516724\n",
      "\t Training loss (single batch): 0.9294891953468323\n",
      "\t Training loss (single batch): 1.565407633781433\n",
      "\t Training loss (single batch): 1.0805448293685913\n",
      "\t Training loss (single batch): 0.9119722247123718\n",
      "\t Training loss (single batch): 1.0494518280029297\n",
      "\t Training loss (single batch): 1.2055996656417847\n",
      "\t Training loss (single batch): 1.1551722288131714\n",
      "\t Training loss (single batch): 0.8885383605957031\n",
      "\t Training loss (single batch): 1.4020839929580688\n",
      "\t Training loss (single batch): 1.001387596130371\n",
      "\t Training loss (single batch): 0.8982821702957153\n",
      "\t Training loss (single batch): 1.7267128229141235\n",
      "\t Training loss (single batch): 1.3007681369781494\n",
      "\t Training loss (single batch): 0.8285430669784546\n",
      "\t Training loss (single batch): 0.8066647052764893\n",
      "\t Training loss (single batch): 1.311232328414917\n",
      "\t Training loss (single batch): 1.3207486867904663\n",
      "\t Training loss (single batch): 1.3183890581130981\n",
      "\t Training loss (single batch): 1.4620169401168823\n",
      "\t Training loss (single batch): 0.9506432414054871\n",
      "\t Training loss (single batch): 1.3137054443359375\n",
      "\t Training loss (single batch): 1.3102283477783203\n",
      "\t Training loss (single batch): 0.8516000509262085\n",
      "\t Training loss (single batch): 1.21847665309906\n",
      "\t Training loss (single batch): 1.4689617156982422\n",
      "\t Training loss (single batch): 1.4481760263442993\n",
      "\t Training loss (single batch): 1.0880988836288452\n",
      "\t Training loss (single batch): 0.9325972199440002\n",
      "\t Training loss (single batch): 1.7675913572311401\n",
      "\t Training loss (single batch): 0.9775497913360596\n",
      "\t Training loss (single batch): 1.3051477670669556\n",
      "\t Training loss (single batch): 1.3790254592895508\n",
      "\t Training loss (single batch): 1.5648049116134644\n",
      "\t Training loss (single batch): 1.5124315023422241\n",
      "\t Training loss (single batch): 1.0204612016677856\n",
      "\t Training loss (single batch): 1.0882360935211182\n",
      "\t Training loss (single batch): 1.463502049446106\n",
      "\t Training loss (single batch): 1.174818515777588\n",
      "\t Training loss (single batch): 1.1174248456954956\n",
      "\t Training loss (single batch): 1.3122406005859375\n",
      "\t Training loss (single batch): 1.5922514200210571\n",
      "\t Training loss (single batch): 1.3768373727798462\n",
      "\t Training loss (single batch): 1.658220887184143\n",
      "\t Training loss (single batch): 1.204089879989624\n",
      "\t Training loss (single batch): 1.3905361890792847\n",
      "\t Training loss (single batch): 1.000662922859192\n",
      "\t Training loss (single batch): 1.2434163093566895\n",
      "\t Training loss (single batch): 1.525146722793579\n",
      "\t Training loss (single batch): 0.9932732582092285\n",
      "\t Training loss (single batch): 1.6513774394989014\n",
      "\t Training loss (single batch): 1.260409951210022\n",
      "\t Training loss (single batch): 1.3010472059249878\n",
      "\t Training loss (single batch): 1.3885592222213745\n",
      "\t Training loss (single batch): 1.1086218357086182\n",
      "\t Training loss (single batch): 1.0160831212997437\n",
      "\t Training loss (single batch): 1.1413389444351196\n",
      "\t Training loss (single batch): 0.7054148316383362\n",
      "\t Training loss (single batch): 1.0705817937850952\n",
      "\t Training loss (single batch): 1.0195049047470093\n",
      "\t Training loss (single batch): 1.3998684883117676\n",
      "\t Training loss (single batch): 0.680232584476471\n",
      "\t Training loss (single batch): 1.1841793060302734\n",
      "\t Training loss (single batch): 1.132259488105774\n",
      "\t Training loss (single batch): 1.0651966333389282\n",
      "\t Training loss (single batch): 0.8180063962936401\n",
      "\t Training loss (single batch): 1.113922357559204\n",
      "\t Training loss (single batch): 1.2094718217849731\n",
      "\t Training loss (single batch): 1.0123252868652344\n",
      "\t Training loss (single batch): 0.6131455898284912\n",
      "\t Training loss (single batch): 0.7784179449081421\n",
      "\t Training loss (single batch): 0.920663595199585\n",
      "\t Training loss (single batch): 0.9188817739486694\n",
      "\t Training loss (single batch): 1.134424090385437\n",
      "\t Training loss (single batch): 0.9169006943702698\n",
      "\t Training loss (single batch): 1.6932642459869385\n",
      "\t Training loss (single batch): 1.1494929790496826\n",
      "\t Training loss (single batch): 1.7516542673110962\n",
      "\t Training loss (single batch): 1.1830356121063232\n",
      "\t Training loss (single batch): 1.545322299003601\n",
      "\t Training loss (single batch): 1.1848514080047607\n",
      "\t Training loss (single batch): 1.3486192226409912\n",
      "\t Training loss (single batch): 1.2806661128997803\n",
      "\t Training loss (single batch): 0.8529680371284485\n",
      "\t Training loss (single batch): 0.9539690017700195\n",
      "\t Training loss (single batch): 0.9318376779556274\n",
      "\t Training loss (single batch): 0.9222905039787292\n",
      "\t Training loss (single batch): 1.530559778213501\n",
      "\t Training loss (single batch): 1.4910366535186768\n",
      "\t Training loss (single batch): 1.1003018617630005\n",
      "\t Training loss (single batch): 1.2113052606582642\n",
      "\t Training loss (single batch): 0.9072160720825195\n",
      "\t Training loss (single batch): 1.1428780555725098\n",
      "\t Training loss (single batch): 1.0763359069824219\n",
      "\t Training loss (single batch): 0.7882171869277954\n",
      "\t Training loss (single batch): 1.0967917442321777\n",
      "\t Training loss (single batch): 1.1513041257858276\n",
      "\t Training loss (single batch): 0.7243120670318604\n",
      "\t Training loss (single batch): 1.2956408262252808\n",
      "\t Training loss (single batch): 1.207590103149414\n",
      "\t Training loss (single batch): 0.9935913681983948\n",
      "\t Training loss (single batch): 0.7704706788063049\n",
      "\t Training loss (single batch): 0.763817310333252\n",
      "\t Training loss (single batch): 1.0944117307662964\n",
      "\t Training loss (single batch): 0.9995135068893433\n",
      "\t Training loss (single batch): 0.9419713020324707\n",
      "\t Training loss (single batch): 1.3929485082626343\n",
      "\t Training loss (single batch): 1.6738566160202026\n",
      "\t Training loss (single batch): 0.9786324501037598\n",
      "\t Training loss (single batch): 1.502181887626648\n",
      "\t Training loss (single batch): 1.3087096214294434\n",
      "\t Training loss (single batch): 0.9542967081069946\n",
      "\t Training loss (single batch): 1.1071460247039795\n",
      "\t Training loss (single batch): 1.155325174331665\n",
      "\t Training loss (single batch): 1.2224866151809692\n",
      "\t Training loss (single batch): 0.8784401416778564\n",
      "\t Training loss (single batch): 0.8321784734725952\n",
      "\t Training loss (single batch): 1.0269794464111328\n",
      "\t Training loss (single batch): 1.038012981414795\n",
      "\t Training loss (single batch): 0.7841400504112244\n",
      "\t Training loss (single batch): 1.6525248289108276\n",
      "\t Training loss (single batch): 1.358006238937378\n",
      "\t Training loss (single batch): 1.0023342370986938\n",
      "\t Training loss (single batch): 0.9776009917259216\n",
      "\t Training loss (single batch): 0.8127080202102661\n",
      "\t Training loss (single batch): 1.612256646156311\n",
      "\t Training loss (single batch): 1.049898624420166\n",
      "\t Training loss (single batch): 1.0593777894973755\n",
      "\t Training loss (single batch): 1.3987727165222168\n",
      "\t Training loss (single batch): 1.086050271987915\n",
      "\t Training loss (single batch): 1.1479077339172363\n",
      "\t Training loss (single batch): 0.8698334097862244\n",
      "\t Training loss (single batch): 1.3014839887619019\n",
      "\t Training loss (single batch): 1.093961238861084\n",
      "\t Training loss (single batch): 1.4732939004898071\n",
      "\t Training loss (single batch): 1.053823709487915\n",
      "\t Training loss (single batch): 1.4638781547546387\n",
      "\t Training loss (single batch): 1.2062662839889526\n",
      "\t Training loss (single batch): 1.3078510761260986\n",
      "\t Training loss (single batch): 1.199171543121338\n",
      "\t Training loss (single batch): 1.015146255493164\n",
      "\t Training loss (single batch): 0.7940171957015991\n",
      "\t Training loss (single batch): 1.0952374935150146\n",
      "\t Training loss (single batch): 1.0481430292129517\n",
      "\t Training loss (single batch): 0.9585649967193604\n",
      "\t Training loss (single batch): 0.8923767805099487\n",
      "\t Training loss (single batch): 1.3841480016708374\n",
      "\t Training loss (single batch): 1.708821177482605\n",
      "\t Training loss (single batch): 0.7046090364456177\n",
      "\t Training loss (single batch): 1.1115071773529053\n",
      "\t Training loss (single batch): 1.0291171073913574\n",
      "\t Training loss (single batch): 1.5763472318649292\n",
      "\t Training loss (single batch): 0.9551685452461243\n",
      "\t Training loss (single batch): 1.4120808839797974\n",
      "\t Training loss (single batch): 0.9458667039871216\n",
      "\t Training loss (single batch): 1.3844436407089233\n",
      "\t Training loss (single batch): 1.1002472639083862\n",
      "\t Training loss (single batch): 0.7721909880638123\n",
      "\t Training loss (single batch): 1.0893412828445435\n",
      "\t Training loss (single batch): 1.4706984758377075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.5185753703117371\n",
      "\t Training loss (single batch): 1.4379003047943115\n",
      "\t Training loss (single batch): 1.2987769842147827\n",
      "\t Training loss (single batch): 0.9231881499290466\n",
      "\t Training loss (single batch): 1.3518832921981812\n",
      "\t Training loss (single batch): 0.9980351328849792\n",
      "\t Training loss (single batch): 0.776905357837677\n",
      "\t Training loss (single batch): 1.1141362190246582\n",
      "\t Training loss (single batch): 1.0698959827423096\n",
      "\t Training loss (single batch): 1.1091123819351196\n",
      "\t Training loss (single batch): 1.4966999292373657\n",
      "\t Training loss (single batch): 0.9140148162841797\n",
      "\t Training loss (single batch): 0.8896597623825073\n",
      "\t Training loss (single batch): 1.6410630941390991\n",
      "\t Training loss (single batch): 1.08522629737854\n",
      "\t Training loss (single batch): 1.0938314199447632\n",
      "\t Training loss (single batch): 1.3662619590759277\n",
      "\t Training loss (single batch): 0.9491459131240845\n",
      "\t Training loss (single batch): 1.5622605085372925\n",
      "\t Training loss (single batch): 0.9029560685157776\n",
      "\t Training loss (single batch): 0.8674898743629456\n",
      "\t Training loss (single batch): 1.0388178825378418\n",
      "\t Training loss (single batch): 1.873477578163147\n",
      "\t Training loss (single batch): 0.9784214496612549\n",
      "\t Training loss (single batch): 0.861429750919342\n",
      "\t Training loss (single batch): 1.1061782836914062\n",
      "\t Training loss (single batch): 1.0827685594558716\n",
      "\t Training loss (single batch): 0.7925231456756592\n",
      "\t Training loss (single batch): 1.1774746179580688\n",
      "\t Training loss (single batch): 0.883143961429596\n",
      "\t Training loss (single batch): 1.3395065069198608\n",
      "\t Training loss (single batch): 1.1059401035308838\n",
      "\t Training loss (single batch): 1.2163004875183105\n",
      "\t Training loss (single batch): 1.0830738544464111\n",
      "\t Training loss (single batch): 1.5002861022949219\n",
      "\t Training loss (single batch): 1.0458807945251465\n",
      "\t Training loss (single batch): 1.2731482982635498\n",
      "\t Training loss (single batch): 0.9956689476966858\n",
      "\t Training loss (single batch): 1.1522330045700073\n",
      "\t Training loss (single batch): 1.307273268699646\n",
      "##################################\n",
      "## EPOCH 92\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1657649278640747\n",
      "\t Training loss (single batch): 0.993202805519104\n",
      "\t Training loss (single batch): 1.6555001735687256\n",
      "\t Training loss (single batch): 1.392635464668274\n",
      "\t Training loss (single batch): 1.2123242616653442\n",
      "\t Training loss (single batch): 1.1544849872589111\n",
      "\t Training loss (single batch): 0.9918434619903564\n",
      "\t Training loss (single batch): 1.2939621210098267\n",
      "\t Training loss (single batch): 1.7628549337387085\n",
      "\t Training loss (single batch): 1.3139208555221558\n",
      "\t Training loss (single batch): 0.9480432868003845\n",
      "\t Training loss (single batch): 0.9850892424583435\n",
      "\t Training loss (single batch): 0.9605063796043396\n",
      "\t Training loss (single batch): 1.2233566045761108\n",
      "\t Training loss (single batch): 1.1107726097106934\n",
      "\t Training loss (single batch): 1.6294792890548706\n",
      "\t Training loss (single batch): 1.064560055732727\n",
      "\t Training loss (single batch): 1.5725336074829102\n",
      "\t Training loss (single batch): 1.379326581954956\n",
      "\t Training loss (single batch): 1.015991449356079\n",
      "\t Training loss (single batch): 0.9995344281196594\n",
      "\t Training loss (single batch): 1.1633771657943726\n",
      "\t Training loss (single batch): 1.0321407318115234\n",
      "\t Training loss (single batch): 0.9274094104766846\n",
      "\t Training loss (single batch): 1.3265345096588135\n",
      "\t Training loss (single batch): 1.0013353824615479\n",
      "\t Training loss (single batch): 1.2851890325546265\n",
      "\t Training loss (single batch): 1.1544690132141113\n",
      "\t Training loss (single batch): 1.653038501739502\n",
      "\t Training loss (single batch): 1.1362239122390747\n",
      "\t Training loss (single batch): 0.9439506530761719\n",
      "\t Training loss (single batch): 1.8727976083755493\n",
      "\t Training loss (single batch): 1.1805909872055054\n",
      "\t Training loss (single batch): 0.8636354804039001\n",
      "\t Training loss (single batch): 1.3444108963012695\n",
      "\t Training loss (single batch): 1.1885735988616943\n",
      "\t Training loss (single batch): 1.2324433326721191\n",
      "\t Training loss (single batch): 1.363863229751587\n",
      "\t Training loss (single batch): 1.3496394157409668\n",
      "\t Training loss (single batch): 1.0920981168746948\n",
      "\t Training loss (single batch): 1.2652943134307861\n",
      "\t Training loss (single batch): 1.0230308771133423\n",
      "\t Training loss (single batch): 1.1608654260635376\n",
      "\t Training loss (single batch): 1.0270867347717285\n",
      "\t Training loss (single batch): 1.3478792905807495\n",
      "\t Training loss (single batch): 0.7010680437088013\n",
      "\t Training loss (single batch): 1.5922276973724365\n",
      "\t Training loss (single batch): 1.5121700763702393\n",
      "\t Training loss (single batch): 1.0494775772094727\n",
      "\t Training loss (single batch): 1.3427913188934326\n",
      "\t Training loss (single batch): 1.0592455863952637\n",
      "\t Training loss (single batch): 0.989404022693634\n",
      "\t Training loss (single batch): 1.0388575792312622\n",
      "\t Training loss (single batch): 1.0524109601974487\n",
      "\t Training loss (single batch): 1.323333740234375\n",
      "\t Training loss (single batch): 1.2146490812301636\n",
      "\t Training loss (single batch): 1.2988890409469604\n",
      "\t Training loss (single batch): 1.1629798412322998\n",
      "\t Training loss (single batch): 1.2456899881362915\n",
      "\t Training loss (single batch): 1.2401623725891113\n",
      "\t Training loss (single batch): 2.0055992603302\n",
      "\t Training loss (single batch): 0.8315587639808655\n",
      "\t Training loss (single batch): 1.3285645246505737\n",
      "\t Training loss (single batch): 1.1031506061553955\n",
      "\t Training loss (single batch): 1.1723765134811401\n",
      "\t Training loss (single batch): 1.1446001529693604\n",
      "\t Training loss (single batch): 1.2891744375228882\n",
      "\t Training loss (single batch): 1.044966697692871\n",
      "\t Training loss (single batch): 1.1706008911132812\n",
      "\t Training loss (single batch): 1.3781412839889526\n",
      "\t Training loss (single batch): 1.2050310373306274\n",
      "\t Training loss (single batch): 1.064335584640503\n",
      "\t Training loss (single batch): 0.8567526340484619\n",
      "\t Training loss (single batch): 1.3820745944976807\n",
      "\t Training loss (single batch): 0.9594219326972961\n",
      "\t Training loss (single batch): 0.7991711497306824\n",
      "\t Training loss (single batch): 0.7683862447738647\n",
      "\t Training loss (single batch): 0.7941799163818359\n",
      "\t Training loss (single batch): 1.0219929218292236\n",
      "\t Training loss (single batch): 1.1996334791183472\n",
      "\t Training loss (single batch): 0.9423166513442993\n",
      "\t Training loss (single batch): 1.4651082754135132\n",
      "\t Training loss (single batch): 1.3023865222930908\n",
      "\t Training loss (single batch): 0.9281952381134033\n",
      "\t Training loss (single batch): 0.8661991357803345\n",
      "\t Training loss (single batch): 1.0172247886657715\n",
      "\t Training loss (single batch): 1.0201274156570435\n",
      "\t Training loss (single batch): 0.88226318359375\n",
      "\t Training loss (single batch): 1.3679944276809692\n",
      "\t Training loss (single batch): 0.8766401410102844\n",
      "\t Training loss (single batch): 1.517082929611206\n",
      "\t Training loss (single batch): 0.9540587663650513\n",
      "\t Training loss (single batch): 1.1832715272903442\n",
      "\t Training loss (single batch): 1.2376387119293213\n",
      "\t Training loss (single batch): 1.2758737802505493\n",
      "\t Training loss (single batch): 1.3830633163452148\n",
      "\t Training loss (single batch): 0.7745944857597351\n",
      "\t Training loss (single batch): 1.6323224306106567\n",
      "\t Training loss (single batch): 1.2898731231689453\n",
      "\t Training loss (single batch): 0.8705945611000061\n",
      "\t Training loss (single batch): 1.1979446411132812\n",
      "\t Training loss (single batch): 1.2082725763320923\n",
      "\t Training loss (single batch): 0.7968946099281311\n",
      "\t Training loss (single batch): 1.5520693063735962\n",
      "\t Training loss (single batch): 0.917542576789856\n",
      "\t Training loss (single batch): 1.582683801651001\n",
      "\t Training loss (single batch): 1.3863369226455688\n",
      "\t Training loss (single batch): 1.0307135581970215\n",
      "\t Training loss (single batch): 1.468924880027771\n",
      "\t Training loss (single batch): 1.7320343255996704\n",
      "\t Training loss (single batch): 1.1781792640686035\n",
      "\t Training loss (single batch): 0.8927723169326782\n",
      "\t Training loss (single batch): 1.0659781694412231\n",
      "\t Training loss (single batch): 1.5652985572814941\n",
      "\t Training loss (single batch): 0.9928869605064392\n",
      "\t Training loss (single batch): 1.1388834714889526\n",
      "\t Training loss (single batch): 1.2250046730041504\n",
      "\t Training loss (single batch): 1.2685641050338745\n",
      "\t Training loss (single batch): 1.3666657209396362\n",
      "\t Training loss (single batch): 1.7623194456100464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.442341685295105\n",
      "\t Training loss (single batch): 1.129091739654541\n",
      "\t Training loss (single batch): 1.2833338975906372\n",
      "\t Training loss (single batch): 0.8010056614875793\n",
      "\t Training loss (single batch): 1.4696056842803955\n",
      "\t Training loss (single batch): 1.4042028188705444\n",
      "\t Training loss (single batch): 1.2953308820724487\n",
      "\t Training loss (single batch): 0.9085913896560669\n",
      "\t Training loss (single batch): 1.345327615737915\n",
      "\t Training loss (single batch): 1.8954954147338867\n",
      "\t Training loss (single batch): 1.322733759880066\n",
      "\t Training loss (single batch): 1.3888742923736572\n",
      "\t Training loss (single batch): 1.1255594491958618\n",
      "\t Training loss (single batch): 1.7894635200500488\n",
      "\t Training loss (single batch): 0.9835927486419678\n",
      "\t Training loss (single batch): 1.1135823726654053\n",
      "\t Training loss (single batch): 0.8596024513244629\n",
      "\t Training loss (single batch): 0.9703556895256042\n",
      "\t Training loss (single batch): 0.7522065043449402\n",
      "\t Training loss (single batch): 1.155562400817871\n",
      "\t Training loss (single batch): 1.0833408832550049\n",
      "\t Training loss (single batch): 1.0947626829147339\n",
      "\t Training loss (single batch): 0.807051420211792\n",
      "\t Training loss (single batch): 0.9385299682617188\n",
      "\t Training loss (single batch): 0.9423683881759644\n",
      "\t Training loss (single batch): 0.894111692905426\n",
      "\t Training loss (single batch): 1.8098162412643433\n",
      "\t Training loss (single batch): 1.4092345237731934\n",
      "\t Training loss (single batch): 1.7334108352661133\n",
      "\t Training loss (single batch): 1.34656822681427\n",
      "\t Training loss (single batch): 0.907683253288269\n",
      "\t Training loss (single batch): 1.5360850095748901\n",
      "\t Training loss (single batch): 1.504707932472229\n",
      "\t Training loss (single batch): 1.3694617748260498\n",
      "\t Training loss (single batch): 1.1339789628982544\n",
      "\t Training loss (single batch): 1.0911990404129028\n",
      "\t Training loss (single batch): 1.339534044265747\n",
      "\t Training loss (single batch): 0.7588623762130737\n",
      "\t Training loss (single batch): 1.5641891956329346\n",
      "\t Training loss (single batch): 0.8446716070175171\n",
      "\t Training loss (single batch): 1.6242852210998535\n",
      "\t Training loss (single batch): 1.3206429481506348\n",
      "\t Training loss (single batch): 1.246082067489624\n",
      "\t Training loss (single batch): 1.293808102607727\n",
      "\t Training loss (single batch): 1.3542683124542236\n",
      "\t Training loss (single batch): 1.5871812105178833\n",
      "\t Training loss (single batch): 1.2912379503250122\n",
      "\t Training loss (single batch): 1.619678258895874\n",
      "\t Training loss (single batch): 1.0825481414794922\n",
      "\t Training loss (single batch): 1.2502018213272095\n",
      "\t Training loss (single batch): 1.1897046566009521\n",
      "\t Training loss (single batch): 1.8206994533538818\n",
      "\t Training loss (single batch): 1.0716822147369385\n",
      "\t Training loss (single batch): 1.3188742399215698\n",
      "\t Training loss (single batch): 1.2213928699493408\n",
      "\t Training loss (single batch): 1.1665613651275635\n",
      "\t Training loss (single batch): 1.2172409296035767\n",
      "\t Training loss (single batch): 0.9899212121963501\n",
      "\t Training loss (single batch): 0.9024171829223633\n",
      "\t Training loss (single batch): 1.3845051527023315\n",
      "\t Training loss (single batch): 0.9529303312301636\n",
      "\t Training loss (single batch): 1.0060232877731323\n",
      "\t Training loss (single batch): 1.0295697450637817\n",
      "\t Training loss (single batch): 1.424464225769043\n",
      "\t Training loss (single batch): 1.4485857486724854\n",
      "\t Training loss (single batch): 1.3615528345108032\n",
      "\t Training loss (single batch): 0.773536205291748\n",
      "\t Training loss (single batch): 1.1614609956741333\n",
      "\t Training loss (single batch): 0.9385352730751038\n",
      "\t Training loss (single batch): 1.4321486949920654\n",
      "\t Training loss (single batch): 1.0371575355529785\n",
      "\t Training loss (single batch): 1.1333119869232178\n",
      "\t Training loss (single batch): 1.399871826171875\n",
      "\t Training loss (single batch): 1.2841203212738037\n",
      "\t Training loss (single batch): 1.3203176259994507\n",
      "\t Training loss (single batch): 1.0277389287948608\n",
      "\t Training loss (single batch): 1.0495797395706177\n",
      "\t Training loss (single batch): 1.8673919439315796\n",
      "\t Training loss (single batch): 1.2325276136398315\n",
      "\t Training loss (single batch): 0.9886472821235657\n",
      "\t Training loss (single batch): 1.0019879341125488\n",
      "\t Training loss (single batch): 1.0983741283416748\n",
      "\t Training loss (single batch): 0.994975209236145\n",
      "\t Training loss (single batch): 1.1255789995193481\n",
      "\t Training loss (single batch): 1.1626760959625244\n",
      "\t Training loss (single batch): 1.8008476495742798\n",
      "\t Training loss (single batch): 1.4840956926345825\n",
      "\t Training loss (single batch): 1.2894171476364136\n",
      "\t Training loss (single batch): 1.0058526992797852\n",
      "\t Training loss (single batch): 1.1474668979644775\n",
      "\t Training loss (single batch): 1.0473759174346924\n",
      "\t Training loss (single batch): 1.3698465824127197\n",
      "\t Training loss (single batch): 1.2594016790390015\n",
      "\t Training loss (single batch): 1.1299610137939453\n",
      "\t Training loss (single batch): 1.3136789798736572\n",
      "\t Training loss (single batch): 1.3786250352859497\n",
      "\t Training loss (single batch): 1.154813528060913\n",
      "\t Training loss (single batch): 1.141895055770874\n",
      "\t Training loss (single batch): 0.8982095122337341\n",
      "\t Training loss (single batch): 1.4342788457870483\n",
      "\t Training loss (single batch): 0.8027186393737793\n",
      "\t Training loss (single batch): 1.4667879343032837\n",
      "\t Training loss (single batch): 1.4894291162490845\n",
      "\t Training loss (single batch): 0.7793262004852295\n",
      "\t Training loss (single batch): 1.1904484033584595\n",
      "\t Training loss (single batch): 0.9943522810935974\n",
      "\t Training loss (single batch): 1.6413719654083252\n",
      "\t Training loss (single batch): 1.1849370002746582\n",
      "\t Training loss (single batch): 1.576657772064209\n",
      "\t Training loss (single batch): 0.934894859790802\n",
      "\t Training loss (single batch): 0.9361851215362549\n",
      "\t Training loss (single batch): 1.186145544052124\n",
      "\t Training loss (single batch): 1.2145063877105713\n",
      "\t Training loss (single batch): 1.107895851135254\n",
      "\t Training loss (single batch): 0.9735171794891357\n",
      "\t Training loss (single batch): 0.92899090051651\n",
      "\t Training loss (single batch): 1.5948395729064941\n",
      "\t Training loss (single batch): 1.0252361297607422\n",
      "\t Training loss (single batch): 1.519433617591858\n",
      "\t Training loss (single batch): 1.6490627527236938\n",
      "\t Training loss (single batch): 0.9781105518341064\n",
      "\t Training loss (single batch): 1.5530104637145996\n",
      "\t Training loss (single batch): 0.8194416165351868\n",
      "\t Training loss (single batch): 1.0970393419265747\n",
      "\t Training loss (single batch): 1.352973461151123\n",
      "\t Training loss (single batch): 1.402052879333496\n",
      "\t Training loss (single batch): 0.9863073229789734\n",
      "\t Training loss (single batch): 1.1969047784805298\n",
      "\t Training loss (single batch): 1.2912873029708862\n",
      "\t Training loss (single batch): 1.3976961374282837\n",
      "\t Training loss (single batch): 1.4862782955169678\n",
      "\t Training loss (single batch): 1.0823794603347778\n",
      "\t Training loss (single batch): 1.1145427227020264\n",
      "\t Training loss (single batch): 1.3508943319320679\n",
      "\t Training loss (single batch): 0.88001549243927\n",
      "\t Training loss (single batch): 0.9017298221588135\n",
      "\t Training loss (single batch): 0.9425203800201416\n",
      "\t Training loss (single batch): 0.8950392603874207\n",
      "\t Training loss (single batch): 1.4931174516677856\n",
      "\t Training loss (single batch): 0.9014458656311035\n",
      "\t Training loss (single batch): 1.234343409538269\n",
      "\t Training loss (single batch): 0.8190781474113464\n",
      "\t Training loss (single batch): 1.365494966506958\n",
      "\t Training loss (single batch): 1.3427002429962158\n",
      "\t Training loss (single batch): 1.3685948848724365\n",
      "\t Training loss (single batch): 1.452264428138733\n",
      "\t Training loss (single batch): 1.2772258520126343\n",
      "\t Training loss (single batch): 1.0845890045166016\n",
      "\t Training loss (single batch): 0.7923732399940491\n",
      "\t Training loss (single batch): 1.376720905303955\n",
      "\t Training loss (single batch): 0.8442285656929016\n",
      "\t Training loss (single batch): 1.12276291847229\n",
      "\t Training loss (single batch): 1.4583171606063843\n",
      "\t Training loss (single batch): 1.616979718208313\n",
      "\t Training loss (single batch): 0.9523099064826965\n",
      "\t Training loss (single batch): 1.1723071336746216\n",
      "\t Training loss (single batch): 1.2586255073547363\n",
      "\t Training loss (single batch): 1.577264428138733\n",
      "\t Training loss (single batch): 0.8975709676742554\n",
      "\t Training loss (single batch): 1.487169861793518\n",
      "\t Training loss (single batch): 1.3242466449737549\n",
      "\t Training loss (single batch): 0.7029383182525635\n",
      "\t Training loss (single batch): 1.0961583852767944\n",
      "\t Training loss (single batch): 1.160390019416809\n",
      "\t Training loss (single batch): 1.0759297609329224\n",
      "\t Training loss (single batch): 1.453057885169983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0635368824005127\n",
      "\t Training loss (single batch): 1.3894315958023071\n",
      "\t Training loss (single batch): 1.1867808103561401\n",
      "\t Training loss (single batch): 1.288845419883728\n",
      "\t Training loss (single batch): 1.3520598411560059\n",
      "\t Training loss (single batch): 1.1763049364089966\n",
      "\t Training loss (single batch): 1.1448732614517212\n",
      "\t Training loss (single batch): 1.382962942123413\n",
      "\t Training loss (single batch): 1.642802119255066\n",
      "\t Training loss (single batch): 1.1695740222930908\n",
      "\t Training loss (single batch): 1.1925328969955444\n",
      "\t Training loss (single batch): 0.7833573222160339\n",
      "\t Training loss (single batch): 1.759321928024292\n",
      "\t Training loss (single batch): 1.5354547500610352\n",
      "\t Training loss (single batch): 0.979071319103241\n",
      "\t Training loss (single batch): 1.1120418310165405\n",
      "\t Training loss (single batch): 0.9825871586799622\n",
      "\t Training loss (single batch): 1.0183701515197754\n",
      "\t Training loss (single batch): 1.470626711845398\n",
      "\t Training loss (single batch): 0.8982774019241333\n",
      "\t Training loss (single batch): 1.3679301738739014\n",
      "\t Training loss (single batch): 1.8042036294937134\n",
      "\t Training loss (single batch): 0.9883884191513062\n",
      "\t Training loss (single batch): 1.2895736694335938\n",
      "\t Training loss (single batch): 1.3428453207015991\n",
      "\t Training loss (single batch): 1.2309225797653198\n",
      "\t Training loss (single batch): 1.3854591846466064\n",
      "\t Training loss (single batch): 1.0920531749725342\n",
      "\t Training loss (single batch): 1.294473648071289\n",
      "\t Training loss (single batch): 1.264798879623413\n",
      "\t Training loss (single batch): 1.0218687057495117\n",
      "##################################\n",
      "## EPOCH 93\n",
      "##################################\n",
      "\t Training loss (single batch): 1.135727047920227\n",
      "\t Training loss (single batch): 1.4371031522750854\n",
      "\t Training loss (single batch): 1.061378836631775\n",
      "\t Training loss (single batch): 1.0763038396835327\n",
      "\t Training loss (single batch): 1.5929992198944092\n",
      "\t Training loss (single batch): 1.3176143169403076\n",
      "\t Training loss (single batch): 1.080511450767517\n",
      "\t Training loss (single batch): 1.6976438760757446\n",
      "\t Training loss (single batch): 1.3386918306350708\n",
      "\t Training loss (single batch): 1.6037718057632446\n",
      "\t Training loss (single batch): 1.1562150716781616\n",
      "\t Training loss (single batch): 1.0866382122039795\n",
      "\t Training loss (single batch): 0.9549092054367065\n",
      "\t Training loss (single batch): 0.8459673523902893\n",
      "\t Training loss (single batch): 1.1170190572738647\n",
      "\t Training loss (single batch): 0.9275553822517395\n",
      "\t Training loss (single batch): 1.2307251691818237\n",
      "\t Training loss (single batch): 1.0357258319854736\n",
      "\t Training loss (single batch): 1.0820820331573486\n",
      "\t Training loss (single batch): 1.2875168323516846\n",
      "\t Training loss (single batch): 0.8801812529563904\n",
      "\t Training loss (single batch): 1.1319719552993774\n",
      "\t Training loss (single batch): 0.9427217841148376\n",
      "\t Training loss (single batch): 1.1864886283874512\n",
      "\t Training loss (single batch): 1.2673962116241455\n",
      "\t Training loss (single batch): 0.9621040225028992\n",
      "\t Training loss (single batch): 1.009285569190979\n",
      "\t Training loss (single batch): 0.5903744101524353\n",
      "\t Training loss (single batch): 0.9890275001525879\n",
      "\t Training loss (single batch): 1.6509379148483276\n",
      "\t Training loss (single batch): 1.0289920568466187\n",
      "\t Training loss (single batch): 1.3830348253250122\n",
      "\t Training loss (single batch): 0.8594918251037598\n",
      "\t Training loss (single batch): 1.1277856826782227\n",
      "\t Training loss (single batch): 1.051585078239441\n",
      "\t Training loss (single batch): 1.146451473236084\n",
      "\t Training loss (single batch): 0.9838346838951111\n",
      "\t Training loss (single batch): 0.8210567235946655\n",
      "\t Training loss (single batch): 0.8351303339004517\n",
      "\t Training loss (single batch): 1.1208937168121338\n",
      "\t Training loss (single batch): 1.567945957183838\n",
      "\t Training loss (single batch): 1.7925302982330322\n",
      "\t Training loss (single batch): 0.9691922664642334\n",
      "\t Training loss (single batch): 1.0025882720947266\n",
      "\t Training loss (single batch): 1.487183690071106\n",
      "\t Training loss (single batch): 1.2804396152496338\n",
      "\t Training loss (single batch): 1.0633659362792969\n",
      "\t Training loss (single batch): 1.0729436874389648\n",
      "\t Training loss (single batch): 1.3257756233215332\n",
      "\t Training loss (single batch): 1.388091802597046\n",
      "\t Training loss (single batch): 1.2573994398117065\n",
      "\t Training loss (single batch): 1.1181422472000122\n",
      "\t Training loss (single batch): 1.0018670558929443\n",
      "\t Training loss (single batch): 0.5728253126144409\n",
      "\t Training loss (single batch): 1.376067876815796\n",
      "\t Training loss (single batch): 1.5761252641677856\n",
      "\t Training loss (single batch): 1.475776195526123\n",
      "\t Training loss (single batch): 1.2147784233093262\n",
      "\t Training loss (single batch): 0.7210829257965088\n",
      "\t Training loss (single batch): 1.3733161687850952\n",
      "\t Training loss (single batch): 1.2147870063781738\n",
      "\t Training loss (single batch): 0.8827152252197266\n",
      "\t Training loss (single batch): 0.9765941500663757\n",
      "\t Training loss (single batch): 1.0827885866165161\n",
      "\t Training loss (single batch): 1.0807982683181763\n",
      "\t Training loss (single batch): 1.2373900413513184\n",
      "\t Training loss (single batch): 1.1446393728256226\n",
      "\t Training loss (single batch): 1.0732027292251587\n",
      "\t Training loss (single batch): 1.259952425956726\n",
      "\t Training loss (single batch): 1.2297214269638062\n",
      "\t Training loss (single batch): 1.3330943584442139\n",
      "\t Training loss (single batch): 1.1006290912628174\n",
      "\t Training loss (single batch): 1.0318591594696045\n",
      "\t Training loss (single batch): 1.1662521362304688\n",
      "\t Training loss (single batch): 1.027225375175476\n",
      "\t Training loss (single batch): 0.7849687337875366\n",
      "\t Training loss (single batch): 1.3583714962005615\n",
      "\t Training loss (single batch): 1.2487393617630005\n",
      "\t Training loss (single batch): 0.7575182318687439\n",
      "\t Training loss (single batch): 1.0015184879302979\n",
      "\t Training loss (single batch): 0.9469422698020935\n",
      "\t Training loss (single batch): 1.1504265069961548\n",
      "\t Training loss (single batch): 0.958482027053833\n",
      "\t Training loss (single batch): 0.7863593101501465\n",
      "\t Training loss (single batch): 1.1672024726867676\n",
      "\t Training loss (single batch): 1.433307409286499\n",
      "\t Training loss (single batch): 1.1316149234771729\n",
      "\t Training loss (single batch): 1.1378072500228882\n",
      "\t Training loss (single batch): 0.9514458775520325\n",
      "\t Training loss (single batch): 0.8352274298667908\n",
      "\t Training loss (single batch): 1.1318384408950806\n",
      "\t Training loss (single batch): 1.257820725440979\n",
      "\t Training loss (single batch): 1.5241528749465942\n",
      "\t Training loss (single batch): 1.2736034393310547\n",
      "\t Training loss (single batch): 0.9890362024307251\n",
      "\t Training loss (single batch): 1.2874585390090942\n",
      "\t Training loss (single batch): 0.9601855278015137\n",
      "\t Training loss (single batch): 1.2166149616241455\n",
      "\t Training loss (single batch): 1.0454243421554565\n",
      "\t Training loss (single batch): 0.8813667297363281\n",
      "\t Training loss (single batch): 1.4762986898422241\n",
      "\t Training loss (single batch): 1.7750310897827148\n",
      "\t Training loss (single batch): 1.3571125268936157\n",
      "\t Training loss (single batch): 1.0233628749847412\n",
      "\t Training loss (single batch): 1.1480152606964111\n",
      "\t Training loss (single batch): 1.3918884992599487\n",
      "\t Training loss (single batch): 1.4708443880081177\n",
      "\t Training loss (single batch): 0.8160849213600159\n",
      "\t Training loss (single batch): 1.3351383209228516\n",
      "\t Training loss (single batch): 1.2340264320373535\n",
      "\t Training loss (single batch): 0.6060807704925537\n",
      "\t Training loss (single batch): 1.293440818786621\n",
      "\t Training loss (single batch): 1.537390947341919\n",
      "\t Training loss (single batch): 1.1091101169586182\n",
      "\t Training loss (single batch): 1.1123545169830322\n",
      "\t Training loss (single batch): 1.0033977031707764\n",
      "\t Training loss (single batch): 1.1782269477844238\n",
      "\t Training loss (single batch): 1.3253562450408936\n",
      "\t Training loss (single batch): 1.348433494567871\n",
      "\t Training loss (single batch): 1.0565178394317627\n",
      "\t Training loss (single batch): 1.144675612449646\n",
      "\t Training loss (single batch): 1.1347675323486328\n",
      "\t Training loss (single batch): 1.4202680587768555\n",
      "\t Training loss (single batch): 0.8457286953926086\n",
      "\t Training loss (single batch): 1.391872763633728\n",
      "\t Training loss (single batch): 0.9828012585639954\n",
      "\t Training loss (single batch): 1.0690500736236572\n",
      "\t Training loss (single batch): 1.6963468790054321\n",
      "\t Training loss (single batch): 1.0609006881713867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9886720776557922\n",
      "\t Training loss (single batch): 1.3346084356307983\n",
      "\t Training loss (single batch): 1.1086094379425049\n",
      "\t Training loss (single batch): 1.220403790473938\n",
      "\t Training loss (single batch): 1.0341477394104004\n",
      "\t Training loss (single batch): 1.3057843446731567\n",
      "\t Training loss (single batch): 1.2221254110336304\n",
      "\t Training loss (single batch): 1.411163568496704\n",
      "\t Training loss (single batch): 1.8868720531463623\n",
      "\t Training loss (single batch): 1.030909538269043\n",
      "\t Training loss (single batch): 0.8335388898849487\n",
      "\t Training loss (single batch): 1.043562889099121\n",
      "\t Training loss (single batch): 1.3400896787643433\n",
      "\t Training loss (single batch): 1.180516004562378\n",
      "\t Training loss (single batch): 1.3779566287994385\n",
      "\t Training loss (single batch): 1.1943243741989136\n",
      "\t Training loss (single batch): 1.2554359436035156\n",
      "\t Training loss (single batch): 0.9629605412483215\n",
      "\t Training loss (single batch): 1.2503447532653809\n",
      "\t Training loss (single batch): 1.7460664510726929\n",
      "\t Training loss (single batch): 0.8957183361053467\n",
      "\t Training loss (single batch): 1.3216822147369385\n",
      "\t Training loss (single batch): 0.897834300994873\n",
      "\t Training loss (single batch): 0.8701313138008118\n",
      "\t Training loss (single batch): 1.0348091125488281\n",
      "\t Training loss (single batch): 1.0089439153671265\n",
      "\t Training loss (single batch): 1.0043071508407593\n",
      "\t Training loss (single batch): 1.140697956085205\n",
      "\t Training loss (single batch): 0.7544419765472412\n",
      "\t Training loss (single batch): 0.7224066853523254\n",
      "\t Training loss (single batch): 1.1735390424728394\n",
      "\t Training loss (single batch): 1.0207997560501099\n",
      "\t Training loss (single batch): 1.3594533205032349\n",
      "\t Training loss (single batch): 0.840003252029419\n",
      "\t Training loss (single batch): 0.9697622656822205\n",
      "\t Training loss (single batch): 1.1486138105392456\n",
      "\t Training loss (single batch): 1.37449312210083\n",
      "\t Training loss (single batch): 1.695183277130127\n",
      "\t Training loss (single batch): 1.3232979774475098\n",
      "\t Training loss (single batch): 1.8382656574249268\n",
      "\t Training loss (single batch): 0.8989047408103943\n",
      "\t Training loss (single batch): 0.9089430570602417\n",
      "\t Training loss (single batch): 1.3254690170288086\n",
      "\t Training loss (single batch): 1.201828122138977\n",
      "\t Training loss (single batch): 1.6435405015945435\n",
      "\t Training loss (single batch): 1.024878740310669\n",
      "\t Training loss (single batch): 1.0599972009658813\n",
      "\t Training loss (single batch): 0.9209703803062439\n",
      "\t Training loss (single batch): 1.3080981969833374\n",
      "\t Training loss (single batch): 1.044859528541565\n",
      "\t Training loss (single batch): 1.40623939037323\n",
      "\t Training loss (single batch): 0.9524984359741211\n",
      "\t Training loss (single batch): 1.315996527671814\n",
      "\t Training loss (single batch): 1.353407859802246\n",
      "\t Training loss (single batch): 1.0927664041519165\n",
      "\t Training loss (single batch): 1.6093336343765259\n",
      "\t Training loss (single batch): 1.0199387073516846\n",
      "\t Training loss (single batch): 1.0608083009719849\n",
      "\t Training loss (single batch): 1.063562273979187\n",
      "\t Training loss (single batch): 1.2184414863586426\n",
      "\t Training loss (single batch): 1.1839830875396729\n",
      "\t Training loss (single batch): 1.2388921976089478\n",
      "\t Training loss (single batch): 1.0882550477981567\n",
      "\t Training loss (single batch): 1.291298747062683\n",
      "\t Training loss (single batch): 1.0540469884872437\n",
      "\t Training loss (single batch): 1.2120357751846313\n",
      "\t Training loss (single batch): 1.0303595066070557\n",
      "\t Training loss (single batch): 1.1642786264419556\n",
      "\t Training loss (single batch): 0.9064872860908508\n",
      "\t Training loss (single batch): 0.9459885954856873\n",
      "\t Training loss (single batch): 1.2279149293899536\n",
      "\t Training loss (single batch): 1.3637950420379639\n",
      "\t Training loss (single batch): 1.090030550956726\n",
      "\t Training loss (single batch): 1.1263227462768555\n",
      "\t Training loss (single batch): 1.6003162860870361\n",
      "\t Training loss (single batch): 1.072252631187439\n",
      "\t Training loss (single batch): 0.9598925709724426\n",
      "\t Training loss (single batch): 1.550412893295288\n",
      "\t Training loss (single batch): 0.8772279024124146\n",
      "\t Training loss (single batch): 0.9274281859397888\n",
      "\t Training loss (single batch): 1.1987210512161255\n",
      "\t Training loss (single batch): 1.6332358121871948\n",
      "\t Training loss (single batch): 1.7706612348556519\n",
      "\t Training loss (single batch): 1.3195239305496216\n",
      "\t Training loss (single batch): 1.0977157354354858\n",
      "\t Training loss (single batch): 1.3850281238555908\n",
      "\t Training loss (single batch): 1.731329321861267\n",
      "\t Training loss (single batch): 1.1290208101272583\n",
      "\t Training loss (single batch): 1.0289838314056396\n",
      "\t Training loss (single batch): 0.899445116519928\n",
      "\t Training loss (single batch): 1.3746176958084106\n",
      "\t Training loss (single batch): 1.2995402812957764\n",
      "\t Training loss (single batch): 1.2694463729858398\n",
      "\t Training loss (single batch): 0.9630471467971802\n",
      "\t Training loss (single batch): 1.5926254987716675\n",
      "\t Training loss (single batch): 0.9015100002288818\n",
      "\t Training loss (single batch): 1.1358058452606201\n",
      "\t Training loss (single batch): 1.4494712352752686\n",
      "\t Training loss (single batch): 1.50068998336792\n",
      "\t Training loss (single batch): 1.0263675451278687\n",
      "\t Training loss (single batch): 1.0111943483352661\n",
      "\t Training loss (single batch): 1.2566595077514648\n",
      "\t Training loss (single batch): 1.0628055334091187\n",
      "\t Training loss (single batch): 0.7495147585868835\n",
      "\t Training loss (single batch): 1.361564040184021\n",
      "\t Training loss (single batch): 1.2263325452804565\n",
      "\t Training loss (single batch): 1.082948923110962\n",
      "\t Training loss (single batch): 1.2038156986236572\n",
      "\t Training loss (single batch): 0.9218844771385193\n",
      "\t Training loss (single batch): 0.9955828785896301\n",
      "\t Training loss (single batch): 1.0082929134368896\n",
      "\t Training loss (single batch): 1.3857789039611816\n",
      "\t Training loss (single batch): 1.0984294414520264\n",
      "\t Training loss (single batch): 1.3746328353881836\n",
      "\t Training loss (single batch): 1.4178816080093384\n",
      "\t Training loss (single batch): 1.227810025215149\n",
      "\t Training loss (single batch): 0.9224628806114197\n",
      "\t Training loss (single batch): 1.0776829719543457\n",
      "\t Training loss (single batch): 1.2663239240646362\n",
      "\t Training loss (single batch): 0.989538311958313\n",
      "\t Training loss (single batch): 1.505241870880127\n",
      "\t Training loss (single batch): 0.9990184307098389\n",
      "\t Training loss (single batch): 1.4312139749526978\n",
      "\t Training loss (single batch): 1.3419573307037354\n",
      "\t Training loss (single batch): 1.544866681098938\n",
      "\t Training loss (single batch): 0.9754337668418884\n",
      "\t Training loss (single batch): 1.3786970376968384\n",
      "\t Training loss (single batch): 1.3420981168746948\n",
      "\t Training loss (single batch): 1.6573858261108398\n",
      "\t Training loss (single batch): 1.6228379011154175\n",
      "\t Training loss (single batch): 1.0708366632461548\n",
      "\t Training loss (single batch): 0.9240745306015015\n",
      "\t Training loss (single batch): 1.2024985551834106\n",
      "\t Training loss (single batch): 0.9261639714241028\n",
      "\t Training loss (single batch): 1.0382941961288452\n",
      "\t Training loss (single batch): 1.1148408651351929\n",
      "\t Training loss (single batch): 0.7585879564285278\n",
      "\t Training loss (single batch): 1.6628262996673584\n",
      "\t Training loss (single batch): 0.9720507860183716\n",
      "\t Training loss (single batch): 1.329482078552246\n",
      "\t Training loss (single batch): 0.8979454636573792\n",
      "\t Training loss (single batch): 1.4572381973266602\n",
      "\t Training loss (single batch): 0.8710789680480957\n",
      "\t Training loss (single batch): 1.1741609573364258\n",
      "\t Training loss (single batch): 1.067587971687317\n",
      "\t Training loss (single batch): 1.423073649406433\n",
      "\t Training loss (single batch): 1.0213472843170166\n",
      "\t Training loss (single batch): 0.9424259662628174\n",
      "\t Training loss (single batch): 1.4599922895431519\n",
      "\t Training loss (single batch): 0.8598485589027405\n",
      "\t Training loss (single batch): 0.8269066214561462\n",
      "\t Training loss (single batch): 1.2812772989273071\n",
      "\t Training loss (single batch): 1.2930803298950195\n",
      "\t Training loss (single batch): 0.7910470366477966\n",
      "\t Training loss (single batch): 1.5000637769699097\n",
      "\t Training loss (single batch): 1.5349199771881104\n",
      "\t Training loss (single batch): 1.4017783403396606\n",
      "\t Training loss (single batch): 0.7374267578125\n",
      "\t Training loss (single batch): 0.986605703830719\n",
      "\t Training loss (single batch): 0.9371129870414734\n",
      "\t Training loss (single batch): 1.319720983505249\n",
      "\t Training loss (single batch): 0.8895983099937439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3810038566589355\n",
      "\t Training loss (single batch): 0.9566414952278137\n",
      "\t Training loss (single batch): 0.9211632013320923\n",
      "\t Training loss (single batch): 1.2056881189346313\n",
      "\t Training loss (single batch): 0.931991457939148\n",
      "\t Training loss (single batch): 1.326352834701538\n",
      "\t Training loss (single batch): 0.8747473955154419\n",
      "\t Training loss (single batch): 1.3347084522247314\n",
      "\t Training loss (single batch): 0.6822368502616882\n",
      "\t Training loss (single batch): 1.083726406097412\n",
      "\t Training loss (single batch): 1.4764710664749146\n",
      "\t Training loss (single batch): 1.0333001613616943\n",
      "\t Training loss (single batch): 1.943091630935669\n",
      "\t Training loss (single batch): 0.934090793132782\n",
      "\t Training loss (single batch): 1.031548023223877\n",
      "\t Training loss (single batch): 1.126711130142212\n",
      "\t Training loss (single batch): 0.821113646030426\n",
      "\t Training loss (single batch): 0.9548752903938293\n",
      "\t Training loss (single batch): 1.3215830326080322\n",
      "\t Training loss (single batch): 0.8562692403793335\n",
      "\t Training loss (single batch): 1.2675282955169678\n",
      "\t Training loss (single batch): 1.1100046634674072\n",
      "\t Training loss (single batch): 1.2104837894439697\n",
      "\t Training loss (single batch): 0.864520788192749\n",
      "\t Training loss (single batch): 1.3453469276428223\n",
      "\t Training loss (single batch): 3.6452994346618652\n",
      "##################################\n",
      "## EPOCH 94\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1266183853149414\n",
      "\t Training loss (single batch): 0.7768486738204956\n",
      "\t Training loss (single batch): 0.9599680304527283\n",
      "\t Training loss (single batch): 1.2914232015609741\n",
      "\t Training loss (single batch): 1.744466781616211\n",
      "\t Training loss (single batch): 1.445358395576477\n",
      "\t Training loss (single batch): 1.3535339832305908\n",
      "\t Training loss (single batch): 1.0302839279174805\n",
      "\t Training loss (single batch): 1.2339075803756714\n",
      "\t Training loss (single batch): 0.9548173546791077\n",
      "\t Training loss (single batch): 1.4939792156219482\n",
      "\t Training loss (single batch): 0.995231568813324\n",
      "\t Training loss (single batch): 1.4599523544311523\n",
      "\t Training loss (single batch): 0.982028603553772\n",
      "\t Training loss (single batch): 1.7538971900939941\n",
      "\t Training loss (single batch): 1.431907296180725\n",
      "\t Training loss (single batch): 1.2548573017120361\n",
      "\t Training loss (single batch): 1.3458105325698853\n",
      "\t Training loss (single batch): 0.9621156454086304\n",
      "\t Training loss (single batch): 0.7690195441246033\n",
      "\t Training loss (single batch): 1.067021369934082\n",
      "\t Training loss (single batch): 1.481616735458374\n",
      "\t Training loss (single batch): 1.238734245300293\n",
      "\t Training loss (single batch): 0.902645468711853\n",
      "\t Training loss (single batch): 1.0807777643203735\n",
      "\t Training loss (single batch): 1.3625097274780273\n",
      "\t Training loss (single batch): 1.2078065872192383\n",
      "\t Training loss (single batch): 1.193274736404419\n",
      "\t Training loss (single batch): 1.1957072019577026\n",
      "\t Training loss (single batch): 1.304462194442749\n",
      "\t Training loss (single batch): 1.4821795225143433\n",
      "\t Training loss (single batch): 1.0932973623275757\n",
      "\t Training loss (single batch): 1.1485365629196167\n",
      "\t Training loss (single batch): 1.4118249416351318\n",
      "\t Training loss (single batch): 1.3010532855987549\n",
      "\t Training loss (single batch): 1.5464533567428589\n",
      "\t Training loss (single batch): 1.2240220308303833\n",
      "\t Training loss (single batch): 1.3290773630142212\n",
      "\t Training loss (single batch): 1.147251009941101\n",
      "\t Training loss (single batch): 1.0220469236373901\n",
      "\t Training loss (single batch): 1.146520733833313\n",
      "\t Training loss (single batch): 1.0052218437194824\n",
      "\t Training loss (single batch): 0.9347769618034363\n",
      "\t Training loss (single batch): 1.438401222229004\n",
      "\t Training loss (single batch): 0.9219856262207031\n",
      "\t Training loss (single batch): 1.0458444356918335\n",
      "\t Training loss (single batch): 0.8956749439239502\n",
      "\t Training loss (single batch): 1.1903570890426636\n",
      "\t Training loss (single batch): 1.4235846996307373\n",
      "\t Training loss (single batch): 1.2029839754104614\n",
      "\t Training loss (single batch): 1.0843333005905151\n",
      "\t Training loss (single batch): 1.140551209449768\n",
      "\t Training loss (single batch): 1.1764748096466064\n",
      "\t Training loss (single batch): 1.4234973192214966\n",
      "\t Training loss (single batch): 1.6762815713882446\n",
      "\t Training loss (single batch): 0.6535625457763672\n",
      "\t Training loss (single batch): 1.4490336179733276\n",
      "\t Training loss (single batch): 0.9897651076316833\n",
      "\t Training loss (single batch): 1.004996657371521\n",
      "\t Training loss (single batch): 1.3187248706817627\n",
      "\t Training loss (single batch): 1.175560474395752\n",
      "\t Training loss (single batch): 1.2648391723632812\n",
      "\t Training loss (single batch): 1.050159215927124\n",
      "\t Training loss (single batch): 1.331713080406189\n",
      "\t Training loss (single batch): 0.8612072467803955\n",
      "\t Training loss (single batch): 1.6111116409301758\n",
      "\t Training loss (single batch): 0.8705580830574036\n",
      "\t Training loss (single batch): 1.4616347551345825\n",
      "\t Training loss (single batch): 0.9651605486869812\n",
      "\t Training loss (single batch): 0.7745251059532166\n",
      "\t Training loss (single batch): 1.507807731628418\n",
      "\t Training loss (single batch): 1.0119761228561401\n",
      "\t Training loss (single batch): 1.8959099054336548\n",
      "\t Training loss (single batch): 1.6126512289047241\n",
      "\t Training loss (single batch): 1.0900425910949707\n",
      "\t Training loss (single batch): 1.0694690942764282\n",
      "\t Training loss (single batch): 0.9957888722419739\n",
      "\t Training loss (single batch): 1.182340383529663\n",
      "\t Training loss (single batch): 1.155753493309021\n",
      "\t Training loss (single batch): 0.8133954405784607\n",
      "\t Training loss (single batch): 1.212314248085022\n",
      "\t Training loss (single batch): 0.6709346175193787\n",
      "\t Training loss (single batch): 1.2572014331817627\n",
      "\t Training loss (single batch): 1.4614728689193726\n",
      "\t Training loss (single batch): 0.9176964163780212\n",
      "\t Training loss (single batch): 1.3164790868759155\n",
      "\t Training loss (single batch): 1.4802042245864868\n",
      "\t Training loss (single batch): 0.8317017555236816\n",
      "\t Training loss (single batch): 1.1027129888534546\n",
      "\t Training loss (single batch): 0.8749039769172668\n",
      "\t Training loss (single batch): 1.1737427711486816\n",
      "\t Training loss (single batch): 0.9838818311691284\n",
      "\t Training loss (single batch): 1.0250270366668701\n",
      "\t Training loss (single batch): 0.8787933588027954\n",
      "\t Training loss (single batch): 0.6954075694084167\n",
      "\t Training loss (single batch): 0.7064334750175476\n",
      "\t Training loss (single batch): 1.0981563329696655\n",
      "\t Training loss (single batch): 1.4967949390411377\n",
      "\t Training loss (single batch): 1.4967659711837769\n",
      "\t Training loss (single batch): 0.7299410104751587\n",
      "\t Training loss (single batch): 1.2270877361297607\n",
      "\t Training loss (single batch): 0.9707334637641907\n",
      "\t Training loss (single batch): 1.4653006792068481\n",
      "\t Training loss (single batch): 1.408699870109558\n",
      "\t Training loss (single batch): 0.8396404981613159\n",
      "\t Training loss (single batch): 1.2057557106018066\n",
      "\t Training loss (single batch): 1.6258121728897095\n",
      "\t Training loss (single batch): 1.1616530418395996\n",
      "\t Training loss (single batch): 0.6925016045570374\n",
      "\t Training loss (single batch): 1.2818939685821533\n",
      "\t Training loss (single batch): 1.1776565313339233\n",
      "\t Training loss (single batch): 1.5478407144546509\n",
      "\t Training loss (single batch): 1.400154709815979\n",
      "\t Training loss (single batch): 0.9508473873138428\n",
      "\t Training loss (single batch): 1.133690357208252\n",
      "\t Training loss (single batch): 1.4476916790008545\n",
      "\t Training loss (single batch): 1.0902163982391357\n",
      "\t Training loss (single batch): 1.1294291019439697\n",
      "\t Training loss (single batch): 1.542339563369751\n",
      "\t Training loss (single batch): 1.2963563203811646\n",
      "\t Training loss (single batch): 1.3262096643447876\n",
      "\t Training loss (single batch): 1.6364232301712036\n",
      "\t Training loss (single batch): 1.4141606092453003\n",
      "\t Training loss (single batch): 1.3379305601119995\n",
      "\t Training loss (single batch): 1.188601016998291\n",
      "\t Training loss (single batch): 1.156855821609497\n",
      "\t Training loss (single batch): 1.1702046394348145\n",
      "\t Training loss (single batch): 1.034544587135315\n",
      "\t Training loss (single batch): 1.5999420881271362\n",
      "\t Training loss (single batch): 0.9389858245849609\n",
      "\t Training loss (single batch): 1.3837518692016602\n",
      "\t Training loss (single batch): 0.9743484258651733\n",
      "\t Training loss (single batch): 0.8479821085929871\n",
      "\t Training loss (single batch): 1.2634083032608032\n",
      "\t Training loss (single batch): 0.9997924566268921\n",
      "\t Training loss (single batch): 1.0326356887817383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3609806299209595\n",
      "\t Training loss (single batch): 1.0574147701263428\n",
      "\t Training loss (single batch): 1.3557485342025757\n",
      "\t Training loss (single batch): 1.2372596263885498\n",
      "\t Training loss (single batch): 1.1927393674850464\n",
      "\t Training loss (single batch): 0.896425187587738\n",
      "\t Training loss (single batch): 0.9516286253929138\n",
      "\t Training loss (single batch): 1.2776212692260742\n",
      "\t Training loss (single batch): 1.1446551084518433\n",
      "\t Training loss (single batch): 1.0081151723861694\n",
      "\t Training loss (single batch): 0.594738245010376\n",
      "\t Training loss (single batch): 1.1524473428726196\n",
      "\t Training loss (single batch): 1.1690254211425781\n",
      "\t Training loss (single batch): 1.0309181213378906\n",
      "\t Training loss (single batch): 0.897841215133667\n",
      "\t Training loss (single batch): 0.9019710421562195\n",
      "\t Training loss (single batch): 1.4344626665115356\n",
      "\t Training loss (single batch): 1.236771583557129\n",
      "\t Training loss (single batch): 1.7297189235687256\n",
      "\t Training loss (single batch): 1.0692627429962158\n",
      "\t Training loss (single batch): 1.4086024761199951\n",
      "\t Training loss (single batch): 1.221859335899353\n",
      "\t Training loss (single batch): 0.9686586260795593\n",
      "\t Training loss (single batch): 1.1863234043121338\n",
      "\t Training loss (single batch): 1.2234516143798828\n",
      "\t Training loss (single batch): 1.3090132474899292\n",
      "\t Training loss (single batch): 1.059108853340149\n",
      "\t Training loss (single batch): 1.1620441675186157\n",
      "\t Training loss (single batch): 1.6303952932357788\n",
      "\t Training loss (single batch): 0.9637584090232849\n",
      "\t Training loss (single batch): 0.989255428314209\n",
      "\t Training loss (single batch): 1.415054440498352\n",
      "\t Training loss (single batch): 1.4704188108444214\n",
      "\t Training loss (single batch): 0.8038347363471985\n",
      "\t Training loss (single batch): 0.7501758337020874\n",
      "\t Training loss (single batch): 1.033286213874817\n",
      "\t Training loss (single batch): 0.8449884653091431\n",
      "\t Training loss (single batch): 1.0525083541870117\n",
      "\t Training loss (single batch): 0.9249100685119629\n",
      "\t Training loss (single batch): 1.045032024383545\n",
      "\t Training loss (single batch): 1.2436453104019165\n",
      "\t Training loss (single batch): 0.9962171316146851\n",
      "\t Training loss (single batch): 1.5261528491973877\n",
      "\t Training loss (single batch): 0.9732391834259033\n",
      "\t Training loss (single batch): 0.9238701462745667\n",
      "\t Training loss (single batch): 0.9816480875015259\n",
      "\t Training loss (single batch): 1.0290507078170776\n",
      "\t Training loss (single batch): 1.2457091808319092\n",
      "\t Training loss (single batch): 1.4049243927001953\n",
      "\t Training loss (single batch): 1.1743134260177612\n",
      "\t Training loss (single batch): 0.7590269446372986\n",
      "\t Training loss (single batch): 1.1513303518295288\n",
      "\t Training loss (single batch): 1.0986793041229248\n",
      "\t Training loss (single batch): 1.4891685247421265\n",
      "\t Training loss (single batch): 1.2015023231506348\n",
      "\t Training loss (single batch): 0.8607203364372253\n",
      "\t Training loss (single batch): 1.3602608442306519\n",
      "\t Training loss (single batch): 1.0047458410263062\n",
      "\t Training loss (single batch): 1.112772822380066\n",
      "\t Training loss (single batch): 1.1795064210891724\n",
      "\t Training loss (single batch): 1.1688413619995117\n",
      "\t Training loss (single batch): 1.6166741847991943\n",
      "\t Training loss (single batch): 0.9029033780097961\n",
      "\t Training loss (single batch): 1.0275791883468628\n",
      "\t Training loss (single batch): 1.456034541130066\n",
      "\t Training loss (single batch): 1.1776440143585205\n",
      "\t Training loss (single batch): 0.8883611559867859\n",
      "\t Training loss (single batch): 0.9421489238739014\n",
      "\t Training loss (single batch): 0.8750429749488831\n",
      "\t Training loss (single batch): 1.0096054077148438\n",
      "\t Training loss (single batch): 1.336938500404358\n",
      "\t Training loss (single batch): 1.0990737676620483\n",
      "\t Training loss (single batch): 0.9054555296897888\n",
      "\t Training loss (single batch): 2.305413007736206\n",
      "\t Training loss (single batch): 1.0157333612442017\n",
      "\t Training loss (single batch): 0.7619138956069946\n",
      "\t Training loss (single batch): 1.3738815784454346\n",
      "\t Training loss (single batch): 1.1921602487564087\n",
      "\t Training loss (single batch): 0.8521028161048889\n",
      "\t Training loss (single batch): 1.1798595190048218\n",
      "\t Training loss (single batch): 1.202364206314087\n",
      "\t Training loss (single batch): 1.7649673223495483\n",
      "\t Training loss (single batch): 1.3610472679138184\n",
      "\t Training loss (single batch): 1.0222314596176147\n",
      "\t Training loss (single batch): 0.9971420764923096\n",
      "\t Training loss (single batch): 0.9462302923202515\n",
      "\t Training loss (single batch): 1.1809780597686768\n",
      "\t Training loss (single batch): 1.2021315097808838\n",
      "\t Training loss (single batch): 0.7446417212486267\n",
      "\t Training loss (single batch): 0.9704070091247559\n",
      "\t Training loss (single batch): 0.9789367318153381\n",
      "\t Training loss (single batch): 1.5588088035583496\n",
      "\t Training loss (single batch): 1.121686339378357\n",
      "\t Training loss (single batch): 1.062688946723938\n",
      "\t Training loss (single batch): 1.696877121925354\n",
      "\t Training loss (single batch): 1.653457760810852\n",
      "\t Training loss (single batch): 1.4955145120620728\n",
      "\t Training loss (single batch): 1.253483533859253\n",
      "\t Training loss (single batch): 0.9228296875953674\n",
      "\t Training loss (single batch): 1.20448637008667\n",
      "\t Training loss (single batch): 1.2775352001190186\n",
      "\t Training loss (single batch): 0.7252718210220337\n",
      "\t Training loss (single batch): 1.0458176136016846\n",
      "\t Training loss (single batch): 1.3689697980880737\n",
      "\t Training loss (single batch): 1.749848484992981\n",
      "\t Training loss (single batch): 0.6598530411720276\n",
      "\t Training loss (single batch): 0.78017258644104\n",
      "\t Training loss (single batch): 1.008055329322815\n",
      "\t Training loss (single batch): 1.344936728477478\n",
      "\t Training loss (single batch): 1.1101022958755493\n",
      "\t Training loss (single batch): 1.5457619428634644\n",
      "\t Training loss (single batch): 1.3671871423721313\n",
      "\t Training loss (single batch): 1.0281522274017334\n",
      "\t Training loss (single batch): 0.8877844214439392\n",
      "\t Training loss (single batch): 0.9924179315567017\n",
      "\t Training loss (single batch): 1.27120840549469\n",
      "\t Training loss (single batch): 1.2600409984588623\n",
      "\t Training loss (single batch): 1.2706433534622192\n",
      "\t Training loss (single batch): 1.4839286804199219\n",
      "\t Training loss (single batch): 1.2112758159637451\n",
      "\t Training loss (single batch): 1.0252183675765991\n",
      "\t Training loss (single batch): 1.1631475687026978\n",
      "\t Training loss (single batch): 1.2542678117752075\n",
      "\t Training loss (single batch): 0.7656872272491455\n",
      "\t Training loss (single batch): 0.7844472527503967\n",
      "\t Training loss (single batch): 1.5653197765350342\n",
      "\t Training loss (single batch): 1.0504626035690308\n",
      "\t Training loss (single batch): 1.0242644548416138\n",
      "\t Training loss (single batch): 1.023016095161438\n",
      "\t Training loss (single batch): 1.6056948900222778\n",
      "\t Training loss (single batch): 1.3079389333724976\n",
      "\t Training loss (single batch): 0.9678145051002502\n",
      "\t Training loss (single batch): 1.509976863861084\n",
      "\t Training loss (single batch): 1.2161363363265991\n",
      "\t Training loss (single batch): 1.535502314567566\n",
      "\t Training loss (single batch): 1.2403149604797363\n",
      "\t Training loss (single batch): 0.8567932844161987\n",
      "\t Training loss (single batch): 1.7046728134155273\n",
      "\t Training loss (single batch): 0.9167709350585938\n",
      "\t Training loss (single batch): 1.1227339506149292\n",
      "\t Training loss (single batch): 1.2655103206634521\n",
      "\t Training loss (single batch): 1.1358771324157715\n",
      "\t Training loss (single batch): 1.2086020708084106\n",
      "\t Training loss (single batch): 1.3601537942886353\n",
      "\t Training loss (single batch): 1.1756949424743652\n",
      "\t Training loss (single batch): 0.9837981462478638\n",
      "\t Training loss (single batch): 1.00388765335083\n",
      "\t Training loss (single batch): 1.0535231828689575\n",
      "\t Training loss (single batch): 1.0880331993103027\n",
      "\t Training loss (single batch): 0.9649221897125244\n",
      "\t Training loss (single batch): 0.9037674069404602\n",
      "\t Training loss (single batch): 1.6886522769927979\n",
      "\t Training loss (single batch): 0.9394956827163696\n",
      "\t Training loss (single batch): 0.9873623251914978\n",
      "\t Training loss (single batch): 1.3739410638809204\n",
      "\t Training loss (single batch): 1.0979506969451904\n",
      "\t Training loss (single batch): 0.9194753170013428\n",
      "\t Training loss (single batch): 0.9890215992927551\n",
      "\t Training loss (single batch): 0.8389950394630432\n",
      "\t Training loss (single batch): 1.5187355279922485\n",
      "\t Training loss (single batch): 0.8428211808204651\n",
      "\t Training loss (single batch): 1.195311188697815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.9884657859802246\n",
      "\t Training loss (single batch): 0.9550580978393555\n",
      "\t Training loss (single batch): 1.358789324760437\n",
      "\t Training loss (single batch): 1.0489709377288818\n",
      "\t Training loss (single batch): 1.1701692342758179\n",
      "\t Training loss (single batch): 1.204100489616394\n",
      "\t Training loss (single batch): 1.6032969951629639\n",
      "\t Training loss (single batch): 1.1334999799728394\n",
      "\t Training loss (single batch): 1.3913516998291016\n",
      "\t Training loss (single batch): 0.8684578537940979\n",
      "\t Training loss (single batch): 1.2096054553985596\n",
      "\t Training loss (single batch): 0.8757158517837524\n",
      "\t Training loss (single batch): 1.4021971225738525\n",
      "\t Training loss (single batch): 1.1972063779830933\n",
      "\t Training loss (single batch): 0.9795968532562256\n",
      "\t Training loss (single batch): 0.9961190819740295\n",
      "\t Training loss (single batch): 1.722198247909546\n",
      "\t Training loss (single batch): 1.0589544773101807\n",
      "\t Training loss (single batch): 0.7350220084190369\n",
      "##################################\n",
      "## EPOCH 95\n",
      "##################################\n",
      "\t Training loss (single batch): 1.145845890045166\n",
      "\t Training loss (single batch): 1.1501071453094482\n",
      "\t Training loss (single batch): 1.2423518896102905\n",
      "\t Training loss (single batch): 1.2433613538742065\n",
      "\t Training loss (single batch): 1.260974407196045\n",
      "\t Training loss (single batch): 1.0165621042251587\n",
      "\t Training loss (single batch): 1.2108306884765625\n",
      "\t Training loss (single batch): 1.085942268371582\n",
      "\t Training loss (single batch): 1.237700343132019\n",
      "\t Training loss (single batch): 0.8369684815406799\n",
      "\t Training loss (single batch): 1.442058801651001\n",
      "\t Training loss (single batch): 1.080570101737976\n",
      "\t Training loss (single batch): 0.8976288437843323\n",
      "\t Training loss (single batch): 1.4606783390045166\n",
      "\t Training loss (single batch): 1.3669990301132202\n",
      "\t Training loss (single batch): 1.189459204673767\n",
      "\t Training loss (single batch): 1.102210521697998\n",
      "\t Training loss (single batch): 0.9818134307861328\n",
      "\t Training loss (single batch): 0.7131125926971436\n",
      "\t Training loss (single batch): 1.181934118270874\n",
      "\t Training loss (single batch): 1.222021460533142\n",
      "\t Training loss (single batch): 0.9448437690734863\n",
      "\t Training loss (single batch): 1.4628376960754395\n",
      "\t Training loss (single batch): 0.9008883833885193\n",
      "\t Training loss (single batch): 0.8249838352203369\n",
      "\t Training loss (single batch): 0.8032342195510864\n",
      "\t Training loss (single batch): 1.0866243839263916\n",
      "\t Training loss (single batch): 0.9160298705101013\n",
      "\t Training loss (single batch): 1.9987101554870605\n",
      "\t Training loss (single batch): 1.8844040632247925\n",
      "\t Training loss (single batch): 1.780302882194519\n",
      "\t Training loss (single batch): 0.8838074803352356\n",
      "\t Training loss (single batch): 1.6778302192687988\n",
      "\t Training loss (single batch): 1.204160451889038\n",
      "\t Training loss (single batch): 0.9905049800872803\n",
      "\t Training loss (single batch): 0.9879504442214966\n",
      "\t Training loss (single batch): 1.1851803064346313\n",
      "\t Training loss (single batch): 1.1884379386901855\n",
      "\t Training loss (single batch): 1.1998565196990967\n",
      "\t Training loss (single batch): 0.7999836802482605\n",
      "\t Training loss (single batch): 1.5582383871078491\n",
      "\t Training loss (single batch): 1.1317970752716064\n",
      "\t Training loss (single batch): 1.0083789825439453\n",
      "\t Training loss (single batch): 1.1035728454589844\n",
      "\t Training loss (single batch): 1.0850811004638672\n",
      "\t Training loss (single batch): 1.0510753393173218\n",
      "\t Training loss (single batch): 0.9863712191581726\n",
      "\t Training loss (single batch): 1.2438502311706543\n",
      "\t Training loss (single batch): 1.5070377588272095\n",
      "\t Training loss (single batch): 1.4569774866104126\n",
      "\t Training loss (single batch): 1.037781834602356\n",
      "\t Training loss (single batch): 1.2454990148544312\n",
      "\t Training loss (single batch): 0.877556562423706\n",
      "\t Training loss (single batch): 0.9403131604194641\n",
      "\t Training loss (single batch): 1.2621018886566162\n",
      "\t Training loss (single batch): 1.577123999595642\n",
      "\t Training loss (single batch): 0.613861620426178\n",
      "\t Training loss (single batch): 1.2135640382766724\n",
      "\t Training loss (single batch): 1.1267309188842773\n",
      "\t Training loss (single batch): 0.9124354720115662\n",
      "\t Training loss (single batch): 0.8368443250656128\n",
      "\t Training loss (single batch): 1.1613192558288574\n",
      "\t Training loss (single batch): 1.1647239923477173\n",
      "\t Training loss (single batch): 1.0703725814819336\n",
      "\t Training loss (single batch): 1.7768009901046753\n",
      "\t Training loss (single batch): 1.1180007457733154\n",
      "\t Training loss (single batch): 2.208444118499756\n",
      "\t Training loss (single batch): 1.1366159915924072\n",
      "\t Training loss (single batch): 1.4127956628799438\n",
      "\t Training loss (single batch): 1.0318890810012817\n",
      "\t Training loss (single batch): 1.0306899547576904\n",
      "\t Training loss (single batch): 1.085357666015625\n",
      "\t Training loss (single batch): 0.8468452095985413\n",
      "\t Training loss (single batch): 0.5846752524375916\n",
      "\t Training loss (single batch): 1.1759922504425049\n",
      "\t Training loss (single batch): 1.2409098148345947\n",
      "\t Training loss (single batch): 1.1509191989898682\n",
      "\t Training loss (single batch): 1.2898766994476318\n",
      "\t Training loss (single batch): 1.3360180854797363\n",
      "\t Training loss (single batch): 1.046984314918518\n",
      "\t Training loss (single batch): 1.054898977279663\n",
      "\t Training loss (single batch): 0.779335081577301\n",
      "\t Training loss (single batch): 1.5603611469268799\n",
      "\t Training loss (single batch): 1.6901456117630005\n",
      "\t Training loss (single batch): 1.2218036651611328\n",
      "\t Training loss (single batch): 0.9732913970947266\n",
      "\t Training loss (single batch): 1.2708284854888916\n",
      "\t Training loss (single batch): 1.173393726348877\n",
      "\t Training loss (single batch): 1.611203908920288\n",
      "\t Training loss (single batch): 0.9995934367179871\n",
      "\t Training loss (single batch): 1.1415075063705444\n",
      "\t Training loss (single batch): 0.9693004488945007\n",
      "\t Training loss (single batch): 1.4913769960403442\n",
      "\t Training loss (single batch): 0.9353480339050293\n",
      "\t Training loss (single batch): 1.5107307434082031\n",
      "\t Training loss (single batch): 0.8605560660362244\n",
      "\t Training loss (single batch): 1.243481159210205\n",
      "\t Training loss (single batch): 1.2019742727279663\n",
      "\t Training loss (single batch): 0.7772383093833923\n",
      "\t Training loss (single batch): 0.960636556148529\n",
      "\t Training loss (single batch): 0.9217453598976135\n",
      "\t Training loss (single batch): 1.1400518417358398\n",
      "\t Training loss (single batch): 0.9241484999656677\n",
      "\t Training loss (single batch): 1.2508676052093506\n",
      "\t Training loss (single batch): 0.675307035446167\n",
      "\t Training loss (single batch): 0.7724927067756653\n",
      "\t Training loss (single batch): 0.9024920463562012\n",
      "\t Training loss (single batch): 0.7639723420143127\n",
      "\t Training loss (single batch): 0.8411187529563904\n",
      "\t Training loss (single batch): 1.2583321332931519\n",
      "\t Training loss (single batch): 1.4775973558425903\n",
      "\t Training loss (single batch): 1.4104548692703247\n",
      "\t Training loss (single batch): 1.0131618976593018\n",
      "\t Training loss (single batch): 0.9045557975769043\n",
      "\t Training loss (single batch): 1.337319016456604\n",
      "\t Training loss (single batch): 1.1583354473114014\n",
      "\t Training loss (single batch): 1.364156723022461\n",
      "\t Training loss (single batch): 0.8131929636001587\n",
      "\t Training loss (single batch): 0.8227654099464417\n",
      "\t Training loss (single batch): 1.2540631294250488\n",
      "\t Training loss (single batch): 0.8823737502098083\n",
      "\t Training loss (single batch): 0.9462283253669739\n",
      "\t Training loss (single batch): 2.0048112869262695\n",
      "\t Training loss (single batch): 1.1110591888427734\n",
      "\t Training loss (single batch): 1.3581181764602661\n",
      "\t Training loss (single batch): 1.128433346748352\n",
      "\t Training loss (single batch): 0.8785337805747986\n",
      "\t Training loss (single batch): 0.7445135116577148\n",
      "\t Training loss (single batch): 1.158299207687378\n",
      "\t Training loss (single batch): 1.3197968006134033\n",
      "\t Training loss (single batch): 1.6894816160202026\n",
      "\t Training loss (single batch): 1.2085245847702026\n",
      "\t Training loss (single batch): 1.017395257949829\n",
      "\t Training loss (single batch): 0.9223646521568298\n",
      "\t Training loss (single batch): 1.199354887008667\n",
      "\t Training loss (single batch): 1.4605450630187988\n",
      "\t Training loss (single batch): 1.221615195274353\n",
      "\t Training loss (single batch): 1.8737272024154663\n",
      "\t Training loss (single batch): 1.1006923913955688\n",
      "\t Training loss (single batch): 1.0638700723648071\n",
      "\t Training loss (single batch): 1.1928627490997314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9937358498573303\n",
      "\t Training loss (single batch): 1.0784138441085815\n",
      "\t Training loss (single batch): 0.9809412956237793\n",
      "\t Training loss (single batch): 0.9963143467903137\n",
      "\t Training loss (single batch): 1.4699087142944336\n",
      "\t Training loss (single batch): 1.483385443687439\n",
      "\t Training loss (single batch): 1.2249287366867065\n",
      "\t Training loss (single batch): 1.0478715896606445\n",
      "\t Training loss (single batch): 0.9187933206558228\n",
      "\t Training loss (single batch): 1.2644801139831543\n",
      "\t Training loss (single batch): 1.522754192352295\n",
      "\t Training loss (single batch): 1.0864553451538086\n",
      "\t Training loss (single batch): 1.4633599519729614\n",
      "\t Training loss (single batch): 1.1795265674591064\n",
      "\t Training loss (single batch): 1.664962649345398\n",
      "\t Training loss (single batch): 1.293929934501648\n",
      "\t Training loss (single batch): 1.1114333868026733\n",
      "\t Training loss (single batch): 1.4049593210220337\n",
      "\t Training loss (single batch): 1.1957018375396729\n",
      "\t Training loss (single batch): 0.9889795780181885\n",
      "\t Training loss (single batch): 0.8399506211280823\n",
      "\t Training loss (single batch): 1.2689151763916016\n",
      "\t Training loss (single batch): 1.2203437089920044\n",
      "\t Training loss (single batch): 0.7388014793395996\n",
      "\t Training loss (single batch): 1.1330293416976929\n",
      "\t Training loss (single batch): 1.239899754524231\n",
      "\t Training loss (single batch): 1.0348434448242188\n",
      "\t Training loss (single batch): 0.9101988077163696\n",
      "\t Training loss (single batch): 1.4265544414520264\n",
      "\t Training loss (single batch): 0.9664856791496277\n",
      "\t Training loss (single batch): 1.161106824874878\n",
      "\t Training loss (single batch): 0.9987020492553711\n",
      "\t Training loss (single batch): 0.8573519587516785\n",
      "\t Training loss (single batch): 1.008775234222412\n",
      "\t Training loss (single batch): 0.9860143661499023\n",
      "\t Training loss (single batch): 1.161797046661377\n",
      "\t Training loss (single batch): 1.6214061975479126\n",
      "\t Training loss (single batch): 0.9899477362632751\n",
      "\t Training loss (single batch): 1.0767697095870972\n",
      "\t Training loss (single batch): 1.3281669616699219\n",
      "\t Training loss (single batch): 0.7455856204032898\n",
      "\t Training loss (single batch): 1.1415698528289795\n",
      "\t Training loss (single batch): 1.3689216375350952\n",
      "\t Training loss (single batch): 1.5941486358642578\n",
      "\t Training loss (single batch): 0.9542649984359741\n",
      "\t Training loss (single batch): 1.3801850080490112\n",
      "\t Training loss (single batch): 1.058176875114441\n",
      "\t Training loss (single batch): 1.1287307739257812\n",
      "\t Training loss (single batch): 1.3900948762893677\n",
      "\t Training loss (single batch): 1.0075076818466187\n",
      "\t Training loss (single batch): 0.9551450610160828\n",
      "\t Training loss (single batch): 1.0469574928283691\n",
      "\t Training loss (single batch): 1.2150400876998901\n",
      "\t Training loss (single batch): 1.1529539823532104\n",
      "\t Training loss (single batch): 1.456765055656433\n",
      "\t Training loss (single batch): 0.9423392415046692\n",
      "\t Training loss (single batch): 1.469793438911438\n",
      "\t Training loss (single batch): 1.354001760482788\n",
      "\t Training loss (single batch): 1.5857093334197998\n",
      "\t Training loss (single batch): 0.9258607029914856\n",
      "\t Training loss (single batch): 0.963117241859436\n",
      "\t Training loss (single batch): 0.8428658246994019\n",
      "\t Training loss (single batch): 1.0669519901275635\n",
      "\t Training loss (single batch): 1.8489642143249512\n",
      "\t Training loss (single batch): 1.4764844179153442\n",
      "\t Training loss (single batch): 1.0190328359603882\n",
      "\t Training loss (single batch): 1.1488271951675415\n",
      "\t Training loss (single batch): 1.328269362449646\n",
      "\t Training loss (single batch): 1.0960756540298462\n",
      "\t Training loss (single batch): 0.9408471584320068\n",
      "\t Training loss (single batch): 1.269758701324463\n",
      "\t Training loss (single batch): 1.1086329221725464\n",
      "\t Training loss (single batch): 1.5001044273376465\n",
      "\t Training loss (single batch): 1.793992519378662\n",
      "\t Training loss (single batch): 1.3629578351974487\n",
      "\t Training loss (single batch): 1.5528286695480347\n",
      "\t Training loss (single batch): 1.0685598850250244\n",
      "\t Training loss (single batch): 1.3903006315231323\n",
      "\t Training loss (single batch): 1.4674204587936401\n",
      "\t Training loss (single batch): 1.2369084358215332\n",
      "\t Training loss (single batch): 1.2222098112106323\n",
      "\t Training loss (single batch): 1.3164433240890503\n",
      "\t Training loss (single batch): 1.1472139358520508\n",
      "\t Training loss (single batch): 0.9111990928649902\n",
      "\t Training loss (single batch): 1.580461859703064\n",
      "\t Training loss (single batch): 1.020409345626831\n",
      "\t Training loss (single batch): 1.3728433847427368\n",
      "\t Training loss (single batch): 1.2527247667312622\n",
      "\t Training loss (single batch): 0.9428753852844238\n",
      "\t Training loss (single batch): 1.3995333909988403\n",
      "\t Training loss (single batch): 1.4445621967315674\n",
      "\t Training loss (single batch): 1.1349632740020752\n",
      "\t Training loss (single batch): 1.0534292459487915\n",
      "\t Training loss (single batch): 1.1675190925598145\n",
      "\t Training loss (single batch): 0.8402646780014038\n",
      "\t Training loss (single batch): 1.1219192743301392\n",
      "\t Training loss (single batch): 1.2264165878295898\n",
      "\t Training loss (single batch): 1.2444257736206055\n",
      "\t Training loss (single batch): 1.5888144969940186\n",
      "\t Training loss (single batch): 1.1810455322265625\n",
      "\t Training loss (single batch): 1.2878299951553345\n",
      "\t Training loss (single batch): 1.2206199169158936\n",
      "\t Training loss (single batch): 1.0094869136810303\n",
      "\t Training loss (single batch): 1.1859400272369385\n",
      "\t Training loss (single batch): 1.2603822946548462\n",
      "\t Training loss (single batch): 1.1206928491592407\n",
      "\t Training loss (single batch): 1.3078645467758179\n",
      "\t Training loss (single batch): 1.193555474281311\n",
      "\t Training loss (single batch): 1.3297322988510132\n",
      "\t Training loss (single batch): 1.1543701887130737\n",
      "\t Training loss (single batch): 1.2913769483566284\n",
      "\t Training loss (single batch): 1.014445424079895\n",
      "\t Training loss (single batch): 1.113373875617981\n",
      "\t Training loss (single batch): 1.392143726348877\n",
      "\t Training loss (single batch): 1.1384271383285522\n",
      "\t Training loss (single batch): 1.4756852388381958\n",
      "\t Training loss (single batch): 1.5044695138931274\n",
      "\t Training loss (single batch): 0.7655636072158813\n",
      "\t Training loss (single batch): 0.7160072922706604\n",
      "\t Training loss (single batch): 1.4932754039764404\n",
      "\t Training loss (single batch): 1.511601448059082\n",
      "\t Training loss (single batch): 1.5610357522964478\n",
      "\t Training loss (single batch): 0.5610542297363281\n",
      "\t Training loss (single batch): 1.263411045074463\n",
      "\t Training loss (single batch): 1.3114018440246582\n",
      "\t Training loss (single batch): 1.7770670652389526\n",
      "\t Training loss (single batch): 0.9826723337173462\n",
      "\t Training loss (single batch): 0.9324398636817932\n",
      "\t Training loss (single batch): 1.1605331897735596\n",
      "\t Training loss (single batch): 1.111772060394287\n",
      "\t Training loss (single batch): 1.0887343883514404\n",
      "\t Training loss (single batch): 1.5561555624008179\n",
      "\t Training loss (single batch): 1.4633655548095703\n",
      "\t Training loss (single batch): 1.096303105354309\n",
      "\t Training loss (single batch): 1.1264410018920898\n",
      "\t Training loss (single batch): 1.1056350469589233\n",
      "\t Training loss (single batch): 1.22751784324646\n",
      "\t Training loss (single batch): 0.9768869280815125\n",
      "\t Training loss (single batch): 1.210557460784912\n",
      "\t Training loss (single batch): 1.2137970924377441\n",
      "\t Training loss (single batch): 1.1088857650756836\n",
      "\t Training loss (single batch): 0.9202073812484741\n",
      "\t Training loss (single batch): 1.159147024154663\n",
      "\t Training loss (single batch): 0.8252786993980408\n",
      "\t Training loss (single batch): 1.442228078842163\n",
      "\t Training loss (single batch): 1.4180692434310913\n",
      "\t Training loss (single batch): 1.3697530031204224\n",
      "\t Training loss (single batch): 0.9115658402442932\n",
      "\t Training loss (single batch): 1.2026599645614624\n",
      "\t Training loss (single batch): 1.0986809730529785\n",
      "\t Training loss (single batch): 0.6465915441513062\n",
      "\t Training loss (single batch): 1.3121600151062012\n",
      "\t Training loss (single batch): 1.2043471336364746\n",
      "\t Training loss (single batch): 0.8034197092056274\n",
      "\t Training loss (single batch): 0.9562796950340271\n",
      "\t Training loss (single batch): 1.1695244312286377\n",
      "\t Training loss (single batch): 1.2838252782821655\n",
      "\t Training loss (single batch): 0.9783686399459839\n",
      "\t Training loss (single batch): 1.1324440240859985\n",
      "\t Training loss (single batch): 1.4596809148788452\n",
      "\t Training loss (single batch): 1.5493992567062378\n",
      "\t Training loss (single batch): 1.2002671957015991\n",
      "\t Training loss (single batch): 1.190016508102417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3019955158233643\n",
      "\t Training loss (single batch): 1.4114317893981934\n",
      "\t Training loss (single batch): 1.4559201002120972\n",
      "\t Training loss (single batch): 1.096659779548645\n",
      "\t Training loss (single batch): 1.3079622983932495\n",
      "\t Training loss (single batch): 1.6830124855041504\n",
      "\t Training loss (single batch): 1.7265980243682861\n",
      "\t Training loss (single batch): 1.8765355348587036\n",
      "\t Training loss (single batch): 1.2727810144424438\n",
      "\t Training loss (single batch): 1.4639047384262085\n",
      "\t Training loss (single batch): 1.2500534057617188\n",
      "\t Training loss (single batch): 1.2722378969192505\n",
      "\t Training loss (single batch): 0.8816617727279663\n",
      "##################################\n",
      "## EPOCH 96\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8684022426605225\n",
      "\t Training loss (single batch): 1.3270913362503052\n",
      "\t Training loss (single batch): 1.4609770774841309\n",
      "\t Training loss (single batch): 1.3841582536697388\n",
      "\t Training loss (single batch): 1.2825038433074951\n",
      "\t Training loss (single batch): 1.1875505447387695\n",
      "\t Training loss (single batch): 1.4828439950942993\n",
      "\t Training loss (single batch): 1.2331100702285767\n",
      "\t Training loss (single batch): 1.2005685567855835\n",
      "\t Training loss (single batch): 0.8826736211776733\n",
      "\t Training loss (single batch): 0.9380981922149658\n",
      "\t Training loss (single batch): 1.2111250162124634\n",
      "\t Training loss (single batch): 0.7958945035934448\n",
      "\t Training loss (single batch): 0.7044726014137268\n",
      "\t Training loss (single batch): 1.3856964111328125\n",
      "\t Training loss (single batch): 1.3978105783462524\n",
      "\t Training loss (single batch): 0.9618503451347351\n",
      "\t Training loss (single batch): 0.938639760017395\n",
      "\t Training loss (single batch): 1.5236395597457886\n",
      "\t Training loss (single batch): 0.869756281375885\n",
      "\t Training loss (single batch): 1.357237458229065\n",
      "\t Training loss (single batch): 1.2136492729187012\n",
      "\t Training loss (single batch): 1.1843432188034058\n",
      "\t Training loss (single batch): 1.0382280349731445\n",
      "\t Training loss (single batch): 1.0703860521316528\n",
      "\t Training loss (single batch): 0.9086164236068726\n",
      "\t Training loss (single batch): 0.9894776344299316\n",
      "\t Training loss (single batch): 1.1174463033676147\n",
      "\t Training loss (single batch): 1.3392822742462158\n",
      "\t Training loss (single batch): 1.097137451171875\n",
      "\t Training loss (single batch): 1.2863181829452515\n",
      "\t Training loss (single batch): 1.1019090414047241\n",
      "\t Training loss (single batch): 1.0878556966781616\n",
      "\t Training loss (single batch): 1.173808217048645\n",
      "\t Training loss (single batch): 1.3146395683288574\n",
      "\t Training loss (single batch): 1.0501197576522827\n",
      "\t Training loss (single batch): 1.0382825136184692\n",
      "\t Training loss (single batch): 1.4743432998657227\n",
      "\t Training loss (single batch): 0.9226987361907959\n",
      "\t Training loss (single batch): 1.487187385559082\n",
      "\t Training loss (single batch): 1.2154923677444458\n",
      "\t Training loss (single batch): 0.8944860100746155\n",
      "\t Training loss (single batch): 1.449133038520813\n",
      "\t Training loss (single batch): 0.9031460285186768\n",
      "\t Training loss (single batch): 0.5697965621948242\n",
      "\t Training loss (single batch): 1.3125125169754028\n",
      "\t Training loss (single batch): 1.090665578842163\n",
      "\t Training loss (single batch): 1.1902941465377808\n",
      "\t Training loss (single batch): 1.2665061950683594\n",
      "\t Training loss (single batch): 1.681894063949585\n",
      "\t Training loss (single batch): 1.485146164894104\n",
      "\t Training loss (single batch): 1.2938083410263062\n",
      "\t Training loss (single batch): 1.1838047504425049\n",
      "\t Training loss (single batch): 1.0089751482009888\n",
      "\t Training loss (single batch): 1.984606146812439\n",
      "\t Training loss (single batch): 1.1720476150512695\n",
      "\t Training loss (single batch): 1.505382776260376\n",
      "\t Training loss (single batch): 1.6609199047088623\n",
      "\t Training loss (single batch): 1.1315890550613403\n",
      "\t Training loss (single batch): 1.2910161018371582\n",
      "\t Training loss (single batch): 1.5946662425994873\n",
      "\t Training loss (single batch): 1.2998378276824951\n",
      "\t Training loss (single batch): 1.0153099298477173\n",
      "\t Training loss (single batch): 1.1536885499954224\n",
      "\t Training loss (single batch): 1.211505651473999\n",
      "\t Training loss (single batch): 0.8673924803733826\n",
      "\t Training loss (single batch): 1.2068190574645996\n",
      "\t Training loss (single batch): 1.4896906614303589\n",
      "\t Training loss (single batch): 1.133438229560852\n",
      "\t Training loss (single batch): 1.207135558128357\n",
      "\t Training loss (single batch): 1.6578660011291504\n",
      "\t Training loss (single batch): 1.2846307754516602\n",
      "\t Training loss (single batch): 1.2130730152130127\n",
      "\t Training loss (single batch): 1.0276185274124146\n",
      "\t Training loss (single batch): 1.2512340545654297\n",
      "\t Training loss (single batch): 0.9396610856056213\n",
      "\t Training loss (single batch): 1.117468237876892\n",
      "\t Training loss (single batch): 1.1266919374465942\n",
      "\t Training loss (single batch): 1.152203917503357\n",
      "\t Training loss (single batch): 1.1654709577560425\n",
      "\t Training loss (single batch): 1.364581823348999\n",
      "\t Training loss (single batch): 1.5634880065917969\n",
      "\t Training loss (single batch): 0.8629180788993835\n",
      "\t Training loss (single batch): 1.0182853937149048\n",
      "\t Training loss (single batch): 1.403031349182129\n",
      "\t Training loss (single batch): 1.3635543584823608\n",
      "\t Training loss (single batch): 1.3824387788772583\n",
      "\t Training loss (single batch): 1.2212672233581543\n",
      "\t Training loss (single batch): 0.6908689737319946\n",
      "\t Training loss (single batch): 1.392367959022522\n",
      "\t Training loss (single batch): 1.5541969537734985\n",
      "\t Training loss (single batch): 1.2722243070602417\n",
      "\t Training loss (single batch): 1.167707085609436\n",
      "\t Training loss (single batch): 1.4458121061325073\n",
      "\t Training loss (single batch): 1.200844168663025\n",
      "\t Training loss (single batch): 1.2479767799377441\n",
      "\t Training loss (single batch): 0.9767431020736694\n",
      "\t Training loss (single batch): 1.2529290914535522\n",
      "\t Training loss (single batch): 0.930082380771637\n",
      "\t Training loss (single batch): 1.076646327972412\n",
      "\t Training loss (single batch): 1.0536868572235107\n",
      "\t Training loss (single batch): 1.2849605083465576\n",
      "\t Training loss (single batch): 1.3974027633666992\n",
      "\t Training loss (single batch): 1.1565332412719727\n",
      "\t Training loss (single batch): 0.8898923397064209\n",
      "\t Training loss (single batch): 0.6979993581771851\n",
      "\t Training loss (single batch): 1.1536048650741577\n",
      "\t Training loss (single batch): 1.3587219715118408\n",
      "\t Training loss (single batch): 1.2639080286026\n",
      "\t Training loss (single batch): 0.8098737597465515\n",
      "\t Training loss (single batch): 1.0923733711242676\n",
      "\t Training loss (single batch): 1.1959556341171265\n",
      "\t Training loss (single batch): 1.2455713748931885\n",
      "\t Training loss (single batch): 1.1506822109222412\n",
      "\t Training loss (single batch): 1.48616623878479\n",
      "\t Training loss (single batch): 1.1418405771255493\n",
      "\t Training loss (single batch): 1.0748205184936523\n",
      "\t Training loss (single batch): 1.3658586740493774\n",
      "\t Training loss (single batch): 1.5361543893814087\n",
      "\t Training loss (single batch): 0.8787944912910461\n",
      "\t Training loss (single batch): 1.1145508289337158\n",
      "\t Training loss (single batch): 1.2988821268081665\n",
      "\t Training loss (single batch): 1.132684350013733\n",
      "\t Training loss (single batch): 1.0301169157028198\n",
      "\t Training loss (single batch): 1.0478217601776123\n",
      "\t Training loss (single batch): 1.102002501487732\n",
      "\t Training loss (single batch): 1.1430834531784058\n",
      "\t Training loss (single batch): 1.1100993156433105\n",
      "\t Training loss (single batch): 1.3428324460983276\n",
      "\t Training loss (single batch): 1.2315714359283447\n",
      "\t Training loss (single batch): 0.8418627977371216\n",
      "\t Training loss (single batch): 0.845034122467041\n",
      "\t Training loss (single batch): 1.628853440284729\n",
      "\t Training loss (single batch): 0.9818124175071716\n",
      "\t Training loss (single batch): 1.0160980224609375\n",
      "\t Training loss (single batch): 0.9562922716140747\n",
      "\t Training loss (single batch): 1.2259502410888672\n",
      "\t Training loss (single batch): 1.5741117000579834\n",
      "\t Training loss (single batch): 1.293163537979126\n",
      "\t Training loss (single batch): 1.3854498863220215\n",
      "\t Training loss (single batch): 0.7351467609405518\n",
      "\t Training loss (single batch): 0.8041818141937256\n",
      "\t Training loss (single batch): 0.8220708966255188\n",
      "\t Training loss (single batch): 1.2489957809448242\n",
      "\t Training loss (single batch): 1.3526936769485474\n",
      "\t Training loss (single batch): 0.8577510118484497\n",
      "\t Training loss (single batch): 1.2369916439056396\n",
      "\t Training loss (single batch): 1.005650520324707\n",
      "\t Training loss (single batch): 1.2834497690200806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1769514083862305\n",
      "\t Training loss (single batch): 1.251267433166504\n",
      "\t Training loss (single batch): 0.7310791611671448\n",
      "\t Training loss (single batch): 0.9282417893409729\n",
      "\t Training loss (single batch): 1.022361397743225\n",
      "\t Training loss (single batch): 0.7680158615112305\n",
      "\t Training loss (single batch): 1.2424049377441406\n",
      "\t Training loss (single batch): 1.3565325736999512\n",
      "\t Training loss (single batch): 1.2018232345581055\n",
      "\t Training loss (single batch): 0.9877737760543823\n",
      "\t Training loss (single batch): 1.4450374841690063\n",
      "\t Training loss (single batch): 1.4134942293167114\n",
      "\t Training loss (single batch): 1.2747691869735718\n",
      "\t Training loss (single batch): 1.1043158769607544\n",
      "\t Training loss (single batch): 1.132979154586792\n",
      "\t Training loss (single batch): 1.0836902856826782\n",
      "\t Training loss (single batch): 1.3542588949203491\n",
      "\t Training loss (single batch): 1.1856762170791626\n",
      "\t Training loss (single batch): 1.3354525566101074\n",
      "\t Training loss (single batch): 1.4033883810043335\n",
      "\t Training loss (single batch): 1.0489765405654907\n",
      "\t Training loss (single batch): 1.5801903009414673\n",
      "\t Training loss (single batch): 1.2496509552001953\n",
      "\t Training loss (single batch): 1.230573058128357\n",
      "\t Training loss (single batch): 1.379080057144165\n",
      "\t Training loss (single batch): 0.7050348520278931\n",
      "\t Training loss (single batch): 1.195110559463501\n",
      "\t Training loss (single batch): 1.1281524896621704\n",
      "\t Training loss (single batch): 1.3917733430862427\n",
      "\t Training loss (single batch): 1.3036600351333618\n",
      "\t Training loss (single batch): 0.7677621841430664\n",
      "\t Training loss (single batch): 1.327510118484497\n",
      "\t Training loss (single batch): 1.220478892326355\n",
      "\t Training loss (single batch): 0.9306082725524902\n",
      "\t Training loss (single batch): 1.5660159587860107\n",
      "\t Training loss (single batch): 1.3223214149475098\n",
      "\t Training loss (single batch): 1.1628267765045166\n",
      "\t Training loss (single batch): 1.4052335023880005\n",
      "\t Training loss (single batch): 1.299850583076477\n",
      "\t Training loss (single batch): 1.1654608249664307\n",
      "\t Training loss (single batch): 0.9505343437194824\n",
      "\t Training loss (single batch): 1.435581088066101\n",
      "\t Training loss (single batch): 1.1865259408950806\n",
      "\t Training loss (single batch): 1.287914752960205\n",
      "\t Training loss (single batch): 1.3357758522033691\n",
      "\t Training loss (single batch): 0.9195696115493774\n",
      "\t Training loss (single batch): 1.4585901498794556\n",
      "\t Training loss (single batch): 1.3942791223526\n",
      "\t Training loss (single batch): 1.2331219911575317\n",
      "\t Training loss (single batch): 1.2802436351776123\n",
      "\t Training loss (single batch): 1.0461565256118774\n",
      "\t Training loss (single batch): 0.9049245715141296\n",
      "\t Training loss (single batch): 0.7597838640213013\n",
      "\t Training loss (single batch): 1.5913652181625366\n",
      "\t Training loss (single batch): 1.8381588459014893\n",
      "\t Training loss (single batch): 0.8348156213760376\n",
      "\t Training loss (single batch): 1.308348298072815\n",
      "\t Training loss (single batch): 1.3646674156188965\n",
      "\t Training loss (single batch): 1.6926592588424683\n",
      "\t Training loss (single batch): 1.546486735343933\n",
      "\t Training loss (single batch): 1.1068089008331299\n",
      "\t Training loss (single batch): 1.3237258195877075\n",
      "\t Training loss (single batch): 1.5581135749816895\n",
      "\t Training loss (single batch): 0.8615601062774658\n",
      "\t Training loss (single batch): 0.917629063129425\n",
      "\t Training loss (single batch): 1.2766187191009521\n",
      "\t Training loss (single batch): 1.11034095287323\n",
      "\t Training loss (single batch): 1.3508583307266235\n",
      "\t Training loss (single batch): 1.0036799907684326\n",
      "\t Training loss (single batch): 1.3396987915039062\n",
      "\t Training loss (single batch): 1.393297791481018\n",
      "\t Training loss (single batch): 1.0019787549972534\n",
      "\t Training loss (single batch): 1.528303623199463\n",
      "\t Training loss (single batch): 1.218928337097168\n",
      "\t Training loss (single batch): 1.6622345447540283\n",
      "\t Training loss (single batch): 1.1965205669403076\n",
      "\t Training loss (single batch): 1.0051652193069458\n",
      "\t Training loss (single batch): 0.949099063873291\n",
      "\t Training loss (single batch): 1.6923270225524902\n",
      "\t Training loss (single batch): 1.4126200675964355\n",
      "\t Training loss (single batch): 1.0906769037246704\n",
      "\t Training loss (single batch): 1.62527596950531\n",
      "\t Training loss (single batch): 1.3041706085205078\n",
      "\t Training loss (single batch): 1.2594826221466064\n",
      "\t Training loss (single batch): 1.33526611328125\n",
      "\t Training loss (single batch): 1.1903324127197266\n",
      "\t Training loss (single batch): 1.3712228536605835\n",
      "\t Training loss (single batch): 1.2567201852798462\n",
      "\t Training loss (single batch): 1.28096604347229\n",
      "\t Training loss (single batch): 0.6804168224334717\n",
      "\t Training loss (single batch): 1.8910703659057617\n",
      "\t Training loss (single batch): 1.4644354581832886\n",
      "\t Training loss (single batch): 0.9838563799858093\n",
      "\t Training loss (single batch): 1.2522897720336914\n",
      "\t Training loss (single batch): 1.4552595615386963\n",
      "\t Training loss (single batch): 1.2027599811553955\n",
      "\t Training loss (single batch): 1.134958028793335\n",
      "\t Training loss (single batch): 0.7643513083457947\n",
      "\t Training loss (single batch): 0.940290629863739\n",
      "\t Training loss (single batch): 0.944845974445343\n",
      "\t Training loss (single batch): 1.1286792755126953\n",
      "\t Training loss (single batch): 1.0402904748916626\n",
      "\t Training loss (single batch): 1.021053671836853\n",
      "\t Training loss (single batch): 1.2623050212860107\n",
      "\t Training loss (single batch): 1.4326239824295044\n",
      "\t Training loss (single batch): 1.0834873914718628\n",
      "\t Training loss (single batch): 1.0915459394454956\n",
      "\t Training loss (single batch): 0.9342016577720642\n",
      "\t Training loss (single batch): 0.8206918239593506\n",
      "\t Training loss (single batch): 1.0585428476333618\n",
      "\t Training loss (single batch): 1.2828803062438965\n",
      "\t Training loss (single batch): 1.3441554307937622\n",
      "\t Training loss (single batch): 1.0053739547729492\n",
      "\t Training loss (single batch): 1.1052298545837402\n",
      "\t Training loss (single batch): 1.249362587928772\n",
      "\t Training loss (single batch): 1.5475022792816162\n",
      "\t Training loss (single batch): 1.2771575450897217\n",
      "\t Training loss (single batch): 0.9774723052978516\n",
      "\t Training loss (single batch): 1.2180970907211304\n",
      "\t Training loss (single batch): 1.5354180335998535\n",
      "\t Training loss (single batch): 1.3455742597579956\n",
      "\t Training loss (single batch): 1.367925763130188\n",
      "\t Training loss (single batch): 1.0545554161071777\n",
      "\t Training loss (single batch): 1.0577664375305176\n",
      "\t Training loss (single batch): 1.470233678817749\n",
      "\t Training loss (single batch): 1.0653157234191895\n",
      "\t Training loss (single batch): 1.9554133415222168\n",
      "\t Training loss (single batch): 1.0963064432144165\n",
      "\t Training loss (single batch): 1.2551759481430054\n",
      "\t Training loss (single batch): 0.8702586889266968\n",
      "\t Training loss (single batch): 1.5793869495391846\n",
      "\t Training loss (single batch): 1.4361671209335327\n",
      "\t Training loss (single batch): 1.2145968675613403\n",
      "\t Training loss (single batch): 0.917921781539917\n",
      "\t Training loss (single batch): 1.2202867269515991\n",
      "\t Training loss (single batch): 1.2084519863128662\n",
      "\t Training loss (single batch): 1.08378005027771\n",
      "\t Training loss (single batch): 0.8980317115783691\n",
      "\t Training loss (single batch): 1.203777551651001\n",
      "\t Training loss (single batch): 1.057944893836975\n",
      "\t Training loss (single batch): 0.9718273282051086\n",
      "\t Training loss (single batch): 0.9764755964279175\n",
      "\t Training loss (single batch): 1.5864146947860718\n",
      "\t Training loss (single batch): 1.1137077808380127\n",
      "\t Training loss (single batch): 1.15989351272583\n",
      "\t Training loss (single batch): 1.318860411643982\n",
      "\t Training loss (single batch): 1.5914593935012817\n",
      "\t Training loss (single batch): 1.2629820108413696\n",
      "\t Training loss (single batch): 1.1204017400741577\n",
      "\t Training loss (single batch): 1.2506024837493896\n",
      "\t Training loss (single batch): 1.0712426900863647\n",
      "\t Training loss (single batch): 1.1298526525497437\n",
      "\t Training loss (single batch): 0.8322654962539673\n",
      "\t Training loss (single batch): 1.0411293506622314\n",
      "\t Training loss (single batch): 0.9637136459350586\n",
      "\t Training loss (single batch): 1.1880744695663452\n",
      "\t Training loss (single batch): 1.2842167615890503\n",
      "\t Training loss (single batch): 1.0602073669433594\n",
      "\t Training loss (single batch): 1.150758147239685\n",
      "\t Training loss (single batch): 1.3798584938049316\n",
      "\t Training loss (single batch): 1.0922218561172485\n",
      "\t Training loss (single batch): 1.2990367412567139\n",
      "\t Training loss (single batch): 1.1914949417114258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3355826139450073\n",
      "\t Training loss (single batch): 1.6025521755218506\n",
      "\t Training loss (single batch): 1.385751724243164\n",
      "\t Training loss (single batch): 1.2779152393341064\n",
      "\t Training loss (single batch): 0.4312167763710022\n",
      "##################################\n",
      "## EPOCH 97\n",
      "##################################\n",
      "\t Training loss (single batch): 0.878874659538269\n",
      "\t Training loss (single batch): 1.143439531326294\n",
      "\t Training loss (single batch): 1.33822500705719\n",
      "\t Training loss (single batch): 1.0012195110321045\n",
      "\t Training loss (single batch): 1.0294004678726196\n",
      "\t Training loss (single batch): 1.2763152122497559\n",
      "\t Training loss (single batch): 1.4585278034210205\n",
      "\t Training loss (single batch): 1.3660238981246948\n",
      "\t Training loss (single batch): 1.6023298501968384\n",
      "\t Training loss (single batch): 0.8639928698539734\n",
      "\t Training loss (single batch): 1.1419123411178589\n",
      "\t Training loss (single batch): 1.2378989458084106\n",
      "\t Training loss (single batch): 0.9509739875793457\n",
      "\t Training loss (single batch): 1.0486969947814941\n",
      "\t Training loss (single batch): 1.334023356437683\n",
      "\t Training loss (single batch): 1.2223989963531494\n",
      "\t Training loss (single batch): 1.0312590599060059\n",
      "\t Training loss (single batch): 0.9331251978874207\n",
      "\t Training loss (single batch): 0.8307926654815674\n",
      "\t Training loss (single batch): 1.2977019548416138\n",
      "\t Training loss (single batch): 0.8679348826408386\n",
      "\t Training loss (single batch): 1.4618935585021973\n",
      "\t Training loss (single batch): 1.1182992458343506\n",
      "\t Training loss (single batch): 1.3533430099487305\n",
      "\t Training loss (single batch): 1.4640922546386719\n",
      "\t Training loss (single batch): 1.2462787628173828\n",
      "\t Training loss (single batch): 0.8615236282348633\n",
      "\t Training loss (single batch): 1.390566349029541\n",
      "\t Training loss (single batch): 0.7200524210929871\n",
      "\t Training loss (single batch): 1.1544708013534546\n",
      "\t Training loss (single batch): 1.3855282068252563\n",
      "\t Training loss (single batch): 1.0343427658081055\n",
      "\t Training loss (single batch): 1.9027117490768433\n",
      "\t Training loss (single batch): 1.1666054725646973\n",
      "\t Training loss (single batch): 1.4521909952163696\n",
      "\t Training loss (single batch): 0.9597744941711426\n",
      "\t Training loss (single batch): 0.6340672969818115\n",
      "\t Training loss (single batch): 1.6338447332382202\n",
      "\t Training loss (single batch): 1.0191303491592407\n",
      "\t Training loss (single batch): 1.3924373388290405\n",
      "\t Training loss (single batch): 0.8534094095230103\n",
      "\t Training loss (single batch): 1.460713267326355\n",
      "\t Training loss (single batch): 1.0491961240768433\n",
      "\t Training loss (single batch): 1.230320692062378\n",
      "\t Training loss (single batch): 1.4069772958755493\n",
      "\t Training loss (single batch): 0.955303966999054\n",
      "\t Training loss (single batch): 1.282300353050232\n",
      "\t Training loss (single batch): 1.2928950786590576\n",
      "\t Training loss (single batch): 1.1065936088562012\n",
      "\t Training loss (single batch): 1.1738994121551514\n",
      "\t Training loss (single batch): 1.937517762184143\n",
      "\t Training loss (single batch): 1.1105855703353882\n",
      "\t Training loss (single batch): 1.2642333507537842\n",
      "\t Training loss (single batch): 1.5975723266601562\n",
      "\t Training loss (single batch): 1.4109293222427368\n",
      "\t Training loss (single batch): 1.1090304851531982\n",
      "\t Training loss (single batch): 1.4018292427062988\n",
      "\t Training loss (single batch): 1.1644096374511719\n",
      "\t Training loss (single batch): 1.1568089723587036\n",
      "\t Training loss (single batch): 0.8258726596832275\n",
      "\t Training loss (single batch): 1.2425109148025513\n",
      "\t Training loss (single batch): 1.0627031326293945\n",
      "\t Training loss (single batch): 1.4050040245056152\n",
      "\t Training loss (single batch): 1.9001712799072266\n",
      "\t Training loss (single batch): 0.9132825136184692\n",
      "\t Training loss (single batch): 0.8023730516433716\n",
      "\t Training loss (single batch): 1.1471763849258423\n",
      "\t Training loss (single batch): 1.0466195344924927\n",
      "\t Training loss (single batch): 1.057202935218811\n",
      "\t Training loss (single batch): 1.3189733028411865\n",
      "\t Training loss (single batch): 1.0006158351898193\n",
      "\t Training loss (single batch): 1.1262764930725098\n",
      "\t Training loss (single batch): 1.5379737615585327\n",
      "\t Training loss (single batch): 0.7534111738204956\n",
      "\t Training loss (single batch): 0.6885039806365967\n",
      "\t Training loss (single batch): 1.5505495071411133\n",
      "\t Training loss (single batch): 0.908203661441803\n",
      "\t Training loss (single batch): 1.517886757850647\n",
      "\t Training loss (single batch): 1.0130833387374878\n",
      "\t Training loss (single batch): 1.1442995071411133\n",
      "\t Training loss (single batch): 0.9965857267379761\n",
      "\t Training loss (single batch): 1.0041463375091553\n",
      "\t Training loss (single batch): 1.235512614250183\n",
      "\t Training loss (single batch): 1.5892436504364014\n",
      "\t Training loss (single batch): 2.204634428024292\n",
      "\t Training loss (single batch): 1.2945879697799683\n",
      "\t Training loss (single batch): 1.5052919387817383\n",
      "\t Training loss (single batch): 1.0194720029830933\n",
      "\t Training loss (single batch): 0.9918337464332581\n",
      "\t Training loss (single batch): 1.4394744634628296\n",
      "\t Training loss (single batch): 1.045649766921997\n",
      "\t Training loss (single batch): 1.2837814092636108\n",
      "\t Training loss (single batch): 0.9199228882789612\n",
      "\t Training loss (single batch): 1.1189104318618774\n",
      "\t Training loss (single batch): 1.308676838874817\n",
      "\t Training loss (single batch): 1.3655630350112915\n",
      "\t Training loss (single batch): 1.210952639579773\n",
      "\t Training loss (single batch): 1.0423533916473389\n",
      "\t Training loss (single batch): 1.4226945638656616\n",
      "\t Training loss (single batch): 1.3753873109817505\n",
      "\t Training loss (single batch): 1.5177630186080933\n",
      "\t Training loss (single batch): 1.0194892883300781\n",
      "\t Training loss (single batch): 1.7278616428375244\n",
      "\t Training loss (single batch): 0.8434463739395142\n",
      "\t Training loss (single batch): 1.3365901708602905\n",
      "\t Training loss (single batch): 0.8855299949645996\n",
      "\t Training loss (single batch): 1.3241463899612427\n",
      "\t Training loss (single batch): 1.3321527242660522\n",
      "\t Training loss (single batch): 0.888899564743042\n",
      "\t Training loss (single batch): 1.2134140729904175\n",
      "\t Training loss (single batch): 1.2305070161819458\n",
      "\t Training loss (single batch): 1.447762131690979\n",
      "\t Training loss (single batch): 1.517704963684082\n",
      "\t Training loss (single batch): 1.3853703737258911\n",
      "\t Training loss (single batch): 1.0822772979736328\n",
      "\t Training loss (single batch): 1.2957926988601685\n",
      "\t Training loss (single batch): 1.0395950078964233\n",
      "\t Training loss (single batch): 1.0127339363098145\n",
      "\t Training loss (single batch): 1.088749885559082\n",
      "\t Training loss (single batch): 0.9713048934936523\n",
      "\t Training loss (single batch): 0.7919641733169556\n",
      "\t Training loss (single batch): 1.2283806800842285\n",
      "\t Training loss (single batch): 1.041624665260315\n",
      "\t Training loss (single batch): 1.190663456916809\n",
      "\t Training loss (single batch): 1.0540355443954468\n",
      "\t Training loss (single batch): 0.8400056958198547\n",
      "\t Training loss (single batch): 1.489964246749878\n",
      "\t Training loss (single batch): 0.8794687390327454\n",
      "\t Training loss (single batch): 1.3887118101119995\n",
      "\t Training loss (single batch): 1.079980492591858\n",
      "\t Training loss (single batch): 1.1346484422683716\n",
      "\t Training loss (single batch): 1.2170695066452026\n",
      "\t Training loss (single batch): 1.0125848054885864\n",
      "\t Training loss (single batch): 1.1361989974975586\n",
      "\t Training loss (single batch): 1.4503191709518433\n",
      "\t Training loss (single batch): 0.8728338479995728\n",
      "\t Training loss (single batch): 1.246193528175354\n",
      "\t Training loss (single batch): 1.1203184127807617\n",
      "\t Training loss (single batch): 1.0196102857589722\n",
      "\t Training loss (single batch): 1.1357226371765137\n",
      "\t Training loss (single batch): 1.2642019987106323\n",
      "\t Training loss (single batch): 0.9485836029052734\n",
      "\t Training loss (single batch): 1.4903074502944946\n",
      "\t Training loss (single batch): 1.0811400413513184\n",
      "\t Training loss (single batch): 1.2139571905136108\n",
      "\t Training loss (single batch): 1.4246402978897095\n",
      "\t Training loss (single batch): 0.9246059060096741\n",
      "\t Training loss (single batch): 1.4510102272033691\n",
      "\t Training loss (single batch): 1.130915880203247\n",
      "\t Training loss (single batch): 1.0183188915252686\n",
      "\t Training loss (single batch): 1.3681834936141968\n",
      "\t Training loss (single batch): 1.053105354309082\n",
      "\t Training loss (single batch): 1.2119590044021606\n",
      "\t Training loss (single batch): 1.416632056236267\n",
      "\t Training loss (single batch): 1.0988569259643555\n",
      "\t Training loss (single batch): 1.021026849746704\n",
      "\t Training loss (single batch): 0.8163363933563232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.3013758659362793\n",
      "\t Training loss (single batch): 1.081723690032959\n",
      "\t Training loss (single batch): 1.5339490175247192\n",
      "\t Training loss (single batch): 1.351961612701416\n",
      "\t Training loss (single batch): 1.0702764987945557\n",
      "\t Training loss (single batch): 1.022630214691162\n",
      "\t Training loss (single batch): 1.1985137462615967\n",
      "\t Training loss (single batch): 1.3875776529312134\n",
      "\t Training loss (single batch): 0.8080635070800781\n",
      "\t Training loss (single batch): 1.220070481300354\n",
      "\t Training loss (single batch): 0.9314050674438477\n",
      "\t Training loss (single batch): 1.2314627170562744\n",
      "\t Training loss (single batch): 1.217640995979309\n",
      "\t Training loss (single batch): 1.1772067546844482\n",
      "\t Training loss (single batch): 1.7140357494354248\n",
      "\t Training loss (single batch): 1.3043400049209595\n",
      "\t Training loss (single batch): 1.0157485008239746\n",
      "\t Training loss (single batch): 1.0596997737884521\n",
      "\t Training loss (single batch): 1.187583565711975\n",
      "\t Training loss (single batch): 1.089462161064148\n",
      "\t Training loss (single batch): 1.1605597734451294\n",
      "\t Training loss (single batch): 1.1081762313842773\n",
      "\t Training loss (single batch): 1.2689496278762817\n",
      "\t Training loss (single batch): 0.9916537404060364\n",
      "\t Training loss (single batch): 0.9483761191368103\n",
      "\t Training loss (single batch): 1.1329165697097778\n",
      "\t Training loss (single batch): 1.4195517301559448\n",
      "\t Training loss (single batch): 1.3365492820739746\n",
      "\t Training loss (single batch): 1.334413766860962\n",
      "\t Training loss (single batch): 1.73086416721344\n",
      "\t Training loss (single batch): 1.3834919929504395\n",
      "\t Training loss (single batch): 2.1005496978759766\n",
      "\t Training loss (single batch): 0.7169598937034607\n",
      "\t Training loss (single batch): 1.02003812789917\n",
      "\t Training loss (single batch): 1.080346703529358\n",
      "\t Training loss (single batch): 1.2838506698608398\n",
      "\t Training loss (single batch): 1.5992752313613892\n",
      "\t Training loss (single batch): 1.2065191268920898\n",
      "\t Training loss (single batch): 1.1584206819534302\n",
      "\t Training loss (single batch): 1.4680753946304321\n",
      "\t Training loss (single batch): 1.138558268547058\n",
      "\t Training loss (single batch): 0.9925199151039124\n",
      "\t Training loss (single batch): 1.1632840633392334\n",
      "\t Training loss (single batch): 1.073168158531189\n",
      "\t Training loss (single batch): 2.015350580215454\n",
      "\t Training loss (single batch): 1.390931248664856\n",
      "\t Training loss (single batch): 1.1028751134872437\n",
      "\t Training loss (single batch): 1.1123806238174438\n",
      "\t Training loss (single batch): 0.9510713219642639\n",
      "\t Training loss (single batch): 0.6487900614738464\n",
      "\t Training loss (single batch): 0.6679479479789734\n",
      "\t Training loss (single batch): 2.0965137481689453\n",
      "\t Training loss (single batch): 1.0760297775268555\n",
      "\t Training loss (single batch): 1.0083519220352173\n",
      "\t Training loss (single batch): 1.149987816810608\n",
      "\t Training loss (single batch): 0.9137660264968872\n",
      "\t Training loss (single batch): 1.561653971672058\n",
      "\t Training loss (single batch): 1.3130422830581665\n",
      "\t Training loss (single batch): 1.3014644384384155\n",
      "\t Training loss (single batch): 0.9474776983261108\n",
      "\t Training loss (single batch): 1.5253455638885498\n",
      "\t Training loss (single batch): 0.9134839177131653\n",
      "\t Training loss (single batch): 0.9167523384094238\n",
      "\t Training loss (single batch): 0.9333436489105225\n",
      "\t Training loss (single batch): 1.2267197370529175\n",
      "\t Training loss (single batch): 1.6521801948547363\n",
      "\t Training loss (single batch): 0.9006600379943848\n",
      "\t Training loss (single batch): 0.9566711187362671\n",
      "\t Training loss (single batch): 1.468435287475586\n",
      "\t Training loss (single batch): 1.240414023399353\n",
      "\t Training loss (single batch): 1.2148189544677734\n",
      "\t Training loss (single batch): 1.289795994758606\n",
      "\t Training loss (single batch): 0.7681152820587158\n",
      "\t Training loss (single batch): 1.6445411443710327\n",
      "\t Training loss (single batch): 1.567997694015503\n",
      "\t Training loss (single batch): 1.5058401823043823\n",
      "\t Training loss (single batch): 1.4300925731658936\n",
      "\t Training loss (single batch): 1.0664608478546143\n",
      "\t Training loss (single batch): 1.568289041519165\n",
      "\t Training loss (single batch): 1.1464192867279053\n",
      "\t Training loss (single batch): 0.9338634610176086\n",
      "\t Training loss (single batch): 0.8280436992645264\n",
      "\t Training loss (single batch): 1.126171350479126\n",
      "\t Training loss (single batch): 1.6778267621994019\n",
      "\t Training loss (single batch): 1.0073752403259277\n",
      "\t Training loss (single batch): 0.9305561780929565\n",
      "\t Training loss (single batch): 1.043082594871521\n",
      "\t Training loss (single batch): 1.5216498374938965\n",
      "\t Training loss (single batch): 1.300206184387207\n",
      "\t Training loss (single batch): 1.2354919910430908\n",
      "\t Training loss (single batch): 1.0640418529510498\n",
      "\t Training loss (single batch): 1.314964771270752\n",
      "\t Training loss (single batch): 1.1438205242156982\n",
      "\t Training loss (single batch): 0.6903728246688843\n",
      "\t Training loss (single batch): 1.1638160943984985\n",
      "\t Training loss (single batch): 1.3482859134674072\n",
      "\t Training loss (single batch): 1.8276269435882568\n",
      "\t Training loss (single batch): 0.8895043730735779\n",
      "\t Training loss (single batch): 1.265526294708252\n",
      "\t Training loss (single batch): 1.0137864351272583\n",
      "\t Training loss (single batch): 0.8727726936340332\n",
      "\t Training loss (single batch): 1.0380375385284424\n",
      "\t Training loss (single batch): 1.3006855249404907\n",
      "\t Training loss (single batch): 1.2803311347961426\n",
      "\t Training loss (single batch): 0.7454321980476379\n",
      "\t Training loss (single batch): 1.1910874843597412\n",
      "\t Training loss (single batch): 1.2307802438735962\n",
      "\t Training loss (single batch): 1.5245530605316162\n",
      "\t Training loss (single batch): 0.9579337239265442\n",
      "\t Training loss (single batch): 1.390444040298462\n",
      "\t Training loss (single batch): 1.0179331302642822\n",
      "\t Training loss (single batch): 1.098149299621582\n",
      "\t Training loss (single batch): 1.1016905307769775\n",
      "\t Training loss (single batch): 1.66421377658844\n",
      "\t Training loss (single batch): 1.7305113077163696\n",
      "\t Training loss (single batch): 0.8791428208351135\n",
      "\t Training loss (single batch): 1.4506109952926636\n",
      "\t Training loss (single batch): 1.4879341125488281\n",
      "\t Training loss (single batch): 1.165936827659607\n",
      "\t Training loss (single batch): 1.3563545942306519\n",
      "\t Training loss (single batch): 0.9642003178596497\n",
      "\t Training loss (single batch): 1.278493881225586\n",
      "\t Training loss (single batch): 1.1700321435928345\n",
      "\t Training loss (single batch): 1.486081600189209\n",
      "\t Training loss (single batch): 1.380204200744629\n",
      "\t Training loss (single batch): 1.191153883934021\n",
      "\t Training loss (single batch): 1.4282948970794678\n",
      "\t Training loss (single batch): 0.8908274173736572\n",
      "\t Training loss (single batch): 0.7209300994873047\n",
      "\t Training loss (single batch): 1.0111455917358398\n",
      "\t Training loss (single batch): 1.3511868715286255\n",
      "\t Training loss (single batch): 1.2518879175186157\n",
      "\t Training loss (single batch): 1.059584617614746\n",
      "\t Training loss (single batch): 1.8668781518936157\n",
      "\t Training loss (single batch): 1.4711928367614746\n",
      "\t Training loss (single batch): 1.1911725997924805\n",
      "\t Training loss (single batch): 1.287404179573059\n",
      "\t Training loss (single batch): 1.6263072490692139\n",
      "\t Training loss (single batch): 1.167055606842041\n",
      "\t Training loss (single batch): 1.0998973846435547\n",
      "\t Training loss (single batch): 0.9786709547042847\n",
      "\t Training loss (single batch): 1.781921625137329\n",
      "\t Training loss (single batch): 1.207443118095398\n",
      "\t Training loss (single batch): 0.9354100823402405\n",
      "\t Training loss (single batch): 1.323928952217102\n",
      "\t Training loss (single batch): 1.258170485496521\n",
      "\t Training loss (single batch): 1.5848195552825928\n",
      "\t Training loss (single batch): 1.3704944849014282\n",
      "\t Training loss (single batch): 1.3397722244262695\n",
      "\t Training loss (single batch): 0.7224451303482056\n",
      "\t Training loss (single batch): 1.5516904592514038\n",
      "\t Training loss (single batch): 1.1954766511917114\n",
      "\t Training loss (single batch): 1.0348538160324097\n",
      "\t Training loss (single batch): 0.7898142337799072\n",
      "\t Training loss (single batch): 0.9661239385604858\n",
      "\t Training loss (single batch): 1.3609929084777832\n",
      "\t Training loss (single batch): 1.2987209558486938\n",
      "\t Training loss (single batch): 0.7208253741264343\n",
      "\t Training loss (single batch): 1.5146647691726685\n",
      "\t Training loss (single batch): 0.9910849332809448\n",
      "##################################\n",
      "## EPOCH 98\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7331427335739136\n",
      "\t Training loss (single batch): 0.8605358600616455\n",
      "\t Training loss (single batch): 1.1935033798217773\n",
      "\t Training loss (single batch): 1.6307801008224487\n",
      "\t Training loss (single batch): 0.9883414506912231\n",
      "\t Training loss (single batch): 1.0420317649841309\n",
      "\t Training loss (single batch): 1.2707628011703491\n",
      "\t Training loss (single batch): 0.8130844235420227\n",
      "\t Training loss (single batch): 1.634833812713623\n",
      "\t Training loss (single batch): 1.9061578512191772\n",
      "\t Training loss (single batch): 0.9080182909965515\n",
      "\t Training loss (single batch): 1.2698993682861328\n",
      "\t Training loss (single batch): 0.7332834005355835\n",
      "\t Training loss (single batch): 1.0291664600372314\n",
      "\t Training loss (single batch): 1.6681585311889648\n",
      "\t Training loss (single batch): 0.7460886836051941\n",
      "\t Training loss (single batch): 0.7508043050765991\n",
      "\t Training loss (single batch): 1.6693204641342163\n",
      "\t Training loss (single batch): 0.9368446469306946\n",
      "\t Training loss (single batch): 1.5770277976989746\n",
      "\t Training loss (single batch): 1.2433748245239258\n",
      "\t Training loss (single batch): 0.8575847148895264\n",
      "\t Training loss (single batch): 0.9690293669700623\n",
      "\t Training loss (single batch): 0.8054126501083374\n",
      "\t Training loss (single batch): 0.7793830037117004\n",
      "\t Training loss (single batch): 1.0785096883773804\n",
      "\t Training loss (single batch): 1.4181675910949707\n",
      "\t Training loss (single batch): 0.8234342336654663\n",
      "\t Training loss (single batch): 0.9152795672416687\n",
      "\t Training loss (single batch): 0.9642981886863708\n",
      "\t Training loss (single batch): 1.208225131034851\n",
      "\t Training loss (single batch): 1.6923350095748901\n",
      "\t Training loss (single batch): 1.1375681161880493\n",
      "\t Training loss (single batch): 0.9981379508972168\n",
      "\t Training loss (single batch): 1.527958869934082\n",
      "\t Training loss (single batch): 1.103906273841858\n",
      "\t Training loss (single batch): 1.336242437362671\n",
      "\t Training loss (single batch): 1.4961403608322144\n",
      "\t Training loss (single batch): 1.075813889503479\n",
      "\t Training loss (single batch): 1.262445092201233\n",
      "\t Training loss (single batch): 0.9146236777305603\n",
      "\t Training loss (single batch): 1.5060473680496216\n",
      "\t Training loss (single batch): 1.317919373512268\n",
      "\t Training loss (single batch): 1.2242807149887085\n",
      "\t Training loss (single batch): 2.0681188106536865\n",
      "\t Training loss (single batch): 1.2594351768493652\n",
      "\t Training loss (single batch): 1.4199618101119995\n",
      "\t Training loss (single batch): 1.6159244775772095\n",
      "\t Training loss (single batch): 1.1167504787445068\n",
      "\t Training loss (single batch): 1.0728404521942139\n",
      "\t Training loss (single batch): 1.5273702144622803\n",
      "\t Training loss (single batch): 1.4848322868347168\n",
      "\t Training loss (single batch): 1.6855874061584473\n",
      "\t Training loss (single batch): 1.077424168586731\n",
      "\t Training loss (single batch): 1.2539739608764648\n",
      "\t Training loss (single batch): 1.0696371793746948\n",
      "\t Training loss (single batch): 1.5585896968841553\n",
      "\t Training loss (single batch): 1.2180553674697876\n",
      "\t Training loss (single batch): 1.2425742149353027\n",
      "\t Training loss (single batch): 1.0766316652297974\n",
      "\t Training loss (single batch): 1.1964969635009766\n",
      "\t Training loss (single batch): 0.6681495308876038\n",
      "\t Training loss (single batch): 1.4472936391830444\n",
      "\t Training loss (single batch): 1.0914592742919922\n",
      "\t Training loss (single batch): 1.1017615795135498\n",
      "\t Training loss (single batch): 0.9766568541526794\n",
      "\t Training loss (single batch): 0.8928909301757812\n",
      "\t Training loss (single batch): 1.176261305809021\n",
      "\t Training loss (single batch): 1.3273645639419556\n",
      "\t Training loss (single batch): 0.8346071839332581\n",
      "\t Training loss (single batch): 0.8064889311790466\n",
      "\t Training loss (single batch): 1.6158783435821533\n",
      "\t Training loss (single batch): 1.6924660205841064\n",
      "\t Training loss (single batch): 1.05375075340271\n",
      "\t Training loss (single batch): 1.0063591003417969\n",
      "\t Training loss (single batch): 1.3553764820098877\n",
      "\t Training loss (single batch): 1.0504424571990967\n",
      "\t Training loss (single batch): 1.186240792274475\n",
      "\t Training loss (single batch): 1.0836052894592285\n",
      "\t Training loss (single batch): 0.8633377552032471\n",
      "\t Training loss (single batch): 1.2891736030578613\n",
      "\t Training loss (single batch): 1.3517459630966187\n",
      "\t Training loss (single batch): 0.8509936332702637\n",
      "\t Training loss (single batch): 1.2888600826263428\n",
      "\t Training loss (single batch): 1.082196831703186\n",
      "\t Training loss (single batch): 0.8354933261871338\n",
      "\t Training loss (single batch): 1.073839783668518\n",
      "\t Training loss (single batch): 1.1184351444244385\n",
      "\t Training loss (single batch): 1.4441237449645996\n",
      "\t Training loss (single batch): 1.1760375499725342\n",
      "\t Training loss (single batch): 1.2276302576065063\n",
      "\t Training loss (single batch): 1.0866718292236328\n",
      "\t Training loss (single batch): 1.0526996850967407\n",
      "\t Training loss (single batch): 1.0855036973953247\n",
      "\t Training loss (single batch): 0.8768190741539001\n",
      "\t Training loss (single batch): 0.8158140182495117\n",
      "\t Training loss (single batch): 1.2827905416488647\n",
      "\t Training loss (single batch): 1.2680538892745972\n",
      "\t Training loss (single batch): 1.1397777795791626\n",
      "\t Training loss (single batch): 1.041043996810913\n",
      "\t Training loss (single batch): 1.2607340812683105\n",
      "\t Training loss (single batch): 0.7711930871009827\n",
      "\t Training loss (single batch): 0.7962139248847961\n",
      "\t Training loss (single batch): 1.707572102546692\n",
      "\t Training loss (single batch): 1.3716278076171875\n",
      "\t Training loss (single batch): 1.548214316368103\n",
      "\t Training loss (single batch): 1.1250958442687988\n",
      "\t Training loss (single batch): 1.2235159873962402\n",
      "\t Training loss (single batch): 1.2408419847488403\n",
      "\t Training loss (single batch): 1.5509061813354492\n",
      "\t Training loss (single batch): 1.2551578283309937\n",
      "\t Training loss (single batch): 1.063128113746643\n",
      "\t Training loss (single batch): 0.8898012638092041\n",
      "\t Training loss (single batch): 1.1919389963150024\n",
      "\t Training loss (single batch): 1.2573659420013428\n",
      "\t Training loss (single batch): 0.8598446846008301\n",
      "\t Training loss (single batch): 0.9562159776687622\n",
      "\t Training loss (single batch): 1.1565214395523071\n",
      "\t Training loss (single batch): 0.8781958222389221\n",
      "\t Training loss (single batch): 1.0020320415496826\n",
      "\t Training loss (single batch): 1.527780294418335\n",
      "\t Training loss (single batch): 1.0475120544433594\n",
      "\t Training loss (single batch): 1.0736048221588135\n",
      "\t Training loss (single batch): 0.9550599455833435\n",
      "\t Training loss (single batch): 0.9976240396499634\n",
      "\t Training loss (single batch): 1.0514298677444458\n",
      "\t Training loss (single batch): 0.7855736017227173\n",
      "\t Training loss (single batch): 1.215326189994812\n",
      "\t Training loss (single batch): 1.3068944215774536\n",
      "\t Training loss (single batch): 1.1293699741363525\n",
      "\t Training loss (single batch): 1.6393731832504272\n",
      "\t Training loss (single batch): 0.8515844941139221\n",
      "\t Training loss (single batch): 1.595076084136963\n",
      "\t Training loss (single batch): 1.112344741821289\n",
      "\t Training loss (single batch): 1.6097978353500366\n",
      "\t Training loss (single batch): 1.0472660064697266\n",
      "\t Training loss (single batch): 1.4585646390914917\n",
      "\t Training loss (single batch): 1.540895938873291\n",
      "\t Training loss (single batch): 0.9891997575759888\n",
      "\t Training loss (single batch): 0.830115556716919\n",
      "\t Training loss (single batch): 1.60122811794281\n",
      "\t Training loss (single batch): 1.0500494241714478\n",
      "\t Training loss (single batch): 1.113750696182251\n",
      "\t Training loss (single batch): 0.8398366570472717\n",
      "\t Training loss (single batch): 0.6438072919845581\n",
      "\t Training loss (single batch): 1.2107352018356323\n",
      "\t Training loss (single batch): 1.6860171556472778\n",
      "\t Training loss (single batch): 1.0406520366668701\n",
      "\t Training loss (single batch): 1.5570204257965088\n",
      "\t Training loss (single batch): 1.185376763343811\n",
      "\t Training loss (single batch): 1.1594555377960205\n",
      "\t Training loss (single batch): 0.7912318110466003\n",
      "\t Training loss (single batch): 1.404722809791565\n",
      "\t Training loss (single batch): 1.4102649688720703\n",
      "\t Training loss (single batch): 1.1361113786697388\n",
      "\t Training loss (single batch): 1.692404866218567\n",
      "\t Training loss (single batch): 0.6832764744758606\n",
      "\t Training loss (single batch): 1.2760136127471924\n",
      "\t Training loss (single batch): 0.8378911018371582\n",
      "\t Training loss (single batch): 1.1528619527816772\n",
      "\t Training loss (single batch): 0.7978312373161316\n",
      "\t Training loss (single batch): 0.9964335560798645\n",
      "\t Training loss (single batch): 0.7267129421234131\n",
      "\t Training loss (single batch): 0.8897810578346252\n",
      "\t Training loss (single batch): 1.5211305618286133\n",
      "\t Training loss (single batch): 1.8140145540237427\n",
      "\t Training loss (single batch): 0.9343193173408508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.5813297033309937\n",
      "\t Training loss (single batch): 1.4479475021362305\n",
      "\t Training loss (single batch): 1.0716735124588013\n",
      "\t Training loss (single batch): 0.8610594868659973\n",
      "\t Training loss (single batch): 1.6506998538970947\n",
      "\t Training loss (single batch): 0.9466586112976074\n",
      "\t Training loss (single batch): 1.3019905090332031\n",
      "\t Training loss (single batch): 1.485424280166626\n",
      "\t Training loss (single batch): 1.1667884588241577\n",
      "\t Training loss (single batch): 1.1526814699172974\n",
      "\t Training loss (single batch): 0.9595087170600891\n",
      "\t Training loss (single batch): 1.1894029378890991\n",
      "\t Training loss (single batch): 1.246029019355774\n",
      "\t Training loss (single batch): 1.074603796005249\n",
      "\t Training loss (single batch): 1.260582447052002\n",
      "\t Training loss (single batch): 1.1635775566101074\n",
      "\t Training loss (single batch): 1.4237319231033325\n",
      "\t Training loss (single batch): 1.2370513677597046\n",
      "\t Training loss (single batch): 0.8445267081260681\n",
      "\t Training loss (single batch): 1.141859769821167\n",
      "\t Training loss (single batch): 0.9354454278945923\n",
      "\t Training loss (single batch): 1.137668251991272\n",
      "\t Training loss (single batch): 1.1090291738510132\n",
      "\t Training loss (single batch): 0.9529956579208374\n",
      "\t Training loss (single batch): 1.1184139251708984\n",
      "\t Training loss (single batch): 1.4079681634902954\n",
      "\t Training loss (single batch): 1.2648258209228516\n",
      "\t Training loss (single batch): 1.0711876153945923\n",
      "\t Training loss (single batch): 1.1018308401107788\n",
      "\t Training loss (single batch): 1.2436543703079224\n",
      "\t Training loss (single batch): 1.4044851064682007\n",
      "\t Training loss (single batch): 1.0538705587387085\n",
      "\t Training loss (single batch): 1.3071749210357666\n",
      "\t Training loss (single batch): 0.9838727712631226\n",
      "\t Training loss (single batch): 1.260324239730835\n",
      "\t Training loss (single batch): 0.8998085856437683\n",
      "\t Training loss (single batch): 1.3717457056045532\n",
      "\t Training loss (single batch): 1.1152704954147339\n",
      "\t Training loss (single batch): 1.0204970836639404\n",
      "\t Training loss (single batch): 1.2158634662628174\n",
      "\t Training loss (single batch): 0.7585815191268921\n",
      "\t Training loss (single batch): 1.1378787755966187\n",
      "\t Training loss (single batch): 1.3192229270935059\n",
      "\t Training loss (single batch): 1.3614311218261719\n",
      "\t Training loss (single batch): 1.4139182567596436\n",
      "\t Training loss (single batch): 1.0414797067642212\n",
      "\t Training loss (single batch): 1.5860086679458618\n",
      "\t Training loss (single batch): 1.315922737121582\n",
      "\t Training loss (single batch): 1.4367802143096924\n",
      "\t Training loss (single batch): 1.651184320449829\n",
      "\t Training loss (single batch): 1.192981243133545\n",
      "\t Training loss (single batch): 1.4160103797912598\n",
      "\t Training loss (single batch): 0.8888534903526306\n",
      "\t Training loss (single batch): 1.0595184564590454\n",
      "\t Training loss (single batch): 1.410461187362671\n",
      "\t Training loss (single batch): 1.2270246744155884\n",
      "\t Training loss (single batch): 1.1772969961166382\n",
      "\t Training loss (single batch): 0.7381468415260315\n",
      "\t Training loss (single batch): 0.993167519569397\n",
      "\t Training loss (single batch): 1.396068811416626\n",
      "\t Training loss (single batch): 1.1446051597595215\n",
      "\t Training loss (single batch): 1.0781246423721313\n",
      "\t Training loss (single batch): 1.1176048517227173\n",
      "\t Training loss (single batch): 1.805680274963379\n",
      "\t Training loss (single batch): 1.5286028385162354\n",
      "\t Training loss (single batch): 1.5650743246078491\n",
      "\t Training loss (single batch): 0.929053544998169\n",
      "\t Training loss (single batch): 1.0397768020629883\n",
      "\t Training loss (single batch): 1.2096867561340332\n",
      "\t Training loss (single batch): 1.4260410070419312\n",
      "\t Training loss (single batch): 1.3695868253707886\n",
      "\t Training loss (single batch): 1.458134412765503\n",
      "\t Training loss (single batch): 1.186804175376892\n",
      "\t Training loss (single batch): 1.06963312625885\n",
      "\t Training loss (single batch): 1.7119460105895996\n",
      "\t Training loss (single batch): 1.2182469367980957\n",
      "\t Training loss (single batch): 1.28831946849823\n",
      "\t Training loss (single batch): 1.1987396478652954\n",
      "\t Training loss (single batch): 1.2501682043075562\n",
      "\t Training loss (single batch): 1.2056885957717896\n",
      "\t Training loss (single batch): 1.1777054071426392\n",
      "\t Training loss (single batch): 1.3682327270507812\n",
      "\t Training loss (single batch): 1.2356083393096924\n",
      "\t Training loss (single batch): 1.6820759773254395\n",
      "\t Training loss (single batch): 0.8356153964996338\n",
      "\t Training loss (single batch): 1.2968065738677979\n",
      "\t Training loss (single batch): 1.2436408996582031\n",
      "\t Training loss (single batch): 1.8105673789978027\n",
      "\t Training loss (single batch): 1.0315864086151123\n",
      "\t Training loss (single batch): 1.5584861040115356\n",
      "\t Training loss (single batch): 1.4585398435592651\n",
      "\t Training loss (single batch): 1.1038769483566284\n",
      "\t Training loss (single batch): 1.3190209865570068\n",
      "\t Training loss (single batch): 1.411102056503296\n",
      "\t Training loss (single batch): 1.7138361930847168\n",
      "\t Training loss (single batch): 1.1465245485305786\n",
      "\t Training loss (single batch): 1.5822430849075317\n",
      "\t Training loss (single batch): 1.0609146356582642\n",
      "\t Training loss (single batch): 1.6857576370239258\n",
      "\t Training loss (single batch): 0.7585287690162659\n",
      "\t Training loss (single batch): 1.4803262948989868\n",
      "\t Training loss (single batch): 0.6760132312774658\n",
      "\t Training loss (single batch): 1.3406001329421997\n",
      "\t Training loss (single batch): 1.0747995376586914\n",
      "\t Training loss (single batch): 1.0676904916763306\n",
      "\t Training loss (single batch): 1.1579030752182007\n",
      "\t Training loss (single batch): 1.6147055625915527\n",
      "\t Training loss (single batch): 1.2057056427001953\n",
      "\t Training loss (single batch): 0.9260076284408569\n",
      "\t Training loss (single batch): 1.013205885887146\n",
      "\t Training loss (single batch): 1.0292917490005493\n",
      "\t Training loss (single batch): 0.8870282769203186\n",
      "\t Training loss (single batch): 1.0087053775787354\n",
      "\t Training loss (single batch): 1.327455997467041\n",
      "\t Training loss (single batch): 1.4454375505447388\n",
      "\t Training loss (single batch): 1.107051134109497\n",
      "\t Training loss (single batch): 1.1130462884902954\n",
      "\t Training loss (single batch): 1.206302285194397\n",
      "\t Training loss (single batch): 1.0132976770401\n",
      "\t Training loss (single batch): 1.2490824460983276\n",
      "\t Training loss (single batch): 1.2125556468963623\n",
      "\t Training loss (single batch): 1.0994210243225098\n",
      "\t Training loss (single batch): 1.2249201536178589\n",
      "\t Training loss (single batch): 1.1075574159622192\n",
      "\t Training loss (single batch): 1.1690658330917358\n",
      "\t Training loss (single batch): 1.537830114364624\n",
      "\t Training loss (single batch): 1.2399516105651855\n",
      "\t Training loss (single batch): 1.3277387619018555\n",
      "\t Training loss (single batch): 0.8240305185317993\n",
      "\t Training loss (single batch): 0.9080944061279297\n",
      "\t Training loss (single batch): 0.987063467502594\n",
      "\t Training loss (single batch): 1.3358043432235718\n",
      "\t Training loss (single batch): 1.3418503999710083\n",
      "\t Training loss (single batch): 0.6701034307479858\n",
      "\t Training loss (single batch): 1.0946406126022339\n",
      "\t Training loss (single batch): 1.3614158630371094\n",
      "\t Training loss (single batch): 1.0267527103424072\n",
      "\t Training loss (single batch): 1.3867602348327637\n",
      "\t Training loss (single batch): 2.066925048828125\n",
      "\t Training loss (single batch): 0.9829997420310974\n",
      "\t Training loss (single batch): 1.4083330631256104\n",
      "\t Training loss (single batch): 1.099534034729004\n",
      "\t Training loss (single batch): 0.7916988134384155\n",
      "\t Training loss (single batch): 0.7726920247077942\n",
      "\t Training loss (single batch): 1.0553350448608398\n",
      "\t Training loss (single batch): 1.411324143409729\n",
      "\t Training loss (single batch): 1.374074935913086\n",
      "\t Training loss (single batch): 1.3698707818984985\n",
      "\t Training loss (single batch): 1.630810022354126\n",
      "\t Training loss (single batch): 0.7341949939727783\n",
      "##################################\n",
      "## EPOCH 99\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5171656608581543\n",
      "\t Training loss (single batch): 1.05617356300354\n",
      "\t Training loss (single batch): 1.1767174005508423\n",
      "\t Training loss (single batch): 1.5119993686676025\n",
      "\t Training loss (single batch): 1.1515889167785645\n",
      "\t Training loss (single batch): 1.8398702144622803\n",
      "\t Training loss (single batch): 1.2690963745117188\n",
      "\t Training loss (single batch): 1.5084214210510254\n",
      "\t Training loss (single batch): 1.368241310119629\n",
      "\t Training loss (single batch): 1.149020791053772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.9976332783699036\n",
      "\t Training loss (single batch): 1.2763599157333374\n",
      "\t Training loss (single batch): 1.0779597759246826\n",
      "\t Training loss (single batch): 1.2553049325942993\n",
      "\t Training loss (single batch): 1.4102733135223389\n",
      "\t Training loss (single batch): 1.0876948833465576\n",
      "\t Training loss (single batch): 1.418114185333252\n",
      "\t Training loss (single batch): 1.5859321355819702\n",
      "\t Training loss (single batch): 0.9674158096313477\n",
      "\t Training loss (single batch): 1.2174506187438965\n",
      "\t Training loss (single batch): 1.8446677923202515\n",
      "\t Training loss (single batch): 1.4392290115356445\n",
      "\t Training loss (single batch): 1.8030732870101929\n",
      "\t Training loss (single batch): 1.3520504236221313\n",
      "\t Training loss (single batch): 1.1530784368515015\n",
      "\t Training loss (single batch): 1.1120349168777466\n",
      "\t Training loss (single batch): 1.1104422807693481\n",
      "\t Training loss (single batch): 1.0262442827224731\n",
      "\t Training loss (single batch): 1.1588208675384521\n",
      "\t Training loss (single batch): 1.6087769269943237\n",
      "\t Training loss (single batch): 1.341697096824646\n",
      "\t Training loss (single batch): 0.8716577291488647\n",
      "\t Training loss (single batch): 1.0747712850570679\n",
      "\t Training loss (single batch): 1.0742902755737305\n",
      "\t Training loss (single batch): 1.43771493434906\n",
      "\t Training loss (single batch): 0.8414193391799927\n",
      "\t Training loss (single batch): 1.727034568786621\n",
      "\t Training loss (single batch): 0.8880184292793274\n",
      "\t Training loss (single batch): 1.1085317134857178\n",
      "\t Training loss (single batch): 1.2348089218139648\n",
      "\t Training loss (single batch): 1.0387974977493286\n",
      "\t Training loss (single batch): 1.8075255155563354\n",
      "\t Training loss (single batch): 1.0594489574432373\n",
      "\t Training loss (single batch): 1.3114372491836548\n",
      "\t Training loss (single batch): 1.2973017692565918\n",
      "\t Training loss (single batch): 1.483770728111267\n",
      "\t Training loss (single batch): 1.0219255685806274\n",
      "\t Training loss (single batch): 1.139217495918274\n",
      "\t Training loss (single batch): 0.9991137385368347\n",
      "\t Training loss (single batch): 1.0057047605514526\n",
      "\t Training loss (single batch): 1.260635256767273\n",
      "\t Training loss (single batch): 1.5553793907165527\n",
      "\t Training loss (single batch): 0.9930760860443115\n",
      "\t Training loss (single batch): 1.2101101875305176\n",
      "\t Training loss (single batch): 1.0882365703582764\n",
      "\t Training loss (single batch): 1.0358127355575562\n",
      "\t Training loss (single batch): 0.9410348534584045\n",
      "\t Training loss (single batch): 0.7978353500366211\n",
      "\t Training loss (single batch): 1.2919342517852783\n",
      "\t Training loss (single batch): 0.9307315945625305\n",
      "\t Training loss (single batch): 1.4499061107635498\n",
      "\t Training loss (single batch): 0.8768912553787231\n",
      "\t Training loss (single batch): 0.7159627079963684\n",
      "\t Training loss (single batch): 1.0380373001098633\n",
      "\t Training loss (single batch): 1.1327776908874512\n",
      "\t Training loss (single batch): 1.4279705286026\n",
      "\t Training loss (single batch): 1.258112907409668\n",
      "\t Training loss (single batch): 1.4699249267578125\n",
      "\t Training loss (single batch): 1.4070136547088623\n",
      "\t Training loss (single batch): 1.2006652355194092\n",
      "\t Training loss (single batch): 1.0459964275360107\n",
      "\t Training loss (single batch): 1.204788327217102\n",
      "\t Training loss (single batch): 1.4467822313308716\n",
      "\t Training loss (single batch): 1.2988989353179932\n",
      "\t Training loss (single batch): 1.145527958869934\n",
      "\t Training loss (single batch): 0.8774358630180359\n",
      "\t Training loss (single batch): 1.271783709526062\n",
      "\t Training loss (single batch): 1.0216178894042969\n",
      "\t Training loss (single batch): 0.7442124485969543\n",
      "\t Training loss (single batch): 1.2671974897384644\n",
      "\t Training loss (single batch): 1.4772475957870483\n",
      "\t Training loss (single batch): 1.2327871322631836\n",
      "\t Training loss (single batch): 1.1493176221847534\n",
      "\t Training loss (single batch): 0.8412973880767822\n",
      "\t Training loss (single batch): 1.6656713485717773\n",
      "\t Training loss (single batch): 0.986910343170166\n",
      "\t Training loss (single batch): 1.1831014156341553\n",
      "\t Training loss (single batch): 0.8934275507926941\n",
      "\t Training loss (single batch): 1.2847903966903687\n",
      "\t Training loss (single batch): 1.264060378074646\n",
      "\t Training loss (single batch): 1.3115390539169312\n",
      "\t Training loss (single batch): 0.9083417057991028\n",
      "\t Training loss (single batch): 1.4281057119369507\n",
      "\t Training loss (single batch): 0.8567085266113281\n",
      "\t Training loss (single batch): 1.3121503591537476\n",
      "\t Training loss (single batch): 0.8524643182754517\n",
      "\t Training loss (single batch): 1.7427685260772705\n",
      "\t Training loss (single batch): 1.4581276178359985\n",
      "\t Training loss (single batch): 1.6593856811523438\n",
      "\t Training loss (single batch): 1.1570841073989868\n",
      "\t Training loss (single batch): 1.162593126296997\n",
      "\t Training loss (single batch): 0.7379659414291382\n",
      "\t Training loss (single batch): 1.0234383344650269\n",
      "\t Training loss (single batch): 1.1095184087753296\n",
      "\t Training loss (single batch): 1.588334083557129\n",
      "\t Training loss (single batch): 1.3947004079818726\n",
      "\t Training loss (single batch): 1.0095902681350708\n",
      "\t Training loss (single batch): 0.8019607663154602\n",
      "\t Training loss (single batch): 0.7668783664703369\n",
      "\t Training loss (single batch): 0.8058309555053711\n",
      "\t Training loss (single batch): 0.7059832811355591\n",
      "\t Training loss (single batch): 1.0864200592041016\n",
      "\t Training loss (single batch): 1.49626624584198\n",
      "\t Training loss (single batch): 0.9814070463180542\n",
      "\t Training loss (single batch): 0.9058084487915039\n",
      "\t Training loss (single batch): 1.2154524326324463\n",
      "\t Training loss (single batch): 1.0784310102462769\n",
      "\t Training loss (single batch): 1.404693603515625\n",
      "\t Training loss (single batch): 0.6822795271873474\n",
      "\t Training loss (single batch): 1.3566266298294067\n",
      "\t Training loss (single batch): 1.3230746984481812\n",
      "\t Training loss (single batch): 1.0991122722625732\n",
      "\t Training loss (single batch): 1.0958242416381836\n",
      "\t Training loss (single batch): 1.314238429069519\n",
      "\t Training loss (single batch): 1.5679693222045898\n",
      "\t Training loss (single batch): 1.088813066482544\n",
      "\t Training loss (single batch): 1.0177048444747925\n",
      "\t Training loss (single batch): 1.2829163074493408\n",
      "\t Training loss (single batch): 1.237902045249939\n",
      "\t Training loss (single batch): 1.2273584604263306\n",
      "\t Training loss (single batch): 0.7697126865386963\n",
      "\t Training loss (single batch): 2.246743679046631\n",
      "\t Training loss (single batch): 0.8082413077354431\n",
      "\t Training loss (single batch): 1.1605875492095947\n",
      "\t Training loss (single batch): 1.7715744972229004\n",
      "\t Training loss (single batch): 1.0691587924957275\n",
      "\t Training loss (single batch): 1.1207493543624878\n",
      "\t Training loss (single batch): 1.3622294664382935\n",
      "\t Training loss (single batch): 1.4267526865005493\n",
      "\t Training loss (single batch): 1.286177635192871\n",
      "\t Training loss (single batch): 0.7552694082260132\n",
      "\t Training loss (single batch): 1.0441267490386963\n",
      "\t Training loss (single batch): 0.9761881232261658\n",
      "\t Training loss (single batch): 0.924014151096344\n",
      "\t Training loss (single batch): 1.1004306077957153\n",
      "\t Training loss (single batch): 1.505061149597168\n",
      "\t Training loss (single batch): 1.7625792026519775\n",
      "\t Training loss (single batch): 1.5721534490585327\n",
      "\t Training loss (single batch): 1.6207367181777954\n",
      "\t Training loss (single batch): 1.0543715953826904\n",
      "\t Training loss (single batch): 1.061943769454956\n",
      "\t Training loss (single batch): 1.0627321004867554\n",
      "\t Training loss (single batch): 1.1913275718688965\n",
      "\t Training loss (single batch): 1.0919804573059082\n",
      "\t Training loss (single batch): 1.2714163064956665\n",
      "\t Training loss (single batch): 1.6840898990631104\n",
      "\t Training loss (single batch): 1.1715489625930786\n",
      "\t Training loss (single batch): 1.2621206045150757\n",
      "\t Training loss (single batch): 1.6758514642715454\n",
      "\t Training loss (single batch): 1.6744076013565063\n",
      "\t Training loss (single batch): 0.870585024356842\n",
      "\t Training loss (single batch): 0.9688596725463867\n",
      "\t Training loss (single batch): 1.3610507249832153\n",
      "\t Training loss (single batch): 1.0589041709899902\n",
      "\t Training loss (single batch): 1.016798734664917\n",
      "\t Training loss (single batch): 1.4680086374282837\n",
      "\t Training loss (single batch): 0.9766822457313538\n",
      "\t Training loss (single batch): 0.9047765135765076\n",
      "\t Training loss (single batch): 0.7613570690155029\n",
      "\t Training loss (single batch): 1.1992422342300415\n",
      "\t Training loss (single batch): 1.1431127786636353\n",
      "\t Training loss (single batch): 0.9874647259712219\n",
      "\t Training loss (single batch): 1.385046362876892\n",
      "\t Training loss (single batch): 1.35808527469635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.1825789213180542\n",
      "\t Training loss (single batch): 1.6606245040893555\n",
      "\t Training loss (single batch): 1.3071460723876953\n",
      "\t Training loss (single batch): 0.958549439907074\n",
      "\t Training loss (single batch): 1.1698408126831055\n",
      "\t Training loss (single batch): 1.4297581911087036\n",
      "\t Training loss (single batch): 1.3579896688461304\n",
      "\t Training loss (single batch): 0.9409142136573792\n",
      "\t Training loss (single batch): 0.6701403260231018\n",
      "\t Training loss (single batch): 1.2375308275222778\n",
      "\t Training loss (single batch): 0.8183053135871887\n",
      "\t Training loss (single batch): 0.9118518233299255\n",
      "\t Training loss (single batch): 1.602758526802063\n",
      "\t Training loss (single batch): 1.3224725723266602\n",
      "\t Training loss (single batch): 1.002163052558899\n",
      "\t Training loss (single batch): 1.1509971618652344\n",
      "\t Training loss (single batch): 1.211553692817688\n",
      "\t Training loss (single batch): 1.6206371784210205\n",
      "\t Training loss (single batch): 1.026116967201233\n",
      "\t Training loss (single batch): 1.2076282501220703\n",
      "\t Training loss (single batch): 0.9924617409706116\n",
      "\t Training loss (single batch): 1.0600976943969727\n",
      "\t Training loss (single batch): 1.1401203870773315\n",
      "\t Training loss (single batch): 1.5263601541519165\n",
      "\t Training loss (single batch): 0.7843729257583618\n",
      "\t Training loss (single batch): 1.3802027702331543\n",
      "\t Training loss (single batch): 1.3599698543548584\n",
      "\t Training loss (single batch): 1.1103742122650146\n",
      "\t Training loss (single batch): 0.940116286277771\n",
      "\t Training loss (single batch): 1.3094111680984497\n",
      "\t Training loss (single batch): 0.9520214796066284\n",
      "\t Training loss (single batch): 1.0777508020401\n",
      "\t Training loss (single batch): 1.4174442291259766\n",
      "\t Training loss (single batch): 1.121415376663208\n",
      "\t Training loss (single batch): 1.0027780532836914\n",
      "\t Training loss (single batch): 0.9928758144378662\n",
      "\t Training loss (single batch): 0.8087355494499207\n",
      "\t Training loss (single batch): 1.3149062395095825\n",
      "\t Training loss (single batch): 1.1932859420776367\n",
      "\t Training loss (single batch): 1.2959017753601074\n",
      "\t Training loss (single batch): 0.9516303539276123\n",
      "\t Training loss (single batch): 0.9659280180931091\n",
      "\t Training loss (single batch): 1.3232672214508057\n",
      "\t Training loss (single batch): 1.3183295726776123\n",
      "\t Training loss (single batch): 1.1353117227554321\n",
      "\t Training loss (single batch): 1.0612884759902954\n",
      "\t Training loss (single batch): 1.5221301317214966\n",
      "\t Training loss (single batch): 1.377880334854126\n",
      "\t Training loss (single batch): 0.7786676287651062\n",
      "\t Training loss (single batch): 0.9832848906517029\n",
      "\t Training loss (single batch): 1.243437647819519\n",
      "\t Training loss (single batch): 1.4032660722732544\n",
      "\t Training loss (single batch): 1.074573278427124\n",
      "\t Training loss (single batch): 1.0355815887451172\n",
      "\t Training loss (single batch): 1.1657297611236572\n",
      "\t Training loss (single batch): 1.287328839302063\n",
      "\t Training loss (single batch): 1.2970012426376343\n",
      "\t Training loss (single batch): 1.3655941486358643\n",
      "\t Training loss (single batch): 0.8039761185646057\n",
      "\t Training loss (single batch): 0.7795243859291077\n",
      "\t Training loss (single batch): 1.3881781101226807\n",
      "\t Training loss (single batch): 1.098287582397461\n",
      "\t Training loss (single batch): 1.5240634679794312\n",
      "\t Training loss (single batch): 0.7441911697387695\n",
      "\t Training loss (single batch): 1.3612298965454102\n",
      "\t Training loss (single batch): 1.5492829084396362\n",
      "\t Training loss (single batch): 1.1762768030166626\n",
      "\t Training loss (single batch): 0.9338460564613342\n",
      "\t Training loss (single batch): 1.2103157043457031\n",
      "\t Training loss (single batch): 1.0914398431777954\n",
      "\t Training loss (single batch): 0.9461219310760498\n",
      "\t Training loss (single batch): 1.0430262088775635\n",
      "\t Training loss (single batch): 1.7191983461380005\n",
      "\t Training loss (single batch): 1.3245447874069214\n",
      "\t Training loss (single batch): 1.323787808418274\n",
      "\t Training loss (single batch): 1.0278493165969849\n",
      "\t Training loss (single batch): 0.993391752243042\n",
      "\t Training loss (single batch): 0.9762908220291138\n",
      "\t Training loss (single batch): 1.7051771879196167\n",
      "\t Training loss (single batch): 1.3699458837509155\n",
      "\t Training loss (single batch): 1.299735426902771\n",
      "\t Training loss (single batch): 0.9237905740737915\n",
      "\t Training loss (single batch): 0.9395478963851929\n",
      "\t Training loss (single batch): 1.262166142463684\n",
      "\t Training loss (single batch): 1.2150474786758423\n",
      "\t Training loss (single batch): 0.889875590801239\n",
      "\t Training loss (single batch): 1.2547017335891724\n",
      "\t Training loss (single batch): 1.0830018520355225\n",
      "\t Training loss (single batch): 1.1281299591064453\n",
      "\t Training loss (single batch): 0.9873952269554138\n",
      "\t Training loss (single batch): 1.7234361171722412\n",
      "\t Training loss (single batch): 0.8338252305984497\n",
      "\t Training loss (single batch): 0.907640278339386\n",
      "\t Training loss (single batch): 2.0733110904693604\n",
      "\t Training loss (single batch): 1.3282232284545898\n",
      "\t Training loss (single batch): 1.1891132593154907\n",
      "\t Training loss (single batch): 1.1439998149871826\n",
      "\t Training loss (single batch): 1.1557073593139648\n",
      "\t Training loss (single batch): 1.2275577783584595\n",
      "\t Training loss (single batch): 1.1214927434921265\n",
      "\t Training loss (single batch): 1.1749951839447021\n",
      "\t Training loss (single batch): 0.9506592750549316\n",
      "\t Training loss (single batch): 1.1786105632781982\n",
      "\t Training loss (single batch): 1.5097113847732544\n",
      "\t Training loss (single batch): 1.4309500455856323\n",
      "\t Training loss (single batch): 1.4840044975280762\n",
      "\t Training loss (single batch): 0.8776785731315613\n",
      "\t Training loss (single batch): 0.8285503387451172\n",
      "\t Training loss (single batch): 1.0396287441253662\n",
      "\t Training loss (single batch): 1.0202617645263672\n",
      "\t Training loss (single batch): 1.3444690704345703\n",
      "\t Training loss (single batch): 1.0558103322982788\n",
      "\t Training loss (single batch): 1.3699939250946045\n",
      "\t Training loss (single batch): 1.2437533140182495\n",
      "\t Training loss (single batch): 1.8429137468338013\n",
      "\t Training loss (single batch): 1.1505153179168701\n",
      "\t Training loss (single batch): 0.9217026829719543\n",
      "\t Training loss (single batch): 1.4111016988754272\n",
      "\t Training loss (single batch): 1.041174292564392\n",
      "\t Training loss (single batch): 1.059470534324646\n",
      "\t Training loss (single batch): 0.8344594836235046\n",
      "\t Training loss (single batch): 0.9110828042030334\n",
      "\t Training loss (single batch): 0.8139172792434692\n",
      "\t Training loss (single batch): 1.387349009513855\n",
      "\t Training loss (single batch): 1.6948686838150024\n",
      "\t Training loss (single batch): 1.142540454864502\n",
      "\t Training loss (single batch): 0.948041558265686\n",
      "\t Training loss (single batch): 1.276383638381958\n",
      "\t Training loss (single batch): 1.6769165992736816\n",
      "\t Training loss (single batch): 1.4376260042190552\n",
      "\t Training loss (single batch): 0.8185520768165588\n",
      "\t Training loss (single batch): 1.1470251083374023\n",
      "\t Training loss (single batch): 1.2976281642913818\n",
      "\t Training loss (single batch): 0.9991287589073181\n",
      "\t Training loss (single batch): 1.056665301322937\n",
      "\t Training loss (single batch): 0.90850430727005\n",
      "\t Training loss (single batch): 2.167417526245117\n",
      "\t Training loss (single batch): 1.2209728956222534\n",
      "\t Training loss (single batch): 1.4202631711959839\n",
      "\t Training loss (single batch): 1.1569156646728516\n",
      "\t Training loss (single batch): 0.77248215675354\n",
      "\t Training loss (single batch): 1.303038477897644\n",
      "\t Training loss (single batch): 1.486169457435608\n",
      "##################################\n",
      "## EPOCH 100\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6761777400970459\n",
      "\t Training loss (single batch): 1.063683271408081\n",
      "\t Training loss (single batch): 0.919762372970581\n",
      "\t Training loss (single batch): 0.9681061506271362\n",
      "\t Training loss (single batch): 0.9176719784736633\n",
      "\t Training loss (single batch): 0.9001478552818298\n",
      "\t Training loss (single batch): 1.235662579536438\n",
      "\t Training loss (single batch): 1.793555498123169\n",
      "\t Training loss (single batch): 1.2744733095169067\n",
      "\t Training loss (single batch): 1.1581230163574219\n",
      "\t Training loss (single batch): 1.8105757236480713\n",
      "\t Training loss (single batch): 1.097334623336792\n",
      "\t Training loss (single batch): 1.5407401323318481\n",
      "\t Training loss (single batch): 1.1904417276382446\n",
      "\t Training loss (single batch): 1.2628716230392456\n",
      "\t Training loss (single batch): 1.1570030450820923\n",
      "\t Training loss (single batch): 1.104331135749817\n",
      "\t Training loss (single batch): 1.155873417854309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.169464349746704\n",
      "\t Training loss (single batch): 1.3207274675369263\n",
      "\t Training loss (single batch): 0.8349696397781372\n",
      "\t Training loss (single batch): 1.7450125217437744\n",
      "\t Training loss (single batch): 1.2664909362792969\n",
      "\t Training loss (single batch): 1.049789309501648\n",
      "\t Training loss (single batch): 1.5554327964782715\n",
      "\t Training loss (single batch): 1.0762830972671509\n",
      "\t Training loss (single batch): 1.2444804906845093\n",
      "\t Training loss (single batch): 0.8567343950271606\n",
      "\t Training loss (single batch): 1.0318994522094727\n",
      "\t Training loss (single batch): 1.2363078594207764\n",
      "\t Training loss (single batch): 1.072996735572815\n",
      "\t Training loss (single batch): 0.9329531788825989\n",
      "\t Training loss (single batch): 1.64449942111969\n",
      "\t Training loss (single batch): 1.2791904211044312\n",
      "\t Training loss (single batch): 1.361505389213562\n",
      "\t Training loss (single batch): 1.473666787147522\n",
      "\t Training loss (single batch): 1.3812545537948608\n",
      "\t Training loss (single batch): 1.4743971824645996\n",
      "\t Training loss (single batch): 1.4918068647384644\n",
      "\t Training loss (single batch): 1.7222559452056885\n",
      "\t Training loss (single batch): 1.3010669946670532\n",
      "\t Training loss (single batch): 1.0052003860473633\n",
      "\t Training loss (single batch): 1.1379269361495972\n",
      "\t Training loss (single batch): 1.469597339630127\n",
      "\t Training loss (single batch): 1.2430936098098755\n",
      "\t Training loss (single batch): 1.316388726234436\n",
      "\t Training loss (single batch): 1.624449610710144\n",
      "\t Training loss (single batch): 0.9148229360580444\n",
      "\t Training loss (single batch): 1.670791745185852\n",
      "\t Training loss (single batch): 1.0207194089889526\n",
      "\t Training loss (single batch): 0.7333932518959045\n",
      "\t Training loss (single batch): 0.9946507811546326\n",
      "\t Training loss (single batch): 1.2114588022232056\n",
      "\t Training loss (single batch): 0.860703706741333\n",
      "\t Training loss (single batch): 1.311835527420044\n",
      "\t Training loss (single batch): 0.9643670320510864\n",
      "\t Training loss (single batch): 1.1805444955825806\n",
      "\t Training loss (single batch): 1.499032974243164\n",
      "\t Training loss (single batch): 1.2538915872573853\n",
      "\t Training loss (single batch): 1.3507486581802368\n",
      "\t Training loss (single batch): 1.2073513269424438\n",
      "\t Training loss (single batch): 0.842373251914978\n",
      "\t Training loss (single batch): 0.945255696773529\n",
      "\t Training loss (single batch): 1.324829339981079\n",
      "\t Training loss (single batch): 2.0562050342559814\n",
      "\t Training loss (single batch): 0.8406991958618164\n",
      "\t Training loss (single batch): 1.0147051811218262\n",
      "\t Training loss (single batch): 1.5188909769058228\n",
      "\t Training loss (single batch): 1.259879469871521\n",
      "\t Training loss (single batch): 1.2534295320510864\n",
      "\t Training loss (single batch): 1.1643157005310059\n",
      "\t Training loss (single batch): 1.1966824531555176\n",
      "\t Training loss (single batch): 1.2212731838226318\n",
      "\t Training loss (single batch): 1.1532652378082275\n",
      "\t Training loss (single batch): 1.4318065643310547\n",
      "\t Training loss (single batch): 1.0471572875976562\n",
      "\t Training loss (single batch): 1.1020280122756958\n",
      "\t Training loss (single batch): 1.3618508577346802\n",
      "\t Training loss (single batch): 1.6422232389450073\n",
      "\t Training loss (single batch): 1.0300654172897339\n",
      "\t Training loss (single batch): 1.3090221881866455\n",
      "\t Training loss (single batch): 0.9016934037208557\n",
      "\t Training loss (single batch): 1.5680519342422485\n",
      "\t Training loss (single batch): 0.9773173928260803\n",
      "\t Training loss (single batch): 1.2525203227996826\n",
      "\t Training loss (single batch): 1.1289381980895996\n",
      "\t Training loss (single batch): 1.393621563911438\n",
      "\t Training loss (single batch): 1.1031025648117065\n",
      "\t Training loss (single batch): 1.065342664718628\n",
      "\t Training loss (single batch): 1.4232113361358643\n",
      "\t Training loss (single batch): 1.5931490659713745\n",
      "\t Training loss (single batch): 1.0118954181671143\n",
      "\t Training loss (single batch): 0.8595953583717346\n",
      "\t Training loss (single batch): 1.4690039157867432\n",
      "\t Training loss (single batch): 1.4254032373428345\n",
      "\t Training loss (single batch): 0.7635502815246582\n",
      "\t Training loss (single batch): 0.9963899850845337\n",
      "\t Training loss (single batch): 1.679673433303833\n",
      "\t Training loss (single batch): 1.3127204179763794\n",
      "\t Training loss (single batch): 0.8913031816482544\n",
      "\t Training loss (single batch): 0.7310708165168762\n",
      "\t Training loss (single batch): 1.3422824144363403\n",
      "\t Training loss (single batch): 0.9742341637611389\n",
      "\t Training loss (single batch): 1.5175399780273438\n",
      "\t Training loss (single batch): 0.8353246450424194\n",
      "\t Training loss (single batch): 1.1715314388275146\n",
      "\t Training loss (single batch): 1.030207872390747\n",
      "\t Training loss (single batch): 1.433048129081726\n",
      "\t Training loss (single batch): 1.246456265449524\n",
      "\t Training loss (single batch): 0.98882657289505\n",
      "\t Training loss (single batch): 0.9701331853866577\n",
      "\t Training loss (single batch): 1.038130283355713\n",
      "\t Training loss (single batch): 1.8641037940979004\n",
      "\t Training loss (single batch): 0.7978288531303406\n",
      "\t Training loss (single batch): 1.5716055631637573\n",
      "\t Training loss (single batch): 1.0677852630615234\n",
      "\t Training loss (single batch): 1.6650974750518799\n",
      "\t Training loss (single batch): 1.0873918533325195\n",
      "\t Training loss (single batch): 1.9216092824935913\n",
      "\t Training loss (single batch): 1.2971230745315552\n",
      "\t Training loss (single batch): 0.7065544724464417\n",
      "\t Training loss (single batch): 1.0712916851043701\n",
      "\t Training loss (single batch): 1.58767831325531\n",
      "\t Training loss (single batch): 0.8644301295280457\n",
      "\t Training loss (single batch): 0.7868090271949768\n",
      "\t Training loss (single batch): 1.1688368320465088\n",
      "\t Training loss (single batch): 1.2391941547393799\n",
      "\t Training loss (single batch): 1.0867351293563843\n",
      "\t Training loss (single batch): 1.2520875930786133\n",
      "\t Training loss (single batch): 0.600155234336853\n",
      "\t Training loss (single batch): 1.4531340599060059\n",
      "\t Training loss (single batch): 0.9965406060218811\n",
      "\t Training loss (single batch): 1.2848327159881592\n",
      "\t Training loss (single batch): 1.5220401287078857\n",
      "\t Training loss (single batch): 1.7331455945968628\n",
      "\t Training loss (single batch): 1.5857031345367432\n",
      "\t Training loss (single batch): 1.156476378440857\n",
      "\t Training loss (single batch): 1.3063852787017822\n",
      "\t Training loss (single batch): 1.0674697160720825\n",
      "\t Training loss (single batch): 0.9706300497055054\n",
      "\t Training loss (single batch): 0.9697235822677612\n",
      "\t Training loss (single batch): 0.7599884867668152\n",
      "\t Training loss (single batch): 1.2063217163085938\n",
      "\t Training loss (single batch): 1.4709137678146362\n",
      "\t Training loss (single batch): 0.9841827750205994\n",
      "\t Training loss (single batch): 1.7434442043304443\n",
      "\t Training loss (single batch): 1.6252031326293945\n",
      "\t Training loss (single batch): 1.24709951877594\n",
      "\t Training loss (single batch): 0.8582562208175659\n",
      "\t Training loss (single batch): 0.9770894646644592\n",
      "\t Training loss (single batch): 1.2418733835220337\n",
      "\t Training loss (single batch): 1.4240775108337402\n",
      "\t Training loss (single batch): 1.2073030471801758\n",
      "\t Training loss (single batch): 1.4889683723449707\n",
      "\t Training loss (single batch): 1.1104073524475098\n",
      "\t Training loss (single batch): 1.038204550743103\n",
      "\t Training loss (single batch): 0.9895803928375244\n",
      "\t Training loss (single batch): 0.9935283064842224\n",
      "\t Training loss (single batch): 1.1928635835647583\n",
      "\t Training loss (single batch): 1.2001665830612183\n",
      "\t Training loss (single batch): 1.0164620876312256\n",
      "\t Training loss (single batch): 1.4424521923065186\n",
      "\t Training loss (single batch): 1.0751502513885498\n",
      "\t Training loss (single batch): 1.3308520317077637\n",
      "\t Training loss (single batch): 1.0391303300857544\n",
      "\t Training loss (single batch): 1.1320405006408691\n",
      "\t Training loss (single batch): 0.9453386664390564\n",
      "\t Training loss (single batch): 1.024928331375122\n",
      "\t Training loss (single batch): 1.5048604011535645\n",
      "\t Training loss (single batch): 1.7306729555130005\n",
      "\t Training loss (single batch): 1.1590652465820312\n",
      "\t Training loss (single batch): 1.1116946935653687\n",
      "\t Training loss (single batch): 1.160772681236267\n",
      "\t Training loss (single batch): 1.3491202592849731\n",
      "\t Training loss (single batch): 1.1433403491973877\n",
      "\t Training loss (single batch): 1.461837887763977\n",
      "\t Training loss (single batch): 1.1603282690048218\n",
      "\t Training loss (single batch): 1.0314968824386597\n",
      "\t Training loss (single batch): 1.2256784439086914\n",
      "\t Training loss (single batch): 1.2073744535446167\n",
      "\t Training loss (single batch): 1.499748945236206\n",
      "\t Training loss (single batch): 1.1590807437896729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.7092276811599731\n",
      "\t Training loss (single batch): 1.1804310083389282\n",
      "\t Training loss (single batch): 1.179866909980774\n",
      "\t Training loss (single batch): 1.149519920349121\n",
      "\t Training loss (single batch): 1.2726030349731445\n",
      "\t Training loss (single batch): 1.1411170959472656\n",
      "\t Training loss (single batch): 1.1647638082504272\n",
      "\t Training loss (single batch): 1.1580708026885986\n",
      "\t Training loss (single batch): 1.1781364679336548\n",
      "\t Training loss (single batch): 1.0193777084350586\n",
      "\t Training loss (single batch): 1.2907243967056274\n",
      "\t Training loss (single batch): 0.6156530380249023\n",
      "\t Training loss (single batch): 1.1302152872085571\n",
      "\t Training loss (single batch): 0.7739677429199219\n",
      "\t Training loss (single batch): 1.509239912033081\n",
      "\t Training loss (single batch): 0.9679361581802368\n",
      "\t Training loss (single batch): 1.2684799432754517\n",
      "\t Training loss (single batch): 1.0101829767227173\n",
      "\t Training loss (single batch): 1.6549524068832397\n",
      "\t Training loss (single batch): 1.7168973684310913\n",
      "\t Training loss (single batch): 1.4026561975479126\n",
      "\t Training loss (single batch): 1.4155445098876953\n",
      "\t Training loss (single batch): 1.8098607063293457\n",
      "\t Training loss (single batch): 1.309212327003479\n",
      "\t Training loss (single batch): 0.882760763168335\n",
      "\t Training loss (single batch): 1.1009455919265747\n",
      "\t Training loss (single batch): 0.8558486104011536\n",
      "\t Training loss (single batch): 1.055751919746399\n",
      "\t Training loss (single batch): 1.5672205686569214\n",
      "\t Training loss (single batch): 1.3785786628723145\n",
      "\t Training loss (single batch): 1.1039890050888062\n",
      "\t Training loss (single batch): 0.8307378888130188\n",
      "\t Training loss (single batch): 1.0757991075515747\n",
      "\t Training loss (single batch): 1.3946906328201294\n",
      "\t Training loss (single batch): 1.3807381391525269\n",
      "\t Training loss (single batch): 1.4265072345733643\n",
      "\t Training loss (single batch): 0.7076559662818909\n",
      "\t Training loss (single batch): 1.1756529808044434\n",
      "\t Training loss (single batch): 1.0081347227096558\n",
      "\t Training loss (single batch): 1.603263020515442\n",
      "\t Training loss (single batch): 1.6279925107955933\n",
      "\t Training loss (single batch): 1.2729493379592896\n",
      "\t Training loss (single batch): 1.284748911857605\n",
      "\t Training loss (single batch): 1.0319249629974365\n",
      "\t Training loss (single batch): 1.2257786989212036\n",
      "\t Training loss (single batch): 0.8804476857185364\n",
      "\t Training loss (single batch): 1.0270761251449585\n",
      "\t Training loss (single batch): 0.6928145885467529\n",
      "\t Training loss (single batch): 0.9127650856971741\n",
      "\t Training loss (single batch): 1.2182996273040771\n",
      "\t Training loss (single batch): 1.3202120065689087\n",
      "\t Training loss (single batch): 1.117440938949585\n",
      "\t Training loss (single batch): 1.4710100889205933\n",
      "\t Training loss (single batch): 1.5587760210037231\n",
      "\t Training loss (single batch): 1.2780375480651855\n",
      "\t Training loss (single batch): 1.1255052089691162\n",
      "\t Training loss (single batch): 0.990977942943573\n",
      "\t Training loss (single batch): 1.5620189905166626\n",
      "\t Training loss (single batch): 1.2955843210220337\n",
      "\t Training loss (single batch): 1.2225642204284668\n",
      "\t Training loss (single batch): 0.7834326028823853\n",
      "\t Training loss (single batch): 1.4209545850753784\n",
      "\t Training loss (single batch): 0.9421212077140808\n",
      "\t Training loss (single batch): 1.8101907968521118\n",
      "\t Training loss (single batch): 1.474661111831665\n",
      "\t Training loss (single batch): 1.0200550556182861\n",
      "\t Training loss (single batch): 1.2495131492614746\n",
      "\t Training loss (single batch): 0.9724668264389038\n",
      "\t Training loss (single batch): 1.324386715888977\n",
      "\t Training loss (single batch): 1.580647349357605\n",
      "\t Training loss (single batch): 1.5488983392715454\n",
      "\t Training loss (single batch): 1.1440922021865845\n",
      "\t Training loss (single batch): 0.6732818484306335\n",
      "\t Training loss (single batch): 1.0398508310317993\n",
      "\t Training loss (single batch): 0.8158411979675293\n",
      "\t Training loss (single batch): 1.2346532344818115\n",
      "\t Training loss (single batch): 1.145907998085022\n",
      "\t Training loss (single batch): 1.996915578842163\n",
      "\t Training loss (single batch): 1.1028392314910889\n",
      "\t Training loss (single batch): 1.3657591342926025\n",
      "\t Training loss (single batch): 1.1425855159759521\n",
      "\t Training loss (single batch): 1.6141635179519653\n",
      "\t Training loss (single batch): 0.9268742203712463\n",
      "\t Training loss (single batch): 1.5101995468139648\n",
      "\t Training loss (single batch): 1.0957489013671875\n",
      "\t Training loss (single batch): 1.370787262916565\n",
      "\t Training loss (single batch): 0.8546937108039856\n",
      "\t Training loss (single batch): 0.9616621732711792\n",
      "\t Training loss (single batch): 1.0398261547088623\n",
      "\t Training loss (single batch): 1.106602668762207\n",
      "\t Training loss (single batch): 0.7842458486557007\n",
      "\t Training loss (single batch): 1.2650532722473145\n",
      "\t Training loss (single batch): 1.681012749671936\n",
      "\t Training loss (single batch): 0.8409640789031982\n",
      "\t Training loss (single batch): 1.3270573616027832\n",
      "\t Training loss (single batch): 0.8153931498527527\n",
      "\t Training loss (single batch): 1.1836422681808472\n",
      "\t Training loss (single batch): 1.1908214092254639\n",
      "\t Training loss (single batch): 0.8526807427406311\n",
      "\t Training loss (single batch): 1.1254894733428955\n",
      "\t Training loss (single batch): 1.2466169595718384\n",
      "\t Training loss (single batch): 1.222469687461853\n",
      "\t Training loss (single batch): 1.3186442852020264\n",
      "\t Training loss (single batch): 1.6575742959976196\n",
      "\t Training loss (single batch): 0.962303876876831\n",
      "\t Training loss (single batch): 1.4824615716934204\n",
      "\t Training loss (single batch): 1.418106198310852\n",
      "\t Training loss (single batch): 0.9192340970039368\n",
      "\t Training loss (single batch): 1.324309229850769\n",
      "\t Training loss (single batch): 1.322835922241211\n",
      "\t Training loss (single batch): 1.296026587486267\n",
      "\t Training loss (single batch): 1.0683097839355469\n",
      "\t Training loss (single batch): 1.3232464790344238\n",
      "\t Training loss (single batch): 1.6029621362686157\n",
      "\t Training loss (single batch): 0.8375341892242432\n",
      "\t Training loss (single batch): 1.1699764728546143\n",
      "\t Training loss (single batch): 1.1585739850997925\n",
      "\t Training loss (single batch): 1.4364264011383057\n",
      "\t Training loss (single batch): 1.1121851205825806\n",
      "\t Training loss (single batch): 1.166724443435669\n",
      "\t Training loss (single batch): 0.7927457094192505\n",
      "\t Training loss (single batch): 0.8469108939170837\n",
      "\t Training loss (single batch): 0.9518355131149292\n",
      "\t Training loss (single batch): 1.2351431846618652\n",
      "\t Training loss (single batch): 1.2388838529586792\n",
      "\t Training loss (single batch): 1.1266144514083862\n",
      "\t Training loss (single batch): 1.322359561920166\n",
      "\t Training loss (single batch): 1.1570115089416504\n",
      "\t Training loss (single batch): 1.263018250465393\n",
      "\t Training loss (single batch): 1.8596442937850952\n",
      "\t Training loss (single batch): 1.5856519937515259\n",
      "\t Training loss (single batch): 1.2788097858428955\n",
      "\t Training loss (single batch): 1.2581192255020142\n",
      "\t Training loss (single batch): 1.0350284576416016\n",
      "\t Training loss (single batch): 1.1800448894500732\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-08bbca687279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m### Save all needed parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Create output dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Save network parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "#%% Train network\n",
    "\n",
    "#%% Check device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Selected device:', device)\n",
    "\n",
    "# Define Dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True, num_workers=1)\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(my_net.parameters(), weight_decay=5e-4)\n",
    "\n",
    "# Define loss function: using MSE since now the words are represented as vectors\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    print('##################################')\n",
    "    print('## EPOCH %d' % (epoch + 1))\n",
    "    print('##################################')\n",
    "    # Iterate batches\n",
    "    for batch_sample in dataloader:\n",
    "        # Extract batch\n",
    "        batch_ = batch_sample['encoded_repr'].to(device)\n",
    "        # Update network\n",
    "        batch_loss = train_batch(my_net, batch_, loss_fn, optimizer)\n",
    "        print('\\t Training loss (single batch):', batch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 5\n",
    "hidden_units = 500\n",
    "num_layers = 2\n",
    "batchsize = 20\n",
    "num_epochs = 100\n",
    "crop_len = 5\n",
    "\n",
    "training_args = {\"embedding_dims\": 5,\n",
    "                \"hidden_units\": 500,\n",
    "                \"num_layers\": 2,\n",
    "                \"batchsize\": 20,\n",
    "                \"num_epochs\": 100,\n",
    "                \"crop_len\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save all needed parameters\n",
    "# Create output dir`\n",
    "out_dir = Path(\"model_W2V\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Save network parameters\n",
    "torch.save(my_net.state_dict(), out_dir / 'net_params.pth')\n",
    "# Save training parameters\n",
    "with open(out_dir / 'training_args.json', 'w') as f:\n",
    "    json.dump(training_args, f, indent=4)\n",
    "# Save encoder dictionary\n",
    "with open(out_dir / 'char_to_number.json', 'w') as f:\n",
    "    json.dump(dataset.word_to_index, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading network and trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dorian was at his house by me players dutch book apple siphons yesterday footed thief movements superstitions amarre wine camillus pebbles pins searched widening temperaments shield moon momentary intentions christian spectators consumption nose untouched isotta see estimate hooded oratory crushed lock cedar illustrated point sovereign recovered stamp dreadful infatuation disk civil christened verities paint devoted take portico beasts eighty jacobean lash intellects droop flowering scolding renewed disappointment gable stock came furry lateness called crumpled down same loses got cheated tussore head cruel buzzed awakened frontleted parts whatever between other nuisance trampling bedside culture happen present sinner fallen grass emperor arrangement feed omnibus lurid bamboo sup consequence tapped demons proposing shaken monotony seizing shaped slander girl unpardonable audace superficial twelve bamboo lives and passion faster schoolgirl kissed sulkily wardour curiously curiosity rich dry gardener lovely callous player intricacies jerking comeliness quickened lashed gold tonic pagan sunlight broker holding fault tulips let faithfully giordano cries nugget trembled singleness spinels stroking sadly branches court thornbury pinnacles blast bread anthony sight colourless reflected informed depresses beard blessed exquisite flew worlds candleshades rustle satin bonnets gardener ways forgot stifling means reverie theories thin asceticism fondled egyptian peer romance wants glimmered beats elocution irresistible mock bitterness heaps digging crimson broideries transformed frank hellenic killed isn taught heavy rendered mauve tulips happier glimmered whole strained creatures indians chapters definition though grossly desired bright dust unbecoming victor curiosity novel loves untouched representative trickle wisdom faded horns some rot marlow rainbow bubbles leer vestige indifference bournemouth collar violets hadn daintily sullenly kneel lean cane more may discords certainly breathing seeds paragraph perplexed boring cookery die cleft deepened kelso deadly fads pollen bells on pilier certainty garments endure singular small strip court denied renunciations artemis rod shooting l audience eighteen experience bullied if lurks crowd ever shimmering analysis hypocrisy year doorway tightening philosophic toned leafless pockets moment "
     ]
    }
   ],
   "source": [
    "# Loading the word representation\n",
    "W1_try = torch.load(\"W1_weights\")\n",
    "W2_try = torch.load(\"W2_weights\")\n",
    "\n",
    "model_dir = Path('model_W2V')\n",
    "\n",
    "training_args = json.load(open(model_dir / 'training_args.json'))\n",
    "\n",
    "#%% Initialize network\n",
    "my_net = Network(training_args[\"embedding_dims\"],\n",
    "                 training_args[\"hidden_units\"],\n",
    "                 training_args[\"num_layers\"],\n",
    "                 dropout_prob=0.3)\n",
    "\n",
    "#%% Load network trained parameters\n",
    "my_net.load_state_dict(torch.load(model_dir / 'net_params.pth', map_location='cpu'))\n",
    "my_net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "\n",
    "chapter_seed = \"dorian was at his house\"\n",
    "chapter_seed = re.split('[ ]', chapter_seed)\n",
    "\n",
    "word_to_number = json.load(open(model_dir / 'word_to_number.json'))\n",
    "number_to_word = {k: v for k,v in enumerate(word_to_number)}\n",
    "\n",
    "np_repr = W2_try.detach().numpy()\n",
    "\n",
    "#%% Find initial state of the RNN\n",
    "with torch.no_grad():\n",
    "    # Encode seed\n",
    "    seed_encoded = encode_text(word_to_number, chapter_seed)\n",
    "    \n",
    "    # Vector representation\n",
    "    seed_repr = create_W2V_representation(seed_encoded, W2_try)\n",
    "    # To tensor\n",
    "    seed_repr = torch.tensor(seed_repr).float()\n",
    "    # Add batch axis\n",
    "    #seed_repr = seed_repr.unsqueeze(0)\n",
    "        \n",
    "    # Forward pass\n",
    "    net_out, net_state = my_net(seed_repr[0].unsqueeze(0).unsqueeze(0))\n",
    "    \n",
    "    for word in seed_repr[1:]:\n",
    "        word.unsqueeze(0)\n",
    "        net_out, net_state = my_net(word.unsqueeze(0).unsqueeze(0), net_state)\n",
    "        \n",
    "    word_vector = net_out.numpy()[0,0,:]\n",
    "    \n",
    "    # Looking for the minimum distances between the guessed representation and the words\n",
    "    next_word_encoded = np.argmin(np.sqrt(np.sum((np_repr - word_vector)**2, axis=1)))\n",
    "        \n",
    "    # Get the most probable last output index\n",
    "    #next_word_encoded = net_out[:, -1, :].argmax().item()\n",
    "    \n",
    "    # Print the seed words\n",
    "    for word in chapter_seed:\n",
    "        print(word, end=' ', flush=True)\n",
    "    \n",
    "    next_word = number_to_word[next_word_encoded]\n",
    "    print(next_word, end=' ', flush=True)\n",
    "\n",
    "#%% Generate chapter\n",
    "tot_word_count = 0\n",
    "while True:\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # The new network input is the one hot encoding of the last chosen word\n",
    "        \n",
    "        net_input = W2_try[next_word_encoded].unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        net_out, net_state = my_net(net_input, net_state)\n",
    "        \n",
    "        word_vector = net_out.numpy()[0,0,:]\n",
    "    \n",
    "        # Looking for the minimum distances between the guessed representation and the words\n",
    "        #  and choosing using a softmax\n",
    "        distances = np.sqrt(np.sum((np_repr - word_vector)**2, axis=1))\n",
    "        distrib = np.array(nn.functional.softmax(torch.from_numpy(1/distances), dim=-1))\n",
    "        #next_char_encoded = np.random.choice(len(distrib.ravel()), size=1, p=distrib.ravel())[0]\n",
    "        next_word_encoded = np.random.choice(len(distrib.ravel()), size=1, p=distrib.ravel())[0]\n",
    "        \n",
    "        # Looking for the minimum distances between the guessed representation and the words\n",
    "        #next_word_encoded = np.argmin(np.sqrt(np.sum((np_repr - word_vector)**2, axis=1)))\n",
    "    \n",
    "        next_word = number_to_word[next_word_encoded]\n",
    "        \n",
    "        print(next_word, end=' ', flush=True)\n",
    "        # Count total letters\n",
    "        tot_word_count += 1\n",
    "        # Break if 20 lines or 300 words\n",
    "        if tot_word_count > 300:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
