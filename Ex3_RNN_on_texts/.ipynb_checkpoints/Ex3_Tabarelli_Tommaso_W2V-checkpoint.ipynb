{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional Neuroscience\n",
    "Accademic year 2019-2020\n",
    "Homework 3\n",
    "\n",
    "Author: Tommaso Tabarelli\n",
    "Period: december 2019\n",
    "\"\"\"\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import reduce\n",
    "from torch import optim, nn\n",
    "from network import Network, train_batch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# Importing packages to implement word2vec\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "        # Call the parent init function (required!)\n",
    "        super().__init__()\n",
    "        # Define recurrent layer\n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                        hidden_size=hidden_units,\n",
    "                        num_layers=layers_num,\n",
    "                        dropout=dropout_prob,\n",
    "                        batch_first=True)\n",
    "        # Define output layer\n",
    "        self.out = nn.Linear(hidden_units, input_size)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        # LSTM\n",
    "        x, rnn_state = self.rnn(x, state)\n",
    "        # Linear layer\n",
    "        x = self.out(x)\n",
    "        return x, rnn_state\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_batch(net, batch_onehot, loss_fn, optimizer):\n",
    "\n",
    "    ### Prepare network input and labels\n",
    "    # Get the labels (the last letter of each sequence)\n",
    "    labels_onehot = batch_onehot[:, -1, :]\n",
    "    labels_numbers = labels_onehot.argmax(dim=1)\n",
    "    # Remove the labels from the input tensor\n",
    "    net_input = batch_onehot[:, :-1, :]\n",
    "    # batch_onehot.shape =   [50, 100, 38]\n",
    "    # labels_onehot.shape =  [50, 38]\n",
    "    # labels_numbers.shape = [50]\n",
    "    # net_input.shape =      [50, 99, 38]\n",
    "\n",
    "    ### Forward pass\n",
    "    # Eventually clear previous recorded gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    net_out, _ = net(net_input)\n",
    "\n",
    "    ### Update network\n",
    "    # Evaluate loss only for last output\n",
    "    loss = loss_fn(net_out[:, -1, :], labels_numbers)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # Return average batch loss\n",
    "    return float(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing word2vec\n",
    "\n",
    "Following the instructions at the following link: https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5746\n",
      "12519\n",
      "the studio was filled with the rich odour of roses\n",
      "['the', 'studio', 'was', 'filled', 'with', 'the', 'rich', 'odour', 'of', 'roses']\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "text = open(\"Picture_of_Dorian_Gray.txt\", 'r').read()\n",
    "\n",
    "# Removing titles\n",
    "text = re.split('\\n{7}', text)[1]\n",
    "\n",
    "# Lowering all text\n",
    "text = text.lower()\n",
    "\n",
    "sentences = re.split('[\\.,!?;:]', text)\n",
    "\n",
    "vocabulary = []\n",
    "all_words = re.split(\"[\\.,!?;:\\n -\\'-]\", text)\n",
    "for word in all_words:\n",
    "    if word not in vocabulary:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary.sort()\n",
    "\n",
    "# Removing the first 3 elements since they are numbers or empty string\n",
    "vocabulary.pop(0)\n",
    "vocabulary.pop(0)\n",
    "vocabulary.pop(0)\n",
    "\n",
    "print(len(text.split(\".\")))\n",
    "print(len(sentences))\n",
    "#print(vocabulary)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "#print(word2idx.keys())\n",
    "\n",
    "print(sentences[0])\n",
    "for sentence in sentences:\n",
    "    # saving index\n",
    "    index = sentences.index(sentence)\n",
    "    sentence = re.split(\"[\\.,!?;:\\n -\\'-]\", sentence)\n",
    "    \n",
    "    # Removing by hand ALL the unwanted characters\n",
    "    for i in range(sentence.count('')):\n",
    "        sentence.pop(sentence.index(''))\n",
    "    for i in range(sentence.count(' ')):\n",
    "        sentence.pop(sentence.index(\" \"))\n",
    "    for i in range(sentence.count('\\n')):\n",
    "        sentence.pop(sentence.index(\"\\n\"))\n",
    "    if (\"152\" in sentence):\n",
    "        sentence.pop(sentence.index(\"152\"))\n",
    "    if (\"1820\" in sentence):\n",
    "        sentence.pop(sentence.index(\"1820\"))\n",
    "    # Overwriting the sentence with a word-split one\n",
    "    sentences[index] = sentence\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"context\" environment to then train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in sentences:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make sure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting to define the Neural Network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer\n",
    "\n",
    "It should be as large as the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(len(vocabulary)).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "\n",
    "It is chosen to be only 1 hidden layer and it depends on the embedding dimension (arbitraryli chosen).\n",
    "\n",
    "W1 is the weight matrix. It has dimensions: [embedding_dims, vocabulary_size]\n",
    "\n",
    "There is no activation function â€” just plain matrix multiplication. (This will be clearer during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, len(vocabulary)).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Variable(torch.randn(len(vocabulary), embedding_dims).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 9.346418637197958\n",
      "Loss at epo 10: 7.211674061661062\n",
      "Loss at epo 20: 6.778484885877758\n",
      "Loss at epo 30: 6.574319957424175\n",
      "Loss at epo 40: 6.451177387590353\n",
      "Loss at epo 50: 6.367360884102869\n",
      "Loss at epo 60: 6.305997703103249\n",
      "Loss at epo 70: 6.258726537012349\n",
      "Loss at epo 80: 6.22101781125469\n",
      "Loss at epo 90: 6.190239186260286\n",
      "Loss at epo 100: 6.164635735624557\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 101\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(my_net.parameters(), weight_decay=5e-4)\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "\n",
    "        # Forward pass\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "        \n",
    "\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "        #optimizer.step()\n",
    "        \n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(W1, \"W1_weights\")\n",
    "torch.save(W2, \"W2_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6779, 5])\n"
     ]
    }
   ],
   "source": [
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved parameters for the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6536, -0.2067, -0.2938,  ..., -0.3121, -0.6741,  1.1771],\n",
      "        [ 0.2630,  1.2787,  1.1708,  ..., -0.9615,  0.1497,  1.2112],\n",
      "        [ 0.5051,  0.5456, -0.4282,  ..., -1.2316,  0.3074,  0.5471],\n",
      "        [-0.2665, -0.3556, -0.5891,  ...,  0.3163,  1.4133,  1.0261],\n",
      "        [ 0.4310,  1.6727,  1.3909,  ...,  1.3273,  0.9603,  0.7538]],\n",
      "       requires_grad=True)\n",
      "tensor([[-4.7673,  0.7313,  1.7859, -1.6416,  3.9699],\n",
      "        [ 0.9328, -0.2321,  0.7114,  0.2274, -0.5491],\n",
      "        [-0.0083, -0.1352,  0.4440, -0.7173, -0.1907],\n",
      "        ...,\n",
      "        [ 0.1161, -1.2554,  1.0307, -1.0508, -0.2169],\n",
      "        [ 1.4459, -1.2231, -0.3555,  0.4521,  1.1497],\n",
      "        [-0.5359,  0.0490,  0.0687, -0.1203, -1.4884]], requires_grad=True)\n",
      "3.3844878673553467\n",
      "4.363911151885986\n"
     ]
    }
   ],
   "source": [
    "W1_try = torch.load(\"W1_weights\")\n",
    "W2_try = torch.load(\"W2_weights\")\n",
    "print(W1_try)\n",
    "print(W2_try)\n",
    "print(torch.max(W1_try).item())\n",
    "print(torch.max(W2_try).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking W2 as the word representation because it depends on context! In other words, I think the center is less biased term, and what I need is to be able to predict words of a given text/style/piece of paper; for this reason context representation is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = W2_try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM network\n",
    "\n",
    "Our network is already well implemented and it uses a LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 5\n",
    "hidden_units = 500\n",
    "num_layers = 2\n",
    "my_net = Network(len(vocabulary), hidden_units, num_layers, dropout_prob=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, batch_W2V, loss_fn, optimizer):\n",
    "\n",
    "    ### Prepare network input and labels\n",
    "    \n",
    "    # Get the labels (the last word of each sequence)\n",
    "    #    (In the batch, take, for every batch, the last word with all coordinates)\n",
    "    labels_ = batch_W2V[:, -1, :]\n",
    "    \n",
    "    # Remove the labels from the input tensor\n",
    "    net_input = batch_W2V[:, :-1, :]\n",
    "\n",
    "    ### Forward pass\n",
    "    # Eventually clear previous recorded gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    net_out, _ = net(net_input)\n",
    "\n",
    "    ### Update network\n",
    "    # Evaluate loss only for last output\n",
    "    loss = loss_fn(net_out[:, -1, :], labels_)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    # Return average batch loss\n",
    "    return float(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to use LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'abandon': 1,\n",
       " 'abdicate': 2,\n",
       " 'abject': 3,\n",
       " 'able': 4,\n",
       " 'aborde': 5,\n",
       " 'about': 6,\n",
       " 'above': 7,\n",
       " 'absence': 8,\n",
       " 'absences': 9,\n",
       " 'absolute': 10,\n",
       " 'absolutely': 11,\n",
       " 'absolution': 12,\n",
       " 'absorb': 13,\n",
       " 'absorbed': 14,\n",
       " 'absorption': 15,\n",
       " 'abstract': 16,\n",
       " 'abstracted': 17,\n",
       " 'abstruse': 18,\n",
       " 'absurd': 19,\n",
       " 'absurdly': 20,\n",
       " 'abused': 21,\n",
       " 'academicians': 22,\n",
       " 'academy': 23,\n",
       " 'acanthus': 24,\n",
       " 'acanthuslike': 25,\n",
       " 'accentuating': 26,\n",
       " 'accept': 27,\n",
       " 'acceptance': 28,\n",
       " 'accepting': 29,\n",
       " 'access': 30,\n",
       " 'accident': 31,\n",
       " 'accidental': 32,\n",
       " 'accompanied': 33,\n",
       " 'accompaniment': 34,\n",
       " 'accompany': 35,\n",
       " 'accord': 36,\n",
       " 'accordance': 37,\n",
       " 'according': 38,\n",
       " 'account': 39,\n",
       " 'accounts': 40,\n",
       " 'accumulate': 41,\n",
       " 'accurate': 42,\n",
       " 'accursed': 43,\n",
       " 'accusing': 44,\n",
       " 'accustomed': 45,\n",
       " 'achilles': 46,\n",
       " 'acid': 47,\n",
       " 'acknowledge': 48,\n",
       " 'acquaintance': 49,\n",
       " 'acquaintances': 50,\n",
       " 'acrobats': 51,\n",
       " 'across': 52,\n",
       " 'act': 53,\n",
       " 'acted': 54,\n",
       " 'acting': 55,\n",
       " 'action': 56,\n",
       " 'actions': 57,\n",
       " 'active': 58,\n",
       " 'activity': 59,\n",
       " 'actor': 60,\n",
       " 'actors': 61,\n",
       " 'actress': 62,\n",
       " 'acts': 63,\n",
       " 'actual': 64,\n",
       " 'actuality': 65,\n",
       " 'actually': 66,\n",
       " 'add': 67,\n",
       " 'added': 68,\n",
       " 'adder': 69,\n",
       " 'adders': 70,\n",
       " 'address': 71,\n",
       " 'addressed': 72,\n",
       " 'adjoining': 73,\n",
       " 'admirable': 74,\n",
       " 'admirably': 75,\n",
       " 'admiral': 76,\n",
       " 'admiration': 77,\n",
       " 'admire': 78,\n",
       " 'admired': 79,\n",
       " 'admirer': 80,\n",
       " 'admit': 81,\n",
       " 'admitted': 82,\n",
       " 'adolphe': 83,\n",
       " 'adonis': 84,\n",
       " 'adopt': 85,\n",
       " 'adopted': 86,\n",
       " 'adoration': 87,\n",
       " 'adore': 88,\n",
       " 'adored': 89,\n",
       " 'adores': 90,\n",
       " 'adrian': 91,\n",
       " 'adriatique': 92,\n",
       " 'advance': 93,\n",
       " 'advanced': 94,\n",
       " 'advantage': 95,\n",
       " 'adventure': 96,\n",
       " 'adventurer': 97,\n",
       " 'adversaries': 98,\n",
       " 'advertise': 99,\n",
       " 'advice': 100,\n",
       " 'aeon': 101,\n",
       " 'affair': 102,\n",
       " 'affect': 103,\n",
       " 'affectations': 104,\n",
       " 'affected': 105,\n",
       " 'affection': 106,\n",
       " 'affectionate': 107,\n",
       " 'affects': 108,\n",
       " 'affinity': 109,\n",
       " 'affirmative': 110,\n",
       " 'affluence': 111,\n",
       " 'afford': 112,\n",
       " 'afraid': 113,\n",
       " 'after': 114,\n",
       " 'afternoon': 115,\n",
       " 'afterwards': 116,\n",
       " 'again': 117,\n",
       " 'against': 118,\n",
       " 'agate': 119,\n",
       " 'agatha': 120,\n",
       " 'age': 121,\n",
       " 'aged': 122,\n",
       " 'ages': 123,\n",
       " 'aging': 124,\n",
       " 'agitated': 125,\n",
       " 'agnew': 126,\n",
       " 'ago': 127,\n",
       " 'agony': 128,\n",
       " 'agree': 129,\n",
       " 'ague': 130,\n",
       " 'ah': 131,\n",
       " 'aid': 132,\n",
       " 'aim': 133,\n",
       " 'aiming': 134,\n",
       " 'aims': 135,\n",
       " 'ain': 136,\n",
       " 'air': 137,\n",
       " 'alan': 138,\n",
       " 'albany': 139,\n",
       " 'alchemist': 140,\n",
       " 'alder': 141,\n",
       " 'alders': 142,\n",
       " 'alexander': 143,\n",
       " 'alfonso': 144,\n",
       " 'algerian': 145,\n",
       " 'algiers': 146,\n",
       " 'alice': 147,\n",
       " 'alien': 148,\n",
       " 'alive': 149,\n",
       " 'all': 150,\n",
       " 'alliance': 151,\n",
       " 'alliterative': 152,\n",
       " 'allow': 153,\n",
       " 'allowed': 154,\n",
       " 'almost': 155,\n",
       " 'aloes': 156,\n",
       " 'aloft': 157,\n",
       " 'alone': 158,\n",
       " 'along': 159,\n",
       " 'alphonso': 160,\n",
       " 'already': 161,\n",
       " 'also': 162,\n",
       " 'altar': 163,\n",
       " 'alter': 164,\n",
       " 'altered': 165,\n",
       " 'alternate': 166,\n",
       " 'alternative': 167,\n",
       " 'although': 168,\n",
       " 'always': 169,\n",
       " 'am': 170,\n",
       " 'amarre': 171,\n",
       " 'amateur': 172,\n",
       " 'amateurs': 173,\n",
       " 'amazed': 174,\n",
       " 'amazement': 175,\n",
       " 'amazing': 176,\n",
       " 'amazon': 177,\n",
       " 'ambassador': 178,\n",
       " 'ambassadors': 179,\n",
       " 'amber': 180,\n",
       " 'ambergris': 181,\n",
       " 'ambitions': 182,\n",
       " 'amends': 183,\n",
       " 'america': 184,\n",
       " 'american': 185,\n",
       " 'americans': 186,\n",
       " 'amethyst': 187,\n",
       " 'amethysts': 188,\n",
       " 'amidst': 189,\n",
       " 'among': 190,\n",
       " 'amongst': 191,\n",
       " 'amour': 192,\n",
       " 'ample': 193,\n",
       " 'amsterdam': 194,\n",
       " 'amuse': 195,\n",
       " 'amused': 196,\n",
       " 'amusement': 197,\n",
       " 'amusing': 198,\n",
       " 'an': 199,\n",
       " 'analyse': 200,\n",
       " 'analysing': 201,\n",
       " 'analysis': 202,\n",
       " 'analyzed': 203,\n",
       " 'anastasius': 204,\n",
       " 'ancestors': 205,\n",
       " 'anchorite': 206,\n",
       " 'and': 207,\n",
       " 'andrew': 208,\n",
       " 'anew': 209,\n",
       " 'angels': 210,\n",
       " 'anger': 211,\n",
       " 'anglomania': 212,\n",
       " 'angrily': 213,\n",
       " 'angry': 214,\n",
       " 'anguish': 215,\n",
       " 'animal': 216,\n",
       " 'animalism': 217,\n",
       " 'animals': 218,\n",
       " 'anne': 219,\n",
       " 'annihilated': 220,\n",
       " 'annihilates': 221,\n",
       " 'announce': 222,\n",
       " 'annoy': 223,\n",
       " 'annoyance': 224,\n",
       " 'annoyed': 225,\n",
       " 'annoying': 226,\n",
       " 'anodyne': 227,\n",
       " 'another': 228,\n",
       " 'answer': 229,\n",
       " 'answered': 230,\n",
       " 'answering': 231,\n",
       " 'anthony': 232,\n",
       " 'antidote': 233,\n",
       " 'antinomianism': 234,\n",
       " 'antinous': 235,\n",
       " 'antique': 236,\n",
       " 'antiquity': 237,\n",
       " 'ants': 238,\n",
       " 'anxiety': 239,\n",
       " 'anxious': 240,\n",
       " 'any': 241,\n",
       " 'anybody': 242,\n",
       " 'anything': 243,\n",
       " 'anywhere': 244,\n",
       " 'apartment': 245,\n",
       " 'apes': 246,\n",
       " 'aphorisms': 247,\n",
       " 'apollo': 248,\n",
       " 'apologize': 249,\n",
       " 'apology': 250,\n",
       " 'apparent': 251,\n",
       " 'apparently': 252,\n",
       " 'appeal': 253,\n",
       " 'appeals': 254,\n",
       " 'appear': 255,\n",
       " 'appearance': 256,\n",
       " 'appearances': 257,\n",
       " 'appeared': 258,\n",
       " 'appeased': 259,\n",
       " 'appetite': 260,\n",
       " 'appetites': 261,\n",
       " 'applaud': 262,\n",
       " 'applause': 263,\n",
       " 'apple': 264,\n",
       " 'apples': 265,\n",
       " 'appointment': 266,\n",
       " 'appreciate': 267,\n",
       " 'appreciation': 268,\n",
       " 'apprehension': 269,\n",
       " 'approaching': 270,\n",
       " 'approval': 271,\n",
       " 'approve': 272,\n",
       " 'approvingly': 273,\n",
       " 'apricot': 274,\n",
       " 'apt': 275,\n",
       " 'arabesques': 276,\n",
       " 'arabian': 277,\n",
       " 'aragon': 278,\n",
       " 'arbiter': 279,\n",
       " 'arbitrary': 280,\n",
       " 'arcade': 281,\n",
       " 'arcades': 282,\n",
       " 'arch': 283,\n",
       " 'archaisms': 284,\n",
       " 'archbishop': 285,\n",
       " 'architectural': 286,\n",
       " 'architecture': 287,\n",
       " 'archway': 288,\n",
       " 'archways': 289,\n",
       " 'arden': 290,\n",
       " 'ardent': 291,\n",
       " 'ardour': 292,\n",
       " 'are': 293,\n",
       " 'argot': 294,\n",
       " 'argue': 295,\n",
       " 'aristocracy': 296,\n",
       " 'aristocratic': 297,\n",
       " 'arm': 298,\n",
       " 'armour': 299,\n",
       " 'arms': 300,\n",
       " 'aromatic': 301,\n",
       " 'around': 302,\n",
       " 'arrange': 303,\n",
       " 'arranged': 304,\n",
       " 'arrangement': 305,\n",
       " 'arrangements': 306,\n",
       " 'arrested': 307,\n",
       " 'arresting': 308,\n",
       " 'arrive': 309,\n",
       " 'arrived': 310,\n",
       " 'arrow': 311,\n",
       " 'arrows': 312,\n",
       " 'art': 313,\n",
       " 'artemis': 314,\n",
       " 'articles': 315,\n",
       " 'articulate': 316,\n",
       " 'artificial': 317,\n",
       " 'artificiality': 318,\n",
       " 'artist': 319,\n",
       " 'artistic': 320,\n",
       " 'artistically': 321,\n",
       " 'artists': 322,\n",
       " 'arts': 323,\n",
       " 'as': 324,\n",
       " 'asbestos': 325,\n",
       " 'ascent': 326,\n",
       " 'asceticism': 327,\n",
       " 'ashamed': 328,\n",
       " 'ashes': 329,\n",
       " 'ashton': 330,\n",
       " 'aside': 331,\n",
       " 'ask': 332,\n",
       " 'asked': 333,\n",
       " 'asking': 334,\n",
       " 'asleep': 335,\n",
       " 'asphodel': 336,\n",
       " 'aspilates': 337,\n",
       " 'ass': 338,\n",
       " 'assented': 339,\n",
       " 'assert': 340,\n",
       " 'assistance': 341,\n",
       " 'assistant': 342,\n",
       " 'assisted': 343,\n",
       " 'associate': 344,\n",
       " 'assume': 345,\n",
       " 'assumed': 346,\n",
       " 'assure': 347,\n",
       " 'assured': 348,\n",
       " 'assures': 349,\n",
       " 'assuring': 350,\n",
       " 'astorre': 351,\n",
       " 'astounding': 352,\n",
       " 'astrakhan': 353,\n",
       " 'astray': 354,\n",
       " 'at': 355,\n",
       " 'atalanta': 356,\n",
       " 'ate': 357,\n",
       " 'athena': 358,\n",
       " 'athenaeum': 359,\n",
       " 'atmosphere': 360,\n",
       " 'atom': 361,\n",
       " 'atoms': 362,\n",
       " 'atone': 363,\n",
       " 'atoned': 364,\n",
       " 'atonement': 365,\n",
       " 'atones': 366,\n",
       " 'attachment': 367,\n",
       " 'attack': 368,\n",
       " 'attacking': 369,\n",
       " 'attain': 370,\n",
       " 'attempt': 371,\n",
       " 'attempted': 372,\n",
       " 'attempts': 373,\n",
       " 'attention': 374,\n",
       " 'attic': 375,\n",
       " 'attitude': 376,\n",
       " 'attracted': 377,\n",
       " 'attraction': 378,\n",
       " 'attractions': 379,\n",
       " 'au': 380,\n",
       " 'auction': 381,\n",
       " 'auctioneer': 382,\n",
       " 'audace': 383,\n",
       " 'audacious': 384,\n",
       " 'audible': 385,\n",
       " 'audience': 386,\n",
       " 'audley': 387,\n",
       " 'aunt': 388,\n",
       " 'australia': 389,\n",
       " 'author': 390,\n",
       " 'authoritative': 391,\n",
       " 'authority': 392,\n",
       " 'autobiography': 393,\n",
       " 'automatons': 394,\n",
       " 'autumn': 395,\n",
       " 'avenue': 396,\n",
       " 'averted': 397,\n",
       " 'avoid': 398,\n",
       " 'awake': 399,\n",
       " 'awakened': 400,\n",
       " 'away': 401,\n",
       " 'awe': 402,\n",
       " 'awful': 403,\n",
       " 'awfully': 404,\n",
       " 'awkward': 405,\n",
       " 'awoke': 406,\n",
       " 'aztecs': 407,\n",
       " 'azur': 408,\n",
       " 'bacchante': 409,\n",
       " 'bachelor': 410,\n",
       " 'bachelors': 411,\n",
       " 'back': 412,\n",
       " 'backed': 413,\n",
       " 'background': 414,\n",
       " 'backs': 415,\n",
       " 'backwards': 416,\n",
       " 'bad': 417,\n",
       " 'badly': 418,\n",
       " 'bag': 419,\n",
       " 'baglioni': 420,\n",
       " 'bags': 421,\n",
       " 'balance': 422,\n",
       " 'balancing': 423,\n",
       " 'balas': 424,\n",
       " 'balasses': 425,\n",
       " 'balcony': 426,\n",
       " 'bald': 427,\n",
       " 'ball': 428,\n",
       " 'balls': 429,\n",
       " 'balms': 430,\n",
       " 'balustrade': 431,\n",
       " 'bamboo': 432,\n",
       " 'band': 433,\n",
       " 'bank': 434,\n",
       " 'banker': 435,\n",
       " 'bankrupt': 436,\n",
       " 'bankruptcies': 437,\n",
       " 'bar': 438,\n",
       " 'barbaric': 439,\n",
       " 'barbi': 440,\n",
       " 'bard': 441,\n",
       " 'bare': 442,\n",
       " 'bareheaded': 443,\n",
       " 'bargain': 444,\n",
       " 'bargaining': 445,\n",
       " 'barge': 446,\n",
       " 'barges': 447,\n",
       " 'barked': 448,\n",
       " 'baronet': 449,\n",
       " 'barrel': 450,\n",
       " 'barren': 451,\n",
       " 'bars': 452,\n",
       " 'bartered': 453,\n",
       " 'basil': 454,\n",
       " 'basis': 455,\n",
       " 'bathed': 456,\n",
       " 'bathroom': 457,\n",
       " 'battened': 458,\n",
       " 'battle': 459,\n",
       " 'bauderike': 460,\n",
       " 'be': 461,\n",
       " 'beaded': 462,\n",
       " 'beads': 463,\n",
       " 'beam': 464,\n",
       " 'beaming': 465,\n",
       " 'beams': 466,\n",
       " 'bear': 467,\n",
       " 'beard': 468,\n",
       " 'bears': 469,\n",
       " 'beast': 470,\n",
       " 'beasts': 471,\n",
       " 'beat': 472,\n",
       " 'beaten': 473,\n",
       " 'beater': 474,\n",
       " 'beaters': 475,\n",
       " 'beating': 476,\n",
       " 'beatrice': 477,\n",
       " 'beats': 478,\n",
       " 'beau': 479,\n",
       " 'beauteous': 480,\n",
       " 'beautiful': 481,\n",
       " 'beautifully': 482,\n",
       " 'beauty': 483,\n",
       " 'became': 484,\n",
       " 'because': 485,\n",
       " 'beckenham': 486,\n",
       " 'become': 487,\n",
       " 'becomes': 488,\n",
       " 'becoming': 489,\n",
       " 'bed': 490,\n",
       " 'bedroom': 491,\n",
       " 'beds': 492,\n",
       " 'bedside': 493,\n",
       " 'bee': 494,\n",
       " 'been': 495,\n",
       " 'beer': 496,\n",
       " 'bees': 497,\n",
       " 'beethoven': 498,\n",
       " 'beetle': 499,\n",
       " 'beetles': 500,\n",
       " 'before': 501,\n",
       " 'beg': 502,\n",
       " 'began': 503,\n",
       " 'beggar': 504,\n",
       " 'begged': 505,\n",
       " 'begin': 506,\n",
       " 'beginning': 507,\n",
       " 'begins': 508,\n",
       " 'begun': 509,\n",
       " 'behave': 510,\n",
       " 'behaved': 511,\n",
       " 'behaves': 512,\n",
       " 'beheld': 513,\n",
       " 'behind': 514,\n",
       " 'behold': 515,\n",
       " 'being': 516,\n",
       " 'belgian': 517,\n",
       " 'belief': 518,\n",
       " 'believe': 519,\n",
       " 'believed': 520,\n",
       " 'believes': 521,\n",
       " 'believing': 522,\n",
       " 'bell': 523,\n",
       " 'bells': 524,\n",
       " 'belong': 525,\n",
       " 'belonged': 526,\n",
       " 'belonging': 527,\n",
       " 'belongs': 528,\n",
       " 'below': 529,\n",
       " 'benches': 530,\n",
       " 'bend': 531,\n",
       " 'bending': 532,\n",
       " 'beneath': 533,\n",
       " 'benefit': 534,\n",
       " 'benefiting': 535,\n",
       " 'bent': 536,\n",
       " 'bepaint': 537,\n",
       " 'bequeathed': 538,\n",
       " 'berkeley': 539,\n",
       " 'berkshire': 540,\n",
       " 'bernal': 541,\n",
       " 'berwick': 542,\n",
       " 'beryl': 543,\n",
       " 'beside': 544,\n",
       " 'besides': 545,\n",
       " 'best': 546,\n",
       " 'bestial': 547,\n",
       " 'better': 548,\n",
       " 'betters': 549,\n",
       " 'betting': 550,\n",
       " 'between': 551,\n",
       " 'beware': 552,\n",
       " 'bewilder': 553,\n",
       " 'beyond': 554,\n",
       " 'bezoar': 555,\n",
       " 'bible': 556,\n",
       " 'bid': 557,\n",
       " 'bill': 558,\n",
       " 'bills': 559,\n",
       " 'binding': 560,\n",
       " 'biology': 561,\n",
       " 'bird': 562,\n",
       " 'birds': 563,\n",
       " 'birrell': 564,\n",
       " 'birth': 565,\n",
       " 'birthday': 566,\n",
       " 'bishop': 567,\n",
       " 'bismuth': 568,\n",
       " 'bit': 569,\n",
       " 'biting': 570,\n",
       " 'bits': 571,\n",
       " 'bitten': 572,\n",
       " 'bitter': 573,\n",
       " 'bitterly': 574,\n",
       " 'bitterness': 575,\n",
       " 'bitters': 576,\n",
       " 'black': 577,\n",
       " 'blackballed': 578,\n",
       " 'blackmailed': 579,\n",
       " 'blade': 580,\n",
       " 'blame': 581,\n",
       " 'blanc': 582,\n",
       " 'blank': 583,\n",
       " 'blasphemy': 584,\n",
       " 'blast': 585,\n",
       " 'blazed': 586,\n",
       " 'blazing': 587,\n",
       " 'blazoned': 588,\n",
       " 'bleached': 589,\n",
       " 'blended': 590,\n",
       " 'blessed': 591,\n",
       " 'blessing': 592,\n",
       " 'blew': 593,\n",
       " 'blind': 594,\n",
       " 'blindly': 595,\n",
       " 'blinds': 596,\n",
       " 'blinking': 597,\n",
       " 'bloated': 598,\n",
       " 'blood': 599,\n",
       " 'bloom': 600,\n",
       " 'bloomed': 601,\n",
       " 'blooms': 602,\n",
       " 'blossom': 603,\n",
       " 'blossoms': 604,\n",
       " 'blotted': 605,\n",
       " 'blow': 606,\n",
       " 'blowing': 607,\n",
       " 'blown': 608,\n",
       " 'blows': 609,\n",
       " 'blue': 610,\n",
       " 'blurred': 611,\n",
       " 'blush': 612,\n",
       " 'blushes': 613,\n",
       " 'blushing': 614,\n",
       " 'boar': 615,\n",
       " 'board': 616,\n",
       " 'body': 617,\n",
       " 'bold': 618,\n",
       " 'boldness': 619,\n",
       " 'bologna': 620,\n",
       " 'bolted': 621,\n",
       " 'bolts': 622,\n",
       " 'bond': 623,\n",
       " 'bonds': 624,\n",
       " 'bone': 625,\n",
       " 'bones': 626,\n",
       " 'boniface': 627,\n",
       " 'bonnet': 628,\n",
       " 'bonnets': 629,\n",
       " 'book': 630,\n",
       " 'books': 631,\n",
       " 'boot': 632,\n",
       " 'booth': 633,\n",
       " 'boots': 634,\n",
       " 'bordered': 635,\n",
       " 'bore': 636,\n",
       " 'bored': 637,\n",
       " 'bores': 638,\n",
       " 'borgia': 639,\n",
       " 'boring': 640,\n",
       " 'born': 641,\n",
       " 'borne': 642,\n",
       " 'borrowed': 643,\n",
       " 'both': 644,\n",
       " 'bother': 645,\n",
       " 'bothering': 646,\n",
       " 'bottle': 647,\n",
       " 'boughs': 648,\n",
       " 'bought': 649,\n",
       " 'bound': 650,\n",
       " 'bounded': 651,\n",
       " 'bouquets': 652,\n",
       " 'bourdon': 653,\n",
       " 'bournemouth': 654,\n",
       " 'bow': 655,\n",
       " 'bowed': 656,\n",
       " 'bowing': 657,\n",
       " 'bowl': 658,\n",
       " 'box': 659,\n",
       " 'boy': 660,\n",
       " 'boyhood': 661,\n",
       " 'boyish': 662,\n",
       " 'boys': 663,\n",
       " 'brabantio': 664,\n",
       " 'brac': 665,\n",
       " 'bracken': 666,\n",
       " 'brain': 667,\n",
       " 'brainless': 668,\n",
       " 'brains': 669,\n",
       " 'branches': 670,\n",
       " 'branded': 671,\n",
       " 'brandon': 672,\n",
       " 'brandy': 673,\n",
       " 'branksome': 674,\n",
       " 'brantome': 675,\n",
       " 'brass': 676,\n",
       " 'braved': 677,\n",
       " 'bravest': 678,\n",
       " 'brawl': 679,\n",
       " 'brawled': 680,\n",
       " 'brawling': 681,\n",
       " 'brazier': 682,\n",
       " 'brazilian': 683,\n",
       " 'bread': 684,\n",
       " 'break': 685,\n",
       " 'breakfast': 686,\n",
       " 'breaking': 687,\n",
       " 'breaks': 688,\n",
       " 'breast': 689,\n",
       " 'breasted': 690,\n",
       " 'breath': 691,\n",
       " 'breathed': 692,\n",
       " 'breathing': 693,\n",
       " 'bred': 694,\n",
       " 'breed': 695,\n",
       " 'brevity': 696,\n",
       " 'bric': 697,\n",
       " 'brickdust': 698,\n",
       " 'brickfields': 699,\n",
       " 'bride': 700,\n",
       " 'brief': 701,\n",
       " 'bright': 702,\n",
       " 'brighter': 703,\n",
       " 'brightly': 704,\n",
       " 'brightness': 705,\n",
       " 'brilliancy': 706,\n",
       " 'brilliant': 707,\n",
       " 'brilliantly': 708,\n",
       " 'bring': 709,\n",
       " 'bringing': 710,\n",
       " 'brings': 711,\n",
       " 'brink': 712,\n",
       " 'bristol': 713,\n",
       " 'british': 714,\n",
       " 'brocade': 715,\n",
       " 'brocades': 716,\n",
       " 'broideries': 717,\n",
       " 'broidery': 718,\n",
       " 'broke': 719,\n",
       " 'broken': 720,\n",
       " 'broker': 721,\n",
       " 'bronze': 722,\n",
       " 'bronzes': 723,\n",
       " 'brood': 724,\n",
       " 'brooded': 725,\n",
       " 'broods': 726,\n",
       " 'brother': 727,\n",
       " 'brothers': 728,\n",
       " 'brougham': 729,\n",
       " 'brought': 730,\n",
       " 'brown': 731,\n",
       " 'browning': 732,\n",
       " 'brows': 733,\n",
       " 'bruise': 734,\n",
       " 'bruno': 735,\n",
       " 'brush': 736,\n",
       " 'brushed': 737,\n",
       " 'brushes': 738,\n",
       " 'brushing': 739,\n",
       " 'brushwork': 740,\n",
       " 'brutal': 741,\n",
       " 'brute': 742,\n",
       " 'bubble': 743,\n",
       " 'bubbles': 744,\n",
       " 'bud': 745,\n",
       " 'built': 746,\n",
       " 'bulky': 747,\n",
       " 'bull': 748,\n",
       " 'bullied': 749,\n",
       " 'bulwark': 750,\n",
       " 'bunch': 751,\n",
       " 'buonarotti': 752,\n",
       " 'burden': 753,\n",
       " 'burdened': 754,\n",
       " 'burdon': 755,\n",
       " 'burgundy': 756,\n",
       " 'buried': 757,\n",
       " 'burlington': 758,\n",
       " 'burn': 759,\n",
       " 'burned': 760,\n",
       " 'burning': 761,\n",
       " 'burnished': 762,\n",
       " 'burns': 763,\n",
       " 'burnt': 764,\n",
       " 'burying': 765,\n",
       " 'bush': 766,\n",
       " 'bushrangers': 767,\n",
       " 'bushy': 768,\n",
       " 'business': 769,\n",
       " 'bustled': 770,\n",
       " 'busy': 771,\n",
       " 'but': 772,\n",
       " 'butler': 773,\n",
       " 'butterflies': 774,\n",
       " 'butterfly': 775,\n",
       " 'button': 776,\n",
       " 'buy': 777,\n",
       " 'buying': 778,\n",
       " 'buzz': 779,\n",
       " 'buzzed': 780,\n",
       " 'by': 781,\n",
       " 'bye': 782,\n",
       " 'cab': 783,\n",
       " 'cabinet': 784,\n",
       " 'cabman': 785,\n",
       " 'cabmen': 786,\n",
       " 'cabs': 787,\n",
       " 'cadence': 788,\n",
       " 'caelestis': 789,\n",
       " 'caesar': 790,\n",
       " 'cafe': 791,\n",
       " 'caged': 792,\n",
       " 'cairo': 793,\n",
       " 'cake': 794,\n",
       " 'cakes': 795,\n",
       " 'caliban': 796,\n",
       " 'caligula': 797,\n",
       " 'call': 798,\n",
       " 'called': 799,\n",
       " 'calling': 800,\n",
       " 'callous': 801,\n",
       " 'callousness': 802,\n",
       " 'calls': 803,\n",
       " 'calm': 804,\n",
       " 'calmly': 805,\n",
       " 'calumnies': 806,\n",
       " 'cambridge': 807,\n",
       " 'came': 808,\n",
       " 'camees': 809,\n",
       " 'camillus': 810,\n",
       " 'camp': 811,\n",
       " 'campanile': 812,\n",
       " 'campbell': 813,\n",
       " 'can': 814,\n",
       " 'candle': 815,\n",
       " 'candleshades': 816,\n",
       " 'candour': 817,\n",
       " 'cane': 818,\n",
       " 'cannot': 819,\n",
       " 'canons': 820,\n",
       " 'canopy': 821,\n",
       " 'canvas': 822,\n",
       " 'cap': 823,\n",
       " 'capable': 824,\n",
       " 'capacity': 825,\n",
       " 'cape': 826,\n",
       " 'capital': 827,\n",
       " 'capri': 828,\n",
       " 'caprice': 829,\n",
       " 'capricious': 830,\n",
       " 'captain': 831,\n",
       " 'capture': 832,\n",
       " 'capulet': 833,\n",
       " 'carbuncles': 834,\n",
       " 'card': 835,\n",
       " 'cardinal': 836,\n",
       " 'cards': 837,\n",
       " 'care': 838,\n",
       " 'cared': 839,\n",
       " 'career': 840,\n",
       " 'careful': 841,\n",
       " 'carefully': 842,\n",
       " 'carelessness': 843,\n",
       " 'cares': 844,\n",
       " 'caressed': 845,\n",
       " 'carlington': 846,\n",
       " 'carlo': 847,\n",
       " 'carlton': 848,\n",
       " 'carnal': 849,\n",
       " 'carnations': 850,\n",
       " 'caroused': 851,\n",
       " 'carpet': 852,\n",
       " 'carriage': 853,\n",
       " 'carried': 854,\n",
       " 'carry': 855,\n",
       " 'carrying': 856,\n",
       " 'cars': 857,\n",
       " 'cart': 858,\n",
       " 'carter': 859,\n",
       " 'carthage': 860,\n",
       " 'cartridges': 861,\n",
       " 'carts': 862,\n",
       " 'carved': 863,\n",
       " 'caryatides': 864,\n",
       " 'case': 865,\n",
       " 'cases': 866,\n",
       " 'cashmere': 867,\n",
       " 'cassone': 868,\n",
       " 'cast': 869,\n",
       " 'caste': 870,\n",
       " 'casual': 871,\n",
       " 'cat': 872,\n",
       " 'catch': 873,\n",
       " 'catching': 874,\n",
       " 'category': 875,\n",
       " 'catherine': 876,\n",
       " 'catholic': 877,\n",
       " 'caught': 878,\n",
       " 'cause': 879,\n",
       " 'caused': 880,\n",
       " 'cave': 881,\n",
       " 'caveman': 882,\n",
       " 'cease': 883,\n",
       " 'ceased': 884,\n",
       " 'cedar': 885,\n",
       " 'ceilan': 886,\n",
       " 'ceiling': 887,\n",
       " 'celebrated': 888,\n",
       " 'cell': 889,\n",
       " 'cells': 890,\n",
       " 'censer': 891,\n",
       " 'censers': 892,\n",
       " 'censure': 893,\n",
       " 'cent': 894,\n",
       " 'centaurs': 895,\n",
       " 'centre': 896,\n",
       " 'centuries': 897,\n",
       " 'century': 898,\n",
       " 'ceremony': 899,\n",
       " 'certain': 900,\n",
       " 'certainly': 901,\n",
       " 'certainty': 902,\n",
       " 'chain': 903,\n",
       " 'chains': 904,\n",
       " 'chair': 905,\n",
       " 'chairs': 906,\n",
       " 'chalice': 907,\n",
       " 'chalk': 908,\n",
       " 'challenge': 909,\n",
       " 'chamber': 910,\n",
       " 'chambers': 911,\n",
       " 'champagne': 912,\n",
       " 'champak': 913,\n",
       " 'champion': 914,\n",
       " 'chance': 915,\n",
       " 'chancing': 916,\n",
       " 'change': 917,\n",
       " 'changed': 918,\n",
       " 'changing': 919,\n",
       " 'chaos': 920,\n",
       " 'chap': 921,\n",
       " 'chapman': 922,\n",
       " 'chaps': 923,\n",
       " 'chapter': 924,\n",
       " 'chapters': 925,\n",
       " 'character': 926,\n",
       " 'characteristic': 927,\n",
       " 'characterizes': 928,\n",
       " 'characters': 929,\n",
       " 'charcoal': 930,\n",
       " 'charge': 931,\n",
       " 'charges': 932,\n",
       " 'chariot': 933,\n",
       " 'charitable': 934,\n",
       " 'charity': 935,\n",
       " 'charles': 936,\n",
       " 'charm': 937,\n",
       " 'charmant': 938,\n",
       " 'charmed': 939,\n",
       " 'charming': 940,\n",
       " 'charmingly': 941,\n",
       " 'charms': 942,\n",
       " 'charpentier': 943,\n",
       " 'chase': 944,\n",
       " 'chased': 945,\n",
       " 'chasing': 946,\n",
       " 'chaste': 947,\n",
       " 'chasubles': 948,\n",
       " 'chat': 949,\n",
       " 'chatter': 950,\n",
       " 'chattered': 951,\n",
       " 'chattering': 952,\n",
       " 'chaud': 953,\n",
       " 'cheated': 954,\n",
       " 'cheek': 955,\n",
       " 'cheeked': 956,\n",
       " 'cheeks': 957,\n",
       " 'cheeky': 958,\n",
       " 'chef': 959,\n",
       " 'chemical': 960,\n",
       " 'chemicals': 961,\n",
       " 'chemist': 962,\n",
       " 'chemistry': 963,\n",
       " 'chequebook': 964,\n",
       " 'cheques': 965,\n",
       " 'cheroot': 966,\n",
       " 'cherries': 967,\n",
       " 'chess': 968,\n",
       " 'chest': 969,\n",
       " 'chestnut': 970,\n",
       " 'chests': 971,\n",
       " 'chicago': 972,\n",
       " 'chief': 973,\n",
       " 'chiefest': 974,\n",
       " 'chiefly': 975,\n",
       " 'child': 976,\n",
       " 'childhood': 977,\n",
       " 'childish': 978,\n",
       " 'childlike': 979,\n",
       " 'children': 980,\n",
       " 'chile': 981,\n",
       " 'chill': 982,\n",
       " 'chilperic': 983,\n",
       " 'chime': 984,\n",
       " 'chimney': 985,\n",
       " 'china': 986,\n",
       " 'chinese': 987,\n",
       " 'chirrup': 988,\n",
       " 'chirruping': 989,\n",
       " 'chiselled': 990,\n",
       " 'chocolate': 991,\n",
       " 'choice': 992,\n",
       " 'choked': 993,\n",
       " 'choking': 994,\n",
       " 'choose': 995,\n",
       " 'chooses': 996,\n",
       " 'chop': 997,\n",
       " 'chopin': 998,\n",
       " 'chord': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the encoded data\n",
    "def encode_text(word_to_index, text):\n",
    "    encoded = [word_to_index[w] for w in text]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Train network\n",
    "    \n",
    "# Define Dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=args.batchsize, shuffle=True, num_workers=1)\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=5e-4)\n",
    "# Define loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Start training\n",
    "for epoch in range(args.num_epochs):\n",
    "    print('##################################')\n",
    "    print('## EPOCH %d' % (epoch + 1))\n",
    "    print('##################################')\n",
    "    # Iterate batches\n",
    "    for batch_sample in dataloader:\n",
    "        # Extract batch\n",
    "        batch_onehot = batch_sample['encoded_onehot'].to(device)\n",
    "        # Update network\n",
    "        batch_loss = train_batch(net, batch_onehot, loss_fn, optimizer)\n",
    "        print('\\t Training loss (single batch):', batch_loss)\n",
    "\n",
    "### Save all needed parameters\n",
    "# Create output dir\n",
    "out_dir = Path(args.out_dir)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Save network parameters\n",
    "torch.save(net.state_dict(), out_dir / 'net_params.pth')\n",
    "# Save training parameters\n",
    "with open(out_dir / 'training_args.json', 'w') as f:\n",
    "    json.dump(vars(args), f, indent=4)\n",
    "# Save encoder dictionary\n",
    "with open(out_dir / 'char_to_number.json', 'w') as f:\n",
    "    json.dump(dataset.char_to_number, f, indent=4)\n",
    "# Save decoder dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 101\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(my_net.parameters(), weight_decay=5e-4)\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        ### Forward pass\n",
    "        # Eventually clear previous recorded gradients\n",
    "        #optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2, _ = my_net(x)\n",
    "        \n",
    "\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "        #optimizer.step()\n",
    "        \n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
