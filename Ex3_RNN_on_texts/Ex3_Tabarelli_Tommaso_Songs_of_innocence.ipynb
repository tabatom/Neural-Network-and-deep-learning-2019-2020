{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional Neuroscience\n",
    "Accademic year 2019-2020\n",
    "Homework 3\n",
    "\n",
    "Author: Tommaso Tabarelli\n",
    "Period: december 2019\n",
    "\"\"\"\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import reduce\n",
    "from torch import optim, nn\n",
    "from network import Network, train_batch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "\tdef __init__(self, input_size, hidden_units, layers_num, dropout_prob=0):\n",
    "\t\t# Call the parent init function (required!)\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Define recurrent layer\n",
    "\t\tself.rnn = nn.LSTM(input_size=input_size, \n",
    "\t\t\t\t\t\thidden_size=hidden_units,\n",
    "\t\t\t\t\t\tnum_layers=layers_num,\n",
    "\t\t\t\t\t\tdropout=dropout_prob,\n",
    "\t\t\t\t\t\tbatch_first=True)\n",
    "\t\t# Define output layer\n",
    "\t\tself.out = nn.Linear(hidden_units, input_size)\n",
    "\n",
    "\tdef forward(self, x, state=None):\n",
    "\t\t# LSTM\n",
    "\t\tx, rnn_state = self.rnn(x, state)\n",
    "\t\t# Linear layer\n",
    "\t\tx = self.out(x)\n",
    "\t\treturn x, rnn_state\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train_batch(net, batch_onehot, loss_fn, optimizer):\n",
    "\n",
    "\t### Prepare network input and labels\n",
    "\t# Get the labels (the last letter of each sequence)\n",
    "\tlabels_onehot = batch_onehot[:, -1, :]\n",
    "\tlabels_numbers = labels_onehot.argmax(dim=1)\n",
    "\t# Remove the labels from the input tensor\n",
    "\tnet_input = batch_onehot[:, :-1, :]\n",
    "\t# batch_onehot.shape =   [50, 100, 38]\n",
    "\t# labels_onehot.shape =  [50, 38]\n",
    "\t# labels_numbers.shape = [50]\n",
    "\t# net_input.shape =      [50, 99, 38]\n",
    "\n",
    "\t### Forward pass\n",
    "\t# Eventually clear previous recorded gradients\n",
    "\toptimizer.zero_grad()\n",
    "\t# Forward pass\n",
    "\tnet_out, _ = net(net_input)\n",
    "\n",
    "\t### Update network\n",
    "\t# Evaluate loss only for last output\n",
    "\tloss = loss_fn(net_out[:, -1, :], labels_numbers)\n",
    "\t# Backward pass\n",
    "\tloss.backward()\n",
    "\t# Update\n",
    "\toptimizer.step()\n",
    "\t# Return average batch loss\n",
    "\treturn float(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--out_dir'], dest='out_dir', nargs=None, const=None, default='model', type=<class 'str'>, choices=None, help='Where to save models and params', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "##############################\n",
    "## PARAMETERS\n",
    "##############################\n",
    "parser = argparse.ArgumentParser(description='Train the Blake sonnet generator network.')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--datasetpath', type=str, default='Songs_of_innocence.txt',\n",
    "                        help='Path of the train txt file')\n",
    "parser.add_argument('--crop_len',    type=int, default=100,\n",
    "                        help='Number of input letters')\n",
    "#parser.add_argument('--alphabet_len',   type=int,   default=,                help='Number of letters in the alphabet')\n",
    "\n",
    "# Network\n",
    "parser.add_argument('--hidden_units',   type=int,   default=128,    help='Number of RNN hidden units')\n",
    "parser.add_argument('--layers_num',     type=int,   default=2,      help='Number of RNN stacked layers')\n",
    "parser.add_argument('--dropout_prob',   type=float, default=0.3,    help='Dropout probability')\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--batchsize',  type=int, default=154,  help='Training batch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=1000, help='Number of training epochs')\n",
    "\n",
    "# Save\n",
    "parser.add_argument('--out_dir', type=str, default='model', help='Where to save models and params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.B.: to change and modify using text and not sonnet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "\n",
      "THE SHEPHERD\n",
      "\n",
      "How sweet is the shepherd’s sweet lot!\n",
      "From the morn to the evening he strays;\n",
      "He shall follow his sheep all the day,\n",
      "And his tongue shall be fillèd with praise.\n",
      "\n",
      "For he hears the lambs’ innocent call,\n",
      "And he hears the ewes’ tender reply;\n",
      "He is watchful while they are in peace,\n",
      "For they know when their shepherd is nigh.\n",
      "48\n",
      "---- \n",
      "Piping down the valleys wild,\n",
      "Piping songs of pleasant glee,\n",
      "On a cloud I saw a child,\n",
      "And he laughing said to me:\n",
      "\n",
      "‘Pipe a song about a Lamb!’\n",
      "So I piped with merry cheer.\n",
      "‘Piper, pipe that song again.’\n",
      "So I piped: he wept to hear.\n",
      "\n",
      "‘Drop thy pipe, thy happy pipe;\n",
      "Sing thy songs of happy cheer!’\n",
      "So I sung the same again,\n",
      "While he wept with joy to hear.\n",
      "\n",
      "\n",
      "‘Piper, sit thee down and write\n",
      "In a book, that all may read.’\n",
      "So he vanished from my sight;\n",
      "And I plucked a hollow reed,\n",
      "\n",
      "And I made a rural pen,\n",
      "And I stained the water clear,\n",
      "And I wrote my happy songs\n",
      "Every child may joy to hear. ----\n",
      "---- \n",
      "Youth of delight! come hither\n",
      "And see the opening morn,\n",
      "Image of Truth new-born.\n",
      "Doubt is fled, and clouds of reason,\n",
      "Dark disputes and artful teazing.\n",
      "Folly is an endless maze;\n",
      "Tangled roots perplex her ways;\n",
      "How many have fallen there!\n",
      "They stumble all night over bones of the dead;\n",
      "And feel—they know not what but care;\n",
      "And wish to lead others, when they should be led.\n",
      " ----\n"
     ]
    }
   ],
   "source": [
    "text = open(\"Songs_of_innocence.txt\").read()\n",
    "\n",
    "ii = 2\n",
    "\n",
    "sonnet_list_ = re.split('\\n{5}', text)\n",
    "\n",
    "print(len(sonnet_list_))\n",
    "print(sonnet_list_[ii])\n",
    "\n",
    "# Removing titles\n",
    "#sonnet_list = list(map(lambda s: re.sub('\\n?[0-9]?[[A-Z]{2,}[ -]?[A-Z]{2,}]*\\n?', '', s), sonnet_list))\n",
    "sonnet_list_ = list(map(lambda s: re.sub('\\n?[0:9]?([A-Z]{2,}[- ]?(\\'S)?){1,}\\n?', '', s), sonnet_list_))\n",
    "\n",
    "# Eliminating the first element since it's empty\n",
    "sonnet_list_.pop(0)\n",
    "\n",
    "print(len(sonnet_list_))\n",
    "\n",
    "print(\"----\", sonnet_list_[0], \"----\")\n",
    "\n",
    "sonnet_list_ = list(map(lambda s: re.sub('\\n{2,3}', '\\n', s), sonnet_list_))\n",
    "\n",
    "print(\"----\", sonnet_list_[-1], \"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "\n",
      "how sweet is the shepherd’s sweet lot!\n",
      "from the morn to the evening he strays;\n",
      "he shall follow his sheep all the day,\n",
      "and his tongue shall be fillèd with praise.\n",
      "for he hears the lambs’ innocent call,\n",
      "and he hears the ewes’ tender reply;\n",
      "he is watchful while they are in peace,\n",
      "for they know when their shepherd is nigh.\n"
     ]
    }
   ],
   "source": [
    "text = open(\"Songs_of_innocence.txt\").read()\n",
    "\n",
    "text = re.sub('\\n?[0:9]?([A-Z]{2,}[- ]?(’S)?){1,}\\n?', '', text)\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "sonnet_list = re.split('\\n{5}', text)\n",
    "\n",
    "sonnet_list = list(map(lambda s: re.sub('\\n{2,3}', '\\n', s), sonnet_list))\n",
    "\n",
    "# Eliminating the first sonnet (it is only empty string)\n",
    "sonnet_list.pop(0)\n",
    "\n",
    "print(len(sonnet_list))\n",
    "\n",
    "print(sonnet_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "piping down the valleys wild,\n",
      "piping songs of pleasant glee,\n",
      "on a cloud i saw a child,\n",
      "and he laughing said to me:\n",
      "‘pipe a song about a lamb!’\n",
      "so i piped with merry cheer.\n",
      "‘piper, pipe that song again.’\n",
      "so i piped: he wept to hear.\n",
      "‘drop thy pipe, thy happy pipe;\n",
      "sing thy songs of happy cheer!’\n",
      "so i sung the same again,\n",
      "while he wept with joy to hear.\n",
      "‘piper, sit thee down and write\n",
      "in a book, that all may read.’\n",
      "so he vanished from my sight;\n",
      "and i plucked a hollow reed,\n",
      "and i made a rural pen,\n",
      "and i stained the water clear,\n",
      "and i wrote my happy songs\n",
      "every child may joy to hear. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "how sweet is the shepherd’s sweet lot!\n",
      "from the morn to the evening he strays;\n",
      "he shall follow his sheep all the day,\n",
      "and his tongue shall be fillèd with praise.\n",
      "for he hears the lambs’ innocent call,\n",
      "and he hears the ewes’ tender reply;\n",
      "he is watchful while they are in peace,\n",
      "for they know when their shepherd is nigh. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "the sun does arise,\n",
      "and make happy the skies;\n",
      "the merry bells ring\n",
      "to welcome the spring;\n",
      "the skylark and thrush,\n",
      "the birds of the bush,\n",
      "sing louder around\n",
      "to the bells’ cheerful sound;\n",
      "while our sports shall be seen\n",
      "on the echoing green.\n",
      "old john, with white hair,\n",
      "does laugh away care,\n",
      "sitting under the oak,\n",
      "among the old folk.\n",
      "they laugh at our play,\n",
      "and soon they all say,\n",
      "‘such, such were the joys\n",
      "when we all—girls and boys—\n",
      "in our youth-time were seen\n",
      "on the echoing green.’\n",
      "till the little ones, weary,\n",
      "no more can be merry:\n",
      "the sun does descend,\n",
      "and our sports have an end.\n",
      "round the laps of their mothers\n",
      "many sisters and brothers,\n",
      "like birds in their nest,\n",
      "are ready for rest,\n",
      "and sport no more seen\n",
      "on the darkening green. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "little lamb, who made thee?\n",
      "does thou know who made thee,\n",
      "gave thee life, and bid thee feed\n",
      "by the stream and o’er the mead;\n",
      "gave thee clothing of delight,\n",
      "softest clothing, woolly, bright;\n",
      "gave thee such a tender voice,\n",
      "making all the vales rejoice?\n",
      "little lamb, who made thee?\n",
      "does thou know who made thee?\n",
      "little lamb, i’ll tell thee;\n",
      "little lamb, i’ll tell thee:\n",
      "he is callèd by thy name,\n",
      "for he calls himself a lamb.\n",
      "he is meek, and he is mild,\n",
      "he became a little child.\n",
      "i a child, and thou a lamb,\n",
      "we are callèd by his name.\n",
      "little lamb, god bless thee!\n",
      "little lamb, god bless thee! \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "my mother bore me in the southern wild,\n",
      "and i am black, but o my soul is white!\n",
      "white as an angel is the english child,\n",
      "but i am black, as if bereaved of light.\n",
      "my mother taught me underneath a tree,\n",
      "and, sitting down before the heat of day,\n",
      "she took me on her lap and kissèd me,\n",
      "and, pointing to the east, began to say:\n",
      "‘look on the rising sun: there god does live,\n",
      "and gives his light, and gives his heat away,\n",
      "and flowers and trees and beasts and men receive\n",
      "comfort in morning, joy in the noonday.\n",
      "‘and we are put on earth a little space,\n",
      "that we may learn to bear the beams of love;\n",
      "and these black bodies and this sunburnt face\n",
      "are but a cloud, and like a shady grove.\n",
      "‘for, when our souls have learned the heat to bear,\n",
      "the cloud will vanish, we shall hear his voice,\n",
      "saying, “come out from the grove, my love and care,\n",
      "and round my golden tent like lambs rejoice.”’\n",
      "thus did my mother say, and kissed me,\n",
      "and thus i say to little english boy.\n",
      "when i from black, and he from white cloud free,\n",
      "and round the tent of god like lambs we joy,\n",
      "i’ll shade him from the heat till he can bear\n",
      "to lean in joy upon our father’s knee;\n",
      "and then i’ll stand and stroke his silver hair,\n",
      "and be like him, and he will then love me. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "merry, merry sparrow!\n",
      "under leaves so green\n",
      "a happy blossom\n",
      "sees you, swift as arrow,\n",
      "seek your cradle narrow,\n",
      "near my bosom.\n",
      "pretty, pretty robin!\n",
      "under leaves so green\n",
      "a happy blossom\n",
      "hears you sobbing, sobbing,\n",
      "pretty, pretty robin,\n",
      "near my bosom. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "when my mother died i was very young,\n",
      "and my father sold me while yet my tongue\n",
      "could scarcely cry ‘weep! weep! weep! weep!’\n",
      "so your chimneys i sweep, and in soot i sleep.\n",
      "there’s little tom dacre, who cried when his head,\n",
      "that curled like a lamb’s back, was shaved; so i said,\n",
      "‘hush, tom! never mind it, for, when your head’s bare,\n",
      "you know that the soot cannot spoil your white hair.’\n",
      "and so he was quiet, and that very night,\n",
      "as tom was a-sleeping, he had such a sight!—\n",
      "that thousands of sweepers, dick, joe, ned, and jack,\n",
      "were all of them locked up in coffins of black.\n",
      "and by came an angel, who had a bright key,\n",
      "and he opened the coffins, and set them all free;\n",
      "then down a green plain, leaping, laughing, they run\n",
      "and wash in a river, and shine in the sun.\n",
      "then naked and white, all their bags left behind,\n",
      "they rise upon clouds, and sport in the wind:\n",
      "and the angel told tom, if he’d be a good boy,\n",
      "he’d have god for his father, and never want joy.\n",
      "and so tom awoke, and we rose in the dark,\n",
      "and got with our bags and our brushes to work.\n",
      "though the morning was cold, tom was happy and warm:\n",
      "so, if all do their duty, they need not fear harm. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "‘father, father, where are you going?\n",
      "o do not walk so fast!\n",
      "speak, father, speak to your little boy,\n",
      "or else i shall be lost.’\n",
      "the night was dark, no father was there,\n",
      "the child was wet with dew;\n",
      "the mire was deep, and the child did weep,\n",
      "and away the vapour flew. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "the little boy lost in the lonely fen,\n",
      "led by the wandering light,\n",
      "began to cry, but god, ever nigh,\n",
      "appeared like his father, in white.\n",
      "he kissed the child, and by the hand led,\n",
      "and to his mother brought,\n",
      "who in sorrow pale, through the lonely dale,\n",
      "her little boy weeping sought. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "when the green woods laugh with the voice of joy,\n",
      "and the dimpling stream runs laughing by;\n",
      "when the air does laugh with our merry wit,\n",
      "and the green hill laughs with the noise of it;\n",
      "when the meadows laugh with lively green,\n",
      "and the grasshopper laughs in the merry scene;\n",
      "when mary and susan and emily\n",
      "with their sweet round mouths sing ‘ha ha he!’\n",
      "when the painted birds laugh in the shade,\n",
      "where our table with cherries and nuts is spread:\n",
      "come live, and be merry, and join with me,\n",
      "to sing the sweet chorus of ‘ha ha he!’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "sweet dreams, form a shade\n",
      "o’er my lovely infant’s head!\n",
      "sweet dreams of pleasant streams\n",
      "by happy, silent, moony beams!\n",
      "sweet sleep, with soft down\n",
      "weave thy brows an infant crown!\n",
      "sweet sleep, angel mild,\n",
      "hover o’er my happy child!\n",
      "sweet smiles, in the night\n",
      "hover over my delight!\n",
      "sweet smiles, mother’s smiles,\n",
      "all the livelong night beguiles.\n",
      "sweet moans, dovelike sighs,\n",
      "chase not slumber from thy eyes!\n",
      "sweet moans, sweeter smiles,\n",
      "all the dovelike moans beguiles.\n",
      "sleep, sleep, happy child!\n",
      "all creation slept and smiled.\n",
      "sleep, sleep, happy sleep,\n",
      "while o’er thee thy mother weep.\n",
      "sweet babe, in thy face\n",
      "holy image i can trace;\n",
      "sweet babe, once like thee\n",
      "thy maker lay, and wept for me:\n",
      "wept for me, for thee, for all,\n",
      "when he was an infant small.\n",
      "thou his image ever see,\n",
      "heavenly face that smiles on thee!\n",
      "smiles on thee, on me, on all,\n",
      "who became an infant small;\n",
      "infant smiles are his own smiles;\n",
      "heaven and earth to peace beguiles. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "to mercy, pity, peace, and love,\n",
      "all pray in their distress,\n",
      "and to these virtues of delight\n",
      "return their thankfulness.\n",
      "for mercy, pity, peace, and love,\n",
      "is god our father dear;\n",
      "and mercy, pity, peace, and love,\n",
      "is man, his child and care.\n",
      "for mercy has a human heart;\n",
      "pity, a human face;\n",
      "and love, the human form divine:\n",
      "and peace the human dress.\n",
      "then every man, of every clime,\n",
      "that prays in his distress,\n",
      "prays to the human form divine:\n",
      "love, mercy, pity, peace.\n",
      "and all must love the human form,\n",
      "in heathen, turk, or jew.\n",
      "where mercy, love, and pity dwell,\n",
      "there god is dwelling too. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "’twas on a holy thursday, their innocent faces clean,\n",
      "the children walking two and two, in red, and blue, and green:\n",
      "grey-headed beadles walked before, with wands as white as snow,\n",
      "till into the high dome of paul’s they like thames waters flow.\n",
      "o what a multitude they seemed, these flowers of london town!\n",
      "seated in companies they sit, with radiance all their own.\n",
      "the hum of multitudes was there, but multitudes of lambs,\n",
      "thousands of little boys and girls raising their innocent hands.\n",
      "now like a mighty wind they raise to heaven the voice of song,\n",
      "or like harmonious thunderings the seats of heaven among:\n",
      "beneath them sit the aged men, wise guardians of the poor.\n",
      "then cherish pity, lest you drive an angel from your door. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "the sun descending in the west,\n",
      "the evening star does shine;\n",
      "the birds are silent in their nest,\n",
      "and i must seek for mine.\n",
      "the moon, like a flower\n",
      "in heaven’s high bower,\n",
      "with silent delight,\n",
      "sits and smiles on the night.\n",
      "farewell, green fields and happy groves,\n",
      "where flocks have took delight,\n",
      "where lambs have nibbled, silent moves\n",
      "the feet of angels bright;\n",
      "unseen, they pour blessing,\n",
      "and joy without ceasing,\n",
      "on each bud and blossom,\n",
      "and each sleeping bosom.\n",
      "they look in every thoughtless nest\n",
      "where birds are covered warm;\n",
      "they visit caves of every beast,\n",
      "to keep them all from harm:\n",
      "if they see any weeping\n",
      "that should have been sleeping,\n",
      "they pour sleep on their head,\n",
      "and sit down by their bed.\n",
      "when wolves and tigers howl for prey,\n",
      "they pitying stand and weep;\n",
      "seeking to drive their thirst away,\n",
      "and keep them from the sheep.\n",
      "but, if they rush dreadful,\n",
      "the angels, most heedful,\n",
      "receive each mild spirit,\n",
      "new worlds to inherit.\n",
      "and there the lion’s ruddy eyes\n",
      "shall flow with tears of gold:\n",
      "and pitying the tender cries,\n",
      "and walking round the fold:\n",
      "saying: ‘wrath by his meekness,\n",
      "and, by his health, sickness,\n",
      "is driven away\n",
      "from our immortal day.\n",
      "‘and now beside thee, bleating lamb,\n",
      "i can lie down and sleep,\n",
      "or think on him who bore thy name,\n",
      "graze after thee, and weep.\n",
      "for, washed in life’s river,\n",
      "my bright mane for ever\n",
      "shall shine like the gold,\n",
      "as i guard o’er the fold.’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "sound the flute!\n",
      "now it’s mute!\n",
      "birds delight,\n",
      "day and night,\n",
      "nightingale,\n",
      "in the dale,\n",
      "lark in sky,—\n",
      "merrily,\n",
      "merrily, merrily to welcome in the year.\n",
      "little boy,\n",
      "full of joy;\n",
      "little girl,\n",
      "sweet and small;\n",
      "cock does crow,\n",
      "so do you;\n",
      "merry voice,\n",
      "infant noise;\n",
      "merrily, merrily to welcome in the year.\n",
      "little lamb,\n",
      "here i am;\n",
      "come and lick\n",
      "my white neck;\n",
      "let me pull\n",
      "your soft wool;\n",
      "let me kiss\n",
      "your soft face;\n",
      "merrily, merrily we welcome in the year. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "  \n",
      "when voices of children are heard on the green,\n",
      "and laughing is heard on the hill,\n",
      "my heart is at rest within my breast,\n",
      "and everything else is still.\n",
      "‘then come home, my children, the sun is gone down,\n",
      "and the dews of night arise;\n",
      "come, come, leave off play, and let us away,\n",
      "till the morning appears in the skies.’\n",
      "‘no, no, let us play, for it is yet day,\n",
      "and we cannot go to sleep;\n",
      "besides, in the sky the little birds fly,\n",
      "and the hills are all covered with sheep.’\n",
      "‘well, well, go and play till the light fades away,\n",
      "and then go home to bed.’\n",
      "the little ones leaped, and shouted, and laughed,\n",
      "and all the hills echoèd. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "‘i have no name;\n",
      "i am but two days old.’\n",
      "what shall i call thee?\n",
      "‘i happy am,\n",
      "joy is my name.’\n",
      "sweet joy befall thee!\n",
      "pretty joy!\n",
      "sweet joy, but two days old.\n",
      "sweet joy i call thee:\n",
      "thou dost smile,\n",
      "i sing the while;\n",
      "sweet joy befall thee! \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "once a dream did weave a shade\n",
      "o’er my angel-guarded bed,\n",
      "that an emmet lost its way\n",
      "where on grass methought i lay.\n",
      "troubled, wildered, and forlorn,\n",
      "dark, benighted, travel-worn,\n",
      "over many a tangled spray,\n",
      "all heart-broke, i heard her say:\n",
      "‘o my children! do they cry,\n",
      "do they hear their father sigh?\n",
      "now they look abroad to see,\n",
      "now return and weep for me.’\n",
      "pitying, i dropped a tear:\n",
      "but i saw a glow-worm near,\n",
      "who replied, ‘what wailing wight\n",
      "calls the watchman of the night?’\n",
      "‘i am set to light the ground,\n",
      "while the beetle goes his round:\n",
      "follow now the beetle’s hum;\n",
      "little wanderer, hie thee home!’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "  \n",
      "can i see another’s woe,\n",
      "and not be in sorrow too?\n",
      "can i see another’s grief,\n",
      "and not seek for kind relief?\n",
      "can i see a falling tear,\n",
      "and not feel my sorrow’s share?\n",
      "can a father see his child\n",
      "weep, nor be with sorrow filled?\n",
      "can a mother sit and hear\n",
      "an infant groan, an infant fear?\n",
      "no, no! never can it be!\n",
      "never, never can it be!\n",
      "and can he who smiles on all\n",
      "hear the wren with sorrows small,\n",
      "hear the small bird’s grief and care,\n",
      "hear the woes that infants bear—\n",
      "and not sit beside the nest,\n",
      "pouring pity in their breast,\n",
      "and not sit the cradle near,\n",
      "weeping tear on infant’s tear?\n",
      "and not sit both night and day,\n",
      "wiping all our tears away?\n",
      "o no! never can it be!\n",
      "never, never can it be!\n",
      "he doth give his joy to all:\n",
      "he becomes an infant small,\n",
      "he becomes a man of woe,\n",
      "he doth feel the sorrow too.\n",
      "think not thou canst sigh a sigh,\n",
      "and thy maker is not by:\n",
      "think not thou canst weep a tear,\n",
      "and thy maker is not near.\n",
      "o he gives to us his joy,\n",
      "that our grief he may destroy:\n",
      "till our grief is fled and gone\n",
      "he doth sit by us and moan. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "\n",
      "hear the voice of the bard,\n",
      "who present, past, and future, sees;\n",
      "whose ears have heard\n",
      "the holy word\n",
      "that walked among the ancient trees;\n",
      "calling the lapséd soul,\n",
      "and weeping in the evening dew;\n",
      "that might control\n",
      "the starry pole,\n",
      "and fallen, fallen light renew!\n",
      "‘o earth, o earth, return!\n",
      "arise from out the dewy grass!\n",
      "night is worn,\n",
      "and the morn\n",
      "rises from the slumbrous mass.\n",
      "‘turn away no more;\n",
      "why wilt thou turn away?\n",
      "the starry floor,\n",
      "the watery shore,\n",
      "is given thee till the break of day.’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "  \n",
      "earth raised up her head\n",
      "from the darkness dread and drear,\n",
      "her light fled,\n",
      "stony, dread,\n",
      "and her locks covered with grey despair.\n",
      "‘prisoned on watery shore,\n",
      "starry jealousy does keep my den\n",
      "cold and hoar;\n",
      "weeping o’er,\n",
      "i hear the father of the ancient men.\n",
      "‘selfish father of men!\n",
      "cruel, jealous, selfish fear!\n",
      "can delight,\n",
      "chained in night,\n",
      "the virgins of youth and morning bear.\n",
      "‘does spring hide its joy,\n",
      "when buds and blossoms grow?\n",
      "does the sower\n",
      "sow by night,\n",
      "or the ploughman in darkness plough?\n",
      "‘break this heavy chain,\n",
      "that does freeze my bones around!\n",
      "selfish, vain,\n",
      "eternal bane,\n",
      "that free love with bondage bound.’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "‘love seeketh not itself to please,\n",
      "nor for itself hath any care,\n",
      "but for another gives its ease,\n",
      "and builds a heaven in hell’s despair.’\n",
      "so sung a little clod of clay,\n",
      "trodden with the cattle’s feet,\n",
      "but a pebble of the brook\n",
      "warbled out these metres meet:\n",
      "‘love seeketh only self to please,\n",
      "to bind another to its delight,\n",
      "joys in another’s loss of ease,\n",
      "and builds a hell in heaven’s despite.’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "is this a holy thing to see\n",
      "in a rich and fruitful land,—\n",
      "babes reduced to misery,\n",
      "fed with cold and usurous hand?\n",
      "is that trembling cry a song?\n",
      "can it be a song of joy?\n",
      "and so many children poor?\n",
      "it is a land of poverty!\n",
      "and their sun does never shine,\n",
      "and their fields are bleak and bare,\n",
      "and their ways are filled with thorns,\n",
      "it is eternal winter there.\n",
      "for where’er the sun does shine,\n",
      "and where’er the rain does fall,\n",
      "babe can never hunger there,\n",
      "nor poverty the mind appal. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "in futurity\n",
      "i prophesy\n",
      "that the earth from sleep\n",
      "(grave the sentence deep)\n",
      "shall arise, and seek\n",
      "for her maker meek;\n",
      "and the desert wild\n",
      "become a garden mild.\n",
      "in the southern clime,\n",
      "where the summer’s prime\n",
      "never fades away,\n",
      "lovely lyca lay.\n",
      "seven summers old\n",
      "lovely lyca told.\n",
      "she had wandered long,\n",
      "hearing wild birds’ song.\n",
      "‘sweet sleep, come to me,\n",
      "underneath this tree;\n",
      "do father, mother, weep?\n",
      "where can lyca sleep?\n",
      "‘lost in desert wild\n",
      "is your little child.\n",
      "how can lyca sleep\n",
      "if her mother weep?\n",
      "‘if her heart does ache,\n",
      "then let lyca wake;\n",
      "if my mother sleep,\n",
      "lyca shall not weep.\n",
      "‘frowning, frowning night,\n",
      "o’er this desert bright\n",
      "let thy moon arise,\n",
      "while i close my eyes.’\n",
      "sleeping lyca lay,\n",
      "while the beasts of prey,\n",
      "come from caverns deep,\n",
      "viewed the maid asleep.\n",
      "the kingly lion stood,\n",
      "and the virgin viewed:\n",
      "then he gambolled round\n",
      "o’er the hallowed ground.\n",
      "leopards, tigers, play\n",
      "round her as she lay;\n",
      "while the lion old\n",
      "bowed his mane of gold,\n",
      "and her bosom lick,\n",
      "and upon her neck,\n",
      "from his eyes of flame,\n",
      "ruby tears there came;\n",
      "while the lioness\n",
      "loosed her slender dress,\n",
      "and naked they conveyed\n",
      "to caves the sleeping maid. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "all the night in woe\n",
      "lyca’s parents go\n",
      "over valleys deep,\n",
      "while the deserts weep.\n",
      "tired and woe-begone,\n",
      "hoarse with making moan,\n",
      "arm in arm, seven days\n",
      "they traced the desert ways.\n",
      "seven nights they sleep\n",
      "among shadows deep,\n",
      "and dream they see their child\n",
      "starved in desert wild.\n",
      "pale through pathless ways\n",
      "the fancied image strays,\n",
      "famished, weeping, weak,\n",
      "with hollow piteous shriek.\n",
      "rising from unrest,\n",
      "the trembling woman pressed\n",
      "with feet of weary woe;\n",
      "she could no further go.\n",
      "in his arms he bore\n",
      "her, armed with sorrow sore;\n",
      "till before their way\n",
      "a couching lion lay.\n",
      "turning back was vain:\n",
      "soon his heavy mane\n",
      "bore them to the ground,\n",
      "then he stalked around,\n",
      "smelling to his prey;\n",
      "but their fears allay\n",
      "when he licks their hands,\n",
      "and silent by them stands.\n",
      "they look upon his eyes,\n",
      "filled with deep surprise;\n",
      "and wondering behold\n",
      "a spirit armed in gold.\n",
      "on his head a crown,\n",
      "on his shoulders down\n",
      "flowed his golden hair.\n",
      "gone was all their care.\n",
      "‘follow me,’ he said;\n",
      "‘weep not for the maid;\n",
      "in my palace deep,\n",
      "lyca lies asleep.’\n",
      "then they followèd\n",
      "where the vision led,\n",
      "and saw their sleeping child\n",
      "among tigers wild.\n",
      "to this day they dwell\n",
      "in a lonely dell,\n",
      "nor fear the wolvish howl\n",
      "nor the lion’s growl. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a little black thing among the snow,\n",
      "crying! ‘weep! weep!’ in notes of woe!\n",
      "‘where are thy father and mother?  say!’—\n",
      "‘they are both gone up to the church to pray.\n",
      "‘because i was happy upon the heath,\n",
      "and smiled among the winter’s snow,\n",
      "they clothed me in the clothes of death,\n",
      "and taught me to sing the notes of woe.\n",
      "‘and because i am happy and dance and sing,\n",
      "they think they have done me no injury,\n",
      "and are gone to praise god and his priest and king,\n",
      "who made up a heaven of our misery.’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "  \n",
      "when the voices of children are heard on the green,\n",
      "and whisperings are in the dale,\n",
      "the days of my youth rise fresh in my mind,\n",
      "my face turns green and pale.\n",
      "then come home, my children, the sun is gone down,\n",
      "and the dews of night arise;\n",
      "your spring and your day are wasted in play,\n",
      "and your winter and night in disguise. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "o rose, thou art sick!\n",
      "the invisible worm,\n",
      "that flies in the night,\n",
      "in the howling storm,\n",
      "has found out thy bed\n",
      "of crimson joy,\n",
      "and his dark secret love\n",
      "does thy life destroy. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "little fly,\n",
      "thy summer’s play\n",
      "my thoughtless hand\n",
      "has brushed away.\n",
      "am not i\n",
      "a fly like thee?\n",
      "or art not thou\n",
      "a man like me?\n",
      "for i dance,\n",
      "and drink, and sing,\n",
      "till some blind hand\n",
      "shall brush my wing.\n",
      "if thought is life\n",
      "and strength and breath,\n",
      "and the want\n",
      "of thought is death;\n",
      "then am i\n",
      "a happy fly.\n",
      "if i live,\n",
      "or if i die. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "i dreamt a dream!  what can it mean?\n",
      "and that i was a maiden queen\n",
      "guarded by an angel mild:\n",
      "witless woe was ne’er beguiled!\n",
      "and i wept both night and day,\n",
      "and he wiped my tears away;\n",
      "and i wept both day and night,\n",
      "and hid from him my heart’s delight.\n",
      "so he took his wings, and fled;\n",
      "then the morn blushed rosy red.\n",
      "i dried my tears, and armed my fears\n",
      "with ten thousand shields and spears.\n",
      "soon my angel came again;\n",
      "i was armed, he came in vain;\n",
      "for the time of youth was fled,\n",
      "and grey hairs were on my head. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "tiger, tiger, burning bright\n",
      "in the forests of the night,\n",
      "what immortal hand or eye\n",
      "could frame thy fearful symmetry?\n",
      "in what distant deeps or skies\n",
      "burnt the fire of thine eyes?\n",
      "on what wings dare he aspire?\n",
      "what the hand dare seize the fire?\n",
      "and what shoulder and what art\n",
      "could twist the sinews of thy heart?\n",
      "and, when thy heart began to beat,\n",
      "what dread hand and what dread feet?\n",
      "what the hammer? what the chain?\n",
      "in what furnace was thy brain?\n",
      "what the anvil? what dread grasp\n",
      "dare its deadly terrors clasp?\n",
      "when the stars threw down their spears,\n",
      "and watered heaven with their tears,\n",
      "did he smile his work to see?\n",
      "did he who made the lamb make thee?\n",
      "tiger, tiger, burning bright\n",
      "in the forests of the night,\n",
      "what immortal hand or eye\n",
      "dare frame thy fearful symmetry? \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a flower was offered to me,\n",
      "such a flower as may never bore;\n",
      "but i said, ‘i’ve a pretty rose tree,’\n",
      "and i passed the sweet flower o’er.\n",
      "then i went to my pretty rose tree,\n",
      "to tend her by day and by night;\n",
      "but my rose turned away with jealousy,\n",
      "and her thorns were my only delight. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " , \n",
      "ah, sunflower, weary of time,\n",
      "who countest the steps of the sun;\n",
      "seeking after that sweet golden clime\n",
      "where the traveller’s journey is done;\n",
      "where the youth pined away with desire,\n",
      "and the pale virgin shrouded in snow,\n",
      "arise from their graves, and aspire\n",
      "where my sunflower wishes to go! \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "the modest rose puts forth a thorn,\n",
      "the humble sheep a threat’ning horn:\n",
      "while the lily white shall in love delight,\n",
      "nor a thorn nor a threat stain her beauty bright. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "i went to the garden of love,\n",
      "and saw what i never had seen;\n",
      "a chapel was built in the midst,\n",
      "where i used to play on the green.\n",
      "and the gates of this chapel were shut,\n",
      "and ‘thou shalt not’ writ over the door;\n",
      "so i turned to the garden of love\n",
      "that so many sweet flowers bore.\n",
      "and i saw it was filled with graves,\n",
      "and tombstones where flowers should be;\n",
      "and priests in black gowns were walking their rounds,\n",
      "and binding with briars my joys and desires. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "dear mother, dear mother, the church is cold;\n",
      "but the alehouse is healthy, and pleasant, and warm.\n",
      "besides, i can tell where i am used well;\n",
      "such usage in heaven will never do well.\n",
      "but, if at the church they would give us some ale,\n",
      "and a pleasant fire our souls to regale,\n",
      "we’d sing and we’d pray all the livelong day,\n",
      "nor ever once wish from the church to stray.\n",
      "then the parson might preach, and drink, and sing,\n",
      "and we’d be as happy as birds in the spring;\n",
      "and modest dame lurch, who is always at church,\n",
      "would not have bandy children, nor fasting, nor birch.\n",
      "and god, like a father, rejoicing to see\n",
      "his children as pleasant and happy as he,\n",
      "would have no more quarrel with the devil or the barrel,\n",
      "but kiss him, and give him both drink and apparel. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "i wander through each chartered street,\n",
      "near where the chartered thames does flow,\n",
      "a mark in every face i meet,\n",
      "marks of weakness, marks of woe.\n",
      "in every cry of every man,\n",
      "in every infant’s cry of fear,\n",
      "in every voice, in every ban,\n",
      "the mind-forged manacles i hear:\n",
      "how the chimney-sweeper’s cry\n",
      "every blackening church appals,\n",
      "and the hapless soldier’s sigh\n",
      "runs in blood down palace-walls.\n",
      "but most, through midnight streets i hear\n",
      "how the youthful harlot’s curse\n",
      "blasts the new-born infant’s tear,\n",
      "and blights with plagues the marriage hearse. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "pity would be no more\n",
      "if we did not make somebody poor,\n",
      "and mercy no more could be\n",
      "if all were as happy as we.\n",
      "and mutual fear brings peace,\n",
      "till the selfish loves increase;\n",
      "then cruelty knits a snare,\n",
      "and spreads his baits with care.\n",
      "he sits down with holy fears,\n",
      "and waters the ground with tears;\n",
      "then humility takes its root\n",
      "underneath his foot.\n",
      "soon spreads the dismal shade\n",
      "of mystery over his head,\n",
      "and the caterpillar and fly\n",
      "feed on the mystery.\n",
      "and it bears the fruit of deceit,\n",
      "ruddy and sweet to eat,\n",
      "and the raven his nest has made\n",
      "in its thickest shade.\n",
      "the gods of the earth and sea\n",
      "sought through nature to find this tree,\n",
      "but their search was all in vain:\n",
      "there grows one in the human brain. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "my mother groaned, my father wept:\n",
      "into the dangerous world i leapt,\n",
      "helpless, naked, piping loud,\n",
      "like a fiend hid in a cloud.\n",
      "struggling in my father’s hands,\n",
      "striving against my swaddling bands,\n",
      "bound and weary, i thought best\n",
      "to sulk upon my mother’s breast. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "i was angry with my friend:\n",
      "i told my wrath, my wrath did end.\n",
      "i was angry with my foe:\n",
      "i told it not, my wrath did grow.\n",
      "and i watered it in fears\n",
      "night and morning with my tears,\n",
      "and i sunnèd it with smiles\n",
      "and with soft deceitful wiles.\n",
      "and it grew both day and night,\n",
      "till it bore an apple bright,\n",
      "and my foe beheld it shine,\n",
      "and he knew that it was mine,—\n",
      "and into my garden stole\n",
      "when the night had veiled the pole;\n",
      "in the morning, glad, i see\n",
      "my foe outstretched beneath the tree. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "‘nought loves another as itself,\n",
      "nor venerates another so,\n",
      "nor is it possible to thought\n",
      "a greater than itself to know.\n",
      "‘and, father, how can i love you\n",
      "or any of my brothers more?\n",
      "i love you like the little bird\n",
      "that picks up crumbs around the door.’\n",
      "the priest sat by and heard the child;\n",
      "in trembling zeal he seized his hair,\n",
      "he led him by his little coat,\n",
      "and all admired his priestly care.\n",
      "and standing on the altar high,\n",
      "‘lo, what a fiend is here!’ said he:\n",
      "‘one who sets reason up for judge\n",
      "of our most holy mystery.’\n",
      "the weeping child could not be heard,\n",
      "the weeping parents wept in vain:\n",
      "they stripped him to his little shirt,\n",
      "and bound him in an iron chain,\n",
      "and burned him in a holy place\n",
      "where many had been burned before;\n",
      "the weeping parents wept in vain.\n",
      "are such things done on albion’s shore? \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "children of the future age,\n",
      "reading this indignant page,\n",
      "know that in a former time\n",
      "love, sweet love, was thought a crime.\n",
      "in the age of gold,\n",
      "free from winter’s cold,\n",
      "youth and maiden bright,\n",
      "to the holy light,\n",
      "naked in the sunny beams delight.\n",
      "once a youthful pair,\n",
      "filled with softest care,\n",
      "met in garden bright\n",
      "where the holy light\n",
      "had just removed the curtains of the night.\n",
      "there, in rising day,\n",
      "on the grass they play;\n",
      "parents were afar,\n",
      "strangers came not near,\n",
      "and the maiden soon forgot her fear.\n",
      "tired with kisses sweet,\n",
      "they agree to meet\n",
      "when the silent sleep\n",
      "waves o’er heaven’s deep,\n",
      "and the weary tired wanderers weep.\n",
      "to her father white\n",
      "came the maiden bright;\n",
      "but his loving look,\n",
      "like the holy book,\n",
      "all her tender limbs with terror shook.\n",
      "ona, pale and weak,\n",
      "to thy father speak!\n",
      "o the trembling fear!\n",
      "o the dismal care\n",
      "that shakes the blossoms of my hoary hair!’ \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "cruelty has a human heart,\n",
      "and jealousy a human face;\n",
      "terror the human form divine,\n",
      "and secrecy the human dress.\n",
      "the human dress is forgèd iron,\n",
      "the human form a fiery forge,\n",
      "the human face a furnace sealed,\n",
      "the human heart its hungry gorge. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "a \n",
      "sleep, sleep, beauty bright,\n",
      "dreaming in the joys of night;\n",
      "sleep, sleep; in thy sleep\n",
      "little sorrows sit and weep.\n",
      "sweet babe, in thy face\n",
      "soft desires i can trace,\n",
      "secret joys and secret smiles,\n",
      "little pretty infant wiles.\n",
      "as thy softest limbs i feel,\n",
      "smiles as of the morning steal\n",
      "o’er thy cheek, and o’er thy breast\n",
      "where thy little heart doth rest.\n",
      "o the cunning wiles that creep\n",
      "in thy little heart asleep!\n",
      "when thy little heart doth wake,\n",
      "then the dreadful light shall break. \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "i love to rise in a summer morn,\n",
      "when the birds sing on every tree;\n",
      "the distant huntsman winds his horn,\n",
      "and the skylark sings with me:\n",
      "o what sweet company!\n",
      "but to go to school in a summer morn,—\n",
      "o it drives all joy away!\n",
      "under a cruel eye outworn,\n",
      "the little ones spend the day\n",
      "in sighing and dismay.\n",
      "ah then at times i drooping sit,\n",
      "and spend many an anxious hour;\n",
      "nor in my book can i take delight,\n",
      "nor sit in learning’s bower,\n",
      "worn through with the dreary shower.\n",
      "how can the bird that is born for joy\n",
      "sit in a cage and sing?\n",
      "how can a child, when fears annoy,\n",
      "but droop his tender wing,\n",
      "and forget his youthful spring!\n",
      "o father and mother if buds are nipped,\n",
      "and blossoms blown away;\n",
      "and if the tender plants are stripped\n",
      "of their joy in the springing day,\n",
      "by sorrow and care’s dismay,—\n",
      "how shall the summer arise in joy,\n",
      "or the summer fruits appear?\n",
      "or how shall we gather what griefs destroy,\n",
      "or bless the mellowing year,\n",
      "when the blasts of winter appear? \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "whate’er is born of mortal birth\n",
      "must be consumèd with the earth,\n",
      "to rise from generation free:\n",
      "then what have i to do with thee?\n",
      "the sexes sprung from shame and pride,\n",
      "blowed in the morn, in evening died;\n",
      "but mercy changed death into sleep;\n",
      "the sexes rose to work and weep.\n",
      "thou, mother of my mortal part,\n",
      "with cruelty didst mould my heart,\n",
      "and with false self-deceiving tears\n",
      "didst blind my nostrils, eyes, and ears,\n",
      "didst close my tongue in senseless clay,\n",
      "and me to mortal life betray.\n",
      "the death of jesus set me free:\n",
      "then what have i to do with thee? \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      " \n",
      "youth of delight! come hither\n",
      "and see the opening morn,\n",
      "image of truth new-born.\n",
      "doubt is fled, and clouds of reason,\n",
      "dark disputes and artful teazing.\n",
      "folly is an endless maze;\n",
      "tangled roots perplex her ways;\n",
      "how many have fallen there!\n",
      "they stumble all night over bones of the dead;\n",
      "and feel—they know not what but care;\n",
      "and wish to lead others, when they should be led.\n",
      " \n",
      "\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ii in range(len(sonnet_list)):\n",
    "    print(\"\\n\\n--------------------\\n\\n\",sonnet_list[ii], \"\\n\\n--------------------\\n\\n\")\n",
    "        #print(\"\\n\\n\\n\", sonnet_list[ii], \"\\n----------\\n\", sonnet_list_[ii], end=\"\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlakeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, transform=None):\n",
    "        \n",
    "        ### Load data\n",
    "        text = open(filepath, 'r').read()\n",
    "\n",
    "        # Removing titles\n",
    "        text = re.sub('\\n?[0:9]?([A-Z]{2,}[- ]?(\\'S)?){1,}\\n?', '', text)\n",
    "\n",
    "        # Lowering all text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Getting the list of sonnets\n",
    "        sonnet_list = re.split('\\n{5}', text)\n",
    "\n",
    "        # Removing the newlines (both 2 and 3 occurencies)\n",
    "        sonnet_list = list(map(lambda s: re.sub('\\n{2,3}', '\\n', s), sonnet_list))\n",
    "\n",
    "        # Eliminating the first sonnet (it is only empty string)\n",
    "        sonnet_list.pop(0)\n",
    "\n",
    "        ### Char to number\n",
    "        alphabet = list(set(text))\t\t# \"set\" function divides the text in single characters (not ordered)\n",
    "        alphabet.sort()\t\t\t\t# sorting not ordered characters\n",
    "        \n",
    "        # Deleting all non wanted characters from the alphabet: keeping only \n",
    "        #   a-z,'\\n', ' ', '.', ',', '?' (the question point is charactesitic in Blake)\n",
    "        alphabet_good = {'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p',\n",
    "                         'q','r','s','t','u','v','w','x','y','z',' ','\\n',',','.','?'}\n",
    "        \n",
    "        print('Found letters:', alphabet)\n",
    "        print('Using only:', alphabet_good)\n",
    "        # Building dictionaries\n",
    "        #char_to_number = {char: number for number, char in enumerate(alphabet)}\n",
    "        #number_to_char = {number: char for number, char in enumerate(alphabet)}\n",
    "\n",
    "        char_to_number = {char: number for number, char in enumerate(alphabet_good)}\n",
    "        number_to_char = {number: char for number, char in enumerate(alphabet_good)}\n",
    "        \n",
    "        ### Store data\n",
    "        self.sonnet_list = sonnet_list\n",
    "        self.transform = transform\n",
    "        self.alphabet = alphabet_good\n",
    "        self.char_to_number = char_to_number\n",
    "        self.number_to_char = number_to_char\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sonnet_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sonnet text\n",
    "        text = self.sonnet_list[idx]\n",
    "        # Encode with numbers\n",
    "        encoded = encode_text(self.char_to_number, text)\n",
    "\n",
    "        # Create sample\n",
    "        sample = {'text': text, 'encoded': encoded}\n",
    "        # Transform (if defined)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "#def encode_text(char_to_number, text):\n",
    "#    encoded = [char_to_number[c] for c in text]\n",
    "#    return encoded\n",
    "\n",
    "def encode_text(char_to_number, text):\n",
    "    i = -1\n",
    "    for c in text:\n",
    "        i+=1\n",
    "        try:\n",
    "            a = char_to_number[c]\n",
    "        except:       # If the character is not in the char_to_number dictionary, then put commas\n",
    "            s = list(text)\n",
    "            s[i]=','\n",
    "            text=''.join(s)\n",
    "    encoded = [char_to_number[c] for c in text]\n",
    "    return encoded\n",
    "\n",
    "def decode_text(number_to_char, encoded):\n",
    "    text = [number_to_char[c] for c in encoded]\n",
    "    # Building proper string from a list of strings (concatenating them)\n",
    "    text = reduce(lambda s1, s2: s1 + s2, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class RandomCrop():\n",
    "    \n",
    "    def __init__(self, crop_len):\n",
    "        self.crop_len = crop_len\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        text = sample['text']\n",
    "        encoded = sample['encoded']\n",
    "        # Randomly choose an index\n",
    "        tot_chars = len(text)\n",
    "        start_idx = np.random.randint(0, tot_chars - self.crop_len)\n",
    "        end_idx = start_idx + self.crop_len\n",
    "        return {**sample,\n",
    "            'text': text[start_idx: end_idx],\n",
    "            'encoded': encoded[start_idx: end_idx]}\n",
    "        \n",
    "\n",
    "def create_one_hot_matrix(encoded, alphabet_len):\n",
    "    # Create one hot matrix\n",
    "    encoded_onehot = np.zeros([len(encoded), alphabet_len])\n",
    "    tot_chars = len(encoded)\n",
    "    # Placing ONEs at the respective position of the letters: \"encoded\" indeed have numbers that encode the letters,\n",
    "    # \tand here it is used as index dimension\n",
    "    encoded_onehot[np.arange(tot_chars), encoded] = 1\n",
    "    return encoded_onehot\n",
    "\n",
    "\n",
    "class OneHotEncoder():\n",
    "    \n",
    "    def __init__(self, alphabet_len):\n",
    "        self.alphabet_len = alphabet_len\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        # Load encoded text with numbers\n",
    "        encoded = np.array(sample['encoded'])\n",
    "        # Create one hot matrix\n",
    "        encoded_onehot = create_one_hot_matrix(encoded, self.alphabet_len)\n",
    "        return {**sample,\n",
    "            'encoded_onehot': encoded_onehot}\n",
    "        \n",
    "                \n",
    "class ToTensor():\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        # Convert one hot encoded text to pytorch tensor\n",
    "        encoded_onehot = torch.tensor(sample['encoded_onehot']).float()\n",
    "        return {'encoded_onehot': encoded_onehot}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n",
      "Found letters: ['\\n', ' ', '!', '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'è', 'é', '—', '‘', '’', '“', '”']\n",
      "Using only: {'a', 't', 'w', 'n', ' ', 'h', 'b', 'o', ',', 'l', '.', 'f', 'z', 'e', 'k', 'g', '\\n', '?', 'v', 'd', 'j', 'x', 'c', 'y', 'm', 's', 'r', 'u', 'i', 'q', 'p'}\n",
      "Found letters: ['\\n', ' ', '!', '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'è', 'é', '—', '‘', '’', '“', '”']\n",
      "Using only: {'a', 't', 'w', 'n', ' ', 'h', 'b', 'o', ',', 'l', '.', 'f', 'z', 'e', 'k', 'g', '\\n', '?', 'v', 'd', 'j', 'x', 'c', 'y', 'm', 's', 'r', 'u', 'i', 'q', 'p'}\n",
      "Alphabet length: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (rnn): LSTM(31, 400, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (out): Linear(in_features=400, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse input arguments\n",
    "training_args = json.load(open('my_training_args.json'))\n",
    "\n",
    "#%% Check device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('Selected device:', device)\n",
    "\n",
    "dataset = BlakeDataset(filepath=training_args['datasetpath'], \n",
    "                    transform=None)\n",
    "\n",
    "#%% Create dataset\n",
    "trans = transforms.Compose([RandomCrop(training_args['crop_len']),\n",
    "                    OneHotEncoder(len(dataset.alphabet)),\n",
    "                    ToTensor()\n",
    "                    ])\n",
    "\n",
    "dataset = BlakeDataset(filepath=training_args['datasetpath'], \n",
    "                    transform=trans)\n",
    "\n",
    "print(\"Alphabet length:\", len(dataset.alphabet))\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(input_size=len(dataset.alphabet), \n",
    "            hidden_units=training_args['hidden_units'], \n",
    "            layers_num=training_args['layers_num'], \n",
    "            dropout_prob=training_args['dropout_prob'])\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasetpath': 'Songs_of_innocence.txt', 'crop_len': 50, 'hidden_units': 400, 'layers_num': 2, 'dropout_prob': 0.1, 'batchsize': 16, 'num_epochs': 1000, 'out_dir': 'model'}\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "## EPOCH 1/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.43574595451355\n",
      "\t Training loss (single batch): 3.4177017211914062\n",
      "\t Training loss (single batch): 3.4129598140716553\n",
      "##################################\n",
      "## EPOCH 2/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.395730972290039\n",
      "\t Training loss (single batch): 3.352689504623413\n",
      "\t Training loss (single batch): 3.4033026695251465\n",
      "##################################\n",
      "## EPOCH 3/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.284884452819824\n",
      "\t Training loss (single batch): 3.1938743591308594\n",
      "\t Training loss (single batch): 2.853177785873413\n",
      "##################################\n",
      "## EPOCH 4/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.76914119720459\n",
      "\t Training loss (single batch): 3.150299072265625\n",
      "\t Training loss (single batch): 3.097045421600342\n",
      "##################################\n",
      "## EPOCH 5/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0489354133605957\n",
      "\t Training loss (single batch): 3.531911849975586\n",
      "\t Training loss (single batch): 2.9433541297912598\n",
      "##################################\n",
      "## EPOCH 6/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.364388942718506\n",
      "\t Training loss (single batch): 2.9607322216033936\n",
      "\t Training loss (single batch): 3.078295946121216\n",
      "##################################\n",
      "## EPOCH 7/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8765604496002197\n",
      "\t Training loss (single batch): 2.995628833770752\n",
      "\t Training loss (single batch): 2.9715077877044678\n",
      "##################################\n",
      "## EPOCH 8/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0408225059509277\n",
      "\t Training loss (single batch): 2.728339433670044\n",
      "\t Training loss (single batch): 2.8863492012023926\n",
      "##################################\n",
      "## EPOCH 9/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.955256700515747\n",
      "\t Training loss (single batch): 3.05513596534729\n",
      "\t Training loss (single batch): 2.9509785175323486\n",
      "##################################\n",
      "## EPOCH 10/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.690946340560913\n",
      "\t Training loss (single batch): 3.072856903076172\n",
      "\t Training loss (single batch): 2.9477322101593018\n",
      "##################################\n",
      "## EPOCH 11/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0711019039154053\n",
      "\t Training loss (single batch): 3.216679096221924\n",
      "\t Training loss (single batch): 3.047257423400879\n",
      "##################################\n",
      "## EPOCH 12/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.510765552520752\n",
      "\t Training loss (single batch): 3.2605228424072266\n",
      "\t Training loss (single batch): 2.91365122795105\n",
      "##################################\n",
      "## EPOCH 13/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1789042949676514\n",
      "\t Training loss (single batch): 3.27843976020813\n",
      "\t Training loss (single batch): 3.191409111022949\n",
      "##################################\n",
      "## EPOCH 14/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.021482467651367\n",
      "\t Training loss (single batch): 3.132007360458374\n",
      "\t Training loss (single batch): 2.881072759628296\n",
      "##################################\n",
      "## EPOCH 15/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.379389524459839\n",
      "\t Training loss (single batch): 3.03105092048645\n",
      "\t Training loss (single batch): 3.03067684173584\n",
      "##################################\n",
      "## EPOCH 16/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.055880546569824\n",
      "\t Training loss (single batch): 3.166883945465088\n",
      "\t Training loss (single batch): 3.2420105934143066\n",
      "##################################\n",
      "## EPOCH 17/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8532602787017822\n",
      "\t Training loss (single batch): 2.9307303428649902\n",
      "\t Training loss (single batch): 3.3091728687286377\n",
      "##################################\n",
      "## EPOCH 18/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9940237998962402\n",
      "\t Training loss (single batch): 2.861713409423828\n",
      "\t Training loss (single batch): 2.808030366897583\n",
      "##################################\n",
      "## EPOCH 19/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.154557228088379\n",
      "\t Training loss (single batch): 3.024791717529297\n",
      "\t Training loss (single batch): 3.1409530639648438\n",
      "##################################\n",
      "## EPOCH 20/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.190493583679199\n",
      "\t Training loss (single batch): 2.8796615600585938\n",
      "\t Training loss (single batch): 2.6077382564544678\n",
      "##################################\n",
      "## EPOCH 21/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0603396892547607\n",
      "\t Training loss (single batch): 3.009409189224243\n",
      "\t Training loss (single batch): 3.2758591175079346\n",
      "##################################\n",
      "## EPOCH 22/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0449085235595703\n",
      "\t Training loss (single batch): 2.9039430618286133\n",
      "\t Training loss (single batch): 3.3234074115753174\n",
      "##################################\n",
      "## EPOCH 23/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0103089809417725\n",
      "\t Training loss (single batch): 3.2006239891052246\n",
      "\t Training loss (single batch): 2.964378595352173\n",
      "##################################\n",
      "## EPOCH 24/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0407423973083496\n",
      "\t Training loss (single batch): 2.951632022857666\n",
      "\t Training loss (single batch): 3.3150250911712646\n",
      "##################################\n",
      "## EPOCH 25/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7297959327697754\n",
      "\t Training loss (single batch): 2.9183950424194336\n",
      "\t Training loss (single batch): 2.8843190670013428\n",
      "##################################\n",
      "## EPOCH 26/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7951207160949707\n",
      "\t Training loss (single batch): 3.2329328060150146\n",
      "\t Training loss (single batch): 2.9477667808532715\n",
      "##################################\n",
      "## EPOCH 27/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.053162097930908\n",
      "\t Training loss (single batch): 2.935553550720215\n",
      "\t Training loss (single batch): 3.058098793029785\n",
      "##################################\n",
      "## EPOCH 28/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.756636142730713\n",
      "\t Training loss (single batch): 2.564383029937744\n",
      "\t Training loss (single batch): 2.833247184753418\n",
      "##################################\n",
      "## EPOCH 29/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3162245750427246\n",
      "\t Training loss (single batch): 3.038207769393921\n",
      "\t Training loss (single batch): 3.0323286056518555\n",
      "##################################\n",
      "## EPOCH 30/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.901740312576294\n",
      "\t Training loss (single batch): 2.8268983364105225\n",
      "\t Training loss (single batch): 3.0650510787963867\n",
      "##################################\n",
      "## EPOCH 31/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9689390659332275\n",
      "\t Training loss (single batch): 3.20500111579895\n",
      "\t Training loss (single batch): 3.370579957962036\n",
      "##################################\n",
      "## EPOCH 32/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.874993324279785\n",
      "\t Training loss (single batch): 2.962299346923828\n",
      "\t Training loss (single batch): 2.7838847637176514\n",
      "##################################\n",
      "## EPOCH 33/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.054865837097168\n",
      "\t Training loss (single batch): 2.988529920578003\n",
      "\t Training loss (single batch): 3.1789450645446777\n",
      "##################################\n",
      "## EPOCH 34/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6927337646484375\n",
      "\t Training loss (single batch): 3.2245469093322754\n",
      "\t Training loss (single batch): 2.961212635040283\n",
      "##################################\n",
      "## EPOCH 35/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.310720443725586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 3.0826447010040283\n",
      "\t Training loss (single batch): 2.9258453845977783\n",
      "##################################\n",
      "## EPOCH 36/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.640077590942383\n",
      "\t Training loss (single batch): 3.325840473175049\n",
      "\t Training loss (single batch): 3.0438756942749023\n",
      "##################################\n",
      "## EPOCH 37/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6155924797058105\n",
      "\t Training loss (single batch): 3.377474308013916\n",
      "\t Training loss (single batch): 2.806382179260254\n",
      "##################################\n",
      "## EPOCH 38/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0209803581237793\n",
      "\t Training loss (single batch): 2.9089670181274414\n",
      "\t Training loss (single batch): 3.083994150161743\n",
      "##################################\n",
      "## EPOCH 39/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8926596641540527\n",
      "\t Training loss (single batch): 3.051866054534912\n",
      "\t Training loss (single batch): 3.248995542526245\n",
      "##################################\n",
      "## EPOCH 40/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9612553119659424\n",
      "\t Training loss (single batch): 3.0062813758850098\n",
      "\t Training loss (single batch): 2.593691825866699\n",
      "##################################\n",
      "## EPOCH 41/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1811776161193848\n",
      "\t Training loss (single batch): 3.2632341384887695\n",
      "\t Training loss (single batch): 3.125096082687378\n",
      "##################################\n",
      "## EPOCH 42/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.932058572769165\n",
      "\t Training loss (single batch): 2.861332654953003\n",
      "\t Training loss (single batch): 2.9651947021484375\n",
      "##################################\n",
      "## EPOCH 43/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9428927898406982\n",
      "\t Training loss (single batch): 2.7865099906921387\n",
      "\t Training loss (single batch): 3.0310215950012207\n",
      "##################################\n",
      "## EPOCH 44/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.806157350540161\n",
      "\t Training loss (single batch): 2.9093122482299805\n",
      "\t Training loss (single batch): 2.810671091079712\n",
      "##################################\n",
      "## EPOCH 45/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6294262409210205\n",
      "\t Training loss (single batch): 2.958991765975952\n",
      "\t Training loss (single batch): 2.7258689403533936\n",
      "##################################\n",
      "## EPOCH 46/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9195871353149414\n",
      "\t Training loss (single batch): 3.388932704925537\n",
      "\t Training loss (single batch): 2.824798107147217\n",
      "##################################\n",
      "## EPOCH 47/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7863142490386963\n",
      "\t Training loss (single batch): 2.8315954208374023\n",
      "\t Training loss (single batch): 2.986353874206543\n",
      "##################################\n",
      "## EPOCH 48/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0221338272094727\n",
      "\t Training loss (single batch): 3.4021167755126953\n",
      "\t Training loss (single batch): 3.012547016143799\n",
      "##################################\n",
      "## EPOCH 49/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0238351821899414\n",
      "\t Training loss (single batch): 2.8663740158081055\n",
      "\t Training loss (single batch): 3.1083853244781494\n",
      "##################################\n",
      "## EPOCH 50/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.846440076828003\n",
      "\t Training loss (single batch): 3.32533860206604\n",
      "\t Training loss (single batch): 3.191948652267456\n",
      "##################################\n",
      "## EPOCH 51/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0600454807281494\n",
      "\t Training loss (single batch): 3.1113295555114746\n",
      "\t Training loss (single batch): 3.071749210357666\n",
      "##################################\n",
      "## EPOCH 52/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.863891363143921\n",
      "\t Training loss (single batch): 2.7216286659240723\n",
      "\t Training loss (single batch): 2.990685224533081\n",
      "##################################\n",
      "## EPOCH 53/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9860782623291016\n",
      "\t Training loss (single batch): 3.1294021606445312\n",
      "\t Training loss (single batch): 2.931575059890747\n",
      "##################################\n",
      "## EPOCH 54/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8027637004852295\n",
      "\t Training loss (single batch): 2.9317433834075928\n",
      "\t Training loss (single batch): 2.847177267074585\n",
      "##################################\n",
      "## EPOCH 55/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1531739234924316\n",
      "\t Training loss (single batch): 2.9937400817871094\n",
      "\t Training loss (single batch): 3.051248073577881\n",
      "##################################\n",
      "## EPOCH 56/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.911350727081299\n",
      "\t Training loss (single batch): 3.2541661262512207\n",
      "\t Training loss (single batch): 2.9598708152770996\n",
      "##################################\n",
      "## EPOCH 57/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8902535438537598\n",
      "\t Training loss (single batch): 2.985830545425415\n",
      "\t Training loss (single batch): 2.9502549171447754\n",
      "##################################\n",
      "## EPOCH 58/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0334537029266357\n",
      "\t Training loss (single batch): 3.3950207233428955\n",
      "\t Training loss (single batch): 2.717224359512329\n",
      "##################################\n",
      "## EPOCH 59/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7402734756469727\n",
      "\t Training loss (single batch): 2.847113609313965\n",
      "\t Training loss (single batch): 2.982877492904663\n",
      "##################################\n",
      "## EPOCH 60/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9590203762054443\n",
      "\t Training loss (single batch): 2.5290584564208984\n",
      "\t Training loss (single batch): 3.1903395652770996\n",
      "##################################\n",
      "## EPOCH 61/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.939755916595459\n",
      "\t Training loss (single batch): 2.9996509552001953\n",
      "\t Training loss (single batch): 2.942192316055298\n",
      "##################################\n",
      "## EPOCH 62/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.887498378753662\n",
      "\t Training loss (single batch): 2.9125380516052246\n",
      "\t Training loss (single batch): 3.0003576278686523\n",
      "##################################\n",
      "## EPOCH 63/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0422005653381348\n",
      "\t Training loss (single batch): 2.8570175170898438\n",
      "\t Training loss (single batch): 3.2991750240325928\n",
      "##################################\n",
      "## EPOCH 64/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.014406442642212\n",
      "\t Training loss (single batch): 2.764423370361328\n",
      "\t Training loss (single batch): 3.4176735877990723\n",
      "##################################\n",
      "## EPOCH 65/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6552600860595703\n",
      "\t Training loss (single batch): 3.0445916652679443\n",
      "\t Training loss (single batch): 2.7904844284057617\n",
      "##################################\n",
      "## EPOCH 66/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7567262649536133\n",
      "\t Training loss (single batch): 2.88264799118042\n",
      "\t Training loss (single batch): 3.047093152999878\n",
      "##################################\n",
      "## EPOCH 67/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.118590831756592\n",
      "\t Training loss (single batch): 2.9752330780029297\n",
      "\t Training loss (single batch): 3.0525691509246826\n",
      "##################################\n",
      "## EPOCH 68/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.728952407836914\n",
      "\t Training loss (single batch): 2.7341957092285156\n",
      "\t Training loss (single batch): 3.1494498252868652\n",
      "##################################\n",
      "## EPOCH 69/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8580901622772217\n",
      "\t Training loss (single batch): 2.647104263305664\n",
      "\t Training loss (single batch): 2.6210341453552246\n",
      "##################################\n",
      "## EPOCH 70/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.923152446746826\n",
      "\t Training loss (single batch): 2.8776488304138184\n",
      "\t Training loss (single batch): 3.5029051303863525\n",
      "##################################\n",
      "## EPOCH 71/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.901402473449707\n",
      "\t Training loss (single batch): 2.777860164642334\n",
      "\t Training loss (single batch): 3.1694788932800293\n",
      "##################################\n",
      "## EPOCH 72/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8340561389923096\n",
      "\t Training loss (single batch): 3.089097023010254\n",
      "\t Training loss (single batch): 2.929450750350952\n",
      "##################################\n",
      "## EPOCH 73/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.034224510192871\n",
      "\t Training loss (single batch): 3.2217793464660645\n",
      "\t Training loss (single batch): 3.161283254623413\n",
      "##################################\n",
      "## EPOCH 74/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8424723148345947\n",
      "\t Training loss (single batch): 3.2643749713897705\n",
      "\t Training loss (single batch): 2.43727707862854\n",
      "##################################\n",
      "## EPOCH 75/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9320199489593506\n",
      "\t Training loss (single batch): 2.918290376663208\n",
      "\t Training loss (single batch): 2.5936315059661865\n",
      "##################################\n",
      "## EPOCH 76/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.3801321983337402\n",
      "\t Training loss (single batch): 2.911292552947998\n",
      "\t Training loss (single batch): 2.944474697113037\n",
      "##################################\n",
      "## EPOCH 77/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6244945526123047\n",
      "\t Training loss (single batch): 3.019019365310669\n",
      "\t Training loss (single batch): 2.617797613143921\n",
      "##################################\n",
      "## EPOCH 78/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7159721851348877\n",
      "\t Training loss (single batch): 2.7788946628570557\n",
      "\t Training loss (single batch): 2.816619873046875\n",
      "##################################\n",
      "## EPOCH 79/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.01409649848938\n",
      "\t Training loss (single batch): 3.1211719512939453\n",
      "\t Training loss (single batch): 2.90234112739563\n",
      "##################################\n",
      "## EPOCH 80/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.229912519454956\n",
      "\t Training loss (single batch): 2.9481136798858643\n",
      "\t Training loss (single batch): 3.2988803386688232\n",
      "##################################\n",
      "## EPOCH 81/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2375142574310303\n",
      "\t Training loss (single batch): 2.678698778152466\n",
      "\t Training loss (single batch): 2.4396300315856934\n",
      "##################################\n",
      "## EPOCH 82/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0288491249084473\n",
      "\t Training loss (single batch): 3.233375310897827\n",
      "\t Training loss (single batch): 2.9537787437438965\n",
      "##################################\n",
      "## EPOCH 83/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7774100303649902\n",
      "\t Training loss (single batch): 3.2415874004364014\n",
      "\t Training loss (single batch): 2.8407814502716064\n",
      "##################################\n",
      "## EPOCH 84/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.172214984893799\n",
      "\t Training loss (single batch): 3.081930160522461\n",
      "\t Training loss (single batch): 2.7468578815460205\n",
      "##################################\n",
      "## EPOCH 85/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.596590042114258\n",
      "\t Training loss (single batch): 2.9504714012145996\n",
      "\t Training loss (single batch): 2.7598490715026855\n",
      "##################################\n",
      "## EPOCH 86/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8754045963287354\n",
      "\t Training loss (single batch): 2.670836925506592\n",
      "\t Training loss (single batch): 3.0229909420013428\n",
      "##################################\n",
      "## EPOCH 87/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7199292182922363\n",
      "\t Training loss (single batch): 3.1295602321624756\n",
      "\t Training loss (single batch): 3.0041816234588623\n",
      "##################################\n",
      "## EPOCH 88/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.811614751815796\n",
      "\t Training loss (single batch): 3.1754448413848877\n",
      "\t Training loss (single batch): 2.654960870742798\n",
      "##################################\n",
      "## EPOCH 89/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.697343111038208\n",
      "\t Training loss (single batch): 3.168388843536377\n",
      "\t Training loss (single batch): 3.230013847351074\n",
      "##################################\n",
      "## EPOCH 90/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.796987771987915\n",
      "\t Training loss (single batch): 2.86806058883667\n",
      "\t Training loss (single batch): 3.1101861000061035\n",
      "##################################\n",
      "## EPOCH 91/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7663156986236572\n",
      "\t Training loss (single batch): 2.8999850749969482\n",
      "\t Training loss (single batch): 3.00606107711792\n",
      "##################################\n",
      "## EPOCH 92/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.853945732116699\n",
      "\t Training loss (single batch): 2.9014782905578613\n",
      "\t Training loss (single batch): 2.8836069107055664\n",
      "##################################\n",
      "## EPOCH 93/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.525761604309082\n",
      "\t Training loss (single batch): 2.436209201812744\n",
      "\t Training loss (single batch): 3.1540396213531494\n",
      "##################################\n",
      "## EPOCH 94/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8716583251953125\n",
      "\t Training loss (single batch): 3.3080499172210693\n",
      "\t Training loss (single batch): 3.3194525241851807\n",
      "##################################\n",
      "## EPOCH 95/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1938283443450928\n",
      "\t Training loss (single batch): 3.1705188751220703\n",
      "\t Training loss (single batch): 3.038426637649536\n",
      "##################################\n",
      "## EPOCH 96/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.024078845977783\n",
      "\t Training loss (single batch): 2.6935882568359375\n",
      "\t Training loss (single batch): 3.0152926445007324\n",
      "##################################\n",
      "## EPOCH 97/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.2604188919067383\n",
      "\t Training loss (single batch): 3.1506102085113525\n",
      "\t Training loss (single batch): 3.1060850620269775\n",
      "##################################\n",
      "## EPOCH 98/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.703835964202881\n",
      "\t Training loss (single batch): 3.2256522178649902\n",
      "\t Training loss (single batch): 3.0771422386169434\n",
      "##################################\n",
      "## EPOCH 99/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.834836483001709\n",
      "\t Training loss (single batch): 3.14863920211792\n",
      "\t Training loss (single batch): 2.892582416534424\n",
      "##################################\n",
      "## EPOCH 100/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.776658296585083\n",
      "\t Training loss (single batch): 2.890681743621826\n",
      "\t Training loss (single batch): 2.769402265548706\n",
      "##################################\n",
      "## EPOCH 101/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9050920009613037\n",
      "\t Training loss (single batch): 3.0174145698547363\n",
      "\t Training loss (single batch): 2.8817944526672363\n",
      "##################################\n",
      "## EPOCH 102/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.212984561920166\n",
      "\t Training loss (single batch): 3.260650157928467\n",
      "\t Training loss (single batch): 2.812189817428589\n",
      "##################################\n",
      "## EPOCH 103/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.635725259780884\n",
      "\t Training loss (single batch): 2.7510178089141846\n",
      "\t Training loss (single batch): 2.7168822288513184\n",
      "##################################\n",
      "## EPOCH 104/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9046897888183594\n",
      "\t Training loss (single batch): 2.973879814147949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 3.071009635925293\n",
      "##################################\n",
      "## EPOCH 105/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.841610908508301\n",
      "\t Training loss (single batch): 2.891688585281372\n",
      "\t Training loss (single batch): 3.390174388885498\n",
      "##################################\n",
      "## EPOCH 106/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9298272132873535\n",
      "\t Training loss (single batch): 2.8597018718719482\n",
      "\t Training loss (single batch): 3.0233235359191895\n",
      "##################################\n",
      "## EPOCH 107/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7831063270568848\n",
      "\t Training loss (single batch): 2.9165091514587402\n",
      "\t Training loss (single batch): 2.6339805126190186\n",
      "##################################\n",
      "## EPOCH 108/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7500500679016113\n",
      "\t Training loss (single batch): 2.885577917098999\n",
      "\t Training loss (single batch): 2.9238288402557373\n",
      "##################################\n",
      "## EPOCH 109/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.906010866165161\n",
      "\t Training loss (single batch): 3.0259971618652344\n",
      "\t Training loss (single batch): 2.7340846061706543\n",
      "##################################\n",
      "## EPOCH 110/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9801535606384277\n",
      "\t Training loss (single batch): 3.1616263389587402\n",
      "\t Training loss (single batch): 3.121995687484741\n",
      "##################################\n",
      "## EPOCH 111/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7549421787261963\n",
      "\t Training loss (single batch): 2.825489044189453\n",
      "\t Training loss (single batch): 2.9560317993164062\n",
      "##################################\n",
      "## EPOCH 112/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.662351131439209\n",
      "\t Training loss (single batch): 3.1155521869659424\n",
      "\t Training loss (single batch): 2.7810494899749756\n",
      "##################################\n",
      "## EPOCH 113/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0027265548706055\n",
      "\t Training loss (single batch): 2.4562439918518066\n",
      "\t Training loss (single batch): 2.8540194034576416\n",
      "##################################\n",
      "## EPOCH 114/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6861557960510254\n",
      "\t Training loss (single batch): 2.775902509689331\n",
      "\t Training loss (single batch): 2.661855459213257\n",
      "##################################\n",
      "## EPOCH 115/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.712674140930176\n",
      "\t Training loss (single batch): 3.104243278503418\n",
      "\t Training loss (single batch): 2.9624783992767334\n",
      "##################################\n",
      "## EPOCH 116/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.962096929550171\n",
      "\t Training loss (single batch): 2.9224581718444824\n",
      "\t Training loss (single batch): 2.7600176334381104\n",
      "##################################\n",
      "## EPOCH 117/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.234908103942871\n",
      "\t Training loss (single batch): 2.8228447437286377\n",
      "\t Training loss (single batch): 2.5042643547058105\n",
      "##################################\n",
      "## EPOCH 118/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7673325538635254\n",
      "\t Training loss (single batch): 2.6154420375823975\n",
      "\t Training loss (single batch): 3.014681339263916\n",
      "##################################\n",
      "## EPOCH 119/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.917823314666748\n",
      "\t Training loss (single batch): 2.77398681640625\n",
      "\t Training loss (single batch): 2.9782001972198486\n",
      "##################################\n",
      "## EPOCH 120/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5119175910949707\n",
      "\t Training loss (single batch): 2.8098840713500977\n",
      "\t Training loss (single batch): 2.7565505504608154\n",
      "##################################\n",
      "## EPOCH 121/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6113312244415283\n",
      "\t Training loss (single batch): 2.5093090534210205\n",
      "\t Training loss (single batch): 2.5846619606018066\n",
      "##################################\n",
      "## EPOCH 122/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.669111490249634\n",
      "\t Training loss (single batch): 2.901289701461792\n",
      "\t Training loss (single batch): 2.8166003227233887\n",
      "##################################\n",
      "## EPOCH 123/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.609570026397705\n",
      "\t Training loss (single batch): 2.481139898300171\n",
      "\t Training loss (single batch): 3.2432126998901367\n",
      "##################################\n",
      "## EPOCH 124/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4937801361083984\n",
      "\t Training loss (single batch): 2.4353721141815186\n",
      "\t Training loss (single batch): 2.9106028079986572\n",
      "##################################\n",
      "## EPOCH 125/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6602330207824707\n",
      "\t Training loss (single batch): 2.5749425888061523\n",
      "\t Training loss (single batch): 2.7993013858795166\n",
      "##################################\n",
      "## EPOCH 126/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.284349203109741\n",
      "\t Training loss (single batch): 2.692584753036499\n",
      "\t Training loss (single batch): 2.8152964115142822\n",
      "##################################\n",
      "## EPOCH 127/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9251444339752197\n",
      "\t Training loss (single batch): 2.297175645828247\n",
      "\t Training loss (single batch): 2.721358299255371\n",
      "##################################\n",
      "## EPOCH 128/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6109976768493652\n",
      "\t Training loss (single batch): 2.6765403747558594\n",
      "\t Training loss (single batch): 2.8209121227264404\n",
      "##################################\n",
      "## EPOCH 129/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.77864670753479\n",
      "\t Training loss (single batch): 2.5923609733581543\n",
      "\t Training loss (single batch): 2.772200345993042\n",
      "##################################\n",
      "## EPOCH 130/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6583504676818848\n",
      "\t Training loss (single batch): 2.6074912548065186\n",
      "\t Training loss (single batch): 2.807219982147217\n",
      "##################################\n",
      "## EPOCH 131/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6776483058929443\n",
      "\t Training loss (single batch): 2.521824836730957\n",
      "\t Training loss (single batch): 2.6488473415374756\n",
      "##################################\n",
      "## EPOCH 132/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.625774383544922\n",
      "\t Training loss (single batch): 2.5602974891662598\n",
      "\t Training loss (single batch): 2.5209906101226807\n",
      "##################################\n",
      "## EPOCH 133/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.445643663406372\n",
      "\t Training loss (single batch): 2.782885789871216\n",
      "\t Training loss (single batch): 2.853160858154297\n",
      "##################################\n",
      "## EPOCH 134/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.596238613128662\n",
      "\t Training loss (single batch): 2.6978955268859863\n",
      "\t Training loss (single batch): 2.2918121814727783\n",
      "##################################\n",
      "## EPOCH 135/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6245172023773193\n",
      "\t Training loss (single batch): 2.3249571323394775\n",
      "\t Training loss (single batch): 2.9695870876312256\n",
      "##################################\n",
      "## EPOCH 136/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.001418113708496\n",
      "\t Training loss (single batch): 2.5757906436920166\n",
      "\t Training loss (single batch): 3.2542474269866943\n",
      "##################################\n",
      "## EPOCH 137/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.776388645172119\n",
      "\t Training loss (single batch): 2.6179521083831787\n",
      "\t Training loss (single batch): 2.841308832168579\n",
      "##################################\n",
      "## EPOCH 138/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5616986751556396\n",
      "\t Training loss (single batch): 2.875844717025757\n",
      "\t Training loss (single batch): 2.7734107971191406\n",
      "##################################\n",
      "## EPOCH 139/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.480743646621704\n",
      "\t Training loss (single batch): 2.6580984592437744\n",
      "\t Training loss (single batch): 2.6430039405822754\n",
      "##################################\n",
      "## EPOCH 140/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7085721492767334\n",
      "\t Training loss (single batch): 2.677532434463501\n",
      "\t Training loss (single batch): 2.6795053482055664\n",
      "##################################\n",
      "## EPOCH 141/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.626812696456909\n",
      "\t Training loss (single batch): 2.3453705310821533\n",
      "\t Training loss (single batch): 2.359250783920288\n",
      "##################################\n",
      "## EPOCH 142/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5752644538879395\n",
      "\t Training loss (single batch): 2.726480722427368\n",
      "\t Training loss (single batch): 3.0531272888183594\n",
      "##################################\n",
      "## EPOCH 143/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.0276615619659424\n",
      "\t Training loss (single batch): 2.707475423812866\n",
      "\t Training loss (single batch): 2.7113733291625977\n",
      "##################################\n",
      "## EPOCH 144/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.428891181945801\n",
      "\t Training loss (single batch): 3.3060855865478516\n",
      "\t Training loss (single batch): 3.1174933910369873\n",
      "##################################\n",
      "## EPOCH 145/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.551711082458496\n",
      "\t Training loss (single batch): 2.6563525199890137\n",
      "\t Training loss (single batch): 2.35136079788208\n",
      "##################################\n",
      "## EPOCH 146/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8734819889068604\n",
      "\t Training loss (single batch): 2.804111957550049\n",
      "\t Training loss (single batch): 2.7811226844787598\n",
      "##################################\n",
      "## EPOCH 147/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2629613876342773\n",
      "\t Training loss (single batch): 2.675881862640381\n",
      "\t Training loss (single batch): 2.504105567932129\n",
      "##################################\n",
      "## EPOCH 148/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.485689401626587\n",
      "\t Training loss (single batch): 2.1777596473693848\n",
      "\t Training loss (single batch): 2.4896047115325928\n",
      "##################################\n",
      "## EPOCH 149/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2648842334747314\n",
      "\t Training loss (single batch): 2.5901684761047363\n",
      "\t Training loss (single batch): 2.457144260406494\n",
      "##################################\n",
      "## EPOCH 150/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3581693172454834\n",
      "\t Training loss (single batch): 2.62160587310791\n",
      "\t Training loss (single batch): 2.6638708114624023\n",
      "##################################\n",
      "## EPOCH 151/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.458946466445923\n",
      "\t Training loss (single batch): 2.3503518104553223\n",
      "\t Training loss (single batch): 2.83392596244812\n",
      "##################################\n",
      "## EPOCH 152/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.83097505569458\n",
      "\t Training loss (single batch): 2.9457249641418457\n",
      "\t Training loss (single batch): 2.86517596244812\n",
      "##################################\n",
      "## EPOCH 153/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.687523126602173\n",
      "\t Training loss (single batch): 2.735233783721924\n",
      "\t Training loss (single batch): 2.6126697063446045\n",
      "##################################\n",
      "## EPOCH 154/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5807363986968994\n",
      "\t Training loss (single batch): 2.5028693675994873\n",
      "\t Training loss (single batch): 2.3434131145477295\n",
      "##################################\n",
      "## EPOCH 155/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.710439443588257\n",
      "\t Training loss (single batch): 2.4819774627685547\n",
      "\t Training loss (single batch): 2.466395378112793\n",
      "##################################\n",
      "## EPOCH 156/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6011910438537598\n",
      "\t Training loss (single batch): 2.27176570892334\n",
      "\t Training loss (single batch): 3.0095605850219727\n",
      "##################################\n",
      "## EPOCH 157/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5388729572296143\n",
      "\t Training loss (single batch): 2.7859153747558594\n",
      "\t Training loss (single batch): 2.1776928901672363\n",
      "##################################\n",
      "## EPOCH 158/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6374378204345703\n",
      "\t Training loss (single batch): 2.973428964614868\n",
      "\t Training loss (single batch): 2.6098055839538574\n",
      "##################################\n",
      "## EPOCH 159/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7620837688446045\n",
      "\t Training loss (single batch): 2.3533382415771484\n",
      "\t Training loss (single batch): 2.67230486869812\n",
      "##################################\n",
      "## EPOCH 160/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6132147312164307\n",
      "\t Training loss (single batch): 2.7516205310821533\n",
      "\t Training loss (single batch): 2.4809248447418213\n",
      "##################################\n",
      "## EPOCH 161/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3242990970611572\n",
      "\t Training loss (single batch): 2.4020440578460693\n",
      "\t Training loss (single batch): 3.1118836402893066\n",
      "##################################\n",
      "## EPOCH 162/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.456108570098877\n",
      "\t Training loss (single batch): 2.579460859298706\n",
      "\t Training loss (single batch): 2.785778760910034\n",
      "##################################\n",
      "## EPOCH 163/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3591601848602295\n",
      "\t Training loss (single batch): 2.869724750518799\n",
      "\t Training loss (single batch): 2.467024564743042\n",
      "##################################\n",
      "## EPOCH 164/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6242594718933105\n",
      "\t Training loss (single batch): 2.670426607131958\n",
      "\t Training loss (single batch): 2.9032816886901855\n",
      "##################################\n",
      "## EPOCH 165/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.656644821166992\n",
      "\t Training loss (single batch): 2.5718843936920166\n",
      "\t Training loss (single batch): 2.472630739212036\n",
      "##################################\n",
      "## EPOCH 166/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5844128131866455\n",
      "\t Training loss (single batch): 2.705977201461792\n",
      "\t Training loss (single batch): 2.7472736835479736\n",
      "##################################\n",
      "## EPOCH 167/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5369553565979004\n",
      "\t Training loss (single batch): 2.7567965984344482\n",
      "\t Training loss (single batch): 3.12774658203125\n",
      "##################################\n",
      "## EPOCH 168/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6939849853515625\n",
      "\t Training loss (single batch): 2.233821153640747\n",
      "\t Training loss (single batch): 2.918635606765747\n",
      "##################################\n",
      "## EPOCH 169/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.449381113052368\n",
      "\t Training loss (single batch): 2.231605291366577\n",
      "\t Training loss (single batch): 2.5278961658477783\n",
      "##################################\n",
      "## EPOCH 170/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.419893980026245\n",
      "\t Training loss (single batch): 2.5634236335754395\n",
      "\t Training loss (single batch): 2.7713520526885986\n",
      "##################################\n",
      "## EPOCH 171/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.771740198135376\n",
      "\t Training loss (single batch): 2.488532781600952\n",
      "\t Training loss (single batch): 3.0729644298553467\n",
      "##################################\n",
      "## EPOCH 172/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8150179386138916\n",
      "\t Training loss (single batch): 2.293259859085083\n",
      "\t Training loss (single batch): 3.011603355407715\n",
      "##################################\n",
      "## EPOCH 173/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.244032859802246\n",
      "\t Training loss (single batch): 2.5241637229919434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.8795409202575684\n",
      "##################################\n",
      "## EPOCH 174/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7600607872009277\n",
      "\t Training loss (single batch): 2.744647979736328\n",
      "\t Training loss (single batch): 2.492595911026001\n",
      "##################################\n",
      "## EPOCH 175/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.227877616882324\n",
      "\t Training loss (single batch): 2.530653476715088\n",
      "\t Training loss (single batch): 2.257291078567505\n",
      "##################################\n",
      "## EPOCH 176/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5286598205566406\n",
      "\t Training loss (single batch): 2.481445789337158\n",
      "\t Training loss (single batch): 2.062704563140869\n",
      "##################################\n",
      "## EPOCH 177/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7077291011810303\n",
      "\t Training loss (single batch): 2.882753372192383\n",
      "\t Training loss (single batch): 2.508575201034546\n",
      "##################################\n",
      "## EPOCH 178/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5015344619750977\n",
      "\t Training loss (single batch): 2.1764068603515625\n",
      "\t Training loss (single batch): 2.501538038253784\n",
      "##################################\n",
      "## EPOCH 179/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2350287437438965\n",
      "\t Training loss (single batch): 2.4578073024749756\n",
      "\t Training loss (single batch): 2.5134103298187256\n",
      "##################################\n",
      "## EPOCH 180/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.467642068862915\n",
      "\t Training loss (single batch): 2.506618022918701\n",
      "\t Training loss (single batch): 2.3080363273620605\n",
      "##################################\n",
      "## EPOCH 181/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.325798273086548\n",
      "\t Training loss (single batch): 2.4368197917938232\n",
      "\t Training loss (single batch): 2.6491594314575195\n",
      "##################################\n",
      "## EPOCH 182/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4463677406311035\n",
      "\t Training loss (single batch): 2.7266621589660645\n",
      "\t Training loss (single batch): 2.654425621032715\n",
      "##################################\n",
      "## EPOCH 183/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6137211322784424\n",
      "\t Training loss (single batch): 1.9448961019515991\n",
      "\t Training loss (single batch): 2.3465733528137207\n",
      "##################################\n",
      "## EPOCH 184/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.382683277130127\n",
      "\t Training loss (single batch): 2.903313398361206\n",
      "\t Training loss (single batch): 2.6761128902435303\n",
      "##################################\n",
      "## EPOCH 185/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.362165927886963\n",
      "\t Training loss (single batch): 2.7680587768554688\n",
      "\t Training loss (single batch): 2.3922483921051025\n",
      "##################################\n",
      "## EPOCH 186/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.654513359069824\n",
      "\t Training loss (single batch): 2.022467613220215\n",
      "\t Training loss (single batch): 2.3188388347625732\n",
      "##################################\n",
      "## EPOCH 187/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3426969051361084\n",
      "\t Training loss (single batch): 2.4193098545074463\n",
      "\t Training loss (single batch): 2.481076955795288\n",
      "##################################\n",
      "## EPOCH 188/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4452431201934814\n",
      "\t Training loss (single batch): 2.4877607822418213\n",
      "\t Training loss (single batch): 2.668524742126465\n",
      "##################################\n",
      "## EPOCH 189/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.660456418991089\n",
      "\t Training loss (single batch): 2.5167911052703857\n",
      "\t Training loss (single batch): 2.152714490890503\n",
      "##################################\n",
      "## EPOCH 190/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0924906730651855\n",
      "\t Training loss (single batch): 2.179279088973999\n",
      "\t Training loss (single batch): 2.682689905166626\n",
      "##################################\n",
      "## EPOCH 191/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7693862915039062\n",
      "\t Training loss (single batch): 2.4995956420898438\n",
      "\t Training loss (single batch): 3.3267014026641846\n",
      "##################################\n",
      "## EPOCH 192/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.759347438812256\n",
      "\t Training loss (single batch): 2.3776090145111084\n",
      "\t Training loss (single batch): 2.125490665435791\n",
      "##################################\n",
      "## EPOCH 193/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.482205629348755\n",
      "\t Training loss (single batch): 2.1483991146087646\n",
      "\t Training loss (single batch): 2.1384177207946777\n",
      "##################################\n",
      "## EPOCH 194/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.214864492416382\n",
      "\t Training loss (single batch): 2.4862656593322754\n",
      "\t Training loss (single batch): 1.940731406211853\n",
      "##################################\n",
      "## EPOCH 195/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3948326110839844\n",
      "\t Training loss (single batch): 2.8676035404205322\n",
      "\t Training loss (single batch): 2.6879396438598633\n",
      "##################################\n",
      "## EPOCH 196/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.830742359161377\n",
      "\t Training loss (single batch): 2.512125015258789\n",
      "\t Training loss (single batch): 2.606592893600464\n",
      "##################################\n",
      "## EPOCH 197/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0726027488708496\n",
      "\t Training loss (single batch): 2.664016008377075\n",
      "\t Training loss (single batch): 2.3061060905456543\n",
      "##################################\n",
      "## EPOCH 198/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0317349433898926\n",
      "\t Training loss (single batch): 2.009056806564331\n",
      "\t Training loss (single batch): 2.5205399990081787\n",
      "##################################\n",
      "## EPOCH 199/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3864147663116455\n",
      "\t Training loss (single batch): 2.7379775047302246\n",
      "\t Training loss (single batch): 1.783841609954834\n",
      "##################################\n",
      "## EPOCH 200/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6682534217834473\n",
      "\t Training loss (single batch): 2.919654130935669\n",
      "\t Training loss (single batch): 2.418034791946411\n",
      "##################################\n",
      "## EPOCH 201/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.509904623031616\n",
      "\t Training loss (single batch): 2.3256216049194336\n",
      "\t Training loss (single batch): 2.8460898399353027\n",
      "##################################\n",
      "## EPOCH 202/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5290989875793457\n",
      "\t Training loss (single batch): 2.5801050662994385\n",
      "\t Training loss (single batch): 2.2868242263793945\n",
      "##################################\n",
      "## EPOCH 203/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.937436580657959\n",
      "\t Training loss (single batch): 1.82319176197052\n",
      "\t Training loss (single batch): 2.4953317642211914\n",
      "##################################\n",
      "## EPOCH 204/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.202975273132324\n",
      "\t Training loss (single batch): 2.824963331222534\n",
      "\t Training loss (single batch): 2.4787096977233887\n",
      "##################################\n",
      "## EPOCH 205/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.570068120956421\n",
      "\t Training loss (single batch): 2.4287781715393066\n",
      "\t Training loss (single batch): 2.092998504638672\n",
      "##################################\n",
      "## EPOCH 206/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.282176971435547\n",
      "\t Training loss (single batch): 2.225407600402832\n",
      "\t Training loss (single batch): 1.9395774602890015\n",
      "##################################\n",
      "## EPOCH 207/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2474124431610107\n",
      "\t Training loss (single batch): 3.265923023223877\n",
      "\t Training loss (single batch): 2.8688690662384033\n",
      "##################################\n",
      "## EPOCH 208/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.9688544273376465\n",
      "\t Training loss (single batch): 2.41462779045105\n",
      "\t Training loss (single batch): 2.603379249572754\n",
      "##################################\n",
      "## EPOCH 209/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.9581475257873535\n",
      "\t Training loss (single batch): 2.3686413764953613\n",
      "\t Training loss (single batch): 2.1088387966156006\n",
      "##################################\n",
      "## EPOCH 210/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7910656929016113\n",
      "\t Training loss (single batch): 2.8500351905822754\n",
      "\t Training loss (single batch): 2.494525194168091\n",
      "##################################\n",
      "## EPOCH 211/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.710008382797241\n",
      "\t Training loss (single batch): 2.3437089920043945\n",
      "\t Training loss (single batch): 3.24796986579895\n",
      "##################################\n",
      "## EPOCH 212/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4472262859344482\n",
      "\t Training loss (single batch): 2.6843743324279785\n",
      "\t Training loss (single batch): 2.2065696716308594\n",
      "##################################\n",
      "## EPOCH 213/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.599905014038086\n",
      "\t Training loss (single batch): 2.0116612911224365\n",
      "\t Training loss (single batch): 2.559364080429077\n",
      "##################################\n",
      "## EPOCH 214/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2402896881103516\n",
      "\t Training loss (single batch): 2.5463383197784424\n",
      "\t Training loss (single batch): 2.6147968769073486\n",
      "##################################\n",
      "## EPOCH 215/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2376437187194824\n",
      "\t Training loss (single batch): 2.4368937015533447\n",
      "\t Training loss (single batch): 2.263087034225464\n",
      "##################################\n",
      "## EPOCH 216/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4414141178131104\n",
      "\t Training loss (single batch): 2.2897140979766846\n",
      "\t Training loss (single batch): 3.0809438228607178\n",
      "##################################\n",
      "## EPOCH 217/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.085233688354492\n",
      "\t Training loss (single batch): 2.4659013748168945\n",
      "\t Training loss (single batch): 2.440284013748169\n",
      "##################################\n",
      "## EPOCH 218/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5449392795562744\n",
      "\t Training loss (single batch): 2.3325862884521484\n",
      "\t Training loss (single batch): 2.396266222000122\n",
      "##################################\n",
      "## EPOCH 219/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.811378002166748\n",
      "\t Training loss (single batch): 2.9446935653686523\n",
      "\t Training loss (single batch): 2.8650028705596924\n",
      "##################################\n",
      "## EPOCH 220/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2929718494415283\n",
      "\t Training loss (single batch): 2.6864712238311768\n",
      "\t Training loss (single batch): 2.3559491634368896\n",
      "##################################\n",
      "## EPOCH 221/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.072007179260254\n",
      "\t Training loss (single batch): 1.7604399919509888\n",
      "\t Training loss (single batch): 2.2124745845794678\n",
      "##################################\n",
      "## EPOCH 222/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6250293254852295\n",
      "\t Training loss (single batch): 2.8601677417755127\n",
      "\t Training loss (single batch): 2.9001128673553467\n",
      "##################################\n",
      "## EPOCH 223/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.228241205215454\n",
      "\t Training loss (single batch): 2.192274808883667\n",
      "\t Training loss (single batch): 2.2692761421203613\n",
      "##################################\n",
      "## EPOCH 224/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.180128335952759\n",
      "\t Training loss (single batch): 2.3888652324676514\n",
      "\t Training loss (single batch): 2.215716600418091\n",
      "##################################\n",
      "## EPOCH 225/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5243968963623047\n",
      "\t Training loss (single batch): 2.079054594039917\n",
      "\t Training loss (single batch): 2.4466981887817383\n",
      "##################################\n",
      "## EPOCH 226/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4150917530059814\n",
      "\t Training loss (single batch): 2.459139347076416\n",
      "\t Training loss (single batch): 2.2843446731567383\n",
      "##################################\n",
      "## EPOCH 227/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.245382308959961\n",
      "\t Training loss (single batch): 2.045300245285034\n",
      "\t Training loss (single batch): 2.521488666534424\n",
      "##################################\n",
      "## EPOCH 228/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7975211143493652\n",
      "\t Training loss (single batch): 2.6052234172821045\n",
      "\t Training loss (single batch): 2.3154006004333496\n",
      "##################################\n",
      "## EPOCH 229/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2254140377044678\n",
      "\t Training loss (single batch): 2.2796947956085205\n",
      "\t Training loss (single batch): 2.781317710876465\n",
      "##################################\n",
      "## EPOCH 230/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9653797149658203\n",
      "\t Training loss (single batch): 2.4597043991088867\n",
      "\t Training loss (single batch): 2.8038699626922607\n",
      "##################################\n",
      "## EPOCH 231/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0084593296051025\n",
      "\t Training loss (single batch): 2.0324833393096924\n",
      "\t Training loss (single batch): 2.8587839603424072\n",
      "##################################\n",
      "## EPOCH 232/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1460230350494385\n",
      "\t Training loss (single batch): 2.2734012603759766\n",
      "\t Training loss (single batch): 2.564305305480957\n",
      "##################################\n",
      "## EPOCH 233/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0934760570526123\n",
      "\t Training loss (single batch): 2.1105659008026123\n",
      "\t Training loss (single batch): 3.3058242797851562\n",
      "##################################\n",
      "## EPOCH 234/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1640825271606445\n",
      "\t Training loss (single batch): 2.2580199241638184\n",
      "\t Training loss (single batch): 2.4796667098999023\n",
      "##################################\n",
      "## EPOCH 235/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.355151653289795\n",
      "\t Training loss (single batch): 2.240506172180176\n",
      "\t Training loss (single batch): 2.8120505809783936\n",
      "##################################\n",
      "## EPOCH 236/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1424150466918945\n",
      "\t Training loss (single batch): 1.9333637952804565\n",
      "\t Training loss (single batch): 2.285017728805542\n",
      "##################################\n",
      "## EPOCH 237/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2010793685913086\n",
      "\t Training loss (single batch): 2.550025224685669\n",
      "\t Training loss (single batch): 2.6121597290039062\n",
      "##################################\n",
      "## EPOCH 238/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6240200996398926\n",
      "\t Training loss (single batch): 2.5037841796875\n",
      "\t Training loss (single batch): 2.5269546508789062\n",
      "##################################\n",
      "## EPOCH 239/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0832929611206055\n",
      "\t Training loss (single batch): 2.1151344776153564\n",
      "\t Training loss (single batch): 2.3442866802215576\n",
      "##################################\n",
      "## EPOCH 240/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.40092396736145\n",
      "\t Training loss (single batch): 2.298793315887451\n",
      "\t Training loss (single batch): 2.432447910308838\n",
      "##################################\n",
      "## EPOCH 241/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9448256492614746\n",
      "\t Training loss (single batch): 2.4271295070648193\n",
      "\t Training loss (single batch): 2.590928077697754\n",
      "##################################\n",
      "## EPOCH 242/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8594398498535156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.3392813205718994\n",
      "\t Training loss (single batch): 2.588060140609741\n",
      "##################################\n",
      "## EPOCH 243/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6277542114257812\n",
      "\t Training loss (single batch): 2.15763783454895\n",
      "\t Training loss (single batch): 1.9508742094039917\n",
      "##################################\n",
      "## EPOCH 244/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.158159017562866\n",
      "\t Training loss (single batch): 2.22627329826355\n",
      "\t Training loss (single batch): 2.5365102291107178\n",
      "##################################\n",
      "## EPOCH 245/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4266114234924316\n",
      "\t Training loss (single batch): 1.9711533784866333\n",
      "\t Training loss (single batch): 1.9874752759933472\n",
      "##################################\n",
      "## EPOCH 246/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0968430042266846\n",
      "\t Training loss (single batch): 2.270009994506836\n",
      "\t Training loss (single batch): 2.4974706172943115\n",
      "##################################\n",
      "## EPOCH 247/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3254001140594482\n",
      "\t Training loss (single batch): 2.442836046218872\n",
      "\t Training loss (single batch): 2.5828964710235596\n",
      "##################################\n",
      "## EPOCH 248/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.391908884048462\n",
      "\t Training loss (single batch): 2.3804566860198975\n",
      "\t Training loss (single batch): 2.9759011268615723\n",
      "##################################\n",
      "## EPOCH 249/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1492202281951904\n",
      "\t Training loss (single batch): 2.472196102142334\n",
      "\t Training loss (single batch): 2.7887136936187744\n",
      "##################################\n",
      "## EPOCH 250/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3492648601531982\n",
      "\t Training loss (single batch): 2.266841411590576\n",
      "\t Training loss (single batch): 2.5533392429351807\n",
      "##################################\n",
      "## EPOCH 251/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0899362564086914\n",
      "\t Training loss (single batch): 1.9324265718460083\n",
      "\t Training loss (single batch): 2.16351580619812\n",
      "##################################\n",
      "## EPOCH 252/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.232652425765991\n",
      "\t Training loss (single batch): 2.38877534866333\n",
      "\t Training loss (single batch): 1.981092095375061\n",
      "##################################\n",
      "## EPOCH 253/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3895411491394043\n",
      "\t Training loss (single batch): 2.6432273387908936\n",
      "\t Training loss (single batch): 2.5542731285095215\n",
      "##################################\n",
      "## EPOCH 254/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.081435203552246\n",
      "\t Training loss (single batch): 2.119624137878418\n",
      "\t Training loss (single batch): 1.8764610290527344\n",
      "##################################\n",
      "## EPOCH 255/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7941943407058716\n",
      "\t Training loss (single batch): 2.654799699783325\n",
      "\t Training loss (single batch): 2.2802422046661377\n",
      "##################################\n",
      "## EPOCH 256/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3298044204711914\n",
      "\t Training loss (single batch): 2.2729830741882324\n",
      "\t Training loss (single batch): 2.0007808208465576\n",
      "##################################\n",
      "## EPOCH 257/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4747567176818848\n",
      "\t Training loss (single batch): 2.2017881870269775\n",
      "\t Training loss (single batch): 1.9829480648040771\n",
      "##################################\n",
      "## EPOCH 258/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.882493495941162\n",
      "\t Training loss (single batch): 2.2930853366851807\n",
      "\t Training loss (single batch): 2.4049909114837646\n",
      "##################################\n",
      "## EPOCH 259/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.045234441757202\n",
      "\t Training loss (single batch): 2.4571173191070557\n",
      "\t Training loss (single batch): 1.9661846160888672\n",
      "##################################\n",
      "## EPOCH 260/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.65305757522583\n",
      "\t Training loss (single batch): 2.0422203540802\n",
      "\t Training loss (single batch): 3.0089006423950195\n",
      "##################################\n",
      "## EPOCH 261/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8110827207565308\n",
      "\t Training loss (single batch): 2.3118226528167725\n",
      "\t Training loss (single batch): 2.544647216796875\n",
      "##################################\n",
      "## EPOCH 262/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0325582027435303\n",
      "\t Training loss (single batch): 2.0912225246429443\n",
      "\t Training loss (single batch): 2.0370588302612305\n",
      "##################################\n",
      "## EPOCH 263/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.311880588531494\n",
      "\t Training loss (single batch): 2.145258903503418\n",
      "\t Training loss (single batch): 2.8209140300750732\n",
      "##################################\n",
      "## EPOCH 264/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2628016471862793\n",
      "\t Training loss (single batch): 2.220348358154297\n",
      "\t Training loss (single batch): 2.6678502559661865\n",
      "##################################\n",
      "## EPOCH 265/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.146414279937744\n",
      "\t Training loss (single batch): 1.9746936559677124\n",
      "\t Training loss (single batch): 2.124485731124878\n",
      "##################################\n",
      "## EPOCH 266/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.916651964187622\n",
      "\t Training loss (single batch): 3.010000228881836\n",
      "\t Training loss (single batch): 2.919764995574951\n",
      "##################################\n",
      "## EPOCH 267/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7455637454986572\n",
      "\t Training loss (single batch): 2.4117228984832764\n",
      "\t Training loss (single batch): 2.225311756134033\n",
      "##################################\n",
      "## EPOCH 268/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3706321716308594\n",
      "\t Training loss (single batch): 2.185241222381592\n",
      "\t Training loss (single batch): 2.870640754699707\n",
      "##################################\n",
      "## EPOCH 269/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.6573667526245117\n",
      "\t Training loss (single batch): 2.465383768081665\n",
      "\t Training loss (single batch): 2.177036762237549\n",
      "##################################\n",
      "## EPOCH 270/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8856738805770874\n",
      "\t Training loss (single batch): 2.129021167755127\n",
      "\t Training loss (single batch): 2.342653512954712\n",
      "##################################\n",
      "## EPOCH 271/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8712466955184937\n",
      "\t Training loss (single batch): 2.3838605880737305\n",
      "\t Training loss (single batch): 2.1937668323516846\n",
      "##################################\n",
      "## EPOCH 272/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.481595993041992\n",
      "\t Training loss (single batch): 1.913106918334961\n",
      "\t Training loss (single batch): 1.991128921508789\n",
      "##################################\n",
      "## EPOCH 273/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.283125400543213\n",
      "\t Training loss (single batch): 2.326451539993286\n",
      "\t Training loss (single batch): 2.1101553440093994\n",
      "##################################\n",
      "## EPOCH 274/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.919149398803711\n",
      "\t Training loss (single batch): 2.837433338165283\n",
      "\t Training loss (single batch): 3.0193941593170166\n",
      "##################################\n",
      "## EPOCH 275/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1064987182617188\n",
      "\t Training loss (single batch): 2.56923246383667\n",
      "\t Training loss (single batch): 2.5444321632385254\n",
      "##################################\n",
      "## EPOCH 276/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.602651834487915\n",
      "\t Training loss (single batch): 2.2386560440063477\n",
      "\t Training loss (single batch): 2.6805005073547363\n",
      "##################################\n",
      "## EPOCH 277/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.616852045059204\n",
      "\t Training loss (single batch): 2.2017087936401367\n",
      "\t Training loss (single batch): 2.5958797931671143\n",
      "##################################\n",
      "## EPOCH 278/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2779481410980225\n",
      "\t Training loss (single batch): 2.3524203300476074\n",
      "\t Training loss (single batch): 2.637117862701416\n",
      "##################################\n",
      "## EPOCH 279/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.581151247024536\n",
      "\t Training loss (single batch): 2.4396469593048096\n",
      "\t Training loss (single batch): 2.2479026317596436\n",
      "##################################\n",
      "## EPOCH 280/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5203378200531006\n",
      "\t Training loss (single batch): 2.5381672382354736\n",
      "\t Training loss (single batch): 1.9035823345184326\n",
      "##################################\n",
      "## EPOCH 281/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3256568908691406\n",
      "\t Training loss (single batch): 2.467607259750366\n",
      "\t Training loss (single batch): 2.569655656814575\n",
      "##################################\n",
      "## EPOCH 282/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7539539337158203\n",
      "\t Training loss (single batch): 2.57051420211792\n",
      "\t Training loss (single batch): 1.9346110820770264\n",
      "##################################\n",
      "## EPOCH 283/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2508153915405273\n",
      "\t Training loss (single batch): 1.9847429990768433\n",
      "\t Training loss (single batch): 2.561532497406006\n",
      "##################################\n",
      "## EPOCH 284/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9615001678466797\n",
      "\t Training loss (single batch): 2.074335813522339\n",
      "\t Training loss (single batch): 2.2782905101776123\n",
      "##################################\n",
      "## EPOCH 285/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7682814598083496\n",
      "\t Training loss (single batch): 2.2046377658843994\n",
      "\t Training loss (single batch): 1.9357386827468872\n",
      "##################################\n",
      "## EPOCH 286/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.90761137008667\n",
      "\t Training loss (single batch): 2.827054023742676\n",
      "\t Training loss (single batch): 2.35086727142334\n",
      "##################################\n",
      "## EPOCH 287/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.623286247253418\n",
      "\t Training loss (single batch): 2.1297152042388916\n",
      "\t Training loss (single batch): 2.9873359203338623\n",
      "##################################\n",
      "## EPOCH 288/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.901818037033081\n",
      "\t Training loss (single batch): 2.0428855419158936\n",
      "\t Training loss (single batch): 2.098332405090332\n",
      "##################################\n",
      "## EPOCH 289/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9353770017623901\n",
      "\t Training loss (single batch): 2.583608627319336\n",
      "\t Training loss (single batch): 2.4781394004821777\n",
      "##################################\n",
      "## EPOCH 290/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.218571662902832\n",
      "\t Training loss (single batch): 2.546041965484619\n",
      "\t Training loss (single batch): 2.927825450897217\n",
      "##################################\n",
      "## EPOCH 291/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.022444486618042\n",
      "\t Training loss (single batch): 2.825073719024658\n",
      "\t Training loss (single batch): 1.4612401723861694\n",
      "##################################\n",
      "## EPOCH 292/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2354257106781006\n",
      "\t Training loss (single batch): 2.00642728805542\n",
      "\t Training loss (single batch): 2.3595645427703857\n",
      "##################################\n",
      "## EPOCH 293/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.575930118560791\n",
      "\t Training loss (single batch): 2.263043165206909\n",
      "\t Training loss (single batch): 2.500919818878174\n",
      "##################################\n",
      "## EPOCH 294/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.040062427520752\n",
      "\t Training loss (single batch): 2.2480459213256836\n",
      "\t Training loss (single batch): 2.451686143875122\n",
      "##################################\n",
      "## EPOCH 295/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.072505235671997\n",
      "\t Training loss (single batch): 2.193303108215332\n",
      "\t Training loss (single batch): 2.682382106781006\n",
      "##################################\n",
      "## EPOCH 296/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.577131986618042\n",
      "\t Training loss (single batch): 1.9873408079147339\n",
      "\t Training loss (single batch): 2.3219926357269287\n",
      "##################################\n",
      "## EPOCH 297/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.101963758468628\n",
      "\t Training loss (single batch): 2.3761556148529053\n",
      "\t Training loss (single batch): 2.4122812747955322\n",
      "##################################\n",
      "## EPOCH 298/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.046842098236084\n",
      "\t Training loss (single batch): 2.4644150733947754\n",
      "\t Training loss (single batch): 2.6264359951019287\n",
      "##################################\n",
      "## EPOCH 299/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7542829513549805\n",
      "\t Training loss (single batch): 2.008277177810669\n",
      "\t Training loss (single batch): 2.1319093704223633\n",
      "##################################\n",
      "## EPOCH 300/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3393282890319824\n",
      "\t Training loss (single batch): 2.4469504356384277\n",
      "\t Training loss (single batch): 2.27642560005188\n",
      "##################################\n",
      "## EPOCH 301/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9894121885299683\n",
      "\t Training loss (single batch): 2.354369640350342\n",
      "\t Training loss (single batch): 2.330838203430176\n",
      "##################################\n",
      "## EPOCH 302/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1538338661193848\n",
      "\t Training loss (single batch): 2.2912511825561523\n",
      "\t Training loss (single batch): 2.1296586990356445\n",
      "##################################\n",
      "## EPOCH 303/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.361071825027466\n",
      "\t Training loss (single batch): 2.4757184982299805\n",
      "\t Training loss (single batch): 1.9709398746490479\n",
      "##################################\n",
      "## EPOCH 304/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5699217319488525\n",
      "\t Training loss (single batch): 2.3708789348602295\n",
      "\t Training loss (single batch): 2.016113042831421\n",
      "##################################\n",
      "## EPOCH 305/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.710839033126831\n",
      "\t Training loss (single batch): 2.635714054107666\n",
      "\t Training loss (single batch): 2.4371790885925293\n",
      "##################################\n",
      "## EPOCH 306/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9637551307678223\n",
      "\t Training loss (single batch): 2.131441354751587\n",
      "\t Training loss (single batch): 2.614748954772949\n",
      "##################################\n",
      "## EPOCH 307/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1259443759918213\n",
      "\t Training loss (single batch): 2.6178224086761475\n",
      "\t Training loss (single batch): 2.1903984546661377\n",
      "##################################\n",
      "## EPOCH 308/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.667804479598999\n",
      "\t Training loss (single batch): 2.1439664363861084\n",
      "\t Training loss (single batch): 2.645228624343872\n",
      "##################################\n",
      "## EPOCH 309/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.326781749725342\n",
      "\t Training loss (single batch): 1.7449359893798828\n",
      "\t Training loss (single batch): 1.724002480506897\n",
      "##################################\n",
      "## EPOCH 310/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2559595108032227\n",
      "\t Training loss (single batch): 2.579597234725952\n",
      "\t Training loss (single batch): 2.713855028152466\n",
      "##################################\n",
      "## EPOCH 311/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7912429571151733\n",
      "\t Training loss (single batch): 2.3209338188171387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.4595484733581543\n",
      "##################################\n",
      "## EPOCH 312/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.508553981781006\n",
      "\t Training loss (single batch): 1.5392355918884277\n",
      "\t Training loss (single batch): 1.6510238647460938\n",
      "##################################\n",
      "## EPOCH 313/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7977155447006226\n",
      "\t Training loss (single batch): 2.1308670043945312\n",
      "\t Training loss (single batch): 2.580653429031372\n",
      "##################################\n",
      "## EPOCH 314/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0452864170074463\n",
      "\t Training loss (single batch): 1.7725178003311157\n",
      "\t Training loss (single batch): 2.1767418384552\n",
      "##################################\n",
      "## EPOCH 315/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1715009212493896\n",
      "\t Training loss (single batch): 2.6241941452026367\n",
      "\t Training loss (single batch): 1.9384509325027466\n",
      "##################################\n",
      "## EPOCH 316/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.318004846572876\n",
      "\t Training loss (single batch): 2.5582661628723145\n",
      "\t Training loss (single batch): 2.0584542751312256\n",
      "##################################\n",
      "## EPOCH 317/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5227506160736084\n",
      "\t Training loss (single batch): 1.847665548324585\n",
      "\t Training loss (single batch): 3.2538373470306396\n",
      "##################################\n",
      "## EPOCH 318/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9809725284576416\n",
      "\t Training loss (single batch): 1.7674099206924438\n",
      "\t Training loss (single batch): 2.4584784507751465\n",
      "##################################\n",
      "## EPOCH 319/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2435083389282227\n",
      "\t Training loss (single batch): 2.123328447341919\n",
      "\t Training loss (single batch): 2.6537210941314697\n",
      "##################################\n",
      "## EPOCH 320/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4192402362823486\n",
      "\t Training loss (single batch): 2.2936344146728516\n",
      "\t Training loss (single batch): 2.298600196838379\n",
      "##################################\n",
      "## EPOCH 321/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.078399896621704\n",
      "\t Training loss (single batch): 2.192800760269165\n",
      "\t Training loss (single batch): 1.7804549932479858\n",
      "##################################\n",
      "## EPOCH 322/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2199740409851074\n",
      "\t Training loss (single batch): 2.3324522972106934\n",
      "\t Training loss (single batch): 1.7574255466461182\n",
      "##################################\n",
      "## EPOCH 323/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.205855131149292\n",
      "\t Training loss (single batch): 2.1620471477508545\n",
      "\t Training loss (single batch): 1.7585606575012207\n",
      "##################################\n",
      "## EPOCH 324/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9991692304611206\n",
      "\t Training loss (single batch): 1.9657806158065796\n",
      "\t Training loss (single batch): 2.637566089630127\n",
      "##################################\n",
      "## EPOCH 325/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7727893590927124\n",
      "\t Training loss (single batch): 2.091275930404663\n",
      "\t Training loss (single batch): 3.300950288772583\n",
      "##################################\n",
      "## EPOCH 326/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7044258117675781\n",
      "\t Training loss (single batch): 2.4296841621398926\n",
      "\t Training loss (single batch): 2.068117141723633\n",
      "##################################\n",
      "## EPOCH 327/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.039475917816162\n",
      "\t Training loss (single batch): 2.316471576690674\n",
      "\t Training loss (single batch): 2.7129554748535156\n",
      "##################################\n",
      "## EPOCH 328/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.293388843536377\n",
      "\t Training loss (single batch): 2.412449598312378\n",
      "\t Training loss (single batch): 2.597903251647949\n",
      "##################################\n",
      "## EPOCH 329/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2660889625549316\n",
      "\t Training loss (single batch): 1.9554049968719482\n",
      "\t Training loss (single batch): 2.0533432960510254\n",
      "##################################\n",
      "## EPOCH 330/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7996588945388794\n",
      "\t Training loss (single batch): 2.0339739322662354\n",
      "\t Training loss (single batch): 1.8072580099105835\n",
      "##################################\n",
      "## EPOCH 331/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0603859424591064\n",
      "\t Training loss (single batch): 2.137484312057495\n",
      "\t Training loss (single batch): 2.5732030868530273\n",
      "##################################\n",
      "## EPOCH 332/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1209235191345215\n",
      "\t Training loss (single batch): 1.5818617343902588\n",
      "\t Training loss (single batch): 2.7246389389038086\n",
      "##################################\n",
      "## EPOCH 333/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4017038345336914\n",
      "\t Training loss (single batch): 2.491914987564087\n",
      "\t Training loss (single batch): 2.806993246078491\n",
      "##################################\n",
      "## EPOCH 334/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.072312355041504\n",
      "\t Training loss (single batch): 2.3973441123962402\n",
      "\t Training loss (single batch): 2.7353031635284424\n",
      "##################################\n",
      "## EPOCH 335/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3517706394195557\n",
      "\t Training loss (single batch): 2.8434324264526367\n",
      "\t Training loss (single batch): 2.6632120609283447\n",
      "##################################\n",
      "## EPOCH 336/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0427534580230713\n",
      "\t Training loss (single batch): 2.555492401123047\n",
      "\t Training loss (single batch): 2.6351473331451416\n",
      "##################################\n",
      "## EPOCH 337/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.464238166809082\n",
      "\t Training loss (single batch): 2.048807144165039\n",
      "\t Training loss (single batch): 2.261552572250366\n",
      "##################################\n",
      "## EPOCH 338/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9160553216934204\n",
      "\t Training loss (single batch): 1.969785451889038\n",
      "\t Training loss (single batch): 2.2789642810821533\n",
      "##################################\n",
      "## EPOCH 339/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7590670585632324\n",
      "\t Training loss (single batch): 2.0196309089660645\n",
      "\t Training loss (single batch): 1.7097383737564087\n",
      "##################################\n",
      "## EPOCH 340/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.360607862472534\n",
      "\t Training loss (single batch): 1.8367592096328735\n",
      "\t Training loss (single batch): 2.0437467098236084\n",
      "##################################\n",
      "## EPOCH 341/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.22845458984375\n",
      "\t Training loss (single batch): 2.319258451461792\n",
      "\t Training loss (single batch): 2.5116078853607178\n",
      "##################################\n",
      "## EPOCH 342/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3374929428100586\n",
      "\t Training loss (single batch): 2.081681489944458\n",
      "\t Training loss (single batch): 2.981987476348877\n",
      "##################################\n",
      "## EPOCH 343/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2016026973724365\n",
      "\t Training loss (single batch): 2.47991681098938\n",
      "\t Training loss (single batch): 1.604679822921753\n",
      "##################################\n",
      "## EPOCH 344/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2148916721343994\n",
      "\t Training loss (single batch): 2.3234200477600098\n",
      "\t Training loss (single batch): 2.6062545776367188\n",
      "##################################\n",
      "## EPOCH 345/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8683855533599854\n",
      "\t Training loss (single batch): 2.079637289047241\n",
      "\t Training loss (single batch): 2.2710728645324707\n",
      "##################################\n",
      "## EPOCH 346/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.9881941080093384\n",
      "\t Training loss (single batch): 2.4323983192443848\n",
      "\t Training loss (single batch): 2.4277305603027344\n",
      "##################################\n",
      "## EPOCH 347/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9767149686813354\n",
      "\t Training loss (single batch): 2.289111852645874\n",
      "\t Training loss (single batch): 1.9840242862701416\n",
      "##################################\n",
      "## EPOCH 348/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.006053924560547\n",
      "\t Training loss (single batch): 2.8431553840637207\n",
      "\t Training loss (single batch): 2.51130747795105\n",
      "##################################\n",
      "## EPOCH 349/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7009791135787964\n",
      "\t Training loss (single batch): 1.8314749002456665\n",
      "\t Training loss (single batch): 1.9156056642532349\n",
      "##################################\n",
      "## EPOCH 350/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.630981922149658\n",
      "\t Training loss (single batch): 2.3041696548461914\n",
      "\t Training loss (single batch): 1.6179176568984985\n",
      "##################################\n",
      "## EPOCH 351/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2741599082946777\n",
      "\t Training loss (single batch): 1.668455958366394\n",
      "\t Training loss (single batch): 2.2559328079223633\n",
      "##################################\n",
      "## EPOCH 352/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8672542572021484\n",
      "\t Training loss (single batch): 1.9704644680023193\n",
      "\t Training loss (single batch): 2.3120808601379395\n",
      "##################################\n",
      "## EPOCH 353/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7802610397338867\n",
      "\t Training loss (single batch): 2.4360177516937256\n",
      "\t Training loss (single batch): 1.9285200834274292\n",
      "##################################\n",
      "## EPOCH 354/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.203235626220703\n",
      "\t Training loss (single batch): 2.0054643154144287\n",
      "\t Training loss (single batch): 2.052302598953247\n",
      "##################################\n",
      "## EPOCH 355/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.029212236404419\n",
      "\t Training loss (single batch): 1.8058440685272217\n",
      "\t Training loss (single batch): 1.5925159454345703\n",
      "##################################\n",
      "## EPOCH 356/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.281003713607788\n",
      "\t Training loss (single batch): 2.382859945297241\n",
      "\t Training loss (single batch): 2.280317783355713\n",
      "##################################\n",
      "## EPOCH 357/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.091888189315796\n",
      "\t Training loss (single batch): 2.1764347553253174\n",
      "\t Training loss (single batch): 1.7897658348083496\n",
      "##################################\n",
      "## EPOCH 358/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8928908109664917\n",
      "\t Training loss (single batch): 2.192702054977417\n",
      "\t Training loss (single batch): 2.1995277404785156\n",
      "##################################\n",
      "## EPOCH 359/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.092207431793213\n",
      "\t Training loss (single batch): 2.662972927093506\n",
      "\t Training loss (single batch): 2.010796546936035\n",
      "##################################\n",
      "## EPOCH 360/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.13822078704834\n",
      "\t Training loss (single batch): 2.070582866668701\n",
      "\t Training loss (single batch): 2.040926933288574\n",
      "##################################\n",
      "## EPOCH 361/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2559409141540527\n",
      "\t Training loss (single batch): 2.3175578117370605\n",
      "\t Training loss (single batch): 1.8913480043411255\n",
      "##################################\n",
      "## EPOCH 362/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.285034656524658\n",
      "\t Training loss (single batch): 2.560710906982422\n",
      "\t Training loss (single batch): 2.0641489028930664\n",
      "##################################\n",
      "## EPOCH 363/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2488489151000977\n",
      "\t Training loss (single batch): 2.228743076324463\n",
      "\t Training loss (single batch): 2.4969608783721924\n",
      "##################################\n",
      "## EPOCH 364/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9822700023651123\n",
      "\t Training loss (single batch): 1.6012563705444336\n",
      "\t Training loss (single batch): 2.760244846343994\n",
      "##################################\n",
      "## EPOCH 365/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1171929836273193\n",
      "\t Training loss (single batch): 2.072754383087158\n",
      "\t Training loss (single batch): 1.956541657447815\n",
      "##################################\n",
      "## EPOCH 366/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.430422306060791\n",
      "\t Training loss (single batch): 2.0215811729431152\n",
      "\t Training loss (single batch): 1.7240047454833984\n",
      "##################################\n",
      "## EPOCH 367/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.135599136352539\n",
      "\t Training loss (single batch): 2.234344244003296\n",
      "\t Training loss (single batch): 2.1004979610443115\n",
      "##################################\n",
      "## EPOCH 368/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.150146484375\n",
      "\t Training loss (single batch): 1.938416600227356\n",
      "\t Training loss (single batch): 2.130251169204712\n",
      "##################################\n",
      "## EPOCH 369/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.400665283203125\n",
      "\t Training loss (single batch): 2.4047346115112305\n",
      "\t Training loss (single batch): 2.18591570854187\n",
      "##################################\n",
      "## EPOCH 370/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1897308826446533\n",
      "\t Training loss (single batch): 2.473153591156006\n",
      "\t Training loss (single batch): 2.54259991645813\n",
      "##################################\n",
      "## EPOCH 371/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.319377899169922\n",
      "\t Training loss (single batch): 2.4921908378601074\n",
      "\t Training loss (single batch): 2.1517174243927\n",
      "##################################\n",
      "## EPOCH 372/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7024379968643188\n",
      "\t Training loss (single batch): 1.968485713005066\n",
      "\t Training loss (single batch): 2.239746570587158\n",
      "##################################\n",
      "## EPOCH 373/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9306015968322754\n",
      "\t Training loss (single batch): 2.146871328353882\n",
      "\t Training loss (single batch): 2.762770891189575\n",
      "##################################\n",
      "## EPOCH 374/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.650601387023926\n",
      "\t Training loss (single batch): 2.0853919982910156\n",
      "\t Training loss (single batch): 2.083667278289795\n",
      "##################################\n",
      "## EPOCH 375/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.756799578666687\n",
      "\t Training loss (single batch): 2.2599949836730957\n",
      "\t Training loss (single batch): 2.3171544075012207\n",
      "##################################\n",
      "## EPOCH 376/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.141789197921753\n",
      "\t Training loss (single batch): 2.2864317893981934\n",
      "\t Training loss (single batch): 1.752387523651123\n",
      "##################################\n",
      "## EPOCH 377/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.007847547531128\n",
      "\t Training loss (single batch): 2.2649662494659424\n",
      "\t Training loss (single batch): 2.334184169769287\n",
      "##################################\n",
      "## EPOCH 378/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.099290609359741\n",
      "\t Training loss (single batch): 2.153351306915283\n",
      "\t Training loss (single batch): 1.776049256324768\n",
      "##################################\n",
      "## EPOCH 379/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1636836528778076\n",
      "\t Training loss (single batch): 2.0606777667999268\n",
      "\t Training loss (single batch): 1.9447948932647705\n",
      "##################################\n",
      "## EPOCH 380/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2348554134368896\n",
      "\t Training loss (single batch): 2.0216097831726074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.2952444553375244\n",
      "##################################\n",
      "## EPOCH 381/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4617867469787598\n",
      "\t Training loss (single batch): 1.804329752922058\n",
      "\t Training loss (single batch): 2.237452507019043\n",
      "##################################\n",
      "## EPOCH 382/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6253979206085205\n",
      "\t Training loss (single batch): 2.028017044067383\n",
      "\t Training loss (single batch): 2.5893454551696777\n",
      "##################################\n",
      "## EPOCH 383/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.125196933746338\n",
      "\t Training loss (single batch): 2.0559980869293213\n",
      "\t Training loss (single batch): 2.129016160964966\n",
      "##################################\n",
      "## EPOCH 384/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7768805027008057\n",
      "\t Training loss (single batch): 2.35585618019104\n",
      "\t Training loss (single batch): 1.5228445529937744\n",
      "##################################\n",
      "## EPOCH 385/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2913596630096436\n",
      "\t Training loss (single batch): 2.008288621902466\n",
      "\t Training loss (single batch): 1.9242609739303589\n",
      "##################################\n",
      "## EPOCH 386/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.568760871887207\n",
      "\t Training loss (single batch): 2.0454957485198975\n",
      "\t Training loss (single batch): 1.8055062294006348\n",
      "##################################\n",
      "## EPOCH 387/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5812158584594727\n",
      "\t Training loss (single batch): 2.048668622970581\n",
      "\t Training loss (single batch): 2.2480218410491943\n",
      "##################################\n",
      "## EPOCH 388/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0355987548828125\n",
      "\t Training loss (single batch): 2.2190959453582764\n",
      "\t Training loss (single batch): 1.7390884160995483\n",
      "##################################\n",
      "## EPOCH 389/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1187989711761475\n",
      "\t Training loss (single batch): 2.047125816345215\n",
      "\t Training loss (single batch): 2.108297109603882\n",
      "##################################\n",
      "## EPOCH 390/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1069047451019287\n",
      "\t Training loss (single batch): 1.6268844604492188\n",
      "\t Training loss (single batch): 2.215073585510254\n",
      "##################################\n",
      "## EPOCH 391/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7601500749588013\n",
      "\t Training loss (single batch): 1.8908116817474365\n",
      "\t Training loss (single batch): 2.0029170513153076\n",
      "##################################\n",
      "## EPOCH 392/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8487920761108398\n",
      "\t Training loss (single batch): 1.9155216217041016\n",
      "\t Training loss (single batch): 3.014819860458374\n",
      "##################################\n",
      "## EPOCH 393/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 3.1083905696868896\n",
      "\t Training loss (single batch): 1.9022736549377441\n",
      "\t Training loss (single batch): 2.1514248847961426\n",
      "##################################\n",
      "## EPOCH 394/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.343660354614258\n",
      "\t Training loss (single batch): 1.8753652572631836\n",
      "\t Training loss (single batch): 2.2806665897369385\n",
      "##################################\n",
      "## EPOCH 395/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2085330486297607\n",
      "\t Training loss (single batch): 1.6185333728790283\n",
      "\t Training loss (single batch): 2.521275043487549\n",
      "##################################\n",
      "## EPOCH 396/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0000157356262207\n",
      "\t Training loss (single batch): 1.6186082363128662\n",
      "\t Training loss (single batch): 2.1532065868377686\n",
      "##################################\n",
      "## EPOCH 397/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3572072982788086\n",
      "\t Training loss (single batch): 2.027575731277466\n",
      "\t Training loss (single batch): 1.6845207214355469\n",
      "##################################\n",
      "## EPOCH 398/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8575412034988403\n",
      "\t Training loss (single batch): 2.4889469146728516\n",
      "\t Training loss (single batch): 2.4833271503448486\n",
      "##################################\n",
      "## EPOCH 399/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.544797897338867\n",
      "\t Training loss (single batch): 2.560622453689575\n",
      "\t Training loss (single batch): 2.2589008808135986\n",
      "##################################\n",
      "## EPOCH 400/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.041748523712158\n",
      "\t Training loss (single batch): 2.0462355613708496\n",
      "\t Training loss (single batch): 1.9386659860610962\n",
      "##################################\n",
      "## EPOCH 401/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.056452751159668\n",
      "\t Training loss (single batch): 2.634021520614624\n",
      "\t Training loss (single batch): 2.7456014156341553\n",
      "##################################\n",
      "## EPOCH 402/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0838329792022705\n",
      "\t Training loss (single batch): 1.92594313621521\n",
      "\t Training loss (single batch): 1.7851917743682861\n",
      "##################################\n",
      "## EPOCH 403/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0473880767822266\n",
      "\t Training loss (single batch): 2.2485530376434326\n",
      "\t Training loss (single batch): 2.384068250656128\n",
      "##################################\n",
      "## EPOCH 404/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.8564631938934326\n",
      "\t Training loss (single batch): 2.2076926231384277\n",
      "\t Training loss (single batch): 1.8829662799835205\n",
      "##################################\n",
      "## EPOCH 405/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4252164363861084\n",
      "\t Training loss (single batch): 1.5870691537857056\n",
      "\t Training loss (single batch): 1.8918753862380981\n",
      "##################################\n",
      "## EPOCH 406/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8777707815170288\n",
      "\t Training loss (single batch): 2.1260883808135986\n",
      "\t Training loss (single batch): 2.0995535850524902\n",
      "##################################\n",
      "## EPOCH 407/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3812544345855713\n",
      "\t Training loss (single batch): 1.9380089044570923\n",
      "\t Training loss (single batch): 2.4128010272979736\n",
      "##################################\n",
      "## EPOCH 408/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.083453893661499\n",
      "\t Training loss (single batch): 1.80059015750885\n",
      "\t Training loss (single batch): 2.3555705547332764\n",
      "##################################\n",
      "## EPOCH 409/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.914462685585022\n",
      "\t Training loss (single batch): 2.2467074394226074\n",
      "\t Training loss (single batch): 2.2792210578918457\n",
      "##################################\n",
      "## EPOCH 410/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.255249500274658\n",
      "\t Training loss (single batch): 2.2163305282592773\n",
      "\t Training loss (single batch): 1.895062804222107\n",
      "##################################\n",
      "## EPOCH 411/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.16587233543396\n",
      "\t Training loss (single batch): 1.8765501976013184\n",
      "\t Training loss (single batch): 1.9038138389587402\n",
      "##################################\n",
      "## EPOCH 412/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3750157356262207\n",
      "\t Training loss (single batch): 2.638552665710449\n",
      "\t Training loss (single batch): 1.332205891609192\n",
      "##################################\n",
      "## EPOCH 413/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7906485795974731\n",
      "\t Training loss (single batch): 2.114068031311035\n",
      "\t Training loss (single batch): 2.79744553565979\n",
      "##################################\n",
      "## EPOCH 414/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.99779212474823\n",
      "\t Training loss (single batch): 1.889908790588379\n",
      "\t Training loss (single batch): 2.484701633453369\n",
      "##################################\n",
      "## EPOCH 415/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.074171781539917\n",
      "\t Training loss (single batch): 2.2360165119171143\n",
      "\t Training loss (single batch): 2.665482997894287\n",
      "##################################\n",
      "## EPOCH 416/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.146357536315918\n",
      "\t Training loss (single batch): 1.991604208946228\n",
      "\t Training loss (single batch): 2.8221845626831055\n",
      "##################################\n",
      "## EPOCH 417/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2346079349517822\n",
      "\t Training loss (single batch): 1.9970006942749023\n",
      "\t Training loss (single batch): 2.1688249111175537\n",
      "##################################\n",
      "## EPOCH 418/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1576452255249023\n",
      "\t Training loss (single batch): 1.7778499126434326\n",
      "\t Training loss (single batch): 2.126979351043701\n",
      "##################################\n",
      "## EPOCH 419/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7492997646331787\n",
      "\t Training loss (single batch): 2.350830554962158\n",
      "\t Training loss (single batch): 1.8611226081848145\n",
      "##################################\n",
      "## EPOCH 420/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.315796136856079\n",
      "\t Training loss (single batch): 2.4136693477630615\n",
      "\t Training loss (single batch): 1.4767990112304688\n",
      "##################################\n",
      "## EPOCH 421/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1311404705047607\n",
      "\t Training loss (single batch): 1.8871301412582397\n",
      "\t Training loss (single batch): 1.4878485202789307\n",
      "##################################\n",
      "## EPOCH 422/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.454200029373169\n",
      "\t Training loss (single batch): 2.2978129386901855\n",
      "\t Training loss (single batch): 1.9069750308990479\n",
      "##################################\n",
      "## EPOCH 423/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8962857723236084\n",
      "\t Training loss (single batch): 2.434077739715576\n",
      "\t Training loss (single batch): 2.0811991691589355\n",
      "##################################\n",
      "## EPOCH 424/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.7879467010498047\n",
      "\t Training loss (single batch): 1.997549295425415\n",
      "\t Training loss (single batch): 2.0248615741729736\n",
      "##################################\n",
      "## EPOCH 425/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.268730878829956\n",
      "\t Training loss (single batch): 3.0961313247680664\n",
      "\t Training loss (single batch): 2.040668249130249\n",
      "##################################\n",
      "## EPOCH 426/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.155729055404663\n",
      "\t Training loss (single batch): 2.2065012454986572\n",
      "\t Training loss (single batch): 3.0970823764801025\n",
      "##################################\n",
      "## EPOCH 427/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.938444972038269\n",
      "\t Training loss (single batch): 1.6556214094161987\n",
      "\t Training loss (single batch): 2.1377289295196533\n",
      "##################################\n",
      "## EPOCH 428/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.089092493057251\n",
      "\t Training loss (single batch): 2.233806848526001\n",
      "\t Training loss (single batch): 2.4828286170959473\n",
      "##################################\n",
      "## EPOCH 429/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.301093101501465\n",
      "\t Training loss (single batch): 2.0169012546539307\n",
      "\t Training loss (single batch): 2.269061326980591\n",
      "##################################\n",
      "## EPOCH 430/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3734726905822754\n",
      "\t Training loss (single batch): 1.7792998552322388\n",
      "\t Training loss (single batch): 2.231206178665161\n",
      "##################################\n",
      "## EPOCH 431/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.651049017906189\n",
      "\t Training loss (single batch): 2.288851499557495\n",
      "\t Training loss (single batch): 2.2246346473693848\n",
      "##################################\n",
      "## EPOCH 432/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.087263584136963\n",
      "\t Training loss (single batch): 2.120335102081299\n",
      "\t Training loss (single batch): 2.255038022994995\n",
      "##################################\n",
      "## EPOCH 433/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2123420238494873\n",
      "\t Training loss (single batch): 1.8682129383087158\n",
      "\t Training loss (single batch): 2.227100372314453\n",
      "##################################\n",
      "## EPOCH 434/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0782017707824707\n",
      "\t Training loss (single batch): 1.5717556476593018\n",
      "\t Training loss (single batch): 2.5621235370635986\n",
      "##################################\n",
      "## EPOCH 435/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.069153308868408\n",
      "\t Training loss (single batch): 2.0994138717651367\n",
      "\t Training loss (single batch): 2.364531993865967\n",
      "##################################\n",
      "## EPOCH 436/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4888741970062256\n",
      "\t Training loss (single batch): 1.6078213453292847\n",
      "\t Training loss (single batch): 2.044482946395874\n",
      "##################################\n",
      "## EPOCH 437/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6754306554794312\n",
      "\t Training loss (single batch): 1.7013065814971924\n",
      "\t Training loss (single batch): 2.115377426147461\n",
      "##################################\n",
      "## EPOCH 438/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4559686183929443\n",
      "\t Training loss (single batch): 1.8125016689300537\n",
      "\t Training loss (single batch): 2.3486855030059814\n",
      "##################################\n",
      "## EPOCH 439/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0116724967956543\n",
      "\t Training loss (single batch): 2.0614750385284424\n",
      "\t Training loss (single batch): 2.3180999755859375\n",
      "##################################\n",
      "## EPOCH 440/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1853222846984863\n",
      "\t Training loss (single batch): 1.9035131931304932\n",
      "\t Training loss (single batch): 1.6697769165039062\n",
      "##################################\n",
      "## EPOCH 441/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.533356189727783\n",
      "\t Training loss (single batch): 1.9341888427734375\n",
      "\t Training loss (single batch): 2.3814494609832764\n",
      "##################################\n",
      "## EPOCH 442/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7412053346633911\n",
      "\t Training loss (single batch): 1.9446964263916016\n",
      "\t Training loss (single batch): 2.006242513656616\n",
      "##################################\n",
      "## EPOCH 443/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8402937650680542\n",
      "\t Training loss (single batch): 2.3489623069763184\n",
      "\t Training loss (single batch): 1.7509286403656006\n",
      "##################################\n",
      "## EPOCH 444/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0150306224823\n",
      "\t Training loss (single batch): 2.292323350906372\n",
      "\t Training loss (single batch): 2.810519218444824\n",
      "##################################\n",
      "## EPOCH 445/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1994824409484863\n",
      "\t Training loss (single batch): 2.043635368347168\n",
      "\t Training loss (single batch): 2.2115371227264404\n",
      "##################################\n",
      "## EPOCH 446/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.264078140258789\n",
      "\t Training loss (single batch): 1.678215503692627\n",
      "\t Training loss (single batch): 2.182467460632324\n",
      "##################################\n",
      "## EPOCH 447/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.34930682182312\n",
      "\t Training loss (single batch): 2.005136728286743\n",
      "\t Training loss (single batch): 2.031661033630371\n",
      "##################################\n",
      "## EPOCH 448/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6641113758087158\n",
      "\t Training loss (single batch): 2.333512306213379\n",
      "\t Training loss (single batch): 1.799877405166626\n",
      "##################################\n",
      "## EPOCH 449/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.160447597503662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.8898413181304932\n",
      "\t Training loss (single batch): 1.9888930320739746\n",
      "##################################\n",
      "## EPOCH 450/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5924253463745117\n",
      "\t Training loss (single batch): 2.2083652019500732\n",
      "\t Training loss (single batch): 2.148249387741089\n",
      "##################################\n",
      "## EPOCH 451/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.268791913986206\n",
      "\t Training loss (single batch): 1.9365302324295044\n",
      "\t Training loss (single batch): 1.6172593832015991\n",
      "##################################\n",
      "## EPOCH 452/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2429983615875244\n",
      "\t Training loss (single batch): 2.1182243824005127\n",
      "\t Training loss (single batch): 1.9227298498153687\n",
      "##################################\n",
      "## EPOCH 453/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.842532992362976\n",
      "\t Training loss (single batch): 2.0072741508483887\n",
      "\t Training loss (single batch): 1.9917739629745483\n",
      "##################################\n",
      "## EPOCH 454/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.854231595993042\n",
      "\t Training loss (single batch): 1.9083590507507324\n",
      "\t Training loss (single batch): 2.1699013710021973\n",
      "##################################\n",
      "## EPOCH 455/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.757269024848938\n",
      "\t Training loss (single batch): 1.8990296125411987\n",
      "\t Training loss (single batch): 2.359325885772705\n",
      "##################################\n",
      "## EPOCH 456/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.189957618713379\n",
      "\t Training loss (single batch): 1.8484152555465698\n",
      "\t Training loss (single batch): 1.9723292589187622\n",
      "##################################\n",
      "## EPOCH 457/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1040544509887695\n",
      "\t Training loss (single batch): 1.8356019258499146\n",
      "\t Training loss (single batch): 1.9739854335784912\n",
      "##################################\n",
      "## EPOCH 458/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8940296173095703\n",
      "\t Training loss (single batch): 1.892691969871521\n",
      "\t Training loss (single batch): 2.255370855331421\n",
      "##################################\n",
      "## EPOCH 459/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8395564556121826\n",
      "\t Training loss (single batch): 1.9458894729614258\n",
      "\t Training loss (single batch): 2.396615505218506\n",
      "##################################\n",
      "## EPOCH 460/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7096301317214966\n",
      "\t Training loss (single batch): 1.8608372211456299\n",
      "\t Training loss (single batch): 1.8080025911331177\n",
      "##################################\n",
      "## EPOCH 461/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0358352661132812\n",
      "\t Training loss (single batch): 2.31877064704895\n",
      "\t Training loss (single batch): 2.166283130645752\n",
      "##################################\n",
      "## EPOCH 462/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.5380983352661133\n",
      "\t Training loss (single batch): 2.1069905757904053\n",
      "\t Training loss (single batch): 2.155320644378662\n",
      "##################################\n",
      "## EPOCH 463/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.64542818069458\n",
      "\t Training loss (single batch): 1.942836880683899\n",
      "\t Training loss (single batch): 1.6145329475402832\n",
      "##################################\n",
      "## EPOCH 464/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6584458351135254\n",
      "\t Training loss (single batch): 2.1522462368011475\n",
      "\t Training loss (single batch): 1.8661001920700073\n",
      "##################################\n",
      "## EPOCH 465/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3619987964630127\n",
      "\t Training loss (single batch): 2.229726791381836\n",
      "\t Training loss (single batch): 1.8417437076568604\n",
      "##################################\n",
      "## EPOCH 466/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.136244773864746\n",
      "\t Training loss (single batch): 1.8342485427856445\n",
      "\t Training loss (single batch): 2.001311779022217\n",
      "##################################\n",
      "## EPOCH 467/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.289938449859619\n",
      "\t Training loss (single batch): 2.040367603302002\n",
      "\t Training loss (single batch): 2.2682554721832275\n",
      "##################################\n",
      "## EPOCH 468/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5122129917144775\n",
      "\t Training loss (single batch): 2.1137027740478516\n",
      "\t Training loss (single batch): 1.8249189853668213\n",
      "##################################\n",
      "## EPOCH 469/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2245595455169678\n",
      "\t Training loss (single batch): 2.070904016494751\n",
      "\t Training loss (single batch): 2.3025431632995605\n",
      "##################################\n",
      "## EPOCH 470/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.94036865234375\n",
      "\t Training loss (single batch): 1.4706488847732544\n",
      "\t Training loss (single batch): 1.751706838607788\n",
      "##################################\n",
      "## EPOCH 471/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.174015998840332\n",
      "\t Training loss (single batch): 2.5551466941833496\n",
      "\t Training loss (single batch): 2.2955434322357178\n",
      "##################################\n",
      "## EPOCH 472/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.092886447906494\n",
      "\t Training loss (single batch): 2.154078245162964\n",
      "\t Training loss (single batch): 1.8320022821426392\n",
      "##################################\n",
      "## EPOCH 473/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.334260940551758\n",
      "\t Training loss (single batch): 2.04689621925354\n",
      "\t Training loss (single batch): 2.1672825813293457\n",
      "##################################\n",
      "## EPOCH 474/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9256582260131836\n",
      "\t Training loss (single batch): 2.0669705867767334\n",
      "\t Training loss (single batch): 1.9809621572494507\n",
      "##################################\n",
      "## EPOCH 475/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6206965446472168\n",
      "\t Training loss (single batch): 2.084456205368042\n",
      "\t Training loss (single batch): 2.1124534606933594\n",
      "##################################\n",
      "## EPOCH 476/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.02429461479187\n",
      "\t Training loss (single batch): 1.8993560075759888\n",
      "\t Training loss (single batch): 1.7546905279159546\n",
      "##################################\n",
      "## EPOCH 477/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0652716159820557\n",
      "\t Training loss (single batch): 1.401140809059143\n",
      "\t Training loss (single batch): 2.269972801208496\n",
      "##################################\n",
      "## EPOCH 478/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.223773241043091\n",
      "\t Training loss (single batch): 1.4018332958221436\n",
      "\t Training loss (single batch): 2.473520040512085\n",
      "##################################\n",
      "## EPOCH 479/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2467246055603027\n",
      "\t Training loss (single batch): 1.5039511919021606\n",
      "\t Training loss (single batch): 1.717747449874878\n",
      "##################################\n",
      "## EPOCH 480/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7575242519378662\n",
      "\t Training loss (single batch): 1.7082184553146362\n",
      "\t Training loss (single batch): 2.3633406162261963\n",
      "##################################\n",
      "## EPOCH 481/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.286545991897583\n",
      "\t Training loss (single batch): 2.0979654788970947\n",
      "\t Training loss (single batch): 1.4426913261413574\n",
      "##################################\n",
      "## EPOCH 482/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9287711381912231\n",
      "\t Training loss (single batch): 1.8047559261322021\n",
      "\t Training loss (single batch): 2.048429250717163\n",
      "##################################\n",
      "## EPOCH 483/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6576058864593506\n",
      "\t Training loss (single batch): 2.658954381942749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.8835257291793823\n",
      "##################################\n",
      "## EPOCH 484/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2468113899230957\n",
      "\t Training loss (single batch): 1.6478410959243774\n",
      "\t Training loss (single batch): 1.668186068534851\n",
      "##################################\n",
      "## EPOCH 485/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3441104888916016\n",
      "\t Training loss (single batch): 1.9800152778625488\n",
      "\t Training loss (single batch): 2.2100393772125244\n",
      "##################################\n",
      "## EPOCH 486/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.655538558959961\n",
      "\t Training loss (single batch): 2.6803736686706543\n",
      "\t Training loss (single batch): 2.412379264831543\n",
      "##################################\n",
      "## EPOCH 487/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1210403442382812\n",
      "\t Training loss (single batch): 1.8290669918060303\n",
      "\t Training loss (single batch): 2.097064256668091\n",
      "##################################\n",
      "## EPOCH 488/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7500600814819336\n",
      "\t Training loss (single batch): 1.508877158164978\n",
      "\t Training loss (single batch): 2.1930735111236572\n",
      "##################################\n",
      "## EPOCH 489/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8641268014907837\n",
      "\t Training loss (single batch): 1.9260458946228027\n",
      "\t Training loss (single batch): 2.6049344539642334\n",
      "##################################\n",
      "## EPOCH 490/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3489649295806885\n",
      "\t Training loss (single batch): 1.8417487144470215\n",
      "\t Training loss (single batch): 2.1160690784454346\n",
      "##################################\n",
      "## EPOCH 491/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6793018579483032\n",
      "\t Training loss (single batch): 1.6581543684005737\n",
      "\t Training loss (single batch): 1.7790069580078125\n",
      "##################################\n",
      "## EPOCH 492/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1809122562408447\n",
      "\t Training loss (single batch): 1.8507057428359985\n",
      "\t Training loss (single batch): 1.8273218870162964\n",
      "##################################\n",
      "## EPOCH 493/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0125479698181152\n",
      "\t Training loss (single batch): 1.891451358795166\n",
      "\t Training loss (single batch): 1.7579121589660645\n",
      "##################################\n",
      "## EPOCH 494/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6296463012695312\n",
      "\t Training loss (single batch): 2.2518699169158936\n",
      "\t Training loss (single batch): 1.5988680124282837\n",
      "##################################\n",
      "## EPOCH 495/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8943063020706177\n",
      "\t Training loss (single batch): 1.7438664436340332\n",
      "\t Training loss (single batch): 1.9519182443618774\n",
      "##################################\n",
      "## EPOCH 496/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.292545795440674\n",
      "\t Training loss (single batch): 1.7549933195114136\n",
      "\t Training loss (single batch): 2.0262646675109863\n",
      "##################################\n",
      "## EPOCH 497/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.952501893043518\n",
      "\t Training loss (single batch): 1.9043306112289429\n",
      "\t Training loss (single batch): 1.752954363822937\n",
      "##################################\n",
      "## EPOCH 498/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7746368646621704\n",
      "\t Training loss (single batch): 1.624700903892517\n",
      "\t Training loss (single batch): 2.3789565563201904\n",
      "##################################\n",
      "## EPOCH 499/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6726561784744263\n",
      "\t Training loss (single batch): 1.7875289916992188\n",
      "\t Training loss (single batch): 2.542940855026245\n",
      "##################################\n",
      "## EPOCH 500/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.864750862121582\n",
      "\t Training loss (single batch): 1.203137755393982\n",
      "\t Training loss (single batch): 1.7452857494354248\n",
      "##################################\n",
      "## EPOCH 501/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3360636234283447\n",
      "\t Training loss (single batch): 1.6416682004928589\n",
      "\t Training loss (single batch): 1.3085235357284546\n",
      "##################################\n",
      "## EPOCH 502/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.115919828414917\n",
      "\t Training loss (single batch): 2.185824155807495\n",
      "\t Training loss (single batch): 1.9248273372650146\n",
      "##################################\n",
      "## EPOCH 503/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.140909194946289\n",
      "\t Training loss (single batch): 2.255864381790161\n",
      "\t Training loss (single batch): 1.8291305303573608\n",
      "##################################\n",
      "## EPOCH 504/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9034698009490967\n",
      "\t Training loss (single batch): 1.836348056793213\n",
      "\t Training loss (single batch): 2.217127561569214\n",
      "##################################\n",
      "## EPOCH 505/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6861642599105835\n",
      "\t Training loss (single batch): 2.4564216136932373\n",
      "\t Training loss (single batch): 1.8558382987976074\n",
      "##################################\n",
      "## EPOCH 506/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9753153324127197\n",
      "\t Training loss (single batch): 1.275443434715271\n",
      "\t Training loss (single batch): 1.924804449081421\n",
      "##################################\n",
      "## EPOCH 507/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7173752784729004\n",
      "\t Training loss (single batch): 2.4040958881378174\n",
      "\t Training loss (single batch): 1.859336256980896\n",
      "##################################\n",
      "## EPOCH 508/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3703205585479736\n",
      "\t Training loss (single batch): 2.0632710456848145\n",
      "\t Training loss (single batch): 1.7614949941635132\n",
      "##################################\n",
      "## EPOCH 509/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7074854373931885\n",
      "\t Training loss (single batch): 1.7663148641586304\n",
      "\t Training loss (single batch): 1.5590702295303345\n",
      "##################################\n",
      "## EPOCH 510/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9139330387115479\n",
      "\t Training loss (single batch): 1.7733632326126099\n",
      "\t Training loss (single batch): 1.992673397064209\n",
      "##################################\n",
      "## EPOCH 511/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.492412805557251\n",
      "\t Training loss (single batch): 1.8060866594314575\n",
      "\t Training loss (single batch): 3.1974518299102783\n",
      "##################################\n",
      "## EPOCH 512/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9112927913665771\n",
      "\t Training loss (single batch): 1.6090201139450073\n",
      "\t Training loss (single batch): 2.084928035736084\n",
      "##################################\n",
      "## EPOCH 513/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.213529586791992\n",
      "\t Training loss (single batch): 2.2665083408355713\n",
      "\t Training loss (single batch): 1.437590479850769\n",
      "##################################\n",
      "## EPOCH 514/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.055929183959961\n",
      "\t Training loss (single batch): 1.5271865129470825\n",
      "\t Training loss (single batch): 1.86646568775177\n",
      "##################################\n",
      "## EPOCH 515/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2687816619873047\n",
      "\t Training loss (single batch): 2.19307804107666\n",
      "\t Training loss (single batch): 2.17228102684021\n",
      "##################################\n",
      "## EPOCH 516/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6756455898284912\n",
      "\t Training loss (single batch): 2.303144693374634\n",
      "\t Training loss (single batch): 2.0399911403656006\n",
      "##################################\n",
      "## EPOCH 517/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.022426128387451\n",
      "\t Training loss (single batch): 2.2077255249023438\n",
      "\t Training loss (single batch): 1.5595202445983887\n",
      "##################################\n",
      "## EPOCH 518/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.852480411529541\n",
      "\t Training loss (single batch): 2.0319714546203613\n",
      "\t Training loss (single batch): 2.3123672008514404\n",
      "##################################\n",
      "## EPOCH 519/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6527773141860962\n",
      "\t Training loss (single batch): 1.826942801475525\n",
      "\t Training loss (single batch): 2.3008921146392822\n",
      "##################################\n",
      "## EPOCH 520/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9422959089279175\n",
      "\t Training loss (single batch): 2.1303138732910156\n",
      "\t Training loss (single batch): 1.7743397951126099\n",
      "##################################\n",
      "## EPOCH 521/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9327870607376099\n",
      "\t Training loss (single batch): 2.1022655963897705\n",
      "\t Training loss (single batch): 2.204070806503296\n",
      "##################################\n",
      "## EPOCH 522/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9822684526443481\n",
      "\t Training loss (single batch): 2.2329461574554443\n",
      "\t Training loss (single batch): 2.2339046001434326\n",
      "##################################\n",
      "## EPOCH 523/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.422663450241089\n",
      "\t Training loss (single batch): 1.9984467029571533\n",
      "\t Training loss (single batch): 2.030097484588623\n",
      "##################################\n",
      "## EPOCH 524/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9604829549789429\n",
      "\t Training loss (single batch): 1.8395689725875854\n",
      "\t Training loss (single batch): 1.902012586593628\n",
      "##################################\n",
      "## EPOCH 525/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.336653709411621\n",
      "\t Training loss (single batch): 1.9055750370025635\n",
      "\t Training loss (single batch): 1.9395965337753296\n",
      "##################################\n",
      "## EPOCH 526/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2925283908843994\n",
      "\t Training loss (single batch): 1.974284052848816\n",
      "\t Training loss (single batch): 1.633028507232666\n",
      "##################################\n",
      "## EPOCH 527/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4315041303634644\n",
      "\t Training loss (single batch): 1.818813443183899\n",
      "\t Training loss (single batch): 1.456741452217102\n",
      "##################################\n",
      "## EPOCH 528/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0151772499084473\n",
      "\t Training loss (single batch): 2.1789839267730713\n",
      "\t Training loss (single batch): 1.35317862033844\n",
      "##################################\n",
      "## EPOCH 529/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3284974098205566\n",
      "\t Training loss (single batch): 1.8367997407913208\n",
      "\t Training loss (single batch): 2.498495578765869\n",
      "##################################\n",
      "## EPOCH 530/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.444472074508667\n",
      "\t Training loss (single batch): 1.354434609413147\n",
      "\t Training loss (single batch): 2.34255051612854\n",
      "##################################\n",
      "## EPOCH 531/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7483841180801392\n",
      "\t Training loss (single batch): 1.911965012550354\n",
      "\t Training loss (single batch): 2.0194973945617676\n",
      "##################################\n",
      "## EPOCH 532/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6624464988708496\n",
      "\t Training loss (single batch): 1.9675077199935913\n",
      "\t Training loss (single batch): 2.1387579441070557\n",
      "##################################\n",
      "## EPOCH 533/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7588865756988525\n",
      "\t Training loss (single batch): 2.3428616523742676\n",
      "\t Training loss (single batch): 1.9697481393814087\n",
      "##################################\n",
      "## EPOCH 534/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4643309116363525\n",
      "\t Training loss (single batch): 1.513079285621643\n",
      "\t Training loss (single batch): 2.0248990058898926\n",
      "##################################\n",
      "## EPOCH 535/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6837701797485352\n",
      "\t Training loss (single batch): 2.686537027359009\n",
      "\t Training loss (single batch): 1.2455288171768188\n",
      "##################################\n",
      "## EPOCH 536/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.216904878616333\n",
      "\t Training loss (single batch): 1.564012050628662\n",
      "\t Training loss (single batch): 2.382050037384033\n",
      "##################################\n",
      "## EPOCH 537/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5858734846115112\n",
      "\t Training loss (single batch): 1.9354383945465088\n",
      "\t Training loss (single batch): 1.5110453367233276\n",
      "##################################\n",
      "## EPOCH 538/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.686065435409546\n",
      "\t Training loss (single batch): 2.5816750526428223\n",
      "\t Training loss (single batch): 1.760622262954712\n",
      "##################################\n",
      "## EPOCH 539/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5249359607696533\n",
      "\t Training loss (single batch): 1.3036879301071167\n",
      "\t Training loss (single batch): 1.9613862037658691\n",
      "##################################\n",
      "## EPOCH 540/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8651357889175415\n",
      "\t Training loss (single batch): 2.1539411544799805\n",
      "\t Training loss (single batch): 2.1264142990112305\n",
      "##################################\n",
      "## EPOCH 541/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0027880668640137\n",
      "\t Training loss (single batch): 1.772646427154541\n",
      "\t Training loss (single batch): 1.6088001728057861\n",
      "##################################\n",
      "## EPOCH 542/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.14150333404541\n",
      "\t Training loss (single batch): 1.5350512266159058\n",
      "\t Training loss (single batch): 2.0205905437469482\n",
      "##################################\n",
      "## EPOCH 543/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8145568370819092\n",
      "\t Training loss (single batch): 1.678072214126587\n",
      "\t Training loss (single batch): 1.898630976676941\n",
      "##################################\n",
      "## EPOCH 544/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8568686246871948\n",
      "\t Training loss (single batch): 2.1514837741851807\n",
      "\t Training loss (single batch): 1.7449966669082642\n",
      "##################################\n",
      "## EPOCH 545/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.824209213256836\n",
      "\t Training loss (single batch): 1.614283800125122\n",
      "\t Training loss (single batch): 2.074024200439453\n",
      "##################################\n",
      "## EPOCH 546/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7586894035339355\n",
      "\t Training loss (single batch): 1.654395580291748\n",
      "\t Training loss (single batch): 1.7866575717926025\n",
      "##################################\n",
      "## EPOCH 547/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7775561809539795\n",
      "\t Training loss (single batch): 1.5894864797592163\n",
      "\t Training loss (single batch): 1.585210919380188\n",
      "##################################\n",
      "## EPOCH 548/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2251627445220947\n",
      "\t Training loss (single batch): 1.7389020919799805\n",
      "\t Training loss (single batch): 1.2529488801956177\n",
      "##################################\n",
      "## EPOCH 549/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.997245192527771\n",
      "\t Training loss (single batch): 1.707971453666687\n",
      "\t Training loss (single batch): 1.4389994144439697\n",
      "##################################\n",
      "## EPOCH 550/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.351412296295166\n",
      "\t Training loss (single batch): 1.7625945806503296\n",
      "\t Training loss (single batch): 1.7202237844467163\n",
      "##################################\n",
      "## EPOCH 551/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9143791198730469\n",
      "\t Training loss (single batch): 2.087082624435425\n",
      "\t Training loss (single batch): 1.7281445264816284\n",
      "##################################\n",
      "## EPOCH 552/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3639466762542725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7620694637298584\n",
      "\t Training loss (single batch): 2.413041114807129\n",
      "##################################\n",
      "## EPOCH 553/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9161033630371094\n",
      "\t Training loss (single batch): 1.8356387615203857\n",
      "\t Training loss (single batch): 1.633104681968689\n",
      "##################################\n",
      "## EPOCH 554/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0644257068634033\n",
      "\t Training loss (single batch): 1.8104933500289917\n",
      "\t Training loss (single batch): 1.5272371768951416\n",
      "##################################\n",
      "## EPOCH 555/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8331079483032227\n",
      "\t Training loss (single batch): 1.270946741104126\n",
      "\t Training loss (single batch): 2.1212034225463867\n",
      "##################################\n",
      "## EPOCH 556/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.885864496231079\n",
      "\t Training loss (single batch): 2.3941292762756348\n",
      "\t Training loss (single batch): 2.7888758182525635\n",
      "##################################\n",
      "## EPOCH 557/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6954370737075806\n",
      "\t Training loss (single batch): 1.6321592330932617\n",
      "\t Training loss (single batch): 1.8897491693496704\n",
      "##################################\n",
      "## EPOCH 558/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1551709175109863\n",
      "\t Training loss (single batch): 1.9945834875106812\n",
      "\t Training loss (single batch): 1.862175464630127\n",
      "##################################\n",
      "## EPOCH 559/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0702481269836426\n",
      "\t Training loss (single batch): 1.8786821365356445\n",
      "\t Training loss (single batch): 1.9464126825332642\n",
      "##################################\n",
      "## EPOCH 560/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.491726875305176\n",
      "\t Training loss (single batch): 1.649399995803833\n",
      "\t Training loss (single batch): 1.9670143127441406\n",
      "##################################\n",
      "## EPOCH 561/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0031800270080566\n",
      "\t Training loss (single batch): 1.8501330614089966\n",
      "\t Training loss (single batch): 2.109158992767334\n",
      "##################################\n",
      "## EPOCH 562/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.144310235977173\n",
      "\t Training loss (single batch): 1.7300888299942017\n",
      "\t Training loss (single batch): 1.9351962804794312\n",
      "##################################\n",
      "## EPOCH 563/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6397607326507568\n",
      "\t Training loss (single batch): 2.1554083824157715\n",
      "\t Training loss (single batch): 1.6748138666152954\n",
      "##################################\n",
      "## EPOCH 564/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0540714263916016\n",
      "\t Training loss (single batch): 1.581490159034729\n",
      "\t Training loss (single batch): 2.107186794281006\n",
      "##################################\n",
      "## EPOCH 565/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6718387603759766\n",
      "\t Training loss (single batch): 1.4843978881835938\n",
      "\t Training loss (single batch): 1.749024748802185\n",
      "##################################\n",
      "## EPOCH 566/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.393911838531494\n",
      "\t Training loss (single batch): 1.5535520315170288\n",
      "\t Training loss (single batch): 1.7854310274124146\n",
      "##################################\n",
      "## EPOCH 567/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.883136510848999\n",
      "\t Training loss (single batch): 1.6718719005584717\n",
      "\t Training loss (single batch): 1.9419407844543457\n",
      "##################################\n",
      "## EPOCH 568/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9870043992996216\n",
      "\t Training loss (single batch): 1.6611195802688599\n",
      "\t Training loss (single batch): 2.051403045654297\n",
      "##################################\n",
      "## EPOCH 569/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1202292442321777\n",
      "\t Training loss (single batch): 1.7888840436935425\n",
      "\t Training loss (single batch): 1.7341187000274658\n",
      "##################################\n",
      "## EPOCH 570/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0791773796081543\n",
      "\t Training loss (single batch): 1.4623881578445435\n",
      "\t Training loss (single batch): 1.5020228624343872\n",
      "##################################\n",
      "## EPOCH 571/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.52886962890625\n",
      "\t Training loss (single batch): 1.7045313119888306\n",
      "\t Training loss (single batch): 1.901347041130066\n",
      "##################################\n",
      "## EPOCH 572/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.989128589630127\n",
      "\t Training loss (single batch): 2.0161099433898926\n",
      "\t Training loss (single batch): 2.2123727798461914\n",
      "##################################\n",
      "## EPOCH 573/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9577698707580566\n",
      "\t Training loss (single batch): 1.8889161348342896\n",
      "\t Training loss (single batch): 1.3498214483261108\n",
      "##################################\n",
      "## EPOCH 574/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.204395055770874\n",
      "\t Training loss (single batch): 1.5231207609176636\n",
      "\t Training loss (single batch): 1.886574149131775\n",
      "##################################\n",
      "## EPOCH 575/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.112092971801758\n",
      "\t Training loss (single batch): 2.2043540477752686\n",
      "\t Training loss (single batch): 2.198899269104004\n",
      "##################################\n",
      "## EPOCH 576/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4318052530288696\n",
      "\t Training loss (single batch): 1.9632810354232788\n",
      "\t Training loss (single batch): 1.7854549884796143\n",
      "##################################\n",
      "## EPOCH 577/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7932807207107544\n",
      "\t Training loss (single batch): 1.8471728563308716\n",
      "\t Training loss (single batch): 2.1848092079162598\n",
      "##################################\n",
      "## EPOCH 578/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6017481088638306\n",
      "\t Training loss (single batch): 1.500006914138794\n",
      "\t Training loss (single batch): 1.8953531980514526\n",
      "##################################\n",
      "## EPOCH 579/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7943753004074097\n",
      "\t Training loss (single batch): 1.547911286354065\n",
      "\t Training loss (single batch): 1.8166537284851074\n",
      "##################################\n",
      "## EPOCH 580/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.764364242553711\n",
      "\t Training loss (single batch): 1.4897292852401733\n",
      "\t Training loss (single batch): 1.6192106008529663\n",
      "##################################\n",
      "## EPOCH 581/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8319497108459473\n",
      "\t Training loss (single batch): 1.5527254343032837\n",
      "\t Training loss (single batch): 2.5835728645324707\n",
      "##################################\n",
      "## EPOCH 582/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.066690683364868\n",
      "\t Training loss (single batch): 1.8311315774917603\n",
      "\t Training loss (single batch): 2.355635643005371\n",
      "##################################\n",
      "## EPOCH 583/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9251368045806885\n",
      "\t Training loss (single batch): 2.2588045597076416\n",
      "\t Training loss (single batch): 1.6494890451431274\n",
      "##################################\n",
      "## EPOCH 584/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.875321388244629\n",
      "\t Training loss (single batch): 1.8267273902893066\n",
      "\t Training loss (single batch): 2.388047695159912\n",
      "##################################\n",
      "## EPOCH 585/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3285455703735352\n",
      "\t Training loss (single batch): 1.6703834533691406\n",
      "\t Training loss (single batch): 1.6627377271652222\n",
      "##################################\n",
      "## EPOCH 586/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6056865453720093\n",
      "\t Training loss (single batch): 1.832149863243103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7441158294677734\n",
      "##################################\n",
      "## EPOCH 587/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8024510145187378\n",
      "\t Training loss (single batch): 1.5997432470321655\n",
      "\t Training loss (single batch): 1.4868451356887817\n",
      "##################################\n",
      "## EPOCH 588/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2276291847229004\n",
      "\t Training loss (single batch): 1.5296990871429443\n",
      "\t Training loss (single batch): 1.723443627357483\n",
      "##################################\n",
      "## EPOCH 589/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0513980388641357\n",
      "\t Training loss (single batch): 1.1263797283172607\n",
      "\t Training loss (single batch): 1.643378734588623\n",
      "##################################\n",
      "## EPOCH 590/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2150628566741943\n",
      "\t Training loss (single batch): 1.6060956716537476\n",
      "\t Training loss (single batch): 1.8564505577087402\n",
      "##################################\n",
      "## EPOCH 591/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3941951990127563\n",
      "\t Training loss (single batch): 1.1240124702453613\n",
      "\t Training loss (single batch): 1.1753779649734497\n",
      "##################################\n",
      "## EPOCH 592/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1432385444641113\n",
      "\t Training loss (single batch): 1.671473503112793\n",
      "\t Training loss (single batch): 1.7713130712509155\n",
      "##################################\n",
      "## EPOCH 593/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5684268474578857\n",
      "\t Training loss (single batch): 1.4982656240463257\n",
      "\t Training loss (single batch): 1.8512369394302368\n",
      "##################################\n",
      "## EPOCH 594/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1870152950286865\n",
      "\t Training loss (single batch): 1.3199760913848877\n",
      "\t Training loss (single batch): 1.3471490144729614\n",
      "##################################\n",
      "## EPOCH 595/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6227861642837524\n",
      "\t Training loss (single batch): 1.628100037574768\n",
      "\t Training loss (single batch): 1.295839548110962\n",
      "##################################\n",
      "## EPOCH 596/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5139596462249756\n",
      "\t Training loss (single batch): 2.2383625507354736\n",
      "\t Training loss (single batch): 1.9266457557678223\n",
      "##################################\n",
      "## EPOCH 597/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.206496477127075\n",
      "\t Training loss (single batch): 1.4338690042495728\n",
      "\t Training loss (single batch): 1.3867334127426147\n",
      "##################################\n",
      "## EPOCH 598/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1693146228790283\n",
      "\t Training loss (single batch): 1.7806642055511475\n",
      "\t Training loss (single batch): 1.5192615985870361\n",
      "##################################\n",
      "## EPOCH 599/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.619661569595337\n",
      "\t Training loss (single batch): 1.916975975036621\n",
      "\t Training loss (single batch): 2.1338937282562256\n",
      "##################################\n",
      "## EPOCH 600/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4756627082824707\n",
      "\t Training loss (single batch): 1.8542073965072632\n",
      "\t Training loss (single batch): 1.7537343502044678\n",
      "##################################\n",
      "## EPOCH 601/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9319864511489868\n",
      "\t Training loss (single batch): 1.7425734996795654\n",
      "\t Training loss (single batch): 1.6871249675750732\n",
      "##################################\n",
      "## EPOCH 602/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7827024459838867\n",
      "\t Training loss (single batch): 1.3316268920898438\n",
      "\t Training loss (single batch): 2.0093507766723633\n",
      "##################################\n",
      "## EPOCH 603/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2836796045303345\n",
      "\t Training loss (single batch): 1.0329314470291138\n",
      "\t Training loss (single batch): 1.157829999923706\n",
      "##################################\n",
      "## EPOCH 604/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7158868312835693\n",
      "\t Training loss (single batch): 1.860451102256775\n",
      "\t Training loss (single batch): 2.08797550201416\n",
      "##################################\n",
      "## EPOCH 605/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.1431071758270264\n",
      "\t Training loss (single batch): 1.4918023347854614\n",
      "\t Training loss (single batch): 2.2891156673431396\n",
      "##################################\n",
      "## EPOCH 606/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5648409128189087\n",
      "\t Training loss (single batch): 1.355940818786621\n",
      "\t Training loss (single batch): 1.930846929550171\n",
      "##################################\n",
      "## EPOCH 607/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6350889205932617\n",
      "\t Training loss (single batch): 1.8780603408813477\n",
      "\t Training loss (single batch): 1.7319649457931519\n",
      "##################################\n",
      "## EPOCH 608/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3343751430511475\n",
      "\t Training loss (single batch): 1.124061942100525\n",
      "\t Training loss (single batch): 1.9635342359542847\n",
      "##################################\n",
      "## EPOCH 609/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2306160926818848\n",
      "\t Training loss (single batch): 1.6455762386322021\n",
      "\t Training loss (single batch): 1.5368329286575317\n",
      "##################################\n",
      "## EPOCH 610/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4929728507995605\n",
      "\t Training loss (single batch): 1.5900131464004517\n",
      "\t Training loss (single batch): 1.8379205465316772\n",
      "##################################\n",
      "## EPOCH 611/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7501262426376343\n",
      "\t Training loss (single batch): 1.3972069025039673\n",
      "\t Training loss (single batch): 1.8169562816619873\n",
      "##################################\n",
      "## EPOCH 612/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9719960689544678\n",
      "\t Training loss (single batch): 1.4132260084152222\n",
      "\t Training loss (single batch): 1.023901104927063\n",
      "##################################\n",
      "## EPOCH 613/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.345165252685547\n",
      "\t Training loss (single batch): 1.2200872898101807\n",
      "\t Training loss (single batch): 1.9897984266281128\n",
      "##################################\n",
      "## EPOCH 614/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.032116651535034\n",
      "\t Training loss (single batch): 1.9388893842697144\n",
      "\t Training loss (single batch): 1.406413197517395\n",
      "##################################\n",
      "## EPOCH 615/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7497010231018066\n",
      "\t Training loss (single batch): 1.62958562374115\n",
      "\t Training loss (single batch): 1.8563587665557861\n",
      "##################################\n",
      "## EPOCH 616/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.245422601699829\n",
      "\t Training loss (single batch): 1.9888718128204346\n",
      "\t Training loss (single batch): 1.8948405981063843\n",
      "##################################\n",
      "## EPOCH 617/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3279186487197876\n",
      "\t Training loss (single batch): 1.9601330757141113\n",
      "\t Training loss (single batch): 1.6315670013427734\n",
      "##################################\n",
      "## EPOCH 618/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0661733150482178\n",
      "\t Training loss (single batch): 1.7241171598434448\n",
      "\t Training loss (single batch): 1.4437462091445923\n",
      "##################################\n",
      "## EPOCH 619/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.551271677017212\n",
      "\t Training loss (single batch): 1.6066547632217407\n",
      "\t Training loss (single batch): 1.442442536354065\n",
      "##################################\n",
      "## EPOCH 620/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6543076038360596\n",
      "\t Training loss (single batch): 1.491844892501831\n",
      "\t Training loss (single batch): 1.7291443347930908\n",
      "##################################\n",
      "## EPOCH 621/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.7879294157028198\n",
      "\t Training loss (single batch): 2.5029988288879395\n",
      "\t Training loss (single batch): 2.679699420928955\n",
      "##################################\n",
      "## EPOCH 622/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3629307746887207\n",
      "\t Training loss (single batch): 2.0430192947387695\n",
      "\t Training loss (single batch): 1.5317795276641846\n",
      "##################################\n",
      "## EPOCH 623/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9741346836090088\n",
      "\t Training loss (single batch): 1.5529390573501587\n",
      "\t Training loss (single batch): 1.566308617591858\n",
      "##################################\n",
      "## EPOCH 624/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.66555917263031\n",
      "\t Training loss (single batch): 1.5320316553115845\n",
      "\t Training loss (single batch): 2.174046516418457\n",
      "##################################\n",
      "## EPOCH 625/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9568991661071777\n",
      "\t Training loss (single batch): 1.4279582500457764\n",
      "\t Training loss (single batch): 1.8612446784973145\n",
      "##################################\n",
      "## EPOCH 626/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0020496845245361\n",
      "\t Training loss (single batch): 1.9485448598861694\n",
      "\t Training loss (single batch): 1.5601379871368408\n",
      "##################################\n",
      "## EPOCH 627/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2870739698410034\n",
      "\t Training loss (single batch): 1.7134662866592407\n",
      "\t Training loss (single batch): 1.761139988899231\n",
      "##################################\n",
      "## EPOCH 628/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5526344776153564\n",
      "\t Training loss (single batch): 1.0735543966293335\n",
      "\t Training loss (single batch): 1.9646192789077759\n",
      "##################################\n",
      "## EPOCH 629/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3984060287475586\n",
      "\t Training loss (single batch): 1.5611575841903687\n",
      "\t Training loss (single batch): 2.0128133296966553\n",
      "##################################\n",
      "## EPOCH 630/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4549177885055542\n",
      "\t Training loss (single batch): 1.5664269924163818\n",
      "\t Training loss (single batch): 1.6984127759933472\n",
      "##################################\n",
      "## EPOCH 631/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6043384075164795\n",
      "\t Training loss (single batch): 1.5260964632034302\n",
      "\t Training loss (single batch): 1.3689351081848145\n",
      "##################################\n",
      "## EPOCH 632/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.992087185382843\n",
      "\t Training loss (single batch): 1.798404335975647\n",
      "\t Training loss (single batch): 1.3528425693511963\n",
      "##################################\n",
      "## EPOCH 633/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3484716415405273\n",
      "\t Training loss (single batch): 1.323195219039917\n",
      "\t Training loss (single batch): 1.4233587980270386\n",
      "##################################\n",
      "## EPOCH 634/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.010257601737976\n",
      "\t Training loss (single batch): 1.7203445434570312\n",
      "\t Training loss (single batch): 1.608756422996521\n",
      "##################################\n",
      "## EPOCH 635/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8261456489562988\n",
      "\t Training loss (single batch): 1.5486747026443481\n",
      "\t Training loss (single batch): 1.973060965538025\n",
      "##################################\n",
      "## EPOCH 636/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.572352409362793\n",
      "\t Training loss (single batch): 2.0060412883758545\n",
      "\t Training loss (single batch): 1.4421308040618896\n",
      "##################################\n",
      "## EPOCH 637/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5112394094467163\n",
      "\t Training loss (single batch): 1.4932905435562134\n",
      "\t Training loss (single batch): 1.2442973852157593\n",
      "##################################\n",
      "## EPOCH 638/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7809489965438843\n",
      "\t Training loss (single batch): 1.5098503828048706\n",
      "\t Training loss (single batch): 1.818615198135376\n",
      "##################################\n",
      "## EPOCH 639/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6382496356964111\n",
      "\t Training loss (single batch): 1.9263461828231812\n",
      "\t Training loss (single batch): 1.7141369581222534\n",
      "##################################\n",
      "## EPOCH 640/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6259711980819702\n",
      "\t Training loss (single batch): 1.7736523151397705\n",
      "\t Training loss (single batch): 1.7763298749923706\n",
      "##################################\n",
      "## EPOCH 641/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2608203887939453\n",
      "\t Training loss (single batch): 1.067790150642395\n",
      "\t Training loss (single batch): 1.6280348300933838\n",
      "##################################\n",
      "## EPOCH 642/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0423494577407837\n",
      "\t Training loss (single batch): 1.5172858238220215\n",
      "\t Training loss (single batch): 1.8501980304718018\n",
      "##################################\n",
      "## EPOCH 643/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.829289197921753\n",
      "\t Training loss (single batch): 1.558897852897644\n",
      "\t Training loss (single batch): 1.4749356508255005\n",
      "##################################\n",
      "## EPOCH 644/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6903507709503174\n",
      "\t Training loss (single batch): 1.2783807516098022\n",
      "\t Training loss (single batch): 1.8334218263626099\n",
      "##################################\n",
      "## EPOCH 645/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8431720733642578\n",
      "\t Training loss (single batch): 1.5339232683181763\n",
      "\t Training loss (single batch): 1.6637966632843018\n",
      "##################################\n",
      "## EPOCH 646/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5715986490249634\n",
      "\t Training loss (single batch): 1.8688409328460693\n",
      "\t Training loss (single batch): 1.8482264280319214\n",
      "##################################\n",
      "## EPOCH 647/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7127844095230103\n",
      "\t Training loss (single batch): 1.5468733310699463\n",
      "\t Training loss (single batch): 1.1941381692886353\n",
      "##################################\n",
      "## EPOCH 648/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8375725746154785\n",
      "\t Training loss (single batch): 1.8796449899673462\n",
      "\t Training loss (single batch): 1.8000117540359497\n",
      "##################################\n",
      "## EPOCH 649/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.813363552093506\n",
      "\t Training loss (single batch): 1.195176601409912\n",
      "\t Training loss (single batch): 1.8240665197372437\n",
      "##################################\n",
      "## EPOCH 650/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4002676010131836\n",
      "\t Training loss (single batch): 0.9364400506019592\n",
      "\t Training loss (single batch): 1.5308167934417725\n",
      "##################################\n",
      "## EPOCH 651/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0775623321533203\n",
      "\t Training loss (single batch): 1.8317726850509644\n",
      "\t Training loss (single batch): 1.3328297138214111\n",
      "##################################\n",
      "## EPOCH 652/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8757086992263794\n",
      "\t Training loss (single batch): 1.8313052654266357\n",
      "\t Training loss (single batch): 1.8904144763946533\n",
      "##################################\n",
      "## EPOCH 653/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4893558025360107\n",
      "\t Training loss (single batch): 1.2832621335983276\n",
      "\t Training loss (single batch): 1.4682906866073608\n",
      "##################################\n",
      "## EPOCH 654/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.2680349349975586\n",
      "\t Training loss (single batch): 0.9785088300704956\n",
      "\t Training loss (single batch): 2.130096673965454\n",
      "##################################\n",
      "## EPOCH 655/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.758406162261963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.42776620388031\n",
      "\t Training loss (single batch): 1.4265174865722656\n",
      "##################################\n",
      "## EPOCH 656/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7594023942947388\n",
      "\t Training loss (single batch): 2.021510601043701\n",
      "\t Training loss (single batch): 1.7308601140975952\n",
      "##################################\n",
      "## EPOCH 657/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8181231021881104\n",
      "\t Training loss (single batch): 1.983553171157837\n",
      "\t Training loss (single batch): 1.4382209777832031\n",
      "##################################\n",
      "## EPOCH 658/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.833127737045288\n",
      "\t Training loss (single batch): 2.4503836631774902\n",
      "\t Training loss (single batch): 1.2929661273956299\n",
      "##################################\n",
      "## EPOCH 659/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8100981712341309\n",
      "\t Training loss (single batch): 1.252934217453003\n",
      "\t Training loss (single batch): 1.4203530550003052\n",
      "##################################\n",
      "## EPOCH 660/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.3572323322296143\n",
      "\t Training loss (single batch): 1.3088408708572388\n",
      "\t Training loss (single batch): 1.8475062847137451\n",
      "##################################\n",
      "## EPOCH 661/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4757637977600098\n",
      "\t Training loss (single batch): 1.3857834339141846\n",
      "\t Training loss (single batch): 1.6756360530853271\n",
      "##################################\n",
      "## EPOCH 662/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4686444997787476\n",
      "\t Training loss (single batch): 1.9212335348129272\n",
      "\t Training loss (single batch): 1.4899251461029053\n",
      "##################################\n",
      "## EPOCH 663/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0254476070404053\n",
      "\t Training loss (single batch): 0.9656479358673096\n",
      "\t Training loss (single batch): 1.7272652387619019\n",
      "##################################\n",
      "## EPOCH 664/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9847548007965088\n",
      "\t Training loss (single batch): 1.7415430545806885\n",
      "\t Training loss (single batch): 1.2274117469787598\n",
      "##################################\n",
      "## EPOCH 665/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4127585887908936\n",
      "\t Training loss (single batch): 1.8500587940216064\n",
      "\t Training loss (single batch): 1.205419659614563\n",
      "##################################\n",
      "## EPOCH 666/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5008330345153809\n",
      "\t Training loss (single batch): 1.384113073348999\n",
      "\t Training loss (single batch): 1.6139432191848755\n",
      "##################################\n",
      "## EPOCH 667/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.96882563829422\n",
      "\t Training loss (single batch): 1.1361889839172363\n",
      "\t Training loss (single batch): 1.576865553855896\n",
      "##################################\n",
      "## EPOCH 668/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0794943571090698\n",
      "\t Training loss (single batch): 1.5869169235229492\n",
      "\t Training loss (single batch): 2.199437141418457\n",
      "##################################\n",
      "## EPOCH 669/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2488868236541748\n",
      "\t Training loss (single batch): 1.5441186428070068\n",
      "\t Training loss (single batch): 1.8509448766708374\n",
      "##################################\n",
      "## EPOCH 670/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9075080156326294\n",
      "\t Training loss (single batch): 1.6333588361740112\n",
      "\t Training loss (single batch): 1.5041226148605347\n",
      "##################################\n",
      "## EPOCH 671/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6422032117843628\n",
      "\t Training loss (single batch): 2.0446693897247314\n",
      "\t Training loss (single batch): 0.9831874966621399\n",
      "##################################\n",
      "## EPOCH 672/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.245316505432129\n",
      "\t Training loss (single batch): 1.6774213314056396\n",
      "\t Training loss (single batch): 1.569410800933838\n",
      "##################################\n",
      "## EPOCH 673/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3049378395080566\n",
      "\t Training loss (single batch): 1.7578456401824951\n",
      "\t Training loss (single batch): 1.7109062671661377\n",
      "##################################\n",
      "## EPOCH 674/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3941744565963745\n",
      "\t Training loss (single batch): 0.9775317907333374\n",
      "\t Training loss (single batch): 2.1277310848236084\n",
      "##################################\n",
      "## EPOCH 675/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2505509853363037\n",
      "\t Training loss (single batch): 1.308638572692871\n",
      "\t Training loss (single batch): 1.7569531202316284\n",
      "##################################\n",
      "## EPOCH 676/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.0887722969055176\n",
      "\t Training loss (single batch): 1.5083115100860596\n",
      "\t Training loss (single batch): 1.905139684677124\n",
      "##################################\n",
      "## EPOCH 677/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5536093711853027\n",
      "\t Training loss (single batch): 1.6120197772979736\n",
      "\t Training loss (single batch): 1.8257389068603516\n",
      "##################################\n",
      "## EPOCH 678/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.844177484512329\n",
      "\t Training loss (single batch): 1.6456775665283203\n",
      "\t Training loss (single batch): 2.018343925476074\n",
      "##################################\n",
      "## EPOCH 679/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5863069295883179\n",
      "\t Training loss (single batch): 1.4152743816375732\n",
      "\t Training loss (single batch): 2.049678325653076\n",
      "##################################\n",
      "## EPOCH 680/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7098584175109863\n",
      "\t Training loss (single batch): 1.1863737106323242\n",
      "\t Training loss (single batch): 1.5578893423080444\n",
      "##################################\n",
      "## EPOCH 681/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4047467708587646\n",
      "\t Training loss (single batch): 1.3034412860870361\n",
      "\t Training loss (single batch): 1.5267162322998047\n",
      "##################################\n",
      "## EPOCH 682/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3585103750228882\n",
      "\t Training loss (single batch): 1.2900545597076416\n",
      "\t Training loss (single batch): 1.1439285278320312\n",
      "##################################\n",
      "## EPOCH 683/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.104630470275879\n",
      "\t Training loss (single batch): 1.3895350694656372\n",
      "\t Training loss (single batch): 1.2307305335998535\n",
      "##################################\n",
      "## EPOCH 684/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1320686340332031\n",
      "\t Training loss (single batch): 1.8235646486282349\n",
      "\t Training loss (single batch): 2.192713975906372\n",
      "##################################\n",
      "## EPOCH 685/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9002329111099243\n",
      "\t Training loss (single batch): 1.4107757806777954\n",
      "\t Training loss (single batch): 1.398864507675171\n",
      "##################################\n",
      "## EPOCH 686/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3025269508361816\n",
      "\t Training loss (single batch): 1.3138649463653564\n",
      "\t Training loss (single batch): 1.6892471313476562\n",
      "##################################\n",
      "## EPOCH 687/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6114451885223389\n",
      "\t Training loss (single batch): 1.2389200925827026\n",
      "\t Training loss (single batch): 1.7064216136932373\n",
      "##################################\n",
      "## EPOCH 688/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8875799179077148\n",
      "\t Training loss (single batch): 1.3406721353530884\n",
      "\t Training loss (single batch): 1.5819565057754517\n",
      "##################################\n",
      "## EPOCH 689/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.657987356185913\n",
      "\t Training loss (single batch): 1.3167197704315186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2353405952453613\n",
      "##################################\n",
      "## EPOCH 690/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6965304613113403\n",
      "\t Training loss (single batch): 1.5816938877105713\n",
      "\t Training loss (single batch): 0.9312922954559326\n",
      "##################################\n",
      "## EPOCH 691/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7554385662078857\n",
      "\t Training loss (single batch): 1.7312700748443604\n",
      "\t Training loss (single batch): 1.3697879314422607\n",
      "##################################\n",
      "## EPOCH 692/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.199037790298462\n",
      "\t Training loss (single batch): 1.6912121772766113\n",
      "\t Training loss (single batch): 1.4804956912994385\n",
      "##################################\n",
      "## EPOCH 693/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.494900107383728\n",
      "\t Training loss (single batch): 1.1430506706237793\n",
      "\t Training loss (single batch): 1.5178824663162231\n",
      "##################################\n",
      "## EPOCH 694/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.265080451965332\n",
      "\t Training loss (single batch): 1.648141860961914\n",
      "\t Training loss (single batch): 1.8886011838912964\n",
      "##################################\n",
      "## EPOCH 695/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1845685243606567\n",
      "\t Training loss (single batch): 1.9772710800170898\n",
      "\t Training loss (single batch): 1.8137494325637817\n",
      "##################################\n",
      "## EPOCH 696/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.319034218788147\n",
      "\t Training loss (single batch): 1.1444734334945679\n",
      "\t Training loss (single batch): 1.6037993431091309\n",
      "##################################\n",
      "## EPOCH 697/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.844586730003357\n",
      "\t Training loss (single batch): 1.9634644985198975\n",
      "\t Training loss (single batch): 1.4671387672424316\n",
      "##################################\n",
      "## EPOCH 698/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.090541124343872\n",
      "\t Training loss (single batch): 1.7659744024276733\n",
      "\t Training loss (single batch): 1.974541425704956\n",
      "##################################\n",
      "## EPOCH 699/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7129443883895874\n",
      "\t Training loss (single batch): 1.601601243019104\n",
      "\t Training loss (single batch): 1.9785946607589722\n",
      "##################################\n",
      "## EPOCH 700/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6200343370437622\n",
      "\t Training loss (single batch): 2.0234880447387695\n",
      "\t Training loss (single batch): 1.639946699142456\n",
      "##################################\n",
      "## EPOCH 701/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.89521062374115\n",
      "\t Training loss (single batch): 0.9069464802742004\n",
      "\t Training loss (single batch): 1.878418207168579\n",
      "##################################\n",
      "## EPOCH 702/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6965219974517822\n",
      "\t Training loss (single batch): 1.3606135845184326\n",
      "\t Training loss (single batch): 1.5874576568603516\n",
      "##################################\n",
      "## EPOCH 703/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7148947715759277\n",
      "\t Training loss (single batch): 1.3060507774353027\n",
      "\t Training loss (single batch): 2.0382847785949707\n",
      "##################################\n",
      "## EPOCH 704/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.706799030303955\n",
      "\t Training loss (single batch): 1.6790709495544434\n",
      "\t Training loss (single batch): 2.3482415676116943\n",
      "##################################\n",
      "## EPOCH 705/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3313026428222656\n",
      "\t Training loss (single batch): 1.3456108570098877\n",
      "\t Training loss (single batch): 1.624685525894165\n",
      "##################################\n",
      "## EPOCH 706/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.250611424446106\n",
      "\t Training loss (single batch): 0.8947867155075073\n",
      "\t Training loss (single batch): 1.702046275138855\n",
      "##################################\n",
      "## EPOCH 707/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7344902753829956\n",
      "\t Training loss (single batch): 2.119793176651001\n",
      "\t Training loss (single batch): 1.8100183010101318\n",
      "##################################\n",
      "## EPOCH 708/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2232555150985718\n",
      "\t Training loss (single batch): 1.2170246839523315\n",
      "\t Training loss (single batch): 1.5093467235565186\n",
      "##################################\n",
      "## EPOCH 709/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.081000804901123\n",
      "\t Training loss (single batch): 1.3818763494491577\n",
      "\t Training loss (single batch): 1.9700922966003418\n",
      "##################################\n",
      "## EPOCH 710/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.729573130607605\n",
      "\t Training loss (single batch): 1.0441958904266357\n",
      "\t Training loss (single batch): 1.298332929611206\n",
      "##################################\n",
      "## EPOCH 711/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2780393362045288\n",
      "\t Training loss (single batch): 1.6302311420440674\n",
      "\t Training loss (single batch): 1.2454546689987183\n",
      "##################################\n",
      "## EPOCH 712/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9977728128433228\n",
      "\t Training loss (single batch): 1.5175226926803589\n",
      "\t Training loss (single batch): 1.0786497592926025\n",
      "##################################\n",
      "## EPOCH 713/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2060987949371338\n",
      "\t Training loss (single batch): 1.4341145753860474\n",
      "\t Training loss (single batch): 1.678006887435913\n",
      "##################################\n",
      "## EPOCH 714/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4856953620910645\n",
      "\t Training loss (single batch): 1.4583475589752197\n",
      "\t Training loss (single batch): 1.0162608623504639\n",
      "##################################\n",
      "## EPOCH 715/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.290740966796875\n",
      "\t Training loss (single batch): 1.1965434551239014\n",
      "\t Training loss (single batch): 1.710755705833435\n",
      "##################################\n",
      "## EPOCH 716/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1435484886169434\n",
      "\t Training loss (single batch): 1.3075406551361084\n",
      "\t Training loss (single batch): 0.8686691522598267\n",
      "##################################\n",
      "## EPOCH 717/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9871413111686707\n",
      "\t Training loss (single batch): 1.3944090604782104\n",
      "\t Training loss (single batch): 1.3912999629974365\n",
      "##################################\n",
      "## EPOCH 718/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6761032342910767\n",
      "\t Training loss (single batch): 1.474645733833313\n",
      "\t Training loss (single batch): 1.382861852645874\n",
      "##################################\n",
      "## EPOCH 719/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8055384159088135\n",
      "\t Training loss (single batch): 1.6007226705551147\n",
      "\t Training loss (single batch): 1.1795974969863892\n",
      "##################################\n",
      "## EPOCH 720/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4052224159240723\n",
      "\t Training loss (single batch): 1.3896883726119995\n",
      "\t Training loss (single batch): 1.518068790435791\n",
      "##################################\n",
      "## EPOCH 721/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5036425590515137\n",
      "\t Training loss (single batch): 1.4325153827667236\n",
      "\t Training loss (single batch): 0.9789348840713501\n",
      "##################################\n",
      "## EPOCH 722/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4209649562835693\n",
      "\t Training loss (single batch): 1.6923120021820068\n",
      "\t Training loss (single batch): 0.8966816663742065\n",
      "##################################\n",
      "## EPOCH 723/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4085363149642944\n",
      "\t Training loss (single batch): 1.3775646686553955\n",
      "\t Training loss (single batch): 2.3924505710601807\n",
      "##################################\n",
      "## EPOCH 724/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.2124723196029663\n",
      "\t Training loss (single batch): 0.916041374206543\n",
      "\t Training loss (single batch): 1.6905442476272583\n",
      "##################################\n",
      "## EPOCH 725/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9091412425041199\n",
      "\t Training loss (single batch): 1.2605388164520264\n",
      "\t Training loss (single batch): 1.7531925439834595\n",
      "##################################\n",
      "## EPOCH 726/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.4092094898223877\n",
      "\t Training loss (single batch): 1.1681504249572754\n",
      "\t Training loss (single batch): 1.4512996673583984\n",
      "##################################\n",
      "## EPOCH 727/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5611519813537598\n",
      "\t Training loss (single batch): 1.5208582878112793\n",
      "\t Training loss (single batch): 1.726356863975525\n",
      "##################################\n",
      "## EPOCH 728/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3946179151535034\n",
      "\t Training loss (single batch): 1.204382300376892\n",
      "\t Training loss (single batch): 1.3652594089508057\n",
      "##################################\n",
      "## EPOCH 729/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.202913522720337\n",
      "\t Training loss (single batch): 1.1141990423202515\n",
      "\t Training loss (single batch): 1.0569171905517578\n",
      "##################################\n",
      "## EPOCH 730/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7680308818817139\n",
      "\t Training loss (single batch): 1.062494158744812\n",
      "\t Training loss (single batch): 1.3089962005615234\n",
      "##################################\n",
      "## EPOCH 731/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5062159299850464\n",
      "\t Training loss (single batch): 1.0167096853256226\n",
      "\t Training loss (single batch): 1.4972667694091797\n",
      "##################################\n",
      "## EPOCH 732/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2799103260040283\n",
      "\t Training loss (single batch): 1.1631190776824951\n",
      "\t Training loss (single batch): 1.5684770345687866\n",
      "##################################\n",
      "## EPOCH 733/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3734955787658691\n",
      "\t Training loss (single batch): 1.7046079635620117\n",
      "\t Training loss (single batch): 1.5001842975616455\n",
      "##################################\n",
      "## EPOCH 734/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4868029356002808\n",
      "\t Training loss (single batch): 1.8081014156341553\n",
      "\t Training loss (single batch): 1.5033677816390991\n",
      "##################################\n",
      "## EPOCH 735/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2947760820388794\n",
      "\t Training loss (single batch): 1.1657553911209106\n",
      "\t Training loss (single batch): 1.9174129962921143\n",
      "##################################\n",
      "## EPOCH 736/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.309503197669983\n",
      "\t Training loss (single batch): 0.8302983045578003\n",
      "\t Training loss (single batch): 1.2586493492126465\n",
      "##################################\n",
      "## EPOCH 737/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3282601833343506\n",
      "\t Training loss (single batch): 2.1081416606903076\n",
      "\t Training loss (single batch): 1.749254584312439\n",
      "##################################\n",
      "## EPOCH 738/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.756145715713501\n",
      "\t Training loss (single batch): 1.6477566957473755\n",
      "\t Training loss (single batch): 1.6286686658859253\n",
      "##################################\n",
      "## EPOCH 739/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2930642366409302\n",
      "\t Training loss (single batch): 1.7880804538726807\n",
      "\t Training loss (single batch): 1.107163667678833\n",
      "##################################\n",
      "## EPOCH 740/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1207232475280762\n",
      "\t Training loss (single batch): 0.751750648021698\n",
      "\t Training loss (single batch): 1.5267369747161865\n",
      "##################################\n",
      "## EPOCH 741/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3958752155303955\n",
      "\t Training loss (single batch): 1.717814326286316\n",
      "\t Training loss (single batch): 1.5501359701156616\n",
      "##################################\n",
      "## EPOCH 742/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3872599601745605\n",
      "\t Training loss (single batch): 1.5412157773971558\n",
      "\t Training loss (single batch): 0.9901095032691956\n",
      "##################################\n",
      "## EPOCH 743/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6123298406600952\n",
      "\t Training loss (single batch): 1.1520189046859741\n",
      "\t Training loss (single batch): 1.435581922531128\n",
      "##################################\n",
      "## EPOCH 744/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8626587390899658\n",
      "\t Training loss (single batch): 1.464994192123413\n",
      "\t Training loss (single batch): 0.9764017462730408\n",
      "##################################\n",
      "## EPOCH 745/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5200251340866089\n",
      "\t Training loss (single batch): 1.6207239627838135\n",
      "\t Training loss (single batch): 0.9518009424209595\n",
      "##################################\n",
      "## EPOCH 746/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.038978934288025\n",
      "\t Training loss (single batch): 1.4705135822296143\n",
      "\t Training loss (single batch): 1.12645423412323\n",
      "##################################\n",
      "## EPOCH 747/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0792399644851685\n",
      "\t Training loss (single batch): 1.0044351816177368\n",
      "\t Training loss (single batch): 1.5266680717468262\n",
      "##################################\n",
      "## EPOCH 748/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0845348834991455\n",
      "\t Training loss (single batch): 1.4318220615386963\n",
      "\t Training loss (single batch): 1.7993091344833374\n",
      "##################################\n",
      "## EPOCH 749/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5244101285934448\n",
      "\t Training loss (single batch): 1.7799819707870483\n",
      "\t Training loss (single batch): 1.7321878671646118\n",
      "##################################\n",
      "## EPOCH 750/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7088016271591187\n",
      "\t Training loss (single batch): 1.0963176488876343\n",
      "\t Training loss (single batch): 1.0086148977279663\n",
      "##################################\n",
      "## EPOCH 751/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5025599002838135\n",
      "\t Training loss (single batch): 1.0572508573532104\n",
      "\t Training loss (single batch): 1.31451416015625\n",
      "##################################\n",
      "## EPOCH 752/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.583961844444275\n",
      "\t Training loss (single batch): 0.9075091481208801\n",
      "\t Training loss (single batch): 1.3144760131835938\n",
      "##################################\n",
      "## EPOCH 753/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4824172854423523\n",
      "\t Training loss (single batch): 1.9688292741775513\n",
      "\t Training loss (single batch): 1.9986666440963745\n",
      "##################################\n",
      "## EPOCH 754/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4499380588531494\n",
      "\t Training loss (single batch): 1.2486988306045532\n",
      "\t Training loss (single batch): 1.1648250818252563\n",
      "##################################\n",
      "## EPOCH 755/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0031192302703857\n",
      "\t Training loss (single batch): 1.2876532077789307\n",
      "\t Training loss (single batch): 1.5435020923614502\n",
      "##################################\n",
      "## EPOCH 756/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5604043006896973\n",
      "\t Training loss (single batch): 1.2306170463562012\n",
      "\t Training loss (single batch): 0.9673148989677429\n",
      "##################################\n",
      "## EPOCH 757/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9954802393913269\n",
      "\t Training loss (single batch): 1.6981621980667114\n",
      "\t Training loss (single batch): 1.566149115562439\n",
      "##################################\n",
      "## EPOCH 758/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1914914846420288\n",
      "\t Training loss (single batch): 0.6927011013031006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 2.0289618968963623\n",
      "##################################\n",
      "## EPOCH 759/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.67409086227417\n",
      "\t Training loss (single batch): 1.4776363372802734\n",
      "\t Training loss (single batch): 1.2149895429611206\n",
      "##################################\n",
      "## EPOCH 760/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4725364446640015\n",
      "\t Training loss (single batch): 1.1255227327346802\n",
      "\t Training loss (single batch): 1.2613141536712646\n",
      "##################################\n",
      "## EPOCH 761/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8839268684387207\n",
      "\t Training loss (single batch): 1.6410918235778809\n",
      "\t Training loss (single batch): 1.8568594455718994\n",
      "##################################\n",
      "## EPOCH 762/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4566993713378906\n",
      "\t Training loss (single batch): 0.5882973074913025\n",
      "\t Training loss (single batch): 1.0715644359588623\n",
      "##################################\n",
      "## EPOCH 763/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3291070461273193\n",
      "\t Training loss (single batch): 1.6376723051071167\n",
      "\t Training loss (single batch): 1.6804602146148682\n",
      "##################################\n",
      "## EPOCH 764/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1808276176452637\n",
      "\t Training loss (single batch): 1.2715203762054443\n",
      "\t Training loss (single batch): 1.1511609554290771\n",
      "##################################\n",
      "## EPOCH 765/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1112189292907715\n",
      "\t Training loss (single batch): 1.2103877067565918\n",
      "\t Training loss (single batch): 1.4962941408157349\n",
      "##################################\n",
      "## EPOCH 766/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1121083498001099\n",
      "\t Training loss (single batch): 1.2272988557815552\n",
      "\t Training loss (single batch): 1.4329547882080078\n",
      "##################################\n",
      "## EPOCH 767/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0888816118240356\n",
      "\t Training loss (single batch): 1.2595765590667725\n",
      "\t Training loss (single batch): 0.8379544019699097\n",
      "##################################\n",
      "## EPOCH 768/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9164466261863708\n",
      "\t Training loss (single batch): 1.0608042478561401\n",
      "\t Training loss (single batch): 1.2295764684677124\n",
      "##################################\n",
      "## EPOCH 769/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9987320303916931\n",
      "\t Training loss (single batch): 0.921924889087677\n",
      "\t Training loss (single batch): 1.1019891500473022\n",
      "##################################\n",
      "## EPOCH 770/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2881784439086914\n",
      "\t Training loss (single batch): 1.0800544023513794\n",
      "\t Training loss (single batch): 1.2560279369354248\n",
      "##################################\n",
      "## EPOCH 771/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2263758182525635\n",
      "\t Training loss (single batch): 0.997224748134613\n",
      "\t Training loss (single batch): 1.0408170223236084\n",
      "##################################\n",
      "## EPOCH 772/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.300235390663147\n",
      "\t Training loss (single batch): 1.1535954475402832\n",
      "\t Training loss (single batch): 1.531617522239685\n",
      "##################################\n",
      "## EPOCH 773/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.432717204093933\n",
      "\t Training loss (single batch): 1.3765430450439453\n",
      "\t Training loss (single batch): 1.306856632232666\n",
      "##################################\n",
      "## EPOCH 774/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3555396795272827\n",
      "\t Training loss (single batch): 1.196326494216919\n",
      "\t Training loss (single batch): 1.4940341711044312\n",
      "##################################\n",
      "## EPOCH 775/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5487126111984253\n",
      "\t Training loss (single batch): 1.5715405941009521\n",
      "\t Training loss (single batch): 1.8593181371688843\n",
      "##################################\n",
      "## EPOCH 776/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2728385925292969\n",
      "\t Training loss (single batch): 1.7402745485305786\n",
      "\t Training loss (single batch): 1.4406514167785645\n",
      "##################################\n",
      "## EPOCH 777/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1686638593673706\n",
      "\t Training loss (single batch): 1.709028720855713\n",
      "\t Training loss (single batch): 1.341376781463623\n",
      "##################################\n",
      "## EPOCH 778/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.282599925994873\n",
      "\t Training loss (single batch): 1.413549780845642\n",
      "\t Training loss (single batch): 1.2718884944915771\n",
      "##################################\n",
      "## EPOCH 779/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.9484729766845703\n",
      "\t Training loss (single batch): 0.8060054183006287\n",
      "\t Training loss (single batch): 1.1145269870758057\n",
      "##################################\n",
      "## EPOCH 780/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0838570594787598\n",
      "\t Training loss (single batch): 1.6384539604187012\n",
      "\t Training loss (single batch): 1.5922884941101074\n",
      "##################################\n",
      "## EPOCH 781/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4703824520111084\n",
      "\t Training loss (single batch): 1.6808149814605713\n",
      "\t Training loss (single batch): 1.1618512868881226\n",
      "##################################\n",
      "## EPOCH 782/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5329453945159912\n",
      "\t Training loss (single batch): 1.2764701843261719\n",
      "\t Training loss (single batch): 1.2402228116989136\n",
      "##################################\n",
      "## EPOCH 783/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6164071559906006\n",
      "\t Training loss (single batch): 0.8264744281768799\n",
      "\t Training loss (single batch): 0.9173610806465149\n",
      "##################################\n",
      "## EPOCH 784/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1091796159744263\n",
      "\t Training loss (single batch): 0.7899053692817688\n",
      "\t Training loss (single batch): 1.5829658508300781\n",
      "##################################\n",
      "## EPOCH 785/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.044875979423523\n",
      "\t Training loss (single batch): 1.0497022867202759\n",
      "\t Training loss (single batch): 1.3618634939193726\n",
      "##################################\n",
      "## EPOCH 786/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9352232217788696\n",
      "\t Training loss (single batch): 0.9952807426452637\n",
      "\t Training loss (single batch): 1.5419617891311646\n",
      "##################################\n",
      "## EPOCH 787/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5533276200294495\n",
      "\t Training loss (single batch): 1.335930585861206\n",
      "\t Training loss (single batch): 1.0258517265319824\n",
      "##################################\n",
      "## EPOCH 788/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8208011388778687\n",
      "\t Training loss (single batch): 1.464313268661499\n",
      "\t Training loss (single batch): 1.4754295349121094\n",
      "##################################\n",
      "## EPOCH 789/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3393152952194214\n",
      "\t Training loss (single batch): 1.9990942478179932\n",
      "\t Training loss (single batch): 1.1941362619400024\n",
      "##################################\n",
      "## EPOCH 790/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.271347999572754\n",
      "\t Training loss (single batch): 0.7142747640609741\n",
      "\t Training loss (single batch): 1.6013448238372803\n",
      "##################################\n",
      "## EPOCH 791/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0919575691223145\n",
      "\t Training loss (single batch): 0.8691385388374329\n",
      "\t Training loss (single batch): 1.5049145221710205\n",
      "##################################\n",
      "## EPOCH 792/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6464991569519043\n",
      "\t Training loss (single batch): 0.809810996055603\n",
      "\t Training loss (single batch): 1.6836659908294678\n",
      "##################################\n",
      "## EPOCH 793/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.059869408607483\n",
      "\t Training loss (single batch): 1.0304052829742432\n",
      "\t Training loss (single batch): 1.4249827861785889\n",
      "##################################\n",
      "## EPOCH 794/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5981485843658447\n",
      "\t Training loss (single batch): 1.5722404718399048\n",
      "\t Training loss (single batch): 1.1316783428192139\n",
      "##################################\n",
      "## EPOCH 795/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8711442947387695\n",
      "\t Training loss (single batch): 1.0317374467849731\n",
      "\t Training loss (single batch): 1.7931606769561768\n",
      "##################################\n",
      "## EPOCH 796/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1348228454589844\n",
      "\t Training loss (single batch): 1.4247372150421143\n",
      "\t Training loss (single batch): 1.157275915145874\n",
      "##################################\n",
      "## EPOCH 797/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3160462379455566\n",
      "\t Training loss (single batch): 1.5288610458374023\n",
      "\t Training loss (single batch): 1.4802374839782715\n",
      "##################################\n",
      "## EPOCH 798/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2049603462219238\n",
      "\t Training loss (single batch): 1.1098741292953491\n",
      "\t Training loss (single batch): 1.230430245399475\n",
      "##################################\n",
      "## EPOCH 799/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3980211019515991\n",
      "\t Training loss (single batch): 1.715685248374939\n",
      "\t Training loss (single batch): 1.2313772439956665\n",
      "##################################\n",
      "## EPOCH 800/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8917086720466614\n",
      "\t Training loss (single batch): 1.27846360206604\n",
      "\t Training loss (single batch): 0.9994475841522217\n",
      "##################################\n",
      "## EPOCH 801/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7766178846359253\n",
      "\t Training loss (single batch): 0.979892909526825\n",
      "\t Training loss (single batch): 1.426393747329712\n",
      "##################################\n",
      "## EPOCH 802/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7276322841644287\n",
      "\t Training loss (single batch): 1.1951539516448975\n",
      "\t Training loss (single batch): 1.5557352304458618\n",
      "##################################\n",
      "## EPOCH 803/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6866375207901\n",
      "\t Training loss (single batch): 0.8614016771316528\n",
      "\t Training loss (single batch): 0.8068368434906006\n",
      "##################################\n",
      "## EPOCH 804/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.8503479957580566\n",
      "\t Training loss (single batch): 1.692591905593872\n",
      "\t Training loss (single batch): 0.9602347016334534\n",
      "##################################\n",
      "## EPOCH 805/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.910411536693573\n",
      "\t Training loss (single batch): 1.0275022983551025\n",
      "\t Training loss (single batch): 1.620790719985962\n",
      "##################################\n",
      "## EPOCH 806/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7759135961532593\n",
      "\t Training loss (single batch): 1.427569031715393\n",
      "\t Training loss (single batch): 1.2391308546066284\n",
      "##################################\n",
      "## EPOCH 807/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2447357177734375\n",
      "\t Training loss (single batch): 1.2798246145248413\n",
      "\t Training loss (single batch): 1.0899543762207031\n",
      "##################################\n",
      "## EPOCH 808/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1107591390609741\n",
      "\t Training loss (single batch): 1.0637322664260864\n",
      "\t Training loss (single batch): 1.2564977407455444\n",
      "##################################\n",
      "## EPOCH 809/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2470440864562988\n",
      "\t Training loss (single batch): 0.5997487902641296\n",
      "\t Training loss (single batch): 0.974386990070343\n",
      "##################################\n",
      "## EPOCH 810/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0523326396942139\n",
      "\t Training loss (single batch): 1.430756688117981\n",
      "\t Training loss (single batch): 2.0194759368896484\n",
      "##################################\n",
      "## EPOCH 811/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0191935300827026\n",
      "\t Training loss (single batch): 0.8836022019386292\n",
      "\t Training loss (single batch): 1.0072343349456787\n",
      "##################################\n",
      "## EPOCH 812/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3311444520950317\n",
      "\t Training loss (single batch): 0.8850654363632202\n",
      "\t Training loss (single batch): 0.9440838098526001\n",
      "##################################\n",
      "## EPOCH 813/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7920300364494324\n",
      "\t Training loss (single batch): 1.086753249168396\n",
      "\t Training loss (single batch): 0.8367881774902344\n",
      "##################################\n",
      "## EPOCH 814/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4162793159484863\n",
      "\t Training loss (single batch): 0.9256466627120972\n",
      "\t Training loss (single batch): 1.2283493280410767\n",
      "##################################\n",
      "## EPOCH 815/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2188549041748047\n",
      "\t Training loss (single batch): 0.9972872138023376\n",
      "\t Training loss (single batch): 0.8635474443435669\n",
      "##################################\n",
      "## EPOCH 816/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6450799703598022\n",
      "\t Training loss (single batch): 0.7544209957122803\n",
      "\t Training loss (single batch): 0.7281519770622253\n",
      "##################################\n",
      "## EPOCH 817/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5957668423652649\n",
      "\t Training loss (single batch): 0.9671413898468018\n",
      "\t Training loss (single batch): 1.2079006433486938\n",
      "##################################\n",
      "## EPOCH 818/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.838097333908081\n",
      "\t Training loss (single batch): 1.185738444328308\n",
      "\t Training loss (single batch): 1.42600679397583\n",
      "##################################\n",
      "## EPOCH 819/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.154162883758545\n",
      "\t Training loss (single batch): 1.0103020668029785\n",
      "\t Training loss (single batch): 1.551798701286316\n",
      "##################################\n",
      "## EPOCH 820/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 2.059626340866089\n",
      "\t Training loss (single batch): 1.5504690408706665\n",
      "\t Training loss (single batch): 0.8477233052253723\n",
      "##################################\n",
      "## EPOCH 821/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1208219528198242\n",
      "\t Training loss (single batch): 1.4338274002075195\n",
      "\t Training loss (single batch): 1.2515052556991577\n",
      "##################################\n",
      "## EPOCH 822/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3494397401809692\n",
      "\t Training loss (single batch): 1.0011497735977173\n",
      "\t Training loss (single batch): 0.9781526923179626\n",
      "##################################\n",
      "## EPOCH 823/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1947720050811768\n",
      "\t Training loss (single batch): 1.2456321716308594\n",
      "\t Training loss (single batch): 1.0713731050491333\n",
      "##################################\n",
      "## EPOCH 824/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8482740521430969\n",
      "\t Training loss (single batch): 0.988204836845398\n",
      "\t Training loss (single batch): 1.444004774093628\n",
      "##################################\n",
      "## EPOCH 825/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9414268136024475\n",
      "\t Training loss (single batch): 1.1362218856811523\n",
      "\t Training loss (single batch): 1.6307622194290161\n",
      "##################################\n",
      "## EPOCH 826/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1016653776168823\n",
      "\t Training loss (single batch): 0.9482442736625671\n",
      "\t Training loss (single batch): 1.7947865724563599\n",
      "##################################\n",
      "## EPOCH 827/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5410809516906738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0219945907592773\n",
      "\t Training loss (single batch): 1.0125222206115723\n",
      "##################################\n",
      "## EPOCH 828/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.7088907957077026\n",
      "\t Training loss (single batch): 0.7579843997955322\n",
      "\t Training loss (single batch): 0.991715669631958\n",
      "##################################\n",
      "## EPOCH 829/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2737884521484375\n",
      "\t Training loss (single batch): 0.5931850671768188\n",
      "\t Training loss (single batch): 1.0620110034942627\n",
      "##################################\n",
      "## EPOCH 830/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3206316232681274\n",
      "\t Training loss (single batch): 0.9627479314804077\n",
      "\t Training loss (single batch): 1.1268012523651123\n",
      "##################################\n",
      "## EPOCH 831/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.047833800315857\n",
      "\t Training loss (single batch): 1.4605859518051147\n",
      "\t Training loss (single batch): 1.4251315593719482\n",
      "##################################\n",
      "## EPOCH 832/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2490239143371582\n",
      "\t Training loss (single batch): 1.028869867324829\n",
      "\t Training loss (single batch): 1.1026768684387207\n",
      "##################################\n",
      "## EPOCH 833/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.91610187292099\n",
      "\t Training loss (single batch): 1.2497342824935913\n",
      "\t Training loss (single batch): 1.140054702758789\n",
      "##################################\n",
      "## EPOCH 834/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.491633653640747\n",
      "\t Training loss (single batch): 0.7912740707397461\n",
      "\t Training loss (single batch): 1.4150521755218506\n",
      "##################################\n",
      "## EPOCH 835/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3185715675354004\n",
      "\t Training loss (single batch): 0.6855498552322388\n",
      "\t Training loss (single batch): 1.085907220840454\n",
      "##################################\n",
      "## EPOCH 836/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1675938367843628\n",
      "\t Training loss (single batch): 1.0874536037445068\n",
      "\t Training loss (single batch): 1.056390643119812\n",
      "##################################\n",
      "## EPOCH 837/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8724561333656311\n",
      "\t Training loss (single batch): 1.0471868515014648\n",
      "\t Training loss (single batch): 1.0757533311843872\n",
      "##################################\n",
      "## EPOCH 838/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6594387292861938\n",
      "\t Training loss (single batch): 0.9998245239257812\n",
      "\t Training loss (single batch): 0.6625216603279114\n",
      "##################################\n",
      "## EPOCH 839/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.117253065109253\n",
      "\t Training loss (single batch): 1.0736945867538452\n",
      "\t Training loss (single batch): 1.5563746690750122\n",
      "##################################\n",
      "## EPOCH 840/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0886577367782593\n",
      "\t Training loss (single batch): 0.8628367185592651\n",
      "\t Training loss (single batch): 1.4374052286148071\n",
      "##################################\n",
      "## EPOCH 841/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.479560375213623\n",
      "\t Training loss (single batch): 1.3947356939315796\n",
      "\t Training loss (single batch): 0.6810349822044373\n",
      "##################################\n",
      "## EPOCH 842/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6941395998001099\n",
      "\t Training loss (single batch): 1.2661439180374146\n",
      "\t Training loss (single batch): 1.0922185182571411\n",
      "##################################\n",
      "## EPOCH 843/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2150816917419434\n",
      "\t Training loss (single batch): 0.5814186334609985\n",
      "\t Training loss (single batch): 1.5288702249526978\n",
      "##################################\n",
      "## EPOCH 844/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1175113916397095\n",
      "\t Training loss (single batch): 0.7975642681121826\n",
      "\t Training loss (single batch): 1.1059428453445435\n",
      "##################################\n",
      "## EPOCH 845/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9605574607849121\n",
      "\t Training loss (single batch): 0.9257615804672241\n",
      "\t Training loss (single batch): 0.9951511025428772\n",
      "##################################\n",
      "## EPOCH 846/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8141452670097351\n",
      "\t Training loss (single batch): 1.1647106409072876\n",
      "\t Training loss (single batch): 1.244832158088684\n",
      "##################################\n",
      "## EPOCH 847/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1825100183486938\n",
      "\t Training loss (single batch): 1.1801378726959229\n",
      "\t Training loss (single batch): 1.227687120437622\n",
      "##################################\n",
      "## EPOCH 848/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2091610431671143\n",
      "\t Training loss (single batch): 0.8640573620796204\n",
      "\t Training loss (single batch): 0.7596418857574463\n",
      "##################################\n",
      "## EPOCH 849/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4785560369491577\n",
      "\t Training loss (single batch): 0.577368438243866\n",
      "\t Training loss (single batch): 1.592171549797058\n",
      "##################################\n",
      "## EPOCH 850/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0840582847595215\n",
      "\t Training loss (single batch): 1.5002583265304565\n",
      "\t Training loss (single batch): 0.8989212512969971\n",
      "##################################\n",
      "## EPOCH 851/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0227245092391968\n",
      "\t Training loss (single batch): 0.8654216527938843\n",
      "\t Training loss (single batch): 1.0669740438461304\n",
      "##################################\n",
      "## EPOCH 852/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3410861492156982\n",
      "\t Training loss (single batch): 0.90218585729599\n",
      "\t Training loss (single batch): 1.385147213935852\n",
      "##################################\n",
      "## EPOCH 853/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3338550329208374\n",
      "\t Training loss (single batch): 0.4342390298843384\n",
      "\t Training loss (single batch): 0.9307662844657898\n",
      "##################################\n",
      "## EPOCH 854/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3483521938323975\n",
      "\t Training loss (single batch): 1.0174503326416016\n",
      "\t Training loss (single batch): 1.3101099729537964\n",
      "##################################\n",
      "## EPOCH 855/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1804993152618408\n",
      "\t Training loss (single batch): 1.099104642868042\n",
      "\t Training loss (single batch): 0.8633831143379211\n",
      "##################################\n",
      "## EPOCH 856/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8132343888282776\n",
      "\t Training loss (single batch): 0.830264151096344\n",
      "\t Training loss (single batch): 1.5722453594207764\n",
      "##################################\n",
      "## EPOCH 857/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.217972755432129\n",
      "\t Training loss (single batch): 0.7270152568817139\n",
      "\t Training loss (single batch): 1.2273826599121094\n",
      "##################################\n",
      "## EPOCH 858/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.478494644165039\n",
      "\t Training loss (single batch): 1.0119355916976929\n",
      "\t Training loss (single batch): 0.8707383871078491\n",
      "##################################\n",
      "## EPOCH 859/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.292616367340088\n",
      "\t Training loss (single batch): 0.9441243410110474\n",
      "\t Training loss (single batch): 0.6564726233482361\n",
      "##################################\n",
      "## EPOCH 860/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7884504199028015\n",
      "\t Training loss (single batch): 0.570400595664978\n",
      "\t Training loss (single batch): 0.9078893065452576\n",
      "##################################\n",
      "## EPOCH 861/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.54632568359375\n",
      "\t Training loss (single batch): 1.4182108640670776\n",
      "\t Training loss (single batch): 0.9283949732780457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "## EPOCH 862/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9642576575279236\n",
      "\t Training loss (single batch): 1.0502434968948364\n",
      "\t Training loss (single batch): 1.0777413845062256\n",
      "##################################\n",
      "## EPOCH 863/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.199897289276123\n",
      "\t Training loss (single batch): 0.479679673910141\n",
      "\t Training loss (single batch): 0.7418422102928162\n",
      "##################################\n",
      "## EPOCH 864/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0976381301879883\n",
      "\t Training loss (single batch): 0.9794875383377075\n",
      "\t Training loss (single batch): 1.4411487579345703\n",
      "##################################\n",
      "## EPOCH 865/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9660755395889282\n",
      "\t Training loss (single batch): 1.0467660427093506\n",
      "\t Training loss (single batch): 0.7135398983955383\n",
      "##################################\n",
      "## EPOCH 866/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6057082414627075\n",
      "\t Training loss (single batch): 1.6044660806655884\n",
      "\t Training loss (single batch): 0.8306192755699158\n",
      "##################################\n",
      "## EPOCH 867/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.063172698020935\n",
      "\t Training loss (single batch): 1.1754405498504639\n",
      "\t Training loss (single batch): 1.0476757287979126\n",
      "##################################\n",
      "## EPOCH 868/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2525867223739624\n",
      "\t Training loss (single batch): 0.6457839608192444\n",
      "\t Training loss (single batch): 1.5886889696121216\n",
      "##################################\n",
      "## EPOCH 869/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.6757169961929321\n",
      "\t Training loss (single batch): 0.4814491271972656\n",
      "\t Training loss (single batch): 1.0809763669967651\n",
      "##################################\n",
      "## EPOCH 870/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1498545408248901\n",
      "\t Training loss (single batch): 1.0934675931930542\n",
      "\t Training loss (single batch): 1.1977367401123047\n",
      "##################################\n",
      "## EPOCH 871/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.351747751235962\n",
      "\t Training loss (single batch): 1.0241819620132446\n",
      "\t Training loss (single batch): 2.0522210597991943\n",
      "##################################\n",
      "## EPOCH 872/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8865976929664612\n",
      "\t Training loss (single batch): 1.08059823513031\n",
      "\t Training loss (single batch): 1.1861971616744995\n",
      "##################################\n",
      "## EPOCH 873/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2651008367538452\n",
      "\t Training loss (single batch): 0.6382577419281006\n",
      "\t Training loss (single batch): 1.33781099319458\n",
      "##################################\n",
      "## EPOCH 874/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9759858846664429\n",
      "\t Training loss (single batch): 0.7718254327774048\n",
      "\t Training loss (single batch): 1.0844311714172363\n",
      "##################################\n",
      "## EPOCH 875/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.797926664352417\n",
      "\t Training loss (single batch): 1.2527904510498047\n",
      "\t Training loss (single batch): 1.3941501379013062\n",
      "##################################\n",
      "## EPOCH 876/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3458846807479858\n",
      "\t Training loss (single batch): 1.0407963991165161\n",
      "\t Training loss (single batch): 1.1084387302398682\n",
      "##################################\n",
      "## EPOCH 877/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7353745698928833\n",
      "\t Training loss (single batch): 0.5805626511573792\n",
      "\t Training loss (single batch): 0.964298665523529\n",
      "##################################\n",
      "## EPOCH 878/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1068474054336548\n",
      "\t Training loss (single batch): 0.7760096788406372\n",
      "\t Training loss (single batch): 0.8250372409820557\n",
      "##################################\n",
      "## EPOCH 879/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.4150848388671875\n",
      "\t Training loss (single batch): 0.7344018220901489\n",
      "\t Training loss (single batch): 0.9500241875648499\n",
      "##################################\n",
      "## EPOCH 880/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4995390772819519\n",
      "\t Training loss (single batch): 1.053658366203308\n",
      "\t Training loss (single batch): 0.9686360955238342\n",
      "##################################\n",
      "## EPOCH 881/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1623976230621338\n",
      "\t Training loss (single batch): 1.054565191268921\n",
      "\t Training loss (single batch): 0.8356111645698547\n",
      "##################################\n",
      "## EPOCH 882/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.86915123462677\n",
      "\t Training loss (single batch): 1.0857343673706055\n",
      "\t Training loss (single batch): 0.9314453601837158\n",
      "##################################\n",
      "## EPOCH 883/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0259320735931396\n",
      "\t Training loss (single batch): 0.6494694352149963\n",
      "\t Training loss (single batch): 0.909816324710846\n",
      "##################################\n",
      "## EPOCH 884/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0851515531539917\n",
      "\t Training loss (single batch): 0.5959808230400085\n",
      "\t Training loss (single batch): 1.3217402696609497\n",
      "##################################\n",
      "## EPOCH 885/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0078765153884888\n",
      "\t Training loss (single batch): 0.9599997997283936\n",
      "\t Training loss (single batch): 0.9304315447807312\n",
      "##################################\n",
      "## EPOCH 886/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0475850105285645\n",
      "\t Training loss (single batch): 0.8084056973457336\n",
      "\t Training loss (single batch): 1.1256911754608154\n",
      "##################################\n",
      "## EPOCH 887/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0139597654342651\n",
      "\t Training loss (single batch): 0.5255411267280579\n",
      "\t Training loss (single batch): 0.8667106628417969\n",
      "##################################\n",
      "## EPOCH 888/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1735754013061523\n",
      "\t Training loss (single batch): 0.5105730295181274\n",
      "\t Training loss (single batch): 0.8175038695335388\n",
      "##################################\n",
      "## EPOCH 889/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1386137008666992\n",
      "\t Training loss (single batch): 0.43610048294067383\n",
      "\t Training loss (single batch): 1.0818456411361694\n",
      "##################################\n",
      "## EPOCH 890/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.5138633251190186\n",
      "\t Training loss (single batch): 0.4731319546699524\n",
      "\t Training loss (single batch): 0.8982009291648865\n",
      "##################################\n",
      "## EPOCH 891/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6616629362106323\n",
      "\t Training loss (single batch): 0.6797529458999634\n",
      "\t Training loss (single batch): 1.1479219198226929\n",
      "##################################\n",
      "## EPOCH 892/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9188337326049805\n",
      "\t Training loss (single batch): 0.8204047679901123\n",
      "\t Training loss (single batch): 1.1233201026916504\n",
      "##################################\n",
      "## EPOCH 893/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8292840123176575\n",
      "\t Training loss (single batch): 1.0396416187286377\n",
      "\t Training loss (single batch): 1.4422863721847534\n",
      "##################################\n",
      "## EPOCH 894/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5066643953323364\n",
      "\t Training loss (single batch): 0.6404048800468445\n",
      "\t Training loss (single batch): 1.1333975791931152\n",
      "##################################\n",
      "## EPOCH 895/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8471335172653198\n",
      "\t Training loss (single batch): 0.9844905734062195\n",
      "\t Training loss (single batch): 1.2172961235046387\n",
      "##################################\n",
      "## EPOCH 896/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 1.0017677545547485\n",
      "\t Training loss (single batch): 0.6782599687576294\n",
      "\t Training loss (single batch): 1.4740432500839233\n",
      "##################################\n",
      "## EPOCH 897/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6852731704711914\n",
      "\t Training loss (single batch): 1.2824437618255615\n",
      "\t Training loss (single batch): 1.0292736291885376\n",
      "##################################\n",
      "## EPOCH 898/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9340311288833618\n",
      "\t Training loss (single batch): 1.002034068107605\n",
      "\t Training loss (single batch): 0.5809177160263062\n",
      "##################################\n",
      "## EPOCH 899/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1865880489349365\n",
      "\t Training loss (single batch): 0.9442340731620789\n",
      "\t Training loss (single batch): 0.5824597477912903\n",
      "##################################\n",
      "## EPOCH 900/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9547919034957886\n",
      "\t Training loss (single batch): 0.7168874144554138\n",
      "\t Training loss (single batch): 1.137865662574768\n",
      "##################################\n",
      "## EPOCH 901/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0064795017242432\n",
      "\t Training loss (single batch): 0.7802387475967407\n",
      "\t Training loss (single batch): 0.8296650052070618\n",
      "##################################\n",
      "## EPOCH 902/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8276006579399109\n",
      "\t Training loss (single batch): 1.0328218936920166\n",
      "\t Training loss (single batch): 0.7623213529586792\n",
      "##################################\n",
      "## EPOCH 903/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.225282907485962\n",
      "\t Training loss (single batch): 0.7398306131362915\n",
      "\t Training loss (single batch): 0.6668117642402649\n",
      "##################################\n",
      "## EPOCH 904/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2517825365066528\n",
      "\t Training loss (single batch): 0.8144708275794983\n",
      "\t Training loss (single batch): 0.8784957528114319\n",
      "##################################\n",
      "## EPOCH 905/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9218555092811584\n",
      "\t Training loss (single batch): 0.8807806968688965\n",
      "\t Training loss (single batch): 0.7561841607093811\n",
      "##################################\n",
      "## EPOCH 906/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5279259085655212\n",
      "\t Training loss (single batch): 0.5920435190200806\n",
      "\t Training loss (single batch): 1.1072373390197754\n",
      "##################################\n",
      "## EPOCH 907/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0285648107528687\n",
      "\t Training loss (single batch): 0.7901856899261475\n",
      "\t Training loss (single batch): 0.833719789981842\n",
      "##################################\n",
      "## EPOCH 908/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0588597059249878\n",
      "\t Training loss (single batch): 0.9193876385688782\n",
      "\t Training loss (single batch): 0.6586125493049622\n",
      "##################################\n",
      "## EPOCH 909/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.501592218875885\n",
      "\t Training loss (single batch): 0.8812704086303711\n",
      "\t Training loss (single batch): 0.7303944826126099\n",
      "##################################\n",
      "## EPOCH 910/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9409970641136169\n",
      "\t Training loss (single batch): 0.98714679479599\n",
      "\t Training loss (single batch): 1.2573370933532715\n",
      "##################################\n",
      "## EPOCH 911/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5195181369781494\n",
      "\t Training loss (single batch): 1.1416819095611572\n",
      "\t Training loss (single batch): 1.227560043334961\n",
      "##################################\n",
      "## EPOCH 912/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0387616157531738\n",
      "\t Training loss (single batch): 0.7739598155021667\n",
      "\t Training loss (single batch): 0.8372989892959595\n",
      "##################################\n",
      "## EPOCH 913/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5815114974975586\n",
      "\t Training loss (single batch): 0.5267975926399231\n",
      "\t Training loss (single batch): 0.33793362975120544\n",
      "##################################\n",
      "## EPOCH 914/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8962001800537109\n",
      "\t Training loss (single batch): 0.6865369081497192\n",
      "\t Training loss (single batch): 1.3699698448181152\n",
      "##################################\n",
      "## EPOCH 915/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0655426979064941\n",
      "\t Training loss (single batch): 0.8519192337989807\n",
      "\t Training loss (single batch): 1.2484811544418335\n",
      "##################################\n",
      "## EPOCH 916/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8564638495445251\n",
      "\t Training loss (single batch): 1.3465255498886108\n",
      "\t Training loss (single batch): 0.6297324299812317\n",
      "##################################\n",
      "## EPOCH 917/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5839728116989136\n",
      "\t Training loss (single batch): 0.758644163608551\n",
      "\t Training loss (single batch): 0.9984264373779297\n",
      "##################################\n",
      "## EPOCH 918/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6433303952217102\n",
      "\t Training loss (single batch): 0.9510051012039185\n",
      "\t Training loss (single batch): 0.7026699185371399\n",
      "##################################\n",
      "## EPOCH 919/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8236692547798157\n",
      "\t Training loss (single batch): 0.5826964974403381\n",
      "\t Training loss (single batch): 0.6971374154090881\n",
      "##################################\n",
      "## EPOCH 920/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.892717719078064\n",
      "\t Training loss (single batch): 0.6354706883430481\n",
      "\t Training loss (single batch): 0.9703992009162903\n",
      "##################################\n",
      "## EPOCH 921/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8604806661605835\n",
      "\t Training loss (single batch): 1.1730753183364868\n",
      "\t Training loss (single batch): 1.080255389213562\n",
      "##################################\n",
      "## EPOCH 922/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7201137542724609\n",
      "\t Training loss (single batch): 0.8150960803031921\n",
      "\t Training loss (single batch): 1.499035358428955\n",
      "##################################\n",
      "## EPOCH 923/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2730555534362793\n",
      "\t Training loss (single batch): 0.4865977168083191\n",
      "\t Training loss (single batch): 1.4626671075820923\n",
      "##################################\n",
      "## EPOCH 924/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8687148690223694\n",
      "\t Training loss (single batch): 0.4694206714630127\n",
      "\t Training loss (single batch): 1.5968166589736938\n",
      "##################################\n",
      "## EPOCH 925/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9846677780151367\n",
      "\t Training loss (single batch): 0.6074752807617188\n",
      "\t Training loss (single batch): 2.15214467048645\n",
      "##################################\n",
      "## EPOCH 926/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6534671783447266\n",
      "\t Training loss (single batch): 0.8536691665649414\n",
      "\t Training loss (single batch): 0.7550497055053711\n",
      "##################################\n",
      "## EPOCH 927/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7566823959350586\n",
      "\t Training loss (single batch): 0.8543704748153687\n",
      "\t Training loss (single batch): 1.8684604167938232\n",
      "##################################\n",
      "## EPOCH 928/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7784828543663025\n",
      "\t Training loss (single batch): 0.8077647089958191\n",
      "\t Training loss (single batch): 0.8440666198730469\n",
      "##################################\n",
      "## EPOCH 929/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.3402721881866455\n",
      "\t Training loss (single batch): 0.9445210099220276\n",
      "\t Training loss (single batch): 0.4400114417076111\n",
      "##################################\n",
      "## EPOCH 930/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7504721879959106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.696546733379364\n",
      "\t Training loss (single batch): 0.7589183449745178\n",
      "##################################\n",
      "## EPOCH 931/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7202404737472534\n",
      "\t Training loss (single batch): 0.9889467358589172\n",
      "\t Training loss (single batch): 1.2727348804473877\n",
      "##################################\n",
      "## EPOCH 932/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9630089998245239\n",
      "\t Training loss (single batch): 0.48543432354927063\n",
      "\t Training loss (single batch): 1.2490557432174683\n",
      "##################################\n",
      "## EPOCH 933/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5969839096069336\n",
      "\t Training loss (single batch): 0.8695554137229919\n",
      "\t Training loss (single batch): 1.3706223964691162\n",
      "##################################\n",
      "## EPOCH 934/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.118812084197998\n",
      "\t Training loss (single batch): 0.7057814002037048\n",
      "\t Training loss (single batch): 0.9372129440307617\n",
      "##################################\n",
      "## EPOCH 935/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1194977760314941\n",
      "\t Training loss (single batch): 0.8398874998092651\n",
      "\t Training loss (single batch): 0.8302949666976929\n",
      "##################################\n",
      "## EPOCH 936/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9311003684997559\n",
      "\t Training loss (single batch): 0.5186216831207275\n",
      "\t Training loss (single batch): 0.549582302570343\n",
      "##################################\n",
      "## EPOCH 937/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.039901614189148\n",
      "\t Training loss (single batch): 0.4139211177825928\n",
      "\t Training loss (single batch): 1.148181438446045\n",
      "##################################\n",
      "## EPOCH 938/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7348911166191101\n",
      "\t Training loss (single batch): 0.7391366958618164\n",
      "\t Training loss (single batch): 0.588910698890686\n",
      "##################################\n",
      "## EPOCH 939/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0390574932098389\n",
      "\t Training loss (single batch): 0.6135021448135376\n",
      "\t Training loss (single batch): 0.42661455273628235\n",
      "##################################\n",
      "## EPOCH 940/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0502957105636597\n",
      "\t Training loss (single batch): 1.308661699295044\n",
      "\t Training loss (single batch): 0.593612790107727\n",
      "##################################\n",
      "## EPOCH 941/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0138962268829346\n",
      "\t Training loss (single batch): 0.6374213695526123\n",
      "\t Training loss (single batch): 0.8628055453300476\n",
      "##################################\n",
      "## EPOCH 942/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0298302173614502\n",
      "\t Training loss (single batch): 1.0640658140182495\n",
      "\t Training loss (single batch): 0.4832209646701813\n",
      "##################################\n",
      "## EPOCH 943/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1301343441009521\n",
      "\t Training loss (single batch): 0.5730882883071899\n",
      "\t Training loss (single batch): 1.4192594289779663\n",
      "##################################\n",
      "## EPOCH 944/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7707486152648926\n",
      "\t Training loss (single batch): 0.5519794821739197\n",
      "\t Training loss (single batch): 0.8313421010971069\n",
      "##################################\n",
      "## EPOCH 945/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6866153478622437\n",
      "\t Training loss (single batch): 0.6212748885154724\n",
      "\t Training loss (single batch): 0.7422736287117004\n",
      "##################################\n",
      "## EPOCH 946/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7113147377967834\n",
      "\t Training loss (single batch): 0.7781956195831299\n",
      "\t Training loss (single batch): 0.7751365900039673\n",
      "##################################\n",
      "## EPOCH 947/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5979205369949341\n",
      "\t Training loss (single batch): 0.6872192025184631\n",
      "\t Training loss (single batch): 0.5214788317680359\n",
      "##################################\n",
      "## EPOCH 948/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.071912169456482\n",
      "\t Training loss (single batch): 0.453602135181427\n",
      "\t Training loss (single batch): 1.1399261951446533\n",
      "##################################\n",
      "## EPOCH 949/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0362268686294556\n",
      "\t Training loss (single batch): 1.0623037815093994\n",
      "\t Training loss (single batch): 0.4565184414386749\n",
      "##################################\n",
      "## EPOCH 950/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9454364776611328\n",
      "\t Training loss (single batch): 1.1163159608840942\n",
      "\t Training loss (single batch): 0.7652097940444946\n",
      "##################################\n",
      "## EPOCH 951/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.935181200504303\n",
      "\t Training loss (single batch): 0.4732200801372528\n",
      "\t Training loss (single batch): 0.8700934052467346\n",
      "##################################\n",
      "## EPOCH 952/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5907312035560608\n",
      "\t Training loss (single batch): 1.0616135597229004\n",
      "\t Training loss (single batch): 1.2335673570632935\n",
      "##################################\n",
      "## EPOCH 953/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6462290287017822\n",
      "\t Training loss (single batch): 0.8052180409431458\n",
      "\t Training loss (single batch): 0.6446020603179932\n",
      "##################################\n",
      "## EPOCH 954/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6727408170700073\n",
      "\t Training loss (single batch): 0.6838465332984924\n",
      "\t Training loss (single batch): 1.0371217727661133\n",
      "##################################\n",
      "## EPOCH 955/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.699550449848175\n",
      "\t Training loss (single batch): 0.8211599588394165\n",
      "\t Training loss (single batch): 0.6771878004074097\n",
      "##################################\n",
      "## EPOCH 956/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5255709886550903\n",
      "\t Training loss (single batch): 0.6902185678482056\n",
      "\t Training loss (single batch): 1.0303738117218018\n",
      "##################################\n",
      "## EPOCH 957/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7771735787391663\n",
      "\t Training loss (single batch): 0.662746787071228\n",
      "\t Training loss (single batch): 0.7950257658958435\n",
      "##################################\n",
      "## EPOCH 958/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0822709798812866\n",
      "\t Training loss (single batch): 1.0947455167770386\n",
      "\t Training loss (single batch): 1.1870204210281372\n",
      "##################################\n",
      "## EPOCH 959/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1511421203613281\n",
      "\t Training loss (single batch): 0.5113620162010193\n",
      "\t Training loss (single batch): 2.066068410873413\n",
      "##################################\n",
      "## EPOCH 960/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.2256300449371338\n",
      "\t Training loss (single batch): 0.5980637669563293\n",
      "\t Training loss (single batch): 0.9535785913467407\n",
      "##################################\n",
      "## EPOCH 961/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6695716381072998\n",
      "\t Training loss (single batch): 0.6787593364715576\n",
      "\t Training loss (single batch): 1.2615464925765991\n",
      "##################################\n",
      "## EPOCH 962/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.164062738418579\n",
      "\t Training loss (single batch): 0.8453579545021057\n",
      "\t Training loss (single batch): 0.396446168422699\n",
      "##################################\n",
      "## EPOCH 963/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8220975399017334\n",
      "\t Training loss (single batch): 0.9564328193664551\n",
      "\t Training loss (single batch): 1.1414543390274048\n",
      "##################################\n",
      "## EPOCH 964/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9473089575767517\n",
      "\t Training loss (single batch): 0.7535101771354675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.7366496920585632\n",
      "##################################\n",
      "## EPOCH 965/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9079659581184387\n",
      "\t Training loss (single batch): 0.9221750497817993\n",
      "\t Training loss (single batch): 0.5256601572036743\n",
      "##################################\n",
      "## EPOCH 966/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9452964067459106\n",
      "\t Training loss (single batch): 0.3396050035953522\n",
      "\t Training loss (single batch): 1.546496868133545\n",
      "##################################\n",
      "## EPOCH 967/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.1138206720352173\n",
      "\t Training loss (single batch): 0.7121904492378235\n",
      "\t Training loss (single batch): 0.6995582580566406\n",
      "##################################\n",
      "## EPOCH 968/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4195690155029297\n",
      "\t Training loss (single batch): 0.6566492319107056\n",
      "\t Training loss (single batch): 1.5941510200500488\n",
      "##################################\n",
      "## EPOCH 969/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7067058086395264\n",
      "\t Training loss (single batch): 0.6727643013000488\n",
      "\t Training loss (single batch): 0.9894540309906006\n",
      "##################################\n",
      "## EPOCH 970/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9025256037712097\n",
      "\t Training loss (single batch): 0.7052203416824341\n",
      "\t Training loss (single batch): 1.047386884689331\n",
      "##################################\n",
      "## EPOCH 971/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.9140669703483582\n",
      "\t Training loss (single batch): 0.9589555263519287\n",
      "\t Training loss (single batch): 0.47874563932418823\n",
      "##################################\n",
      "## EPOCH 972/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7410425543785095\n",
      "\t Training loss (single batch): 0.5446048974990845\n",
      "\t Training loss (single batch): 0.627045750617981\n",
      "##################################\n",
      "## EPOCH 973/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6661836504936218\n",
      "\t Training loss (single batch): 0.4687316417694092\n",
      "\t Training loss (single batch): 1.8569852113723755\n",
      "##################################\n",
      "## EPOCH 974/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7550052404403687\n",
      "\t Training loss (single batch): 0.4470624029636383\n",
      "\t Training loss (single batch): 1.3520671129226685\n",
      "##################################\n",
      "## EPOCH 975/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0802581310272217\n",
      "\t Training loss (single batch): 0.5404514670372009\n",
      "\t Training loss (single batch): 0.7534969449043274\n",
      "##################################\n",
      "## EPOCH 976/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 1.0570646524429321\n",
      "\t Training loss (single batch): 0.5444904565811157\n",
      "\t Training loss (single batch): 0.8321438431739807\n",
      "##################################\n",
      "## EPOCH 977/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5468387603759766\n",
      "\t Training loss (single batch): 0.3267728090286255\n",
      "\t Training loss (single batch): 0.8771734237670898\n",
      "##################################\n",
      "## EPOCH 978/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7296097278594971\n",
      "\t Training loss (single batch): 1.2224836349487305\n",
      "\t Training loss (single batch): 0.6221802234649658\n",
      "##################################\n",
      "## EPOCH 979/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8354729413986206\n",
      "\t Training loss (single batch): 0.6285098791122437\n",
      "\t Training loss (single batch): 0.6829198598861694\n",
      "##################################\n",
      "## EPOCH 980/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.879345178604126\n",
      "\t Training loss (single batch): 0.8527927398681641\n",
      "\t Training loss (single batch): 0.6439197063446045\n",
      "##################################\n",
      "## EPOCH 981/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.47281187772750854\n",
      "\t Training loss (single batch): 0.5009409189224243\n",
      "\t Training loss (single batch): 1.0108517408370972\n",
      "##################################\n",
      "## EPOCH 982/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8874146342277527\n",
      "\t Training loss (single batch): 0.6764965057373047\n",
      "\t Training loss (single batch): 0.6012583374977112\n",
      "##################################\n",
      "## EPOCH 983/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7211430072784424\n",
      "\t Training loss (single batch): 0.675577700138092\n",
      "\t Training loss (single batch): 0.7132915258407593\n",
      "##################################\n",
      "## EPOCH 984/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6358726620674133\n",
      "\t Training loss (single batch): 0.787147581577301\n",
      "\t Training loss (single batch): 0.5856619477272034\n",
      "##################################\n",
      "## EPOCH 985/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.6155273914337158\n",
      "\t Training loss (single batch): 0.6222920417785645\n",
      "\t Training loss (single batch): 0.7487977743148804\n",
      "##################################\n",
      "## EPOCH 986/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4555293917655945\n",
      "\t Training loss (single batch): 0.5847195386886597\n",
      "\t Training loss (single batch): 0.8961486220359802\n",
      "##################################\n",
      "## EPOCH 987/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4554237425327301\n",
      "\t Training loss (single batch): 0.4991017282009125\n",
      "\t Training loss (single batch): 0.6170353293418884\n",
      "##################################\n",
      "## EPOCH 988/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7267014384269714\n",
      "\t Training loss (single batch): 0.4254150390625\n",
      "\t Training loss (single batch): 0.6131930947303772\n",
      "##################################\n",
      "## EPOCH 989/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8008615374565125\n",
      "\t Training loss (single batch): 0.7901740670204163\n",
      "\t Training loss (single batch): 1.047834873199463\n",
      "##################################\n",
      "## EPOCH 990/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8544519543647766\n",
      "\t Training loss (single batch): 0.6696040034294128\n",
      "\t Training loss (single batch): 0.4710657000541687\n",
      "##################################\n",
      "## EPOCH 991/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5760599970817566\n",
      "\t Training loss (single batch): 0.6869425177574158\n",
      "\t Training loss (single batch): 0.7983690500259399\n",
      "##################################\n",
      "## EPOCH 992/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.31275153160095215\n",
      "\t Training loss (single batch): 0.35631459951400757\n",
      "\t Training loss (single batch): 0.8792354464530945\n",
      "##################################\n",
      "## EPOCH 993/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.85682213306427\n",
      "\t Training loss (single batch): 1.0705775022506714\n",
      "\t Training loss (single batch): 0.9526805877685547\n",
      "##################################\n",
      "## EPOCH 994/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.5983127951622009\n",
      "\t Training loss (single batch): 0.5239779949188232\n",
      "\t Training loss (single batch): 0.8343244791030884\n",
      "##################################\n",
      "## EPOCH 995/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.42359381914138794\n",
      "\t Training loss (single batch): 0.2987350821495056\n",
      "\t Training loss (single batch): 1.0208146572113037\n",
      "##################################\n",
      "## EPOCH 996/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7150052785873413\n",
      "\t Training loss (single batch): 0.46257278323173523\n",
      "\t Training loss (single batch): 0.3386131227016449\n",
      "##################################\n",
      "## EPOCH 997/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.4971323609352112\n",
      "\t Training loss (single batch): 1.1333560943603516\n",
      "\t Training loss (single batch): 0.9345722794532776\n",
      "##################################\n",
      "## EPOCH 998/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.8332691192626953\n",
      "\t Training loss (single batch): 0.20283257961273193\n",
      "\t Training loss (single batch): 0.5821318030357361\n",
      "##################################\n",
      "## EPOCH 999/1000\n",
      "##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training loss (single batch): 0.32562971115112305\n",
      "\t Training loss (single batch): 0.9431481957435608\n",
      "\t Training loss (single batch): 0.7876369953155518\n",
      "##################################\n",
      "## EPOCH 1000/1000\n",
      "##################################\n",
      "\t Training loss (single batch): 0.7334100008010864\n",
      "\t Training loss (single batch): 0.7262476086616516\n",
      "\t Training loss (single batch): 0.5707294344902039\n"
     ]
    }
   ],
   "source": [
    "#%% Train network\n",
    "\n",
    "# Define Dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=training_args['batchsize'], shuffle=True, num_workers=1)\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=5e-4)\n",
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Start training\n",
    "for epoch in range(int(training_args['num_epochs'])):\n",
    "    print('##################################')\n",
    "    print('## EPOCH '+str(epoch + 1)+\"/\"+str(int(training_args['num_epochs'])))\n",
    "    print('##################################')\n",
    "    # Iterate batches\n",
    "    for batch_sample in dataloader:\n",
    "        # Extract batch\n",
    "        batch_onehot = batch_sample['encoded_onehot'].to(device)\n",
    "        # Update network\n",
    "        batch_loss = train_batch(net, batch_onehot, loss_fn, optimizer)\n",
    "        print('\\t Training loss (single batch):', batch_loss)\n",
    "\n",
    "### Save all needed parameters\n",
    "# Create output dir\n",
    "out_dir = Path(training_args['out_dir'])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Save network parameters\n",
    "torch.save(net.state_dict(), out_dir / 'net_params.pth')\n",
    "\n",
    "# Adding alphabet length\n",
    "training_args[\"alphabet_len\"] = len(dataset.alphabet)\n",
    "# Save training parameters\n",
    "with open(out_dir / 'training_args.json', 'w') as f:\n",
    "    json.dump(training_args, f, indent=4)\n",
    "# Save encoder dictionary\n",
    "with open(out_dir / 'char_to_number.json', 'w') as f:\n",
    "    json.dump(dataset.char_to_number, f, indent=4)\n",
    "# Save decoder dictionary\n",
    "with open(out_dir / 'number_to_char.json', 'w') as f:\n",
    "    json.dump(dataset.number_to_char, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n",
      "dict_keys(['encoded_onehot'])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True, num_workers=1)\n",
    "for b in dataloader:\n",
    "    print(b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save all needed parameters\n",
    "# Create output dir\n",
    "out_dir = Path(training_args['out_dir'])\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Save network parameters\n",
    "torch.save(net.state_dict(), out_dir / 'net_params.pth')\n",
    "\n",
    "# Adding alphabet length\n",
    "training_args[\"alphabet_len\"] = len(dataset.alphabet)\n",
    "# Save training parameters\n",
    "with open(out_dir / 'training_args.json', 'w') as f:\n",
    "    json.dump(training_args, f, indent=4)\n",
    "# Save encoder dictionary\n",
    "with open(out_dir / 'char_to_number.json', 'w') as f:\n",
    "    json.dump(dataset.char_to_number, f, indent=4)\n",
    "# Save decoder dictionary\n",
    "with open(out_dir / 'number_to_char.json', 'w') as f:\n",
    "    json.dump(dataset.number_to_char, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "                   = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: model_Blake_2019-12-10_09:13\n",
      "burning tiger little lambe\n",
      "s,ade, hererat n wist lolen,\n",
      "and npghe gillad,wafw,bey iroen,\n",
      "in the bayadtawhe hey roarn,\n",
      "fitl woat dfskeog,\n",
      "leve yiad the ly is aud a the s and afd whot oread,\n",
      "du csangheef he wheakirilgs,\n",
      "and hath foren,\n",
      "wiu beekin the merrill.d,\n",
      "ezy onaptoeg theu nothen,, and hearttor iso ssark,\n",
      "fu mrce cislds,f, cton t ored ond hapt feep,\n",
      "iw e ghe sill thoue deasn,\n",
      "an the haghlene he desal.\n",
      "anvt bo if feev,\n",
      "and boslofu woagn deed,\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "##############################\n",
    "## PARAMETERS\n",
    "##############################\n",
    "#parser = argparse.ArgumentParser(description='Generate sonnet starting from a given text')\n",
    "\n",
    "#parser.add_argument('--sonnet_seed', type=str, default='the', help='Initial text of the sonnet')\n",
    "#parser.add_argument('--model_dir',   type=str, default='pretrained_models/model_3', help='Network model directory')\n",
    "\n",
    "##############################\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "### Parse input arguments\n",
    "#args = parser.parse_args()\n",
    "\n",
    "model_dir = Path(\"model_Blake_2019-12-03_22:22/\")\n",
    "sonnet_seed = \"burning tiger little lamb\"\n",
    "\n",
    "#%% Load training parameters\n",
    "model_dir = Path(\"model_Blake_2019-12-10_09:13\")\n",
    "print ('Loading model from: %s' % model_dir)\n",
    "training_args = json.load(open(model_dir / 'training_args.json'))\n",
    "\n",
    "#%% Load encoder and decoder dictionaries\n",
    "number_to_char = json.load(open(model_dir / 'number_to_char.json'))\n",
    "char_to_number = json.load(open(model_dir / 'char_to_number.json'))\n",
    "\n",
    "#%% Initialize network\n",
    "net = Network(input_size=training_args['alphabet_len'], \n",
    "            hidden_units=training_args['hidden_units'], \n",
    "            layers_num=training_args['layers_num'])\n",
    "\n",
    "#%% Load network trained parameters\n",
    "net.load_state_dict(torch.load(model_dir / 'net_params.pth', map_location='cpu'))\n",
    "net.eval() # Evaluation mode (e.g. disable dropout)\n",
    "\n",
    "#%% Find initial state of the RNN\n",
    "with torch.no_grad():\n",
    "    # Encode seed\n",
    "    seed_encoded = encode_text(char_to_number, sonnet_seed)\n",
    "    # One hot matrix\n",
    "    seed_onehot = create_one_hot_matrix(seed_encoded, training_args['alphabet_len'])\n",
    "    # To tensor\n",
    "    seed_onehot = torch.tensor(seed_onehot).float()\n",
    "    # Add batch axis\n",
    "    seed_onehot = seed_onehot.unsqueeze(0)\n",
    "    # Forward pass\n",
    "    net_out, net_state = net(seed_onehot)\n",
    "    # Get the most probable last output index\n",
    "    # ---------- sampling using softmax ----------\n",
    "    next_char_encoded = net_out[:, -1, :].argmax().item()\n",
    "    # Print the seed letters\n",
    "    print(sonnet_seed, end='', flush=True)\n",
    "    print(number_to_char[str(next_char_encoded)])\n",
    "\n",
    "#%% Generate sonnet\n",
    "new_line_count = 0\n",
    "tot_char_count = 0\n",
    "while True:\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # The new network input is the one hot encoding of the last chosen letter\n",
    "        net_input = create_one_hot_matrix([next_char_encoded], len(dataset.alphabet))\n",
    "        net_input = torch.tensor(net_input).float()\n",
    "        net_input = net_input.unsqueeze(0)\n",
    "        # Forward pass\n",
    "        net_out, net_state = net(net_input, net_state)\n",
    "        # Get the most probable letter index\n",
    "        distrib = np.array(nn.functional.softmax(net_out, dim=-1))\n",
    "        next_char_encoded = np.random.choice(len(distrib.ravel()), size=1, p=distrib.ravel())[0]\n",
    "        \n",
    "        #next_char_encoded = net_out.argmax().item()\n",
    "        \n",
    "        # Decode the letter\n",
    "        next_char = number_to_char[str(next_char_encoded)]\n",
    "        print(next_char, end='', flush=True)\n",
    "        # Count total letters\n",
    "        tot_char_count += 1\n",
    "        # Count new lines\n",
    "        if next_char == '\\n':\n",
    "            new_line_count += 1\n",
    "        # Break if 14 lines or 2000 letters\n",
    "        if new_line_count == 14 or tot_char_count > 2000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasetpath': 'Songs_of_innocence.txt', 'crop_len': 50, 'hidden_units': 400, 'layers_num': 2, 'dropout_prob': 0.1, 'batchsize': 16, 'num_epochs': 1000, 'out_dir': 'model', 'alphabet_len': 31}\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
