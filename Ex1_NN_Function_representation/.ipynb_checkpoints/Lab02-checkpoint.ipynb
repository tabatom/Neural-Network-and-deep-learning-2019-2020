{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "ICT FOR LIFE AND HEALTH - Department of Information Engineering\n",
    "PHYSICS OF DATA - Department of Physics and Astronomy\n",
    "COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY - Department of Psychology\n",
    "\n",
    "A.A. 2019/20 (6 CFU)\n",
    "Dr. Alberto Testolin, Dr. Federico Chiariotti\n",
    "\n",
    "Author: Dr. Matteo Gadaleta\n",
    "\n",
    "Lab. 02 - Linear regression with artificial neurons\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#%% Define the true model and generate some noisy samples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seed\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple quadratic model\n",
    "$ y = a + bx + cx^2 $\n",
    "\n",
    "$ a = -1.45, \\quad b = 1.12, \\quad c = 2.3 $\n",
    "\n",
    "$\\beta = (a,b,c)$\n",
    "\n",
    "$ y = \\beta \\cdot (1,x,x^2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Define a simple quadratic model\n",
    "# y = a + b * x + c * x^2\n",
    "# a = -1.45, b = 1.12, c = 2.3\n",
    "\n",
    "beta_true = [-1.45, 1.12, 2.3]\n",
    "\n",
    "def poly_model(x, beta):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "        x: x vector\n",
    "        beta: polynomial parameters\n",
    "        noise: enable noisy sampling\n",
    "    \"\"\"\n",
    "    pol_order = len(beta)\n",
    "    x_matrix = np.array([x**i for i in range(pol_order)]).transpose()\n",
    "    y_true = np.matmul(x_matrix, beta)\n",
    "    return y_true\n",
    "    \n",
    "### Generate 20 train points\n",
    "num_train_points = 10\n",
    "x_train = np.random.rand(num_train_points)\n",
    "y_train = poly_model(x_train, beta_true)\n",
    "noise = np.random.randn(len(y_train)) * 0.2\n",
    "y_train = y_train + noise\n",
    "\n",
    "### Generate 20 test points\n",
    "num_test_points = 10\n",
    "x_test = np.random.rand(num_test_points)\n",
    "y_test = poly_model(x_test, beta_true)\n",
    "noise = np.random.randn(len(y_test)) * 0.2\n",
    "y_test = y_test + noise\n",
    "\n",
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "x_highres = np.linspace(0,1,1000)\n",
    "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='g', ls='', marker='.', label='Test data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Activation function\n",
    "\n",
    "# Define activation function\n",
    "from scipy.special import expit\n",
    "act = expit\n",
    "# 1st derivative\n",
    "act_der = lambda x: act(x) * (1 - act(x))\n",
    "\n",
    "# Plot activation function\n",
    "x_plot = np.linspace(-5,5,1000)\n",
    "y_act = act(x_plot)\n",
    "y_act_der = act_der(x_plot)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_plot, y_act, label='Sigmoid function')\n",
    "plt.plot(x_plot, y_act_der, label='Sigmoid 1st derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Network class\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No):\n",
    "            \n",
    "        ### WEIGHT INITIALIZATION (Xavier)\n",
    "        # Initialize hidden weights and biases (layer 1)\n",
    "        Wh1 = (np.random.rand(Nh1, Ni) - 0.5) * np.sqrt(12 / (Nh1 + Ni))\n",
    "        Bh1 = np.zeros([Nh1, 1])\n",
    "        self.WBh1 = np.concatenate([Wh1, Bh1], 1) # Weight matrix including biases\n",
    "        # Initialize hidden weights and biases (layer 2)\n",
    "        Wh2 = (np.random.rand(Nh2, Nh1) - 0.5) * np.sqrt(12 / (Nh2 + Nh1))\n",
    "        Bh2 = np.zeros([Nh2, 1])\n",
    "        self.WBh2 = np.concatenate([Wh2, Bh2], 1) # Weight matrix including biases\n",
    "        # Initialize output weights and biases\n",
    "        Wo = (np.random.rand(No, Nh2) - 0.5) * np.sqrt(12 / (No + Nh2))\n",
    "        Bo = np.zeros([No, 1])\n",
    "        self.WBo = np.concatenate([Wo, Bo], 1) # Weight matrix including biases\n",
    "        \n",
    "        ### ACTIVATION FUNCTION\n",
    "        self.act = expit\n",
    "        self.act_der = lambda x: act(x) * (1 - act(x))\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        x = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(x, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        if additional_out:\n",
    "            return Y.squeeze(), Z2\n",
    "        \n",
    "        return Y.squeeze()\n",
    "        \n",
    "    def update(self, x, label, lr):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(X, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        # Evaluate the derivative terms\n",
    "        D1 = Y - label\n",
    "        D2 = Z2\n",
    "        D3 = self.WBo[:,:-1]\n",
    "        D4 = self.act_der(H2)\n",
    "        D5 = Z1\n",
    "        D6 = self.WBh2[:,:-1]\n",
    "        D7 = self.act_der(H1)\n",
    "        D8 = X\n",
    "        \n",
    "        # Layer Error\n",
    "        Eo = D1\n",
    "        Eh2 = np.matmul(Eo, D3) * D4\n",
    "        Eh1 = np.matmul(Eh2, D6) * D7\n",
    "        \n",
    "        \n",
    "        # Derivative for weight matrices\n",
    "        dWBo = np.matmul(Eo.reshape(-1,1), D2.reshape(1,-1))\n",
    "        dWBh2 = np.matmul(Eh2.reshape(-1,1), D5.reshape(1,-1))\n",
    "        dWBh1 = np.matmul(Eh1.reshape(-1,1), D8.reshape(1,-1))\n",
    "        \n",
    "        # Update the weights\n",
    "        self.WBh1 -= lr * dWBh1\n",
    "        self.WBh2 -= lr * dWBh2\n",
    "        self.WBo -= lr * dWBo\n",
    "        \n",
    "        # Evaluate loss function\n",
    "        loss = (Y - label)**2/2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def plot_weights(self):\n",
    "    \n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "        axs[0].hist(self.WBh1.flatten(), 20)\n",
    "        axs[1].hist(self.WBh2.flatten(), 50)\n",
    "        axs[2].hist(self.WBo.flatten(), 20)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st hidden layer weigth matrix shape: (200, 2)\n",
      "2nd hidden layer weigth matrix shape: (200, 201)\n",
      "Output layer weigth matrix shape: (1, 201)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PARAMETERS\n",
    "Ni = 1 # Number of inputs\n",
    "Nh1 = 200 # Number of hidden neurons (layer 1)\n",
    "Nh2 = 200 # Number of hidden neurons (layer 2)\n",
    "No = 1 # Number of outputs\n",
    "\n",
    "### Initialize network\n",
    "net = Network(Ni, Nh1, Nh2, No)\n",
    "\n",
    "# Access the class members\n",
    "print('1st hidden layer weigth matrix shape:', net.WBh1.shape)\n",
    "print('2nd hidden layer weigth matrix shape:', net.WBh2.shape)\n",
    "print('Output layer weigth matrix shape:', net.WBo.shape)\n",
    "\n",
    "# Plot weights\n",
    "plt.close('all')\n",
    "net.plot_weights()\n",
    "\n",
    "#%% FORWARD PASS (before training)\n",
    "\n",
    "# Define the x vector\n",
    "x_highres = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Evaluate the output for each input (this can be done as a batch, but for now let's do 1 input at a time)\n",
    "initial_net_output = []\n",
    "for x in x_highres:\n",
    "    net_out = net.forward(x)\n",
    "    initial_net_output.append(net_out)\n",
    "initial_net_output = np.array(initial_net_output)\n",
    "    \n",
    "# Or in just 1 line of pythonic code!!\n",
    "initial_net_output = np.array([net.forward(x) for x in x_highres])\n",
    "\n",
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "x_highres = np.linspace(0,1,1000)\n",
    "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_highres, initial_net_output, color='g', ls='--', label='Network output (random weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.: the first histogram is not uniform due to the fact that the weights are not uniform chosen (see lecture of 24/10/2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here parameters have to be set.\n",
    "\n",
    "N.B.: there are some parameters we do not know for now... Try to figure them out and launch the NN (about 20k epochs: maybe start with 5k)\n",
    "\n",
    "en_decay = flag (T or F) to enable the decay of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - lr: 0.09997 - Train loss: 30.53963 - Test loss: 0.61090\n",
      "Epoch 2 - lr: 0.09994 - Train loss: 0.72997 - Test loss: 0.41115\n",
      "Epoch 3 - lr: 0.09991 - Train loss: 0.67056 - Test loss: 0.38893\n",
      "Epoch 4 - lr: 0.09988 - Train loss: 0.67158 - Test loss: 0.38686\n",
      "Epoch 5 - lr: 0.09985 - Train loss: 0.67450 - Test loss: 0.38680\n",
      "Epoch 6 - lr: 0.09982 - Train loss: 0.67579 - Test loss: 0.38685\n",
      "Epoch 7 - lr: 0.09979 - Train loss: 0.67625 - Test loss: 0.38687\n",
      "Epoch 8 - lr: 0.09975 - Train loss: 0.67639 - Test loss: 0.38687\n",
      "Epoch 9 - lr: 0.09972 - Train loss: 0.67641 - Test loss: 0.38686\n",
      "Epoch 10 - lr: 0.09969 - Train loss: 0.67640 - Test loss: 0.38684\n",
      "Epoch 11 - lr: 0.09966 - Train loss: 0.67636 - Test loss: 0.38683\n",
      "Epoch 12 - lr: 0.09963 - Train loss: 0.67633 - Test loss: 0.38681\n",
      "Epoch 13 - lr: 0.09960 - Train loss: 0.67629 - Test loss: 0.38680\n",
      "Epoch 14 - lr: 0.09957 - Train loss: 0.67625 - Test loss: 0.38678\n",
      "Epoch 15 - lr: 0.09954 - Train loss: 0.67621 - Test loss: 0.38677\n",
      "Epoch 16 - lr: 0.09951 - Train loss: 0.67617 - Test loss: 0.38675\n",
      "Epoch 17 - lr: 0.09948 - Train loss: 0.67613 - Test loss: 0.38674\n",
      "Epoch 18 - lr: 0.09945 - Train loss: 0.67609 - Test loss: 0.38672\n",
      "Epoch 19 - lr: 0.09942 - Train loss: 0.67605 - Test loss: 0.38671\n",
      "Epoch 20 - lr: 0.09939 - Train loss: 0.67601 - Test loss: 0.38669\n",
      "Epoch 21 - lr: 0.09936 - Train loss: 0.67597 - Test loss: 0.38667\n",
      "Epoch 22 - lr: 0.09933 - Train loss: 0.67593 - Test loss: 0.38666\n",
      "Epoch 23 - lr: 0.09930 - Train loss: 0.67589 - Test loss: 0.38664\n",
      "Epoch 24 - lr: 0.09927 - Train loss: 0.67585 - Test loss: 0.38662\n",
      "Epoch 25 - lr: 0.09924 - Train loss: 0.67581 - Test loss: 0.38661\n",
      "Epoch 26 - lr: 0.09920 - Train loss: 0.67577 - Test loss: 0.38659\n",
      "Epoch 27 - lr: 0.09917 - Train loss: 0.67573 - Test loss: 0.38658\n",
      "Epoch 28 - lr: 0.09914 - Train loss: 0.67569 - Test loss: 0.38656\n",
      "Epoch 29 - lr: 0.09911 - Train loss: 0.67565 - Test loss: 0.38654\n",
      "Epoch 30 - lr: 0.09908 - Train loss: 0.67561 - Test loss: 0.38653\n",
      "Epoch 31 - lr: 0.09905 - Train loss: 0.67557 - Test loss: 0.38651\n",
      "Epoch 32 - lr: 0.09902 - Train loss: 0.67553 - Test loss: 0.38649\n",
      "Epoch 33 - lr: 0.09899 - Train loss: 0.67549 - Test loss: 0.38647\n",
      "Epoch 34 - lr: 0.09896 - Train loss: 0.67544 - Test loss: 0.38646\n",
      "Epoch 35 - lr: 0.09893 - Train loss: 0.67540 - Test loss: 0.38644\n",
      "Epoch 36 - lr: 0.09890 - Train loss: 0.67536 - Test loss: 0.38642\n",
      "Epoch 37 - lr: 0.09887 - Train loss: 0.67532 - Test loss: 0.38640\n",
      "Epoch 38 - lr: 0.09884 - Train loss: 0.67528 - Test loss: 0.38639\n",
      "Epoch 39 - lr: 0.09881 - Train loss: 0.67524 - Test loss: 0.38637\n",
      "Epoch 40 - lr: 0.09878 - Train loss: 0.67519 - Test loss: 0.38635\n",
      "Epoch 41 - lr: 0.09875 - Train loss: 0.67515 - Test loss: 0.38633\n",
      "Epoch 42 - lr: 0.09872 - Train loss: 0.67511 - Test loss: 0.38631\n",
      "Epoch 43 - lr: 0.09869 - Train loss: 0.67506 - Test loss: 0.38630\n",
      "Epoch 44 - lr: 0.09866 - Train loss: 0.67502 - Test loss: 0.38628\n",
      "Epoch 45 - lr: 0.09863 - Train loss: 0.67498 - Test loss: 0.38626\n",
      "Epoch 46 - lr: 0.09860 - Train loss: 0.67494 - Test loss: 0.38624\n",
      "Epoch 47 - lr: 0.09857 - Train loss: 0.67489 - Test loss: 0.38622\n",
      "Epoch 48 - lr: 0.09854 - Train loss: 0.67485 - Test loss: 0.38620\n",
      "Epoch 49 - lr: 0.09851 - Train loss: 0.67480 - Test loss: 0.38618\n",
      "Epoch 50 - lr: 0.09848 - Train loss: 0.67476 - Test loss: 0.38616\n",
      "Epoch 51 - lr: 0.09845 - Train loss: 0.67472 - Test loss: 0.38614\n",
      "Epoch 52 - lr: 0.09842 - Train loss: 0.67467 - Test loss: 0.38612\n",
      "Epoch 53 - lr: 0.09839 - Train loss: 0.67463 - Test loss: 0.38610\n",
      "Epoch 54 - lr: 0.09836 - Train loss: 0.67458 - Test loss: 0.38609\n",
      "Epoch 55 - lr: 0.09833 - Train loss: 0.67454 - Test loss: 0.38607\n",
      "Epoch 56 - lr: 0.09830 - Train loss: 0.67449 - Test loss: 0.38604\n",
      "Epoch 57 - lr: 0.09827 - Train loss: 0.67445 - Test loss: 0.38602\n",
      "Epoch 58 - lr: 0.09824 - Train loss: 0.67440 - Test loss: 0.38600\n",
      "Epoch 59 - lr: 0.09820 - Train loss: 0.67436 - Test loss: 0.38598\n",
      "Epoch 60 - lr: 0.09817 - Train loss: 0.67431 - Test loss: 0.38596\n",
      "Epoch 61 - lr: 0.09814 - Train loss: 0.67426 - Test loss: 0.38594\n",
      "Epoch 62 - lr: 0.09811 - Train loss: 0.67422 - Test loss: 0.38592\n",
      "Epoch 63 - lr: 0.09808 - Train loss: 0.67417 - Test loss: 0.38590\n",
      "Epoch 64 - lr: 0.09805 - Train loss: 0.67412 - Test loss: 0.38588\n",
      "Epoch 65 - lr: 0.09802 - Train loss: 0.67408 - Test loss: 0.38586\n",
      "Epoch 66 - lr: 0.09799 - Train loss: 0.67403 - Test loss: 0.38583\n",
      "Epoch 67 - lr: 0.09796 - Train loss: 0.67398 - Test loss: 0.38581\n",
      "Epoch 68 - lr: 0.09793 - Train loss: 0.67393 - Test loss: 0.38579\n",
      "Epoch 69 - lr: 0.09790 - Train loss: 0.67389 - Test loss: 0.38577\n",
      "Epoch 70 - lr: 0.09787 - Train loss: 0.67384 - Test loss: 0.38574\n",
      "Epoch 71 - lr: 0.09784 - Train loss: 0.67379 - Test loss: 0.38572\n",
      "Epoch 72 - lr: 0.09781 - Train loss: 0.67374 - Test loss: 0.38570\n",
      "Epoch 73 - lr: 0.09778 - Train loss: 0.67369 - Test loss: 0.38568\n",
      "Epoch 74 - lr: 0.09775 - Train loss: 0.67364 - Test loss: 0.38565\n",
      "Epoch 75 - lr: 0.09772 - Train loss: 0.67359 - Test loss: 0.38563\n",
      "Epoch 76 - lr: 0.09769 - Train loss: 0.67354 - Test loss: 0.38560\n",
      "Epoch 77 - lr: 0.09766 - Train loss: 0.67349 - Test loss: 0.38558\n",
      "Epoch 78 - lr: 0.09763 - Train loss: 0.67344 - Test loss: 0.38556\n",
      "Epoch 79 - lr: 0.09760 - Train loss: 0.67339 - Test loss: 0.38553\n",
      "Epoch 80 - lr: 0.09757 - Train loss: 0.67334 - Test loss: 0.38551\n",
      "Epoch 81 - lr: 0.09754 - Train loss: 0.67329 - Test loss: 0.38548\n",
      "Epoch 82 - lr: 0.09751 - Train loss: 0.67324 - Test loss: 0.38546\n",
      "Epoch 83 - lr: 0.09748 - Train loss: 0.67319 - Test loss: 0.38543\n",
      "Epoch 84 - lr: 0.09745 - Train loss: 0.67313 - Test loss: 0.38540\n",
      "Epoch 85 - lr: 0.09742 - Train loss: 0.67308 - Test loss: 0.38538\n",
      "Epoch 86 - lr: 0.09739 - Train loss: 0.67303 - Test loss: 0.38535\n",
      "Epoch 87 - lr: 0.09736 - Train loss: 0.67298 - Test loss: 0.38533\n",
      "Epoch 88 - lr: 0.09733 - Train loss: 0.67292 - Test loss: 0.38530\n",
      "Epoch 89 - lr: 0.09730 - Train loss: 0.67287 - Test loss: 0.38527\n",
      "Epoch 90 - lr: 0.09727 - Train loss: 0.67282 - Test loss: 0.38525\n",
      "Epoch 91 - lr: 0.09724 - Train loss: 0.67276 - Test loss: 0.38522\n",
      "Epoch 92 - lr: 0.09722 - Train loss: 0.67271 - Test loss: 0.38519\n",
      "Epoch 93 - lr: 0.09719 - Train loss: 0.67265 - Test loss: 0.38516\n",
      "Epoch 94 - lr: 0.09716 - Train loss: 0.67260 - Test loss: 0.38513\n",
      "Epoch 95 - lr: 0.09713 - Train loss: 0.67254 - Test loss: 0.38510\n",
      "Epoch 96 - lr: 0.09710 - Train loss: 0.67249 - Test loss: 0.38508\n",
      "Epoch 97 - lr: 0.09707 - Train loss: 0.67243 - Test loss: 0.38505\n",
      "Epoch 98 - lr: 0.09704 - Train loss: 0.67237 - Test loss: 0.38502\n",
      "Epoch 99 - lr: 0.09701 - Train loss: 0.67232 - Test loss: 0.38499\n",
      "Epoch 100 - lr: 0.09698 - Train loss: 0.67226 - Test loss: 0.38496\n",
      "Epoch 101 - lr: 0.09695 - Train loss: 0.67220 - Test loss: 0.38493\n",
      "Epoch 102 - lr: 0.09692 - Train loss: 0.67214 - Test loss: 0.38489\n",
      "Epoch 103 - lr: 0.09689 - Train loss: 0.67208 - Test loss: 0.38486\n",
      "Epoch 104 - lr: 0.09686 - Train loss: 0.67202 - Test loss: 0.38483\n",
      "Epoch 105 - lr: 0.09683 - Train loss: 0.67196 - Test loss: 0.38480\n",
      "Epoch 106 - lr: 0.09680 - Train loss: 0.67190 - Test loss: 0.38477\n",
      "Epoch 107 - lr: 0.09677 - Train loss: 0.67184 - Test loss: 0.38473\n",
      "Epoch 108 - lr: 0.09674 - Train loss: 0.67178 - Test loss: 0.38470\n",
      "Epoch 109 - lr: 0.09671 - Train loss: 0.67172 - Test loss: 0.38467\n",
      "Epoch 110 - lr: 0.09668 - Train loss: 0.67166 - Test loss: 0.38463\n",
      "Epoch 111 - lr: 0.09665 - Train loss: 0.67160 - Test loss: 0.38460\n",
      "Epoch 112 - lr: 0.09662 - Train loss: 0.67154 - Test loss: 0.38457\n",
      "Epoch 113 - lr: 0.09659 - Train loss: 0.67147 - Test loss: 0.38453\n",
      "Epoch 114 - lr: 0.09656 - Train loss: 0.67141 - Test loss: 0.38449\n",
      "Epoch 115 - lr: 0.09653 - Train loss: 0.67134 - Test loss: 0.38446\n",
      "Epoch 116 - lr: 0.09650 - Train loss: 0.67128 - Test loss: 0.38442\n",
      "Epoch 117 - lr: 0.09647 - Train loss: 0.67122 - Test loss: 0.38439\n",
      "Epoch 118 - lr: 0.09644 - Train loss: 0.67115 - Test loss: 0.38435\n",
      "Epoch 119 - lr: 0.09641 - Train loss: 0.67108 - Test loss: 0.38431\n",
      "Epoch 120 - lr: 0.09638 - Train loss: 0.67102 - Test loss: 0.38427\n",
      "Epoch 121 - lr: 0.09635 - Train loss: 0.67095 - Test loss: 0.38423\n",
      "Epoch 122 - lr: 0.09632 - Train loss: 0.67088 - Test loss: 0.38419\n",
      "Epoch 123 - lr: 0.09629 - Train loss: 0.67081 - Test loss: 0.38416\n",
      "Epoch 124 - lr: 0.09626 - Train loss: 0.67075 - Test loss: 0.38412\n",
      "Epoch 125 - lr: 0.09624 - Train loss: 0.67068 - Test loss: 0.38407\n",
      "Epoch 126 - lr: 0.09621 - Train loss: 0.67061 - Test loss: 0.38403\n",
      "Epoch 127 - lr: 0.09618 - Train loss: 0.67054 - Test loss: 0.38399\n",
      "Epoch 128 - lr: 0.09615 - Train loss: 0.67046 - Test loss: 0.38395\n",
      "Epoch 129 - lr: 0.09612 - Train loss: 0.67039 - Test loss: 0.38391\n",
      "Epoch 130 - lr: 0.09609 - Train loss: 0.67032 - Test loss: 0.38386\n",
      "Epoch 131 - lr: 0.09606 - Train loss: 0.67025 - Test loss: 0.38382\n",
      "Epoch 132 - lr: 0.09603 - Train loss: 0.67017 - Test loss: 0.38378\n",
      "Epoch 133 - lr: 0.09600 - Train loss: 0.67010 - Test loss: 0.38373\n",
      "Epoch 134 - lr: 0.09597 - Train loss: 0.67003 - Test loss: 0.38368\n",
      "Epoch 135 - lr: 0.09594 - Train loss: 0.66995 - Test loss: 0.38364\n",
      "Epoch 136 - lr: 0.09591 - Train loss: 0.66987 - Test loss: 0.38359\n",
      "Epoch 137 - lr: 0.09588 - Train loss: 0.66980 - Test loss: 0.38354\n",
      "Epoch 138 - lr: 0.09585 - Train loss: 0.66972 - Test loss: 0.38350\n",
      "Epoch 139 - lr: 0.09582 - Train loss: 0.66964 - Test loss: 0.38345\n",
      "Epoch 140 - lr: 0.09579 - Train loss: 0.66956 - Test loss: 0.38340\n",
      "Epoch 141 - lr: 0.09576 - Train loss: 0.66948 - Test loss: 0.38335\n",
      "Epoch 142 - lr: 0.09573 - Train loss: 0.66940 - Test loss: 0.38330\n",
      "Epoch 143 - lr: 0.09570 - Train loss: 0.66932 - Test loss: 0.38324\n",
      "Epoch 144 - lr: 0.09568 - Train loss: 0.66924 - Test loss: 0.38319\n",
      "Epoch 145 - lr: 0.09565 - Train loss: 0.66916 - Test loss: 0.38314\n",
      "Epoch 146 - lr: 0.09562 - Train loss: 0.66907 - Test loss: 0.38309\n",
      "Epoch 147 - lr: 0.09559 - Train loss: 0.66899 - Test loss: 0.38303\n",
      "Epoch 148 - lr: 0.09556 - Train loss: 0.66890 - Test loss: 0.38298\n",
      "Epoch 149 - lr: 0.09553 - Train loss: 0.66882 - Test loss: 0.38292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 - lr: 0.09550 - Train loss: 0.66873 - Test loss: 0.38286\n",
      "Epoch 151 - lr: 0.09547 - Train loss: 0.66864 - Test loss: 0.38280\n",
      "Epoch 152 - lr: 0.09544 - Train loss: 0.66855 - Test loss: 0.38274\n",
      "Epoch 153 - lr: 0.09541 - Train loss: 0.66846 - Test loss: 0.38269\n",
      "Epoch 154 - lr: 0.09538 - Train loss: 0.66837 - Test loss: 0.38262\n",
      "Epoch 155 - lr: 0.09535 - Train loss: 0.66828 - Test loss: 0.38256\n",
      "Epoch 156 - lr: 0.09532 - Train loss: 0.66819 - Test loss: 0.38250\n",
      "Epoch 157 - lr: 0.09529 - Train loss: 0.66810 - Test loss: 0.38244\n",
      "Epoch 158 - lr: 0.09526 - Train loss: 0.66800 - Test loss: 0.38237\n",
      "Epoch 159 - lr: 0.09524 - Train loss: 0.66791 - Test loss: 0.38231\n",
      "Epoch 160 - lr: 0.09521 - Train loss: 0.66781 - Test loss: 0.38224\n",
      "Epoch 161 - lr: 0.09518 - Train loss: 0.66771 - Test loss: 0.38217\n",
      "Epoch 162 - lr: 0.09515 - Train loss: 0.66761 - Test loss: 0.38211\n",
      "Epoch 163 - lr: 0.09512 - Train loss: 0.66752 - Test loss: 0.38204\n",
      "Epoch 164 - lr: 0.09509 - Train loss: 0.66741 - Test loss: 0.38197\n",
      "Epoch 165 - lr: 0.09506 - Train loss: 0.66731 - Test loss: 0.38189\n",
      "Epoch 166 - lr: 0.09503 - Train loss: 0.66721 - Test loss: 0.38182\n",
      "Epoch 167 - lr: 0.09500 - Train loss: 0.66711 - Test loss: 0.38175\n",
      "Epoch 168 - lr: 0.09497 - Train loss: 0.66700 - Test loss: 0.38167\n",
      "Epoch 169 - lr: 0.09494 - Train loss: 0.66690 - Test loss: 0.38159\n",
      "Epoch 170 - lr: 0.09491 - Train loss: 0.66679 - Test loss: 0.38152\n",
      "Epoch 171 - lr: 0.09489 - Train loss: 0.66668 - Test loss: 0.38144\n",
      "Epoch 172 - lr: 0.09486 - Train loss: 0.66657 - Test loss: 0.38136\n",
      "Epoch 173 - lr: 0.09483 - Train loss: 0.66646 - Test loss: 0.38128\n",
      "Epoch 174 - lr: 0.09480 - Train loss: 0.66635 - Test loss: 0.38119\n",
      "Epoch 175 - lr: 0.09477 - Train loss: 0.66624 - Test loss: 0.38111\n",
      "Epoch 176 - lr: 0.09474 - Train loss: 0.66612 - Test loss: 0.38102\n",
      "Epoch 177 - lr: 0.09471 - Train loss: 0.66601 - Test loss: 0.38094\n",
      "Epoch 178 - lr: 0.09468 - Train loss: 0.66589 - Test loss: 0.38085\n",
      "Epoch 179 - lr: 0.09465 - Train loss: 0.66577 - Test loss: 0.38076\n",
      "Epoch 180 - lr: 0.09462 - Train loss: 0.66565 - Test loss: 0.38067\n",
      "Epoch 181 - lr: 0.09459 - Train loss: 0.66553 - Test loss: 0.38057\n",
      "Epoch 182 - lr: 0.09457 - Train loss: 0.66541 - Test loss: 0.38048\n",
      "Epoch 183 - lr: 0.09454 - Train loss: 0.66528 - Test loss: 0.38038\n",
      "Epoch 184 - lr: 0.09451 - Train loss: 0.66516 - Test loss: 0.38028\n",
      "Epoch 185 - lr: 0.09448 - Train loss: 0.66503 - Test loss: 0.38018\n",
      "Epoch 186 - lr: 0.09445 - Train loss: 0.66490 - Test loss: 0.38008\n",
      "Epoch 187 - lr: 0.09442 - Train loss: 0.66477 - Test loss: 0.37998\n",
      "Epoch 188 - lr: 0.09439 - Train loss: 0.66464 - Test loss: 0.37988\n",
      "Epoch 189 - lr: 0.09436 - Train loss: 0.66451 - Test loss: 0.37977\n",
      "Epoch 190 - lr: 0.09433 - Train loss: 0.66437 - Test loss: 0.37966\n",
      "Epoch 191 - lr: 0.09430 - Train loss: 0.66424 - Test loss: 0.37955\n",
      "Epoch 192 - lr: 0.09428 - Train loss: 0.66410 - Test loss: 0.37944\n",
      "Epoch 193 - lr: 0.09425 - Train loss: 0.66396 - Test loss: 0.37932\n",
      "Epoch 194 - lr: 0.09422 - Train loss: 0.66382 - Test loss: 0.37921\n",
      "Epoch 195 - lr: 0.09419 - Train loss: 0.66368 - Test loss: 0.37909\n",
      "Epoch 196 - lr: 0.09416 - Train loss: 0.66353 - Test loss: 0.37897\n",
      "Epoch 197 - lr: 0.09413 - Train loss: 0.66338 - Test loss: 0.37885\n",
      "Epoch 198 - lr: 0.09410 - Train loss: 0.66324 - Test loss: 0.37872\n",
      "Epoch 199 - lr: 0.09407 - Train loss: 0.66309 - Test loss: 0.37860\n",
      "Epoch 200 - lr: 0.09404 - Train loss: 0.66293 - Test loss: 0.37847\n",
      "Epoch 201 - lr: 0.09402 - Train loss: 0.66278 - Test loss: 0.37834\n",
      "Epoch 202 - lr: 0.09399 - Train loss: 0.66263 - Test loss: 0.37820\n",
      "Epoch 203 - lr: 0.09396 - Train loss: 0.66247 - Test loss: 0.37807\n",
      "Epoch 204 - lr: 0.09393 - Train loss: 0.66231 - Test loss: 0.37793\n",
      "Epoch 205 - lr: 0.09390 - Train loss: 0.66215 - Test loss: 0.37779\n",
      "Epoch 206 - lr: 0.09387 - Train loss: 0.66198 - Test loss: 0.37765\n",
      "Epoch 207 - lr: 0.09384 - Train loss: 0.66182 - Test loss: 0.37750\n",
      "Epoch 208 - lr: 0.09381 - Train loss: 0.66165 - Test loss: 0.37735\n",
      "Epoch 209 - lr: 0.09378 - Train loss: 0.66148 - Test loss: 0.37720\n",
      "Epoch 210 - lr: 0.09376 - Train loss: 0.66131 - Test loss: 0.37705\n",
      "Epoch 211 - lr: 0.09373 - Train loss: 0.66113 - Test loss: 0.37689\n",
      "Epoch 212 - lr: 0.09370 - Train loss: 0.66096 - Test loss: 0.37673\n",
      "Epoch 213 - lr: 0.09367 - Train loss: 0.66078 - Test loss: 0.37657\n",
      "Epoch 214 - lr: 0.09364 - Train loss: 0.66060 - Test loss: 0.37641\n",
      "Epoch 215 - lr: 0.09361 - Train loss: 0.66042 - Test loss: 0.37624\n",
      "Epoch 216 - lr: 0.09358 - Train loss: 0.66023 - Test loss: 0.37607\n",
      "Epoch 217 - lr: 0.09355 - Train loss: 0.66004 - Test loss: 0.37590\n",
      "Epoch 218 - lr: 0.09353 - Train loss: 0.65985 - Test loss: 0.37572\n",
      "Epoch 219 - lr: 0.09350 - Train loss: 0.65966 - Test loss: 0.37554\n",
      "Epoch 220 - lr: 0.09347 - Train loss: 0.65946 - Test loss: 0.37536\n",
      "Epoch 221 - lr: 0.09344 - Train loss: 0.65926 - Test loss: 0.37517\n",
      "Epoch 222 - lr: 0.09341 - Train loss: 0.65906 - Test loss: 0.37498\n",
      "Epoch 223 - lr: 0.09338 - Train loss: 0.65886 - Test loss: 0.37479\n",
      "Epoch 224 - lr: 0.09335 - Train loss: 0.65865 - Test loss: 0.37459\n",
      "Epoch 225 - lr: 0.09333 - Train loss: 0.65845 - Test loss: 0.37440\n",
      "Epoch 226 - lr: 0.09330 - Train loss: 0.65823 - Test loss: 0.37419\n",
      "Epoch 227 - lr: 0.09327 - Train loss: 0.65802 - Test loss: 0.37399\n",
      "Epoch 228 - lr: 0.09324 - Train loss: 0.65780 - Test loss: 0.37378\n",
      "Epoch 229 - lr: 0.09321 - Train loss: 0.65758 - Test loss: 0.37357\n",
      "Epoch 230 - lr: 0.09318 - Train loss: 0.65736 - Test loss: 0.37335\n",
      "Epoch 231 - lr: 0.09315 - Train loss: 0.65713 - Test loss: 0.37313\n",
      "Epoch 232 - lr: 0.09313 - Train loss: 0.65690 - Test loss: 0.37290\n",
      "Epoch 233 - lr: 0.09310 - Train loss: 0.65667 - Test loss: 0.37268\n",
      "Epoch 234 - lr: 0.09307 - Train loss: 0.65644 - Test loss: 0.37244\n",
      "Epoch 235 - lr: 0.09304 - Train loss: 0.65620 - Test loss: 0.37221\n",
      "Epoch 236 - lr: 0.09301 - Train loss: 0.65596 - Test loss: 0.37197\n",
      "Epoch 237 - lr: 0.09298 - Train loss: 0.65571 - Test loss: 0.37172\n",
      "Epoch 238 - lr: 0.09295 - Train loss: 0.65546 - Test loss: 0.37148\n",
      "Epoch 239 - lr: 0.09293 - Train loss: 0.65521 - Test loss: 0.37123\n",
      "Epoch 240 - lr: 0.09290 - Train loss: 0.65496 - Test loss: 0.37097\n",
      "Epoch 241 - lr: 0.09287 - Train loss: 0.65470 - Test loss: 0.37071\n",
      "Epoch 242 - lr: 0.09284 - Train loss: 0.65443 - Test loss: 0.37044\n",
      "Epoch 243 - lr: 0.09281 - Train loss: 0.65417 - Test loss: 0.37018\n",
      "Epoch 244 - lr: 0.09278 - Train loss: 0.65390 - Test loss: 0.36990\n",
      "Epoch 245 - lr: 0.09275 - Train loss: 0.65362 - Test loss: 0.36962\n",
      "Epoch 246 - lr: 0.09273 - Train loss: 0.65335 - Test loss: 0.36934\n",
      "Epoch 247 - lr: 0.09270 - Train loss: 0.65306 - Test loss: 0.36906\n",
      "Epoch 248 - lr: 0.09267 - Train loss: 0.65278 - Test loss: 0.36876\n",
      "Epoch 249 - lr: 0.09264 - Train loss: 0.65249 - Test loss: 0.36847\n",
      "Epoch 250 - lr: 0.09261 - Train loss: 0.65219 - Test loss: 0.36817\n",
      "Epoch 251 - lr: 0.09258 - Train loss: 0.65189 - Test loss: 0.36786\n",
      "Epoch 252 - lr: 0.09256 - Train loss: 0.65159 - Test loss: 0.36755\n",
      "Epoch 253 - lr: 0.09253 - Train loss: 0.65128 - Test loss: 0.36724\n",
      "Epoch 254 - lr: 0.09250 - Train loss: 0.65097 - Test loss: 0.36692\n",
      "Epoch 255 - lr: 0.09247 - Train loss: 0.65065 - Test loss: 0.36659\n",
      "Epoch 256 - lr: 0.09244 - Train loss: 0.65033 - Test loss: 0.36626\n",
      "Epoch 257 - lr: 0.09241 - Train loss: 0.65000 - Test loss: 0.36593\n",
      "Epoch 258 - lr: 0.09238 - Train loss: 0.64967 - Test loss: 0.36559\n",
      "Epoch 259 - lr: 0.09236 - Train loss: 0.64933 - Test loss: 0.36525\n",
      "Epoch 260 - lr: 0.09233 - Train loss: 0.64899 - Test loss: 0.36490\n",
      "Epoch 261 - lr: 0.09230 - Train loss: 0.64864 - Test loss: 0.36454\n",
      "Epoch 262 - lr: 0.09227 - Train loss: 0.64829 - Test loss: 0.36418\n",
      "Epoch 263 - lr: 0.09224 - Train loss: 0.64793 - Test loss: 0.36381\n",
      "Epoch 264 - lr: 0.09221 - Train loss: 0.64757 - Test loss: 0.36344\n",
      "Epoch 265 - lr: 0.09219 - Train loss: 0.64720 - Test loss: 0.36307\n",
      "Epoch 266 - lr: 0.09216 - Train loss: 0.64682 - Test loss: 0.36268\n",
      "Epoch 267 - lr: 0.09213 - Train loss: 0.64644 - Test loss: 0.36230\n",
      "Epoch 268 - lr: 0.09210 - Train loss: 0.64605 - Test loss: 0.36190\n",
      "Epoch 269 - lr: 0.09207 - Train loss: 0.64565 - Test loss: 0.36150\n",
      "Epoch 270 - lr: 0.09204 - Train loss: 0.64525 - Test loss: 0.36110\n",
      "Epoch 271 - lr: 0.09202 - Train loss: 0.64484 - Test loss: 0.36069\n",
      "Epoch 272 - lr: 0.09199 - Train loss: 0.64443 - Test loss: 0.36027\n",
      "Epoch 273 - lr: 0.09196 - Train loss: 0.64400 - Test loss: 0.35985\n",
      "Epoch 274 - lr: 0.09193 - Train loss: 0.64357 - Test loss: 0.35942\n",
      "Epoch 275 - lr: 0.09190 - Train loss: 0.64314 - Test loss: 0.35899\n",
      "Epoch 276 - lr: 0.09188 - Train loss: 0.64269 - Test loss: 0.35855\n",
      "Epoch 277 - lr: 0.09185 - Train loss: 0.64224 - Test loss: 0.35810\n",
      "Epoch 278 - lr: 0.09182 - Train loss: 0.64178 - Test loss: 0.35765\n",
      "Epoch 279 - lr: 0.09179 - Train loss: 0.64131 - Test loss: 0.35719\n",
      "Epoch 280 - lr: 0.09176 - Train loss: 0.64084 - Test loss: 0.35673\n",
      "Epoch 281 - lr: 0.09173 - Train loss: 0.64035 - Test loss: 0.35626\n",
      "Epoch 282 - lr: 0.09171 - Train loss: 0.63986 - Test loss: 0.35578\n",
      "Epoch 283 - lr: 0.09168 - Train loss: 0.63936 - Test loss: 0.35530\n",
      "Epoch 284 - lr: 0.09165 - Train loss: 0.63885 - Test loss: 0.35481\n",
      "Epoch 285 - lr: 0.09162 - Train loss: 0.63833 - Test loss: 0.35431\n",
      "Epoch 286 - lr: 0.09159 - Train loss: 0.63780 - Test loss: 0.35381\n",
      "Epoch 287 - lr: 0.09157 - Train loss: 0.63726 - Test loss: 0.35330\n",
      "Epoch 288 - lr: 0.09154 - Train loss: 0.63671 - Test loss: 0.35278\n",
      "Epoch 289 - lr: 0.09151 - Train loss: 0.63615 - Test loss: 0.35226\n",
      "Epoch 290 - lr: 0.09148 - Train loss: 0.63558 - Test loss: 0.35173\n",
      "Epoch 291 - lr: 0.09145 - Train loss: 0.63500 - Test loss: 0.35119\n",
      "Epoch 292 - lr: 0.09143 - Train loss: 0.63441 - Test loss: 0.35065\n",
      "Epoch 293 - lr: 0.09140 - Train loss: 0.63381 - Test loss: 0.35010\n",
      "Epoch 294 - lr: 0.09137 - Train loss: 0.63319 - Test loss: 0.34955\n",
      "Epoch 295 - lr: 0.09134 - Train loss: 0.63257 - Test loss: 0.34899\n",
      "Epoch 296 - lr: 0.09131 - Train loss: 0.63193 - Test loss: 0.34842\n",
      "Epoch 297 - lr: 0.09129 - Train loss: 0.63128 - Test loss: 0.34784\n",
      "Epoch 298 - lr: 0.09126 - Train loss: 0.63062 - Test loss: 0.34726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 - lr: 0.09123 - Train loss: 0.62994 - Test loss: 0.34667\n",
      "Epoch 300 - lr: 0.09120 - Train loss: 0.62925 - Test loss: 0.34607\n",
      "Epoch 301 - lr: 0.09117 - Train loss: 0.62854 - Test loss: 0.34547\n",
      "Epoch 302 - lr: 0.09115 - Train loss: 0.62782 - Test loss: 0.34485\n",
      "Epoch 303 - lr: 0.09112 - Train loss: 0.62709 - Test loss: 0.34424\n",
      "Epoch 304 - lr: 0.09109 - Train loss: 0.62634 - Test loss: 0.34361\n",
      "Epoch 305 - lr: 0.09106 - Train loss: 0.62558 - Test loss: 0.34298\n",
      "Epoch 306 - lr: 0.09103 - Train loss: 0.62479 - Test loss: 0.34234\n",
      "Epoch 307 - lr: 0.09101 - Train loss: 0.62400 - Test loss: 0.34170\n",
      "Epoch 308 - lr: 0.09098 - Train loss: 0.62318 - Test loss: 0.34104\n",
      "Epoch 309 - lr: 0.09095 - Train loss: 0.62235 - Test loss: 0.34038\n",
      "Epoch 310 - lr: 0.09092 - Train loss: 0.62150 - Test loss: 0.33972\n",
      "Epoch 311 - lr: 0.09089 - Train loss: 0.62063 - Test loss: 0.33904\n",
      "Epoch 312 - lr: 0.09087 - Train loss: 0.61974 - Test loss: 0.33836\n",
      "Epoch 313 - lr: 0.09084 - Train loss: 0.61883 - Test loss: 0.33768\n",
      "Epoch 314 - lr: 0.09081 - Train loss: 0.61790 - Test loss: 0.33699\n",
      "Epoch 315 - lr: 0.09078 - Train loss: 0.61695 - Test loss: 0.33629\n",
      "Epoch 316 - lr: 0.09075 - Train loss: 0.61598 - Test loss: 0.33558\n",
      "Epoch 317 - lr: 0.09073 - Train loss: 0.61499 - Test loss: 0.33487\n",
      "Epoch 318 - lr: 0.09070 - Train loss: 0.61397 - Test loss: 0.33415\n",
      "Epoch 319 - lr: 0.09067 - Train loss: 0.61293 - Test loss: 0.33343\n",
      "Epoch 320 - lr: 0.09064 - Train loss: 0.61187 - Test loss: 0.33269\n",
      "Epoch 321 - lr: 0.09061 - Train loss: 0.61078 - Test loss: 0.33196\n",
      "Epoch 322 - lr: 0.09059 - Train loss: 0.60966 - Test loss: 0.33122\n",
      "Epoch 323 - lr: 0.09056 - Train loss: 0.60852 - Test loss: 0.33047\n",
      "Epoch 324 - lr: 0.09053 - Train loss: 0.60735 - Test loss: 0.32972\n",
      "Epoch 325 - lr: 0.09050 - Train loss: 0.60615 - Test loss: 0.32896\n",
      "Epoch 326 - lr: 0.09048 - Train loss: 0.60492 - Test loss: 0.32820\n",
      "Epoch 327 - lr: 0.09045 - Train loss: 0.60366 - Test loss: 0.32743\n",
      "Epoch 328 - lr: 0.09042 - Train loss: 0.60238 - Test loss: 0.32666\n",
      "Epoch 329 - lr: 0.09039 - Train loss: 0.60105 - Test loss: 0.32589\n",
      "Epoch 330 - lr: 0.09036 - Train loss: 0.59970 - Test loss: 0.32511\n",
      "Epoch 331 - lr: 0.09034 - Train loss: 0.59831 - Test loss: 0.32433\n",
      "Epoch 332 - lr: 0.09031 - Train loss: 0.59689 - Test loss: 0.32355\n",
      "Epoch 333 - lr: 0.09028 - Train loss: 0.59543 - Test loss: 0.32276\n",
      "Epoch 334 - lr: 0.09025 - Train loss: 0.59393 - Test loss: 0.32198\n",
      "Epoch 335 - lr: 0.09023 - Train loss: 0.59239 - Test loss: 0.32119\n",
      "Epoch 336 - lr: 0.09020 - Train loss: 0.59081 - Test loss: 0.32040\n",
      "Epoch 337 - lr: 0.09017 - Train loss: 0.58920 - Test loss: 0.31961\n",
      "Epoch 338 - lr: 0.09014 - Train loss: 0.58754 - Test loss: 0.31882\n",
      "Epoch 339 - lr: 0.09012 - Train loss: 0.58583 - Test loss: 0.31804\n",
      "Epoch 340 - lr: 0.09009 - Train loss: 0.58408 - Test loss: 0.31725\n",
      "Epoch 341 - lr: 0.09006 - Train loss: 0.58229 - Test loss: 0.31647\n",
      "Epoch 342 - lr: 0.09003 - Train loss: 0.58044 - Test loss: 0.31569\n",
      "Epoch 343 - lr: 0.09001 - Train loss: 0.57855 - Test loss: 0.31491\n",
      "Epoch 344 - lr: 0.08998 - Train loss: 0.57660 - Test loss: 0.31414\n",
      "Epoch 345 - lr: 0.08995 - Train loss: 0.57460 - Test loss: 0.31337\n",
      "Epoch 346 - lr: 0.08992 - Train loss: 0.57254 - Test loss: 0.31260\n",
      "Epoch 347 - lr: 0.08989 - Train loss: 0.57042 - Test loss: 0.31184\n",
      "Epoch 348 - lr: 0.08987 - Train loss: 0.56825 - Test loss: 0.31108\n",
      "Epoch 349 - lr: 0.08984 - Train loss: 0.56601 - Test loss: 0.31033\n",
      "Epoch 350 - lr: 0.08981 - Train loss: 0.56370 - Test loss: 0.30958\n",
      "Epoch 351 - lr: 0.08978 - Train loss: 0.56133 - Test loss: 0.30883\n",
      "Epoch 352 - lr: 0.08976 - Train loss: 0.55888 - Test loss: 0.30809\n",
      "Epoch 353 - lr: 0.08973 - Train loss: 0.55636 - Test loss: 0.30735\n",
      "Epoch 354 - lr: 0.08970 - Train loss: 0.55377 - Test loss: 0.30662\n",
      "Epoch 355 - lr: 0.08967 - Train loss: 0.55109 - Test loss: 0.30588\n",
      "Epoch 356 - lr: 0.08965 - Train loss: 0.54834 - Test loss: 0.30515\n",
      "Epoch 357 - lr: 0.08962 - Train loss: 0.54550 - Test loss: 0.30441\n",
      "Epoch 358 - lr: 0.08959 - Train loss: 0.54257 - Test loss: 0.30367\n",
      "Epoch 359 - lr: 0.08956 - Train loss: 0.53955 - Test loss: 0.30293\n",
      "Epoch 360 - lr: 0.08954 - Train loss: 0.53644 - Test loss: 0.30218\n",
      "Epoch 361 - lr: 0.08951 - Train loss: 0.53322 - Test loss: 0.30142\n",
      "Epoch 362 - lr: 0.08948 - Train loss: 0.52991 - Test loss: 0.30065\n",
      "Epoch 363 - lr: 0.08945 - Train loss: 0.52650 - Test loss: 0.29986\n",
      "Epoch 364 - lr: 0.08943 - Train loss: 0.52297 - Test loss: 0.29905\n",
      "Epoch 365 - lr: 0.08940 - Train loss: 0.51934 - Test loss: 0.29822\n",
      "Epoch 366 - lr: 0.08937 - Train loss: 0.51559 - Test loss: 0.29735\n",
      "Epoch 367 - lr: 0.08934 - Train loss: 0.51172 - Test loss: 0.29645\n",
      "Epoch 368 - lr: 0.08932 - Train loss: 0.50773 - Test loss: 0.29550\n",
      "Epoch 369 - lr: 0.08929 - Train loss: 0.50362 - Test loss: 0.29451\n",
      "Epoch 370 - lr: 0.08926 - Train loss: 0.49938 - Test loss: 0.29345\n",
      "Epoch 371 - lr: 0.08923 - Train loss: 0.49502 - Test loss: 0.29233\n",
      "Epoch 372 - lr: 0.08921 - Train loss: 0.49051 - Test loss: 0.29113\n",
      "Epoch 373 - lr: 0.08918 - Train loss: 0.48588 - Test loss: 0.28985\n",
      "Epoch 374 - lr: 0.08915 - Train loss: 0.48110 - Test loss: 0.28847\n",
      "Epoch 375 - lr: 0.08913 - Train loss: 0.47618 - Test loss: 0.28698\n",
      "Epoch 376 - lr: 0.08910 - Train loss: 0.47112 - Test loss: 0.28538\n",
      "Epoch 377 - lr: 0.08907 - Train loss: 0.46591 - Test loss: 0.28364\n",
      "Epoch 378 - lr: 0.08904 - Train loss: 0.46056 - Test loss: 0.28177\n",
      "Epoch 379 - lr: 0.08902 - Train loss: 0.45505 - Test loss: 0.27975\n",
      "Epoch 380 - lr: 0.08899 - Train loss: 0.44940 - Test loss: 0.27758\n",
      "Epoch 381 - lr: 0.08896 - Train loss: 0.44361 - Test loss: 0.27524\n",
      "Epoch 382 - lr: 0.08893 - Train loss: 0.43767 - Test loss: 0.27273\n",
      "Epoch 383 - lr: 0.08891 - Train loss: 0.43158 - Test loss: 0.27004\n",
      "Epoch 384 - lr: 0.08888 - Train loss: 0.42535 - Test loss: 0.26719\n",
      "Epoch 385 - lr: 0.08885 - Train loss: 0.41899 - Test loss: 0.26415\n",
      "Epoch 386 - lr: 0.08882 - Train loss: 0.41250 - Test loss: 0.26095\n",
      "Epoch 387 - lr: 0.08880 - Train loss: 0.40588 - Test loss: 0.25758\n",
      "Epoch 388 - lr: 0.08877 - Train loss: 0.39914 - Test loss: 0.25407\n",
      "Epoch 389 - lr: 0.08874 - Train loss: 0.39230 - Test loss: 0.25041\n",
      "Epoch 390 - lr: 0.08872 - Train loss: 0.38536 - Test loss: 0.24663\n",
      "Epoch 391 - lr: 0.08869 - Train loss: 0.37834 - Test loss: 0.24274\n",
      "Epoch 392 - lr: 0.08866 - Train loss: 0.37125 - Test loss: 0.23878\n",
      "Epoch 393 - lr: 0.08863 - Train loss: 0.36411 - Test loss: 0.23475\n",
      "Epoch 394 - lr: 0.08861 - Train loss: 0.35694 - Test loss: 0.23068\n",
      "Epoch 395 - lr: 0.08858 - Train loss: 0.34974 - Test loss: 0.22661\n",
      "Epoch 396 - lr: 0.08855 - Train loss: 0.34256 - Test loss: 0.22254\n",
      "Epoch 397 - lr: 0.08853 - Train loss: 0.33540 - Test loss: 0.21851\n",
      "Epoch 398 - lr: 0.08850 - Train loss: 0.32829 - Test loss: 0.21454\n",
      "Epoch 399 - lr: 0.08847 - Train loss: 0.32125 - Test loss: 0.21064\n",
      "Epoch 400 - lr: 0.08844 - Train loss: 0.31430 - Test loss: 0.20683\n",
      "Epoch 401 - lr: 0.08842 - Train loss: 0.30746 - Test loss: 0.20313\n",
      "Epoch 402 - lr: 0.08839 - Train loss: 0.30076 - Test loss: 0.19954\n",
      "Epoch 403 - lr: 0.08836 - Train loss: 0.29420 - Test loss: 0.19607\n",
      "Epoch 404 - lr: 0.08834 - Train loss: 0.28780 - Test loss: 0.19272\n",
      "Epoch 405 - lr: 0.08831 - Train loss: 0.28158 - Test loss: 0.18951\n",
      "Epoch 406 - lr: 0.08828 - Train loss: 0.27554 - Test loss: 0.18642\n",
      "Epoch 407 - lr: 0.08825 - Train loss: 0.26968 - Test loss: 0.18346\n",
      "Epoch 408 - lr: 0.08823 - Train loss: 0.26402 - Test loss: 0.18063\n",
      "Epoch 409 - lr: 0.08820 - Train loss: 0.25854 - Test loss: 0.17792\n",
      "Epoch 410 - lr: 0.08817 - Train loss: 0.25325 - Test loss: 0.17535\n",
      "Epoch 411 - lr: 0.08815 - Train loss: 0.24813 - Test loss: 0.17289\n",
      "Epoch 412 - lr: 0.08812 - Train loss: 0.24319 - Test loss: 0.17056\n",
      "Epoch 413 - lr: 0.08809 - Train loss: 0.23841 - Test loss: 0.16836\n",
      "Epoch 414 - lr: 0.08806 - Train loss: 0.23378 - Test loss: 0.16629\n",
      "Epoch 415 - lr: 0.08804 - Train loss: 0.22931 - Test loss: 0.16434\n",
      "Epoch 416 - lr: 0.08801 - Train loss: 0.22497 - Test loss: 0.16252\n",
      "Epoch 417 - lr: 0.08798 - Train loss: 0.22076 - Test loss: 0.16084\n",
      "Epoch 418 - lr: 0.08796 - Train loss: 0.21668 - Test loss: 0.15928\n",
      "Epoch 419 - lr: 0.08793 - Train loss: 0.21272 - Test loss: 0.15786\n",
      "Epoch 420 - lr: 0.08790 - Train loss: 0.20886 - Test loss: 0.15656\n",
      "Epoch 421 - lr: 0.08788 - Train loss: 0.20512 - Test loss: 0.15540\n",
      "Epoch 422 - lr: 0.08785 - Train loss: 0.20148 - Test loss: 0.15436\n",
      "Epoch 423 - lr: 0.08782 - Train loss: 0.19793 - Test loss: 0.15345\n",
      "Epoch 424 - lr: 0.08779 - Train loss: 0.19449 - Test loss: 0.15265\n",
      "Epoch 425 - lr: 0.08777 - Train loss: 0.19115 - Test loss: 0.15197\n",
      "Epoch 426 - lr: 0.08774 - Train loss: 0.18790 - Test loss: 0.15141\n",
      "Epoch 427 - lr: 0.08771 - Train loss: 0.18475 - Test loss: 0.15094\n",
      "Epoch 428 - lr: 0.08769 - Train loss: 0.18169 - Test loss: 0.15057\n",
      "Epoch 429 - lr: 0.08766 - Train loss: 0.17873 - Test loss: 0.15029\n",
      "Epoch 430 - lr: 0.08763 - Train loss: 0.17588 - Test loss: 0.15008\n",
      "Epoch 431 - lr: 0.08761 - Train loss: 0.17312 - Test loss: 0.14995\n",
      "Epoch 432 - lr: 0.08758 - Train loss: 0.17047 - Test loss: 0.14986\n",
      "Epoch 433 - lr: 0.08755 - Train loss: 0.16793 - Test loss: 0.14983\n",
      "Epoch 434 - lr: 0.08753 - Train loss: 0.16550 - Test loss: 0.14982\n",
      "Epoch 435 - lr: 0.08750 - Train loss: 0.16317 - Test loss: 0.14983\n",
      "Epoch 436 - lr: 0.08747 - Train loss: 0.16096 - Test loss: 0.14984\n",
      "Epoch 437 - lr: 0.08744 - Train loss: 0.15887 - Test loss: 0.14985\n",
      "Epoch 438 - lr: 0.08742 - Train loss: 0.15690 - Test loss: 0.14982\n",
      "Epoch 439 - lr: 0.08739 - Train loss: 0.15505 - Test loss: 0.14976\n",
      "Epoch 440 - lr: 0.08736 - Train loss: 0.15332 - Test loss: 0.14965\n",
      "Epoch 441 - lr: 0.08734 - Train loss: 0.15171 - Test loss: 0.14946\n",
      "Epoch 442 - lr: 0.08731 - Train loss: 0.15023 - Test loss: 0.14920\n",
      "Epoch 443 - lr: 0.08728 - Train loss: 0.14887 - Test loss: 0.14884\n",
      "Epoch 444 - lr: 0.08726 - Train loss: 0.14764 - Test loss: 0.14837\n",
      "Epoch 445 - lr: 0.08723 - Train loss: 0.14654 - Test loss: 0.14778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 446 - lr: 0.08720 - Train loss: 0.14556 - Test loss: 0.14708\n",
      "Epoch 447 - lr: 0.08718 - Train loss: 0.14469 - Test loss: 0.14624\n",
      "Epoch 448 - lr: 0.08715 - Train loss: 0.14395 - Test loss: 0.14526\n",
      "Epoch 449 - lr: 0.08712 - Train loss: 0.14332 - Test loss: 0.14415\n",
      "Epoch 450 - lr: 0.08710 - Train loss: 0.14280 - Test loss: 0.14290\n",
      "Epoch 451 - lr: 0.08707 - Train loss: 0.14239 - Test loss: 0.14151\n",
      "Epoch 452 - lr: 0.08704 - Train loss: 0.14208 - Test loss: 0.13999\n",
      "Epoch 453 - lr: 0.08702 - Train loss: 0.14186 - Test loss: 0.13834\n",
      "Epoch 454 - lr: 0.08699 - Train loss: 0.14173 - Test loss: 0.13658\n",
      "Epoch 455 - lr: 0.08696 - Train loss: 0.14168 - Test loss: 0.13471\n",
      "Epoch 456 - lr: 0.08694 - Train loss: 0.14170 - Test loss: 0.13274\n",
      "Epoch 457 - lr: 0.08691 - Train loss: 0.14179 - Test loss: 0.13069\n",
      "Epoch 458 - lr: 0.08688 - Train loss: 0.14194 - Test loss: 0.12857\n",
      "Epoch 459 - lr: 0.08686 - Train loss: 0.14214 - Test loss: 0.12640\n",
      "Epoch 460 - lr: 0.08683 - Train loss: 0.14238 - Test loss: 0.12418\n",
      "Epoch 461 - lr: 0.08680 - Train loss: 0.14266 - Test loss: 0.12194\n",
      "Epoch 462 - lr: 0.08678 - Train loss: 0.14297 - Test loss: 0.11968\n",
      "Epoch 463 - lr: 0.08675 - Train loss: 0.14331 - Test loss: 0.11741\n",
      "Epoch 464 - lr: 0.08672 - Train loss: 0.14367 - Test loss: 0.11516\n",
      "Epoch 465 - lr: 0.08670 - Train loss: 0.14403 - Test loss: 0.11293\n",
      "Epoch 466 - lr: 0.08667 - Train loss: 0.14441 - Test loss: 0.11072\n",
      "Epoch 467 - lr: 0.08664 - Train loss: 0.14479 - Test loss: 0.10856\n",
      "Epoch 468 - lr: 0.08662 - Train loss: 0.14517 - Test loss: 0.10644\n",
      "Epoch 469 - lr: 0.08659 - Train loss: 0.14554 - Test loss: 0.10438\n",
      "Epoch 470 - lr: 0.08656 - Train loss: 0.14591 - Test loss: 0.10237\n",
      "Epoch 471 - lr: 0.08654 - Train loss: 0.14627 - Test loss: 0.10042\n",
      "Epoch 472 - lr: 0.08651 - Train loss: 0.14662 - Test loss: 0.09854\n",
      "Epoch 473 - lr: 0.08648 - Train loss: 0.14696 - Test loss: 0.09672\n",
      "Epoch 474 - lr: 0.08646 - Train loss: 0.14728 - Test loss: 0.09498\n",
      "Epoch 475 - lr: 0.08643 - Train loss: 0.14759 - Test loss: 0.09329\n",
      "Epoch 476 - lr: 0.08640 - Train loss: 0.14788 - Test loss: 0.09168\n",
      "Epoch 477 - lr: 0.08638 - Train loss: 0.14816 - Test loss: 0.09014\n",
      "Epoch 478 - lr: 0.08635 - Train loss: 0.14842 - Test loss: 0.08866\n",
      "Epoch 479 - lr: 0.08632 - Train loss: 0.14866 - Test loss: 0.08725\n",
      "Epoch 480 - lr: 0.08630 - Train loss: 0.14889 - Test loss: 0.08590\n",
      "Epoch 481 - lr: 0.08627 - Train loss: 0.14910 - Test loss: 0.08461\n",
      "Epoch 482 - lr: 0.08624 - Train loss: 0.14930 - Test loss: 0.08338\n",
      "Epoch 483 - lr: 0.08622 - Train loss: 0.14949 - Test loss: 0.08221\n",
      "Epoch 484 - lr: 0.08619 - Train loss: 0.14966 - Test loss: 0.08110\n",
      "Epoch 485 - lr: 0.08617 - Train loss: 0.14981 - Test loss: 0.08004\n",
      "Epoch 486 - lr: 0.08614 - Train loss: 0.14996 - Test loss: 0.07903\n",
      "Epoch 487 - lr: 0.08611 - Train loss: 0.15009 - Test loss: 0.07807\n",
      "Epoch 488 - lr: 0.08609 - Train loss: 0.15021 - Test loss: 0.07715\n",
      "Epoch 489 - lr: 0.08606 - Train loss: 0.15032 - Test loss: 0.07628\n",
      "Epoch 490 - lr: 0.08603 - Train loss: 0.15042 - Test loss: 0.07545\n",
      "Epoch 491 - lr: 0.08601 - Train loss: 0.15051 - Test loss: 0.07467\n",
      "Epoch 492 - lr: 0.08598 - Train loss: 0.15059 - Test loss: 0.07392\n",
      "Epoch 493 - lr: 0.08595 - Train loss: 0.15066 - Test loss: 0.07320\n",
      "Epoch 494 - lr: 0.08593 - Train loss: 0.15072 - Test loss: 0.07253\n",
      "Epoch 495 - lr: 0.08590 - Train loss: 0.15078 - Test loss: 0.07188\n",
      "Epoch 496 - lr: 0.08587 - Train loss: 0.15082 - Test loss: 0.07127\n",
      "Epoch 497 - lr: 0.08585 - Train loss: 0.15086 - Test loss: 0.07068\n",
      "Epoch 498 - lr: 0.08582 - Train loss: 0.15090 - Test loss: 0.07013\n",
      "Epoch 499 - lr: 0.08580 - Train loss: 0.15093 - Test loss: 0.06960\n",
      "Epoch 500 - lr: 0.08577 - Train loss: 0.15095 - Test loss: 0.06910\n",
      "Epoch 501 - lr: 0.08574 - Train loss: 0.15097 - Test loss: 0.06862\n",
      "Epoch 502 - lr: 0.08572 - Train loss: 0.15098 - Test loss: 0.06816\n",
      "Epoch 503 - lr: 0.08569 - Train loss: 0.15099 - Test loss: 0.06773\n",
      "Epoch 504 - lr: 0.08566 - Train loss: 0.15099 - Test loss: 0.06732\n",
      "Epoch 505 - lr: 0.08564 - Train loss: 0.15099 - Test loss: 0.06693\n",
      "Epoch 506 - lr: 0.08561 - Train loss: 0.15099 - Test loss: 0.06655\n",
      "Epoch 507 - lr: 0.08559 - Train loss: 0.15098 - Test loss: 0.06620\n",
      "Epoch 508 - lr: 0.08556 - Train loss: 0.15097 - Test loss: 0.06586\n",
      "Epoch 509 - lr: 0.08553 - Train loss: 0.15096 - Test loss: 0.06554\n",
      "Epoch 510 - lr: 0.08551 - Train loss: 0.15094 - Test loss: 0.06523\n",
      "Epoch 511 - lr: 0.08548 - Train loss: 0.15092 - Test loss: 0.06494\n",
      "Epoch 512 - lr: 0.08545 - Train loss: 0.15090 - Test loss: 0.06466\n",
      "Epoch 513 - lr: 0.08543 - Train loss: 0.15087 - Test loss: 0.06439\n",
      "Epoch 514 - lr: 0.08540 - Train loss: 0.15084 - Test loss: 0.06414\n",
      "Epoch 515 - lr: 0.08538 - Train loss: 0.15082 - Test loss: 0.06390\n",
      "Epoch 516 - lr: 0.08535 - Train loss: 0.15078 - Test loss: 0.06367\n",
      "Epoch 517 - lr: 0.08532 - Train loss: 0.15075 - Test loss: 0.06346\n",
      "Epoch 518 - lr: 0.08530 - Train loss: 0.15071 - Test loss: 0.06325\n",
      "Epoch 519 - lr: 0.08527 - Train loss: 0.15068 - Test loss: 0.06305\n",
      "Epoch 520 - lr: 0.08524 - Train loss: 0.15064 - Test loss: 0.06287\n",
      "Epoch 521 - lr: 0.08522 - Train loss: 0.15060 - Test loss: 0.06269\n",
      "Epoch 522 - lr: 0.08519 - Train loss: 0.15055 - Test loss: 0.06252\n",
      "Epoch 523 - lr: 0.08517 - Train loss: 0.15051 - Test loss: 0.06236\n",
      "Epoch 524 - lr: 0.08514 - Train loss: 0.15047 - Test loss: 0.06221\n",
      "Epoch 525 - lr: 0.08511 - Train loss: 0.15042 - Test loss: 0.06206\n",
      "Epoch 526 - lr: 0.08509 - Train loss: 0.15037 - Test loss: 0.06192\n",
      "Epoch 527 - lr: 0.08506 - Train loss: 0.15032 - Test loss: 0.06179\n",
      "Epoch 528 - lr: 0.08504 - Train loss: 0.15028 - Test loss: 0.06167\n",
      "Epoch 529 - lr: 0.08501 - Train loss: 0.15022 - Test loss: 0.06155\n",
      "Epoch 530 - lr: 0.08498 - Train loss: 0.15017 - Test loss: 0.06144\n",
      "Epoch 531 - lr: 0.08496 - Train loss: 0.15012 - Test loss: 0.06133\n",
      "Epoch 532 - lr: 0.08493 - Train loss: 0.15007 - Test loss: 0.06123\n",
      "Epoch 533 - lr: 0.08491 - Train loss: 0.15001 - Test loss: 0.06114\n",
      "Epoch 534 - lr: 0.08488 - Train loss: 0.14996 - Test loss: 0.06105\n",
      "Epoch 535 - lr: 0.08485 - Train loss: 0.14990 - Test loss: 0.06096\n",
      "Epoch 536 - lr: 0.08483 - Train loss: 0.14985 - Test loss: 0.06088\n",
      "Epoch 537 - lr: 0.08480 - Train loss: 0.14979 - Test loss: 0.06081\n",
      "Epoch 538 - lr: 0.08477 - Train loss: 0.14973 - Test loss: 0.06074\n",
      "Epoch 539 - lr: 0.08475 - Train loss: 0.14967 - Test loss: 0.06067\n",
      "Epoch 540 - lr: 0.08472 - Train loss: 0.14962 - Test loss: 0.06061\n",
      "Epoch 541 - lr: 0.08470 - Train loss: 0.14956 - Test loss: 0.06055\n",
      "Epoch 542 - lr: 0.08467 - Train loss: 0.14950 - Test loss: 0.06049\n",
      "Epoch 543 - lr: 0.08464 - Train loss: 0.14944 - Test loss: 0.06044\n",
      "Epoch 544 - lr: 0.08462 - Train loss: 0.14938 - Test loss: 0.06039\n",
      "Epoch 545 - lr: 0.08459 - Train loss: 0.14931 - Test loss: 0.06034\n",
      "Epoch 546 - lr: 0.08457 - Train loss: 0.14925 - Test loss: 0.06030\n",
      "Epoch 547 - lr: 0.08454 - Train loss: 0.14919 - Test loss: 0.06026\n",
      "Epoch 548 - lr: 0.08451 - Train loss: 0.14913 - Test loss: 0.06022\n",
      "Epoch 549 - lr: 0.08449 - Train loss: 0.14907 - Test loss: 0.06019\n",
      "Epoch 550 - lr: 0.08446 - Train loss: 0.14900 - Test loss: 0.06015\n",
      "Epoch 551 - lr: 0.08444 - Train loss: 0.14894 - Test loss: 0.06012\n",
      "Epoch 552 - lr: 0.08441 - Train loss: 0.14888 - Test loss: 0.06010\n",
      "Epoch 553 - lr: 0.08439 - Train loss: 0.14881 - Test loss: 0.06007\n",
      "Epoch 554 - lr: 0.08436 - Train loss: 0.14875 - Test loss: 0.06005\n",
      "Epoch 555 - lr: 0.08433 - Train loss: 0.14868 - Test loss: 0.06003\n",
      "Epoch 556 - lr: 0.08431 - Train loss: 0.14862 - Test loss: 0.06001\n",
      "Epoch 557 - lr: 0.08428 - Train loss: 0.14856 - Test loss: 0.05999\n",
      "Epoch 558 - lr: 0.08426 - Train loss: 0.14849 - Test loss: 0.05997\n",
      "Epoch 559 - lr: 0.08423 - Train loss: 0.14842 - Test loss: 0.05996\n",
      "Epoch 560 - lr: 0.08420 - Train loss: 0.14836 - Test loss: 0.05995\n",
      "Epoch 561 - lr: 0.08418 - Train loss: 0.14829 - Test loss: 0.05994\n",
      "Epoch 562 - lr: 0.08415 - Train loss: 0.14823 - Test loss: 0.05993\n",
      "Epoch 563 - lr: 0.08413 - Train loss: 0.14816 - Test loss: 0.05992\n",
      "Epoch 564 - lr: 0.08410 - Train loss: 0.14810 - Test loss: 0.05991\n",
      "Epoch 565 - lr: 0.08407 - Train loss: 0.14803 - Test loss: 0.05991\n",
      "Epoch 566 - lr: 0.08405 - Train loss: 0.14796 - Test loss: 0.05991\n",
      "Epoch 567 - lr: 0.08402 - Train loss: 0.14790 - Test loss: 0.05990\n",
      "Epoch 568 - lr: 0.08400 - Train loss: 0.14783 - Test loss: 0.05990\n",
      "Epoch 569 - lr: 0.08397 - Train loss: 0.14776 - Test loss: 0.05990\n",
      "Epoch 570 - lr: 0.08395 - Train loss: 0.14770 - Test loss: 0.05990\n",
      "Epoch 571 - lr: 0.08392 - Train loss: 0.14763 - Test loss: 0.05990\n",
      "Epoch 572 - lr: 0.08389 - Train loss: 0.14756 - Test loss: 0.05991\n",
      "Epoch 573 - lr: 0.08387 - Train loss: 0.14750 - Test loss: 0.05991\n",
      "Epoch 574 - lr: 0.08384 - Train loss: 0.14743 - Test loss: 0.05991\n",
      "Epoch 575 - lr: 0.08382 - Train loss: 0.14736 - Test loss: 0.05992\n",
      "Epoch 576 - lr: 0.08379 - Train loss: 0.14729 - Test loss: 0.05992\n",
      "Epoch 577 - lr: 0.08377 - Train loss: 0.14723 - Test loss: 0.05993\n",
      "Epoch 578 - lr: 0.08374 - Train loss: 0.14716 - Test loss: 0.05994\n",
      "Epoch 579 - lr: 0.08371 - Train loss: 0.14709 - Test loss: 0.05995\n",
      "Epoch 580 - lr: 0.08369 - Train loss: 0.14702 - Test loss: 0.05996\n",
      "Epoch 581 - lr: 0.08366 - Train loss: 0.14696 - Test loss: 0.05996\n",
      "Epoch 582 - lr: 0.08364 - Train loss: 0.14689 - Test loss: 0.05997\n",
      "Epoch 583 - lr: 0.08361 - Train loss: 0.14682 - Test loss: 0.05998\n",
      "Epoch 584 - lr: 0.08359 - Train loss: 0.14675 - Test loss: 0.06000\n",
      "Epoch 585 - lr: 0.08356 - Train loss: 0.14669 - Test loss: 0.06001\n",
      "Epoch 586 - lr: 0.08353 - Train loss: 0.14662 - Test loss: 0.06002\n",
      "Epoch 587 - lr: 0.08351 - Train loss: 0.14655 - Test loss: 0.06003\n",
      "Epoch 588 - lr: 0.08348 - Train loss: 0.14648 - Test loss: 0.06004\n",
      "Epoch 589 - lr: 0.08346 - Train loss: 0.14642 - Test loss: 0.06006\n",
      "Epoch 590 - lr: 0.08343 - Train loss: 0.14635 - Test loss: 0.06007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591 - lr: 0.08341 - Train loss: 0.14628 - Test loss: 0.06009\n",
      "Epoch 592 - lr: 0.08338 - Train loss: 0.14621 - Test loss: 0.06010\n",
      "Epoch 593 - lr: 0.08336 - Train loss: 0.14614 - Test loss: 0.06011\n",
      "Epoch 594 - lr: 0.08333 - Train loss: 0.14608 - Test loss: 0.06013\n",
      "Epoch 595 - lr: 0.08330 - Train loss: 0.14601 - Test loss: 0.06014\n",
      "Epoch 596 - lr: 0.08328 - Train loss: 0.14594 - Test loss: 0.06016\n",
      "Epoch 597 - lr: 0.08325 - Train loss: 0.14587 - Test loss: 0.06017\n",
      "Epoch 598 - lr: 0.08323 - Train loss: 0.14581 - Test loss: 0.06019\n",
      "Epoch 599 - lr: 0.08320 - Train loss: 0.14574 - Test loss: 0.06021\n",
      "Epoch 600 - lr: 0.08318 - Train loss: 0.14567 - Test loss: 0.06022\n",
      "Epoch 601 - lr: 0.08315 - Train loss: 0.14560 - Test loss: 0.06024\n",
      "Epoch 602 - lr: 0.08313 - Train loss: 0.14553 - Test loss: 0.06026\n",
      "Epoch 603 - lr: 0.08310 - Train loss: 0.14547 - Test loss: 0.06027\n",
      "Epoch 604 - lr: 0.08307 - Train loss: 0.14540 - Test loss: 0.06029\n",
      "Epoch 605 - lr: 0.08305 - Train loss: 0.14533 - Test loss: 0.06031\n",
      "Epoch 606 - lr: 0.08302 - Train loss: 0.14526 - Test loss: 0.06032\n",
      "Epoch 607 - lr: 0.08300 - Train loss: 0.14520 - Test loss: 0.06034\n",
      "Epoch 608 - lr: 0.08297 - Train loss: 0.14513 - Test loss: 0.06036\n",
      "Epoch 609 - lr: 0.08295 - Train loss: 0.14506 - Test loss: 0.06037\n",
      "Epoch 610 - lr: 0.08292 - Train loss: 0.14499 - Test loss: 0.06039\n",
      "Epoch 611 - lr: 0.08290 - Train loss: 0.14493 - Test loss: 0.06041\n",
      "Epoch 612 - lr: 0.08287 - Train loss: 0.14486 - Test loss: 0.06043\n",
      "Epoch 613 - lr: 0.08285 - Train loss: 0.14479 - Test loss: 0.06044\n",
      "Epoch 614 - lr: 0.08282 - Train loss: 0.14472 - Test loss: 0.06046\n",
      "Epoch 615 - lr: 0.08279 - Train loss: 0.14465 - Test loss: 0.06048\n",
      "Epoch 616 - lr: 0.08277 - Train loss: 0.14459 - Test loss: 0.06050\n",
      "Epoch 617 - lr: 0.08274 - Train loss: 0.14452 - Test loss: 0.06051\n",
      "Epoch 618 - lr: 0.08272 - Train loss: 0.14445 - Test loss: 0.06053\n",
      "Epoch 619 - lr: 0.08269 - Train loss: 0.14439 - Test loss: 0.06055\n",
      "Epoch 620 - lr: 0.08267 - Train loss: 0.14432 - Test loss: 0.06057\n",
      "Epoch 621 - lr: 0.08264 - Train loss: 0.14425 - Test loss: 0.06058\n",
      "Epoch 622 - lr: 0.08262 - Train loss: 0.14418 - Test loss: 0.06060\n",
      "Epoch 623 - lr: 0.08259 - Train loss: 0.14412 - Test loss: 0.06062\n",
      "Epoch 624 - lr: 0.08257 - Train loss: 0.14405 - Test loss: 0.06064\n",
      "Epoch 625 - lr: 0.08254 - Train loss: 0.14398 - Test loss: 0.06065\n",
      "Epoch 626 - lr: 0.08252 - Train loss: 0.14391 - Test loss: 0.06067\n",
      "Epoch 627 - lr: 0.08249 - Train loss: 0.14385 - Test loss: 0.06069\n",
      "Epoch 628 - lr: 0.08246 - Train loss: 0.14378 - Test loss: 0.06071\n",
      "Epoch 629 - lr: 0.08244 - Train loss: 0.14371 - Test loss: 0.06072\n",
      "Epoch 630 - lr: 0.08241 - Train loss: 0.14364 - Test loss: 0.06074\n",
      "Epoch 631 - lr: 0.08239 - Train loss: 0.14358 - Test loss: 0.06076\n",
      "Epoch 632 - lr: 0.08236 - Train loss: 0.14351 - Test loss: 0.06077\n",
      "Epoch 633 - lr: 0.08234 - Train loss: 0.14344 - Test loss: 0.06079\n",
      "Epoch 634 - lr: 0.08231 - Train loss: 0.14338 - Test loss: 0.06081\n",
      "Epoch 635 - lr: 0.08229 - Train loss: 0.14331 - Test loss: 0.06082\n",
      "Epoch 636 - lr: 0.08226 - Train loss: 0.14324 - Test loss: 0.06084\n",
      "Epoch 637 - lr: 0.08224 - Train loss: 0.14317 - Test loss: 0.06086\n",
      "Epoch 638 - lr: 0.08221 - Train loss: 0.14311 - Test loss: 0.06087\n",
      "Epoch 639 - lr: 0.08219 - Train loss: 0.14304 - Test loss: 0.06089\n",
      "Epoch 640 - lr: 0.08216 - Train loss: 0.14297 - Test loss: 0.06090\n",
      "Epoch 641 - lr: 0.08214 - Train loss: 0.14291 - Test loss: 0.06092\n",
      "Epoch 642 - lr: 0.08211 - Train loss: 0.14284 - Test loss: 0.06094\n",
      "Epoch 643 - lr: 0.08209 - Train loss: 0.14277 - Test loss: 0.06095\n",
      "Epoch 644 - lr: 0.08206 - Train loss: 0.14270 - Test loss: 0.06097\n",
      "Epoch 645 - lr: 0.08204 - Train loss: 0.14264 - Test loss: 0.06098\n",
      "Epoch 646 - lr: 0.08201 - Train loss: 0.14257 - Test loss: 0.06100\n",
      "Epoch 647 - lr: 0.08198 - Train loss: 0.14250 - Test loss: 0.06101\n",
      "Epoch 648 - lr: 0.08196 - Train loss: 0.14244 - Test loss: 0.06103\n",
      "Epoch 649 - lr: 0.08193 - Train loss: 0.14237 - Test loss: 0.06104\n",
      "Epoch 650 - lr: 0.08191 - Train loss: 0.14230 - Test loss: 0.06106\n",
      "Epoch 651 - lr: 0.08188 - Train loss: 0.14223 - Test loss: 0.06107\n",
      "Epoch 652 - lr: 0.08186 - Train loss: 0.14217 - Test loss: 0.06109\n",
      "Epoch 653 - lr: 0.08183 - Train loss: 0.14210 - Test loss: 0.06110\n",
      "Epoch 654 - lr: 0.08181 - Train loss: 0.14203 - Test loss: 0.06111\n",
      "Epoch 655 - lr: 0.08178 - Train loss: 0.14197 - Test loss: 0.06113\n",
      "Epoch 656 - lr: 0.08176 - Train loss: 0.14190 - Test loss: 0.06114\n",
      "Epoch 657 - lr: 0.08173 - Train loss: 0.14183 - Test loss: 0.06116\n",
      "Epoch 658 - lr: 0.08171 - Train loss: 0.14177 - Test loss: 0.06117\n",
      "Epoch 659 - lr: 0.08168 - Train loss: 0.14170 - Test loss: 0.06118\n",
      "Epoch 660 - lr: 0.08166 - Train loss: 0.14163 - Test loss: 0.06120\n",
      "Epoch 661 - lr: 0.08163 - Train loss: 0.14156 - Test loss: 0.06121\n",
      "Epoch 662 - lr: 0.08161 - Train loss: 0.14150 - Test loss: 0.06122\n",
      "Epoch 663 - lr: 0.08158 - Train loss: 0.14143 - Test loss: 0.06124\n",
      "Epoch 664 - lr: 0.08156 - Train loss: 0.14136 - Test loss: 0.06125\n",
      "Epoch 665 - lr: 0.08153 - Train loss: 0.14130 - Test loss: 0.06126\n",
      "Epoch 666 - lr: 0.08151 - Train loss: 0.14123 - Test loss: 0.06127\n",
      "Epoch 667 - lr: 0.08148 - Train loss: 0.14116 - Test loss: 0.06129\n",
      "Epoch 668 - lr: 0.08146 - Train loss: 0.14109 - Test loss: 0.06130\n",
      "Epoch 669 - lr: 0.08143 - Train loss: 0.14103 - Test loss: 0.06131\n",
      "Epoch 670 - lr: 0.08141 - Train loss: 0.14096 - Test loss: 0.06132\n",
      "Epoch 671 - lr: 0.08138 - Train loss: 0.14089 - Test loss: 0.06133\n",
      "Epoch 672 - lr: 0.08136 - Train loss: 0.14083 - Test loss: 0.06134\n",
      "Epoch 673 - lr: 0.08133 - Train loss: 0.14076 - Test loss: 0.06136\n",
      "Epoch 674 - lr: 0.08131 - Train loss: 0.14069 - Test loss: 0.06137\n",
      "Epoch 675 - lr: 0.08128 - Train loss: 0.14062 - Test loss: 0.06138\n",
      "Epoch 676 - lr: 0.08126 - Train loss: 0.14056 - Test loss: 0.06139\n",
      "Epoch 677 - lr: 0.08123 - Train loss: 0.14049 - Test loss: 0.06140\n",
      "Epoch 678 - lr: 0.08121 - Train loss: 0.14042 - Test loss: 0.06141\n",
      "Epoch 679 - lr: 0.08118 - Train loss: 0.14035 - Test loss: 0.06142\n",
      "Epoch 680 - lr: 0.08116 - Train loss: 0.14029 - Test loss: 0.06143\n",
      "Epoch 681 - lr: 0.08113 - Train loss: 0.14022 - Test loss: 0.06144\n",
      "Epoch 682 - lr: 0.08111 - Train loss: 0.14015 - Test loss: 0.06145\n",
      "Epoch 683 - lr: 0.08108 - Train loss: 0.14009 - Test loss: 0.06146\n",
      "Epoch 684 - lr: 0.08106 - Train loss: 0.14002 - Test loss: 0.06147\n",
      "Epoch 685 - lr: 0.08103 - Train loss: 0.13995 - Test loss: 0.06148\n",
      "Epoch 686 - lr: 0.08101 - Train loss: 0.13988 - Test loss: 0.06149\n",
      "Epoch 687 - lr: 0.08098 - Train loss: 0.13982 - Test loss: 0.06150\n",
      "Epoch 688 - lr: 0.08096 - Train loss: 0.13975 - Test loss: 0.06151\n",
      "Epoch 689 - lr: 0.08093 - Train loss: 0.13968 - Test loss: 0.06152\n",
      "Epoch 690 - lr: 0.08091 - Train loss: 0.13961 - Test loss: 0.06152\n",
      "Epoch 691 - lr: 0.08088 - Train loss: 0.13954 - Test loss: 0.06153\n",
      "Epoch 692 - lr: 0.08086 - Train loss: 0.13948 - Test loss: 0.06154\n",
      "Epoch 693 - lr: 0.08084 - Train loss: 0.13941 - Test loss: 0.06155\n",
      "Epoch 694 - lr: 0.08081 - Train loss: 0.13934 - Test loss: 0.06156\n",
      "Epoch 695 - lr: 0.08079 - Train loss: 0.13927 - Test loss: 0.06156\n",
      "Epoch 696 - lr: 0.08076 - Train loss: 0.13921 - Test loss: 0.06157\n",
      "Epoch 697 - lr: 0.08074 - Train loss: 0.13914 - Test loss: 0.06158\n",
      "Epoch 698 - lr: 0.08071 - Train loss: 0.13907 - Test loss: 0.06159\n",
      "Epoch 699 - lr: 0.08069 - Train loss: 0.13900 - Test loss: 0.06159\n",
      "Epoch 700 - lr: 0.08066 - Train loss: 0.13893 - Test loss: 0.06160\n",
      "Epoch 701 - lr: 0.08064 - Train loss: 0.13887 - Test loss: 0.06161\n",
      "Epoch 702 - lr: 0.08061 - Train loss: 0.13880 - Test loss: 0.06161\n",
      "Epoch 703 - lr: 0.08059 - Train loss: 0.13873 - Test loss: 0.06162\n",
      "Epoch 704 - lr: 0.08056 - Train loss: 0.13866 - Test loss: 0.06163\n",
      "Epoch 705 - lr: 0.08054 - Train loss: 0.13859 - Test loss: 0.06163\n",
      "Epoch 706 - lr: 0.08051 - Train loss: 0.13853 - Test loss: 0.06164\n",
      "Epoch 707 - lr: 0.08049 - Train loss: 0.13846 - Test loss: 0.06165\n",
      "Epoch 708 - lr: 0.08046 - Train loss: 0.13839 - Test loss: 0.06165\n",
      "Epoch 709 - lr: 0.08044 - Train loss: 0.13832 - Test loss: 0.06166\n",
      "Epoch 710 - lr: 0.08041 - Train loss: 0.13825 - Test loss: 0.06166\n",
      "Epoch 711 - lr: 0.08039 - Train loss: 0.13818 - Test loss: 0.06167\n",
      "Epoch 712 - lr: 0.08036 - Train loss: 0.13811 - Test loss: 0.06167\n",
      "Epoch 713 - lr: 0.08034 - Train loss: 0.13805 - Test loss: 0.06168\n",
      "Epoch 714 - lr: 0.08032 - Train loss: 0.13798 - Test loss: 0.06168\n",
      "Epoch 715 - lr: 0.08029 - Train loss: 0.13791 - Test loss: 0.06169\n",
      "Epoch 716 - lr: 0.08027 - Train loss: 0.13784 - Test loss: 0.06169\n",
      "Epoch 717 - lr: 0.08024 - Train loss: 0.13777 - Test loss: 0.06169\n",
      "Epoch 718 - lr: 0.08022 - Train loss: 0.13770 - Test loss: 0.06170\n",
      "Epoch 719 - lr: 0.08019 - Train loss: 0.13763 - Test loss: 0.06170\n",
      "Epoch 720 - lr: 0.08017 - Train loss: 0.13756 - Test loss: 0.06171\n",
      "Epoch 721 - lr: 0.08014 - Train loss: 0.13750 - Test loss: 0.06171\n",
      "Epoch 722 - lr: 0.08012 - Train loss: 0.13743 - Test loss: 0.06171\n",
      "Epoch 723 - lr: 0.08009 - Train loss: 0.13736 - Test loss: 0.06172\n",
      "Epoch 724 - lr: 0.08007 - Train loss: 0.13729 - Test loss: 0.06172\n",
      "Epoch 725 - lr: 0.08004 - Train loss: 0.13722 - Test loss: 0.06172\n",
      "Epoch 726 - lr: 0.08002 - Train loss: 0.13715 - Test loss: 0.06172\n",
      "Epoch 727 - lr: 0.08000 - Train loss: 0.13708 - Test loss: 0.06173\n",
      "Epoch 728 - lr: 0.07997 - Train loss: 0.13701 - Test loss: 0.06173\n",
      "Epoch 729 - lr: 0.07995 - Train loss: 0.13694 - Test loss: 0.06173\n",
      "Epoch 730 - lr: 0.07992 - Train loss: 0.13687 - Test loss: 0.06173\n",
      "Epoch 731 - lr: 0.07990 - Train loss: 0.13680 - Test loss: 0.06174\n",
      "Epoch 732 - lr: 0.07987 - Train loss: 0.13673 - Test loss: 0.06174\n",
      "Epoch 733 - lr: 0.07985 - Train loss: 0.13666 - Test loss: 0.06174\n",
      "Epoch 734 - lr: 0.07982 - Train loss: 0.13659 - Test loss: 0.06174\n",
      "Epoch 735 - lr: 0.07980 - Train loss: 0.13652 - Test loss: 0.06174\n",
      "Epoch 736 - lr: 0.07977 - Train loss: 0.13645 - Test loss: 0.06174\n",
      "Epoch 737 - lr: 0.07975 - Train loss: 0.13638 - Test loss: 0.06174\n",
      "Epoch 738 - lr: 0.07973 - Train loss: 0.13631 - Test loss: 0.06174\n",
      "Epoch 739 - lr: 0.07970 - Train loss: 0.13624 - Test loss: 0.06174\n",
      "Epoch 740 - lr: 0.07968 - Train loss: 0.13617 - Test loss: 0.06174\n",
      "Epoch 741 - lr: 0.07965 - Train loss: 0.13610 - Test loss: 0.06174\n",
      "Epoch 742 - lr: 0.07963 - Train loss: 0.13603 - Test loss: 0.06174\n",
      "Epoch 743 - lr: 0.07960 - Train loss: 0.13595 - Test loss: 0.06174\n",
      "Epoch 744 - lr: 0.07958 - Train loss: 0.13588 - Test loss: 0.06174\n",
      "Epoch 745 - lr: 0.07955 - Train loss: 0.13581 - Test loss: 0.06174\n",
      "Epoch 746 - lr: 0.07953 - Train loss: 0.13574 - Test loss: 0.06174\n",
      "Epoch 747 - lr: 0.07951 - Train loss: 0.13567 - Test loss: 0.06174\n",
      "Epoch 748 - lr: 0.07948 - Train loss: 0.13560 - Test loss: 0.06174\n",
      "Epoch 749 - lr: 0.07946 - Train loss: 0.13553 - Test loss: 0.06174\n",
      "Epoch 750 - lr: 0.07943 - Train loss: 0.13545 - Test loss: 0.06174\n",
      "Epoch 751 - lr: 0.07941 - Train loss: 0.13538 - Test loss: 0.06174\n",
      "Epoch 752 - lr: 0.07938 - Train loss: 0.13531 - Test loss: 0.06174\n",
      "Epoch 753 - lr: 0.07936 - Train loss: 0.13524 - Test loss: 0.06173\n",
      "Epoch 754 - lr: 0.07934 - Train loss: 0.13517 - Test loss: 0.06173\n",
      "Epoch 755 - lr: 0.07931 - Train loss: 0.13509 - Test loss: 0.06173\n",
      "Epoch 756 - lr: 0.07929 - Train loss: 0.13502 - Test loss: 0.06173\n",
      "Epoch 757 - lr: 0.07926 - Train loss: 0.13495 - Test loss: 0.06172\n",
      "Epoch 758 - lr: 0.07924 - Train loss: 0.13487 - Test loss: 0.06172\n",
      "Epoch 759 - lr: 0.07921 - Train loss: 0.13480 - Test loss: 0.06172\n",
      "Epoch 760 - lr: 0.07919 - Train loss: 0.13473 - Test loss: 0.06171\n",
      "Epoch 761 - lr: 0.07917 - Train loss: 0.13466 - Test loss: 0.06171\n",
      "Epoch 762 - lr: 0.07914 - Train loss: 0.13458 - Test loss: 0.06171\n",
      "Epoch 763 - lr: 0.07912 - Train loss: 0.13451 - Test loss: 0.06170\n",
      "Epoch 764 - lr: 0.07909 - Train loss: 0.13443 - Test loss: 0.06170\n",
      "Epoch 765 - lr: 0.07907 - Train loss: 0.13436 - Test loss: 0.06170\n",
      "Epoch 766 - lr: 0.07904 - Train loss: 0.13429 - Test loss: 0.06169\n",
      "Epoch 767 - lr: 0.07902 - Train loss: 0.13421 - Test loss: 0.06169\n",
      "Epoch 768 - lr: 0.07900 - Train loss: 0.13414 - Test loss: 0.06168\n",
      "Epoch 769 - lr: 0.07897 - Train loss: 0.13406 - Test loss: 0.06168\n",
      "Epoch 770 - lr: 0.07895 - Train loss: 0.13399 - Test loss: 0.06167\n",
      "Epoch 771 - lr: 0.07892 - Train loss: 0.13391 - Test loss: 0.06167\n",
      "Epoch 772 - lr: 0.07890 - Train loss: 0.13384 - Test loss: 0.06166\n",
      "Epoch 773 - lr: 0.07887 - Train loss: 0.13376 - Test loss: 0.06166\n",
      "Epoch 774 - lr: 0.07885 - Train loss: 0.13369 - Test loss: 0.06165\n",
      "Epoch 775 - lr: 0.07883 - Train loss: 0.13361 - Test loss: 0.06165\n",
      "Epoch 776 - lr: 0.07880 - Train loss: 0.13353 - Test loss: 0.06164\n",
      "Epoch 777 - lr: 0.07878 - Train loss: 0.13346 - Test loss: 0.06163\n",
      "Epoch 778 - lr: 0.07875 - Train loss: 0.13338 - Test loss: 0.06163\n",
      "Epoch 779 - lr: 0.07873 - Train loss: 0.13331 - Test loss: 0.06162\n",
      "Epoch 780 - lr: 0.07870 - Train loss: 0.13323 - Test loss: 0.06161\n",
      "Epoch 781 - lr: 0.07868 - Train loss: 0.13315 - Test loss: 0.06161\n",
      "Epoch 782 - lr: 0.07866 - Train loss: 0.13307 - Test loss: 0.06160\n",
      "Epoch 783 - lr: 0.07863 - Train loss: 0.13300 - Test loss: 0.06159\n",
      "Epoch 784 - lr: 0.07861 - Train loss: 0.13292 - Test loss: 0.06158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 785 - lr: 0.07858 - Train loss: 0.13284 - Test loss: 0.06157\n",
      "Epoch 786 - lr: 0.07856 - Train loss: 0.13276 - Test loss: 0.06157\n",
      "Epoch 787 - lr: 0.07854 - Train loss: 0.13269 - Test loss: 0.06156\n",
      "Epoch 788 - lr: 0.07851 - Train loss: 0.13261 - Test loss: 0.06155\n",
      "Epoch 789 - lr: 0.07849 - Train loss: 0.13253 - Test loss: 0.06154\n",
      "Epoch 790 - lr: 0.07846 - Train loss: 0.13245 - Test loss: 0.06153\n",
      "Epoch 791 - lr: 0.07844 - Train loss: 0.13237 - Test loss: 0.06152\n",
      "Epoch 792 - lr: 0.07842 - Train loss: 0.13229 - Test loss: 0.06151\n",
      "Epoch 793 - lr: 0.07839 - Train loss: 0.13221 - Test loss: 0.06150\n",
      "Epoch 794 - lr: 0.07837 - Train loss: 0.13213 - Test loss: 0.06149\n",
      "Epoch 795 - lr: 0.07834 - Train loss: 0.13205 - Test loss: 0.06148\n",
      "Epoch 796 - lr: 0.07832 - Train loss: 0.13197 - Test loss: 0.06147\n",
      "Epoch 797 - lr: 0.07829 - Train loss: 0.13189 - Test loss: 0.06146\n",
      "Epoch 798 - lr: 0.07827 - Train loss: 0.13181 - Test loss: 0.06145\n",
      "Epoch 799 - lr: 0.07825 - Train loss: 0.13173 - Test loss: 0.06144\n",
      "Epoch 800 - lr: 0.07822 - Train loss: 0.13165 - Test loss: 0.06143\n",
      "Epoch 801 - lr: 0.07820 - Train loss: 0.13157 - Test loss: 0.06142\n",
      "Epoch 802 - lr: 0.07817 - Train loss: 0.13148 - Test loss: 0.06141\n",
      "Epoch 803 - lr: 0.07815 - Train loss: 0.13140 - Test loss: 0.06140\n",
      "Epoch 804 - lr: 0.07813 - Train loss: 0.13132 - Test loss: 0.06139\n",
      "Epoch 805 - lr: 0.07810 - Train loss: 0.13124 - Test loss: 0.06138\n",
      "Epoch 806 - lr: 0.07808 - Train loss: 0.13115 - Test loss: 0.06136\n",
      "Epoch 807 - lr: 0.07805 - Train loss: 0.13107 - Test loss: 0.06135\n",
      "Epoch 808 - lr: 0.07803 - Train loss: 0.13099 - Test loss: 0.06134\n",
      "Epoch 809 - lr: 0.07801 - Train loss: 0.13091 - Test loss: 0.06133\n",
      "Epoch 810 - lr: 0.07798 - Train loss: 0.13082 - Test loss: 0.06131\n",
      "Epoch 811 - lr: 0.07796 - Train loss: 0.13074 - Test loss: 0.06130\n",
      "Epoch 812 - lr: 0.07794 - Train loss: 0.13065 - Test loss: 0.06129\n",
      "Epoch 813 - lr: 0.07791 - Train loss: 0.13057 - Test loss: 0.06127\n",
      "Epoch 814 - lr: 0.07789 - Train loss: 0.13049 - Test loss: 0.06126\n",
      "Epoch 815 - lr: 0.07786 - Train loss: 0.13040 - Test loss: 0.06125\n",
      "Epoch 816 - lr: 0.07784 - Train loss: 0.13032 - Test loss: 0.06123\n",
      "Epoch 817 - lr: 0.07782 - Train loss: 0.13023 - Test loss: 0.06122\n",
      "Epoch 818 - lr: 0.07779 - Train loss: 0.13015 - Test loss: 0.06121\n",
      "Epoch 819 - lr: 0.07777 - Train loss: 0.13006 - Test loss: 0.06119\n",
      "Epoch 820 - lr: 0.07774 - Train loss: 0.12998 - Test loss: 0.06118\n",
      "Epoch 821 - lr: 0.07772 - Train loss: 0.12989 - Test loss: 0.06116\n",
      "Epoch 822 - lr: 0.07770 - Train loss: 0.12980 - Test loss: 0.06115\n",
      "Epoch 823 - lr: 0.07767 - Train loss: 0.12972 - Test loss: 0.06113\n",
      "Epoch 824 - lr: 0.07765 - Train loss: 0.12963 - Test loss: 0.06112\n",
      "Epoch 825 - lr: 0.07762 - Train loss: 0.12955 - Test loss: 0.06111\n",
      "Epoch 826 - lr: 0.07760 - Train loss: 0.12946 - Test loss: 0.06109\n",
      "Epoch 827 - lr: 0.07758 - Train loss: 0.12937 - Test loss: 0.06108\n",
      "Epoch 828 - lr: 0.07755 - Train loss: 0.12929 - Test loss: 0.06106\n",
      "Epoch 829 - lr: 0.07753 - Train loss: 0.12920 - Test loss: 0.06104\n",
      "Epoch 830 - lr: 0.07751 - Train loss: 0.12911 - Test loss: 0.06103\n",
      "Epoch 831 - lr: 0.07748 - Train loss: 0.12903 - Test loss: 0.06101\n",
      "Epoch 832 - lr: 0.07746 - Train loss: 0.12894 - Test loss: 0.06100\n",
      "Epoch 833 - lr: 0.07743 - Train loss: 0.12885 - Test loss: 0.06098\n",
      "Epoch 834 - lr: 0.07741 - Train loss: 0.12877 - Test loss: 0.06097\n",
      "Epoch 835 - lr: 0.07739 - Train loss: 0.12868 - Test loss: 0.06095\n",
      "Epoch 836 - lr: 0.07736 - Train loss: 0.12859 - Test loss: 0.06094\n",
      "Epoch 837 - lr: 0.07734 - Train loss: 0.12851 - Test loss: 0.06092\n",
      "Epoch 838 - lr: 0.07732 - Train loss: 0.12842 - Test loss: 0.06090\n",
      "Epoch 839 - lr: 0.07729 - Train loss: 0.12833 - Test loss: 0.06089\n",
      "Epoch 840 - lr: 0.07727 - Train loss: 0.12825 - Test loss: 0.06087\n",
      "Epoch 841 - lr: 0.07724 - Train loss: 0.12816 - Test loss: 0.06086\n",
      "Epoch 842 - lr: 0.07722 - Train loss: 0.12807 - Test loss: 0.06084\n",
      "Epoch 843 - lr: 0.07720 - Train loss: 0.12799 - Test loss: 0.06082\n",
      "Epoch 844 - lr: 0.07717 - Train loss: 0.12790 - Test loss: 0.06081\n",
      "Epoch 845 - lr: 0.07715 - Train loss: 0.12781 - Test loss: 0.06079\n",
      "Epoch 846 - lr: 0.07713 - Train loss: 0.12773 - Test loss: 0.06078\n",
      "Epoch 847 - lr: 0.07710 - Train loss: 0.12764 - Test loss: 0.06076\n",
      "Epoch 848 - lr: 0.07708 - Train loss: 0.12755 - Test loss: 0.06074\n",
      "Epoch 849 - lr: 0.07705 - Train loss: 0.12747 - Test loss: 0.06073\n",
      "Epoch 850 - lr: 0.07703 - Train loss: 0.12738 - Test loss: 0.06071\n",
      "Epoch 851 - lr: 0.07701 - Train loss: 0.12729 - Test loss: 0.06070\n",
      "Epoch 852 - lr: 0.07698 - Train loss: 0.12721 - Test loss: 0.06068\n",
      "Epoch 853 - lr: 0.07696 - Train loss: 0.12712 - Test loss: 0.06067\n",
      "Epoch 854 - lr: 0.07694 - Train loss: 0.12704 - Test loss: 0.06065\n",
      "Epoch 855 - lr: 0.07691 - Train loss: 0.12695 - Test loss: 0.06063\n",
      "Epoch 856 - lr: 0.07689 - Train loss: 0.12686 - Test loss: 0.06062\n",
      "Epoch 857 - lr: 0.07687 - Train loss: 0.12678 - Test loss: 0.06060\n",
      "Epoch 858 - lr: 0.07684 - Train loss: 0.12669 - Test loss: 0.06059\n",
      "Epoch 859 - lr: 0.07682 - Train loss: 0.12661 - Test loss: 0.06057\n",
      "Epoch 860 - lr: 0.07680 - Train loss: 0.12652 - Test loss: 0.06056\n",
      "Epoch 861 - lr: 0.07677 - Train loss: 0.12644 - Test loss: 0.06054\n",
      "Epoch 862 - lr: 0.07675 - Train loss: 0.12635 - Test loss: 0.06053\n",
      "Epoch 863 - lr: 0.07672 - Train loss: 0.12627 - Test loss: 0.06051\n",
      "Epoch 864 - lr: 0.07670 - Train loss: 0.12618 - Test loss: 0.06050\n",
      "Epoch 865 - lr: 0.07668 - Train loss: 0.12610 - Test loss: 0.06048\n",
      "Epoch 866 - lr: 0.07665 - Train loss: 0.12601 - Test loss: 0.06047\n",
      "Epoch 867 - lr: 0.07663 - Train loss: 0.12593 - Test loss: 0.06045\n",
      "Epoch 868 - lr: 0.07661 - Train loss: 0.12585 - Test loss: 0.06044\n",
      "Epoch 869 - lr: 0.07658 - Train loss: 0.12576 - Test loss: 0.06042\n",
      "Epoch 870 - lr: 0.07656 - Train loss: 0.12568 - Test loss: 0.06041\n",
      "Epoch 871 - lr: 0.07654 - Train loss: 0.12559 - Test loss: 0.06039\n",
      "Epoch 872 - lr: 0.07651 - Train loss: 0.12551 - Test loss: 0.06038\n",
      "Epoch 873 - lr: 0.07649 - Train loss: 0.12543 - Test loss: 0.06036\n",
      "Epoch 874 - lr: 0.07647 - Train loss: 0.12534 - Test loss: 0.06035\n",
      "Epoch 875 - lr: 0.07644 - Train loss: 0.12526 - Test loss: 0.06033\n",
      "Epoch 876 - lr: 0.07642 - Train loss: 0.12518 - Test loss: 0.06032\n",
      "Epoch 877 - lr: 0.07640 - Train loss: 0.12509 - Test loss: 0.06031\n",
      "Epoch 878 - lr: 0.07637 - Train loss: 0.12501 - Test loss: 0.06029\n",
      "Epoch 879 - lr: 0.07635 - Train loss: 0.12493 - Test loss: 0.06028\n",
      "Epoch 880 - lr: 0.07632 - Train loss: 0.12484 - Test loss: 0.06026\n",
      "Epoch 881 - lr: 0.07630 - Train loss: 0.12476 - Test loss: 0.06025\n",
      "Epoch 882 - lr: 0.07628 - Train loss: 0.12468 - Test loss: 0.06023\n",
      "Epoch 883 - lr: 0.07625 - Train loss: 0.12459 - Test loss: 0.06022\n",
      "Epoch 884 - lr: 0.07623 - Train loss: 0.12451 - Test loss: 0.06021\n",
      "Epoch 885 - lr: 0.07621 - Train loss: 0.12443 - Test loss: 0.06019\n",
      "Epoch 886 - lr: 0.07618 - Train loss: 0.12434 - Test loss: 0.06018\n",
      "Epoch 887 - lr: 0.07616 - Train loss: 0.12426 - Test loss: 0.06016\n",
      "Epoch 888 - lr: 0.07614 - Train loss: 0.12418 - Test loss: 0.06015\n",
      "Epoch 889 - lr: 0.07611 - Train loss: 0.12409 - Test loss: 0.06014\n",
      "Epoch 890 - lr: 0.07609 - Train loss: 0.12401 - Test loss: 0.06012\n",
      "Epoch 891 - lr: 0.07607 - Train loss: 0.12392 - Test loss: 0.06011\n",
      "Epoch 892 - lr: 0.07604 - Train loss: 0.12384 - Test loss: 0.06009\n",
      "Epoch 893 - lr: 0.07602 - Train loss: 0.12376 - Test loss: 0.06008\n",
      "Epoch 894 - lr: 0.07600 - Train loss: 0.12367 - Test loss: 0.06007\n",
      "Epoch 895 - lr: 0.07597 - Train loss: 0.12359 - Test loss: 0.06005\n",
      "Epoch 896 - lr: 0.07595 - Train loss: 0.12350 - Test loss: 0.06004\n",
      "Epoch 897 - lr: 0.07593 - Train loss: 0.12342 - Test loss: 0.06002\n",
      "Epoch 898 - lr: 0.07590 - Train loss: 0.12333 - Test loss: 0.06001\n",
      "Epoch 899 - lr: 0.07588 - Train loss: 0.12325 - Test loss: 0.06000\n",
      "Epoch 900 - lr: 0.07586 - Train loss: 0.12316 - Test loss: 0.05998\n",
      "Epoch 901 - lr: 0.07583 - Train loss: 0.12308 - Test loss: 0.05997\n",
      "Epoch 902 - lr: 0.07581 - Train loss: 0.12299 - Test loss: 0.05995\n",
      "Epoch 903 - lr: 0.07579 - Train loss: 0.12291 - Test loss: 0.05994\n",
      "Epoch 904 - lr: 0.07576 - Train loss: 0.12282 - Test loss: 0.05992\n",
      "Epoch 905 - lr: 0.07574 - Train loss: 0.12274 - Test loss: 0.05991\n",
      "Epoch 906 - lr: 0.07572 - Train loss: 0.12265 - Test loss: 0.05990\n",
      "Epoch 907 - lr: 0.07569 - Train loss: 0.12256 - Test loss: 0.05988\n",
      "Epoch 908 - lr: 0.07567 - Train loss: 0.12248 - Test loss: 0.05987\n",
      "Epoch 909 - lr: 0.07565 - Train loss: 0.12239 - Test loss: 0.05985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 910 - lr: 0.07563 - Train loss: 0.12230 - Test loss: 0.05984\n",
      "Epoch 911 - lr: 0.07560 - Train loss: 0.12221 - Test loss: 0.05982\n",
      "Epoch 912 - lr: 0.07558 - Train loss: 0.12213 - Test loss: 0.05981\n",
      "Epoch 913 - lr: 0.07556 - Train loss: 0.12204 - Test loss: 0.05979\n",
      "Epoch 914 - lr: 0.07553 - Train loss: 0.12195 - Test loss: 0.05978\n",
      "Epoch 915 - lr: 0.07551 - Train loss: 0.12186 - Test loss: 0.05976\n",
      "Epoch 916 - lr: 0.07549 - Train loss: 0.12177 - Test loss: 0.05975\n",
      "Epoch 917 - lr: 0.07546 - Train loss: 0.12168 - Test loss: 0.05973\n",
      "Epoch 918 - lr: 0.07544 - Train loss: 0.12159 - Test loss: 0.05972\n",
      "Epoch 919 - lr: 0.07542 - Train loss: 0.12150 - Test loss: 0.05970\n",
      "Epoch 920 - lr: 0.07539 - Train loss: 0.12141 - Test loss: 0.05969\n",
      "Epoch 921 - lr: 0.07537 - Train loss: 0.12132 - Test loss: 0.05967\n",
      "Epoch 922 - lr: 0.07535 - Train loss: 0.12123 - Test loss: 0.05965\n",
      "Epoch 923 - lr: 0.07532 - Train loss: 0.12114 - Test loss: 0.05964\n",
      "Epoch 924 - lr: 0.07530 - Train loss: 0.12104 - Test loss: 0.05962\n",
      "Epoch 925 - lr: 0.07528 - Train loss: 0.12095 - Test loss: 0.05961\n",
      "Epoch 926 - lr: 0.07525 - Train loss: 0.12086 - Test loss: 0.05959\n",
      "Epoch 927 - lr: 0.07523 - Train loss: 0.12076 - Test loss: 0.05957\n",
      "Epoch 928 - lr: 0.07521 - Train loss: 0.12067 - Test loss: 0.05956\n",
      "Epoch 929 - lr: 0.07519 - Train loss: 0.12058 - Test loss: 0.05954\n",
      "Epoch 930 - lr: 0.07516 - Train loss: 0.12048 - Test loss: 0.05952\n",
      "Epoch 931 - lr: 0.07514 - Train loss: 0.12039 - Test loss: 0.05951\n",
      "Epoch 932 - lr: 0.07512 - Train loss: 0.12029 - Test loss: 0.05949\n",
      "Epoch 933 - lr: 0.07509 - Train loss: 0.12020 - Test loss: 0.05947\n",
      "Epoch 934 - lr: 0.07507 - Train loss: 0.12010 - Test loss: 0.05946\n",
      "Epoch 935 - lr: 0.07505 - Train loss: 0.12000 - Test loss: 0.05944\n",
      "Epoch 936 - lr: 0.07502 - Train loss: 0.11991 - Test loss: 0.05942\n",
      "Epoch 937 - lr: 0.07500 - Train loss: 0.11981 - Test loss: 0.05941\n",
      "Epoch 938 - lr: 0.07498 - Train loss: 0.11971 - Test loss: 0.05939\n",
      "Epoch 939 - lr: 0.07495 - Train loss: 0.11961 - Test loss: 0.05937\n",
      "Epoch 940 - lr: 0.07493 - Train loss: 0.11952 - Test loss: 0.05935\n",
      "Epoch 941 - lr: 0.07491 - Train loss: 0.11942 - Test loss: 0.05933\n",
      "Epoch 942 - lr: 0.07489 - Train loss: 0.11932 - Test loss: 0.05932\n",
      "Epoch 943 - lr: 0.07486 - Train loss: 0.11922 - Test loss: 0.05930\n",
      "Epoch 944 - lr: 0.07484 - Train loss: 0.11912 - Test loss: 0.05928\n",
      "Epoch 945 - lr: 0.07482 - Train loss: 0.11902 - Test loss: 0.05926\n",
      "Epoch 946 - lr: 0.07479 - Train loss: 0.11891 - Test loss: 0.05924\n",
      "Epoch 947 - lr: 0.07477 - Train loss: 0.11881 - Test loss: 0.05923\n",
      "Epoch 948 - lr: 0.07475 - Train loss: 0.11871 - Test loss: 0.05921\n",
      "Epoch 949 - lr: 0.07473 - Train loss: 0.11861 - Test loss: 0.05919\n",
      "Epoch 950 - lr: 0.07470 - Train loss: 0.11851 - Test loss: 0.05917\n",
      "Epoch 951 - lr: 0.07468 - Train loss: 0.11840 - Test loss: 0.05915\n",
      "Epoch 952 - lr: 0.07466 - Train loss: 0.11830 - Test loss: 0.05913\n",
      "Epoch 953 - lr: 0.07463 - Train loss: 0.11819 - Test loss: 0.05911\n",
      "Epoch 954 - lr: 0.07461 - Train loss: 0.11809 - Test loss: 0.05909\n",
      "Epoch 955 - lr: 0.07459 - Train loss: 0.11798 - Test loss: 0.05907\n",
      "Epoch 956 - lr: 0.07456 - Train loss: 0.11788 - Test loss: 0.05905\n",
      "Epoch 957 - lr: 0.07454 - Train loss: 0.11777 - Test loss: 0.05903\n",
      "Epoch 958 - lr: 0.07452 - Train loss: 0.11767 - Test loss: 0.05901\n",
      "Epoch 959 - lr: 0.07450 - Train loss: 0.11756 - Test loss: 0.05899\n",
      "Epoch 960 - lr: 0.07447 - Train loss: 0.11745 - Test loss: 0.05897\n",
      "Epoch 961 - lr: 0.07445 - Train loss: 0.11734 - Test loss: 0.05895\n",
      "Epoch 962 - lr: 0.07443 - Train loss: 0.11724 - Test loss: 0.05893\n",
      "Epoch 963 - lr: 0.07440 - Train loss: 0.11713 - Test loss: 0.05891\n",
      "Epoch 964 - lr: 0.07438 - Train loss: 0.11702 - Test loss: 0.05889\n",
      "Epoch 965 - lr: 0.07436 - Train loss: 0.11691 - Test loss: 0.05887\n",
      "Epoch 966 - lr: 0.07434 - Train loss: 0.11680 - Test loss: 0.05885\n",
      "Epoch 967 - lr: 0.07431 - Train loss: 0.11669 - Test loss: 0.05883\n",
      "Epoch 968 - lr: 0.07429 - Train loss: 0.11658 - Test loss: 0.05881\n",
      "Epoch 969 - lr: 0.07427 - Train loss: 0.11647 - Test loss: 0.05879\n",
      "Epoch 970 - lr: 0.07424 - Train loss: 0.11636 - Test loss: 0.05877\n",
      "Epoch 971 - lr: 0.07422 - Train loss: 0.11625 - Test loss: 0.05874\n",
      "Epoch 972 - lr: 0.07420 - Train loss: 0.11613 - Test loss: 0.05872\n",
      "Epoch 973 - lr: 0.07418 - Train loss: 0.11602 - Test loss: 0.05870\n",
      "Epoch 974 - lr: 0.07415 - Train loss: 0.11591 - Test loss: 0.05868\n",
      "Epoch 975 - lr: 0.07413 - Train loss: 0.11579 - Test loss: 0.05866\n",
      "Epoch 976 - lr: 0.07411 - Train loss: 0.11568 - Test loss: 0.05864\n",
      "Epoch 977 - lr: 0.07409 - Train loss: 0.11557 - Test loss: 0.05861\n",
      "Epoch 978 - lr: 0.07406 - Train loss: 0.11545 - Test loss: 0.05859\n",
      "Epoch 979 - lr: 0.07404 - Train loss: 0.11534 - Test loss: 0.05857\n",
      "Epoch 980 - lr: 0.07402 - Train loss: 0.11522 - Test loss: 0.05855\n",
      "Epoch 981 - lr: 0.07399 - Train loss: 0.11511 - Test loss: 0.05852\n",
      "Epoch 982 - lr: 0.07397 - Train loss: 0.11499 - Test loss: 0.05850\n",
      "Epoch 983 - lr: 0.07395 - Train loss: 0.11488 - Test loss: 0.05848\n",
      "Epoch 984 - lr: 0.07393 - Train loss: 0.11476 - Test loss: 0.05846\n",
      "Epoch 985 - lr: 0.07390 - Train loss: 0.11464 - Test loss: 0.05843\n",
      "Epoch 986 - lr: 0.07388 - Train loss: 0.11452 - Test loss: 0.05841\n",
      "Epoch 987 - lr: 0.07386 - Train loss: 0.11441 - Test loss: 0.05839\n",
      "Epoch 988 - lr: 0.07384 - Train loss: 0.11429 - Test loss: 0.05836\n",
      "Epoch 989 - lr: 0.07381 - Train loss: 0.11417 - Test loss: 0.05834\n",
      "Epoch 990 - lr: 0.07379 - Train loss: 0.11405 - Test loss: 0.05832\n",
      "Epoch 991 - lr: 0.07377 - Train loss: 0.11393 - Test loss: 0.05829\n",
      "Epoch 992 - lr: 0.07375 - Train loss: 0.11381 - Test loss: 0.05827\n",
      "Epoch 993 - lr: 0.07372 - Train loss: 0.11369 - Test loss: 0.05825\n",
      "Epoch 994 - lr: 0.07370 - Train loss: 0.11357 - Test loss: 0.05822\n",
      "Epoch 995 - lr: 0.07368 - Train loss: 0.11345 - Test loss: 0.05820\n",
      "Epoch 996 - lr: 0.07365 - Train loss: 0.11333 - Test loss: 0.05817\n",
      "Epoch 997 - lr: 0.07363 - Train loss: 0.11321 - Test loss: 0.05815\n",
      "Epoch 998 - lr: 0.07361 - Train loss: 0.11309 - Test loss: 0.05812\n",
      "Epoch 999 - lr: 0.07359 - Train loss: 0.11297 - Test loss: 0.05810\n",
      "Epoch 1000 - lr: 0.07356 - Train loss: 0.11284 - Test loss: 0.05808\n",
      "Epoch 1001 - lr: 0.07354 - Train loss: 0.11272 - Test loss: 0.05805\n",
      "Epoch 1002 - lr: 0.07352 - Train loss: 0.11260 - Test loss: 0.05803\n",
      "Epoch 1003 - lr: 0.07350 - Train loss: 0.11247 - Test loss: 0.05800\n",
      "Epoch 1004 - lr: 0.07347 - Train loss: 0.11235 - Test loss: 0.05798\n",
      "Epoch 1005 - lr: 0.07345 - Train loss: 0.11222 - Test loss: 0.05795\n",
      "Epoch 1006 - lr: 0.07343 - Train loss: 0.11210 - Test loss: 0.05792\n",
      "Epoch 1007 - lr: 0.07341 - Train loss: 0.11197 - Test loss: 0.05790\n",
      "Epoch 1008 - lr: 0.07338 - Train loss: 0.11185 - Test loss: 0.05787\n",
      "Epoch 1009 - lr: 0.07336 - Train loss: 0.11172 - Test loss: 0.05785\n",
      "Epoch 1010 - lr: 0.07334 - Train loss: 0.11160 - Test loss: 0.05782\n",
      "Epoch 1011 - lr: 0.07332 - Train loss: 0.11147 - Test loss: 0.05779\n",
      "Epoch 1012 - lr: 0.07329 - Train loss: 0.11134 - Test loss: 0.05777\n",
      "Epoch 1013 - lr: 0.07327 - Train loss: 0.11121 - Test loss: 0.05774\n",
      "Epoch 1014 - lr: 0.07325 - Train loss: 0.11109 - Test loss: 0.05771\n",
      "Epoch 1015 - lr: 0.07323 - Train loss: 0.11096 - Test loss: 0.05769\n",
      "Epoch 1016 - lr: 0.07320 - Train loss: 0.11083 - Test loss: 0.05766\n",
      "Epoch 1017 - lr: 0.07318 - Train loss: 0.11070 - Test loss: 0.05763\n",
      "Epoch 1018 - lr: 0.07316 - Train loss: 0.11057 - Test loss: 0.05761\n",
      "Epoch 1019 - lr: 0.07314 - Train loss: 0.11044 - Test loss: 0.05758\n",
      "Epoch 1020 - lr: 0.07311 - Train loss: 0.11030 - Test loss: 0.05755\n",
      "Epoch 1021 - lr: 0.07309 - Train loss: 0.11017 - Test loss: 0.05752\n",
      "Epoch 1022 - lr: 0.07307 - Train loss: 0.11004 - Test loss: 0.05749\n",
      "Epoch 1023 - lr: 0.07305 - Train loss: 0.10991 - Test loss: 0.05746\n",
      "Epoch 1024 - lr: 0.07302 - Train loss: 0.10977 - Test loss: 0.05744\n",
      "Epoch 1025 - lr: 0.07300 - Train loss: 0.10964 - Test loss: 0.05741\n",
      "Epoch 1026 - lr: 0.07298 - Train loss: 0.10950 - Test loss: 0.05738\n",
      "Epoch 1027 - lr: 0.07296 - Train loss: 0.10937 - Test loss: 0.05735\n",
      "Epoch 1028 - lr: 0.07293 - Train loss: 0.10923 - Test loss: 0.05732\n",
      "Epoch 1029 - lr: 0.07291 - Train loss: 0.10910 - Test loss: 0.05729\n",
      "Epoch 1030 - lr: 0.07289 - Train loss: 0.10896 - Test loss: 0.05726\n",
      "Epoch 1031 - lr: 0.07287 - Train loss: 0.10882 - Test loss: 0.05723\n",
      "Epoch 1032 - lr: 0.07285 - Train loss: 0.10868 - Test loss: 0.05720\n",
      "Epoch 1033 - lr: 0.07282 - Train loss: 0.10854 - Test loss: 0.05717\n",
      "Epoch 1034 - lr: 0.07280 - Train loss: 0.10840 - Test loss: 0.05714\n",
      "Epoch 1035 - lr: 0.07278 - Train loss: 0.10826 - Test loss: 0.05710\n",
      "Epoch 1036 - lr: 0.07276 - Train loss: 0.10812 - Test loss: 0.05707\n",
      "Epoch 1037 - lr: 0.07273 - Train loss: 0.10798 - Test loss: 0.05704\n",
      "Epoch 1038 - lr: 0.07271 - Train loss: 0.10784 - Test loss: 0.05701\n",
      "Epoch 1039 - lr: 0.07269 - Train loss: 0.10769 - Test loss: 0.05698\n",
      "Epoch 1040 - lr: 0.07267 - Train loss: 0.10755 - Test loss: 0.05694\n",
      "Epoch 1041 - lr: 0.07264 - Train loss: 0.10741 - Test loss: 0.05691\n",
      "Epoch 1042 - lr: 0.07262 - Train loss: 0.10726 - Test loss: 0.05688\n",
      "Epoch 1043 - lr: 0.07260 - Train loss: 0.10711 - Test loss: 0.05684\n",
      "Epoch 1044 - lr: 0.07258 - Train loss: 0.10697 - Test loss: 0.05681\n",
      "Epoch 1045 - lr: 0.07255 - Train loss: 0.10682 - Test loss: 0.05678\n",
      "Epoch 1046 - lr: 0.07253 - Train loss: 0.10667 - Test loss: 0.05674\n",
      "Epoch 1047 - lr: 0.07251 - Train loss: 0.10652 - Test loss: 0.05671\n",
      "Epoch 1048 - lr: 0.07249 - Train loss: 0.10637 - Test loss: 0.05667\n",
      "Epoch 1049 - lr: 0.07247 - Train loss: 0.10622 - Test loss: 0.05664\n",
      "Epoch 1050 - lr: 0.07244 - Train loss: 0.10607 - Test loss: 0.05660\n",
      "Epoch 1051 - lr: 0.07242 - Train loss: 0.10591 - Test loss: 0.05656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1052 - lr: 0.07240 - Train loss: 0.10576 - Test loss: 0.05653\n",
      "Epoch 1053 - lr: 0.07238 - Train loss: 0.10560 - Test loss: 0.05649\n",
      "Epoch 1054 - lr: 0.07235 - Train loss: 0.10545 - Test loss: 0.05645\n",
      "Epoch 1055 - lr: 0.07233 - Train loss: 0.10529 - Test loss: 0.05642\n",
      "Epoch 1056 - lr: 0.07231 - Train loss: 0.10513 - Test loss: 0.05638\n",
      "Epoch 1057 - lr: 0.07229 - Train loss: 0.10498 - Test loss: 0.05634\n",
      "Epoch 1058 - lr: 0.07227 - Train loss: 0.10482 - Test loss: 0.05630\n",
      "Epoch 1059 - lr: 0.07224 - Train loss: 0.10466 - Test loss: 0.05626\n",
      "Epoch 1060 - lr: 0.07222 - Train loss: 0.10449 - Test loss: 0.05622\n",
      "Epoch 1061 - lr: 0.07220 - Train loss: 0.10433 - Test loss: 0.05618\n",
      "Epoch 1062 - lr: 0.07218 - Train loss: 0.10417 - Test loss: 0.05614\n",
      "Epoch 1063 - lr: 0.07216 - Train loss: 0.10400 - Test loss: 0.05610\n",
      "Epoch 1064 - lr: 0.07213 - Train loss: 0.10384 - Test loss: 0.05606\n",
      "Epoch 1065 - lr: 0.07211 - Train loss: 0.10367 - Test loss: 0.05602\n",
      "Epoch 1066 - lr: 0.07209 - Train loss: 0.10351 - Test loss: 0.05598\n",
      "Epoch 1067 - lr: 0.07207 - Train loss: 0.10334 - Test loss: 0.05593\n",
      "Epoch 1068 - lr: 0.07204 - Train loss: 0.10317 - Test loss: 0.05589\n",
      "Epoch 1069 - lr: 0.07202 - Train loss: 0.10300 - Test loss: 0.05585\n",
      "Epoch 1070 - lr: 0.07200 - Train loss: 0.10282 - Test loss: 0.05580\n",
      "Epoch 1071 - lr: 0.07198 - Train loss: 0.10265 - Test loss: 0.05576\n",
      "Epoch 1072 - lr: 0.07196 - Train loss: 0.10248 - Test loss: 0.05571\n",
      "Epoch 1073 - lr: 0.07193 - Train loss: 0.10230 - Test loss: 0.05567\n",
      "Epoch 1074 - lr: 0.07191 - Train loss: 0.10213 - Test loss: 0.05562\n",
      "Epoch 1075 - lr: 0.07189 - Train loss: 0.10195 - Test loss: 0.05558\n",
      "Epoch 1076 - lr: 0.07187 - Train loss: 0.10177 - Test loss: 0.05553\n",
      "Epoch 1077 - lr: 0.07185 - Train loss: 0.10159 - Test loss: 0.05548\n",
      "Epoch 1078 - lr: 0.07182 - Train loss: 0.10141 - Test loss: 0.05544\n",
      "Epoch 1079 - lr: 0.07180 - Train loss: 0.10123 - Test loss: 0.05539\n",
      "Epoch 1080 - lr: 0.07178 - Train loss: 0.10104 - Test loss: 0.05534\n",
      "Epoch 1081 - lr: 0.07176 - Train loss: 0.10086 - Test loss: 0.05529\n",
      "Epoch 1082 - lr: 0.07174 - Train loss: 0.10067 - Test loss: 0.05524\n",
      "Epoch 1083 - lr: 0.07171 - Train loss: 0.10049 - Test loss: 0.05519\n",
      "Epoch 1084 - lr: 0.07169 - Train loss: 0.10030 - Test loss: 0.05514\n",
      "Epoch 1085 - lr: 0.07167 - Train loss: 0.10011 - Test loss: 0.05509\n",
      "Epoch 1086 - lr: 0.07165 - Train loss: 0.09992 - Test loss: 0.05504\n",
      "Epoch 1087 - lr: 0.07163 - Train loss: 0.09973 - Test loss: 0.05499\n",
      "Epoch 1088 - lr: 0.07160 - Train loss: 0.09953 - Test loss: 0.05493\n",
      "Epoch 1089 - lr: 0.07158 - Train loss: 0.09934 - Test loss: 0.05488\n",
      "Epoch 1090 - lr: 0.07156 - Train loss: 0.09914 - Test loss: 0.05483\n",
      "Epoch 1091 - lr: 0.07154 - Train loss: 0.09895 - Test loss: 0.05477\n",
      "Epoch 1092 - lr: 0.07152 - Train loss: 0.09875 - Test loss: 0.05472\n",
      "Epoch 1093 - lr: 0.07149 - Train loss: 0.09855 - Test loss: 0.05466\n",
      "Epoch 1094 - lr: 0.07147 - Train loss: 0.09835 - Test loss: 0.05460\n",
      "Epoch 1095 - lr: 0.07145 - Train loss: 0.09814 - Test loss: 0.05455\n",
      "Epoch 1096 - lr: 0.07143 - Train loss: 0.09794 - Test loss: 0.05449\n",
      "Epoch 1097 - lr: 0.07141 - Train loss: 0.09773 - Test loss: 0.05443\n",
      "Epoch 1098 - lr: 0.07138 - Train loss: 0.09753 - Test loss: 0.05437\n",
      "Epoch 1099 - lr: 0.07136 - Train loss: 0.09732 - Test loss: 0.05431\n",
      "Epoch 1100 - lr: 0.07134 - Train loss: 0.09711 - Test loss: 0.05425\n",
      "Epoch 1101 - lr: 0.07132 - Train loss: 0.09690 - Test loss: 0.05419\n",
      "Epoch 1102 - lr: 0.07130 - Train loss: 0.09669 - Test loss: 0.05413\n",
      "Epoch 1103 - lr: 0.07127 - Train loss: 0.09647 - Test loss: 0.05407\n",
      "Epoch 1104 - lr: 0.07125 - Train loss: 0.09626 - Test loss: 0.05401\n",
      "Epoch 1105 - lr: 0.07123 - Train loss: 0.09604 - Test loss: 0.05394\n",
      "Epoch 1106 - lr: 0.07121 - Train loss: 0.09582 - Test loss: 0.05388\n",
      "Epoch 1107 - lr: 0.07119 - Train loss: 0.09560 - Test loss: 0.05382\n",
      "Epoch 1108 - lr: 0.07117 - Train loss: 0.09538 - Test loss: 0.05375\n",
      "Epoch 1109 - lr: 0.07114 - Train loss: 0.09516 - Test loss: 0.05369\n",
      "Epoch 1110 - lr: 0.07112 - Train loss: 0.09493 - Test loss: 0.05362\n",
      "Epoch 1111 - lr: 0.07110 - Train loss: 0.09471 - Test loss: 0.05355\n",
      "Epoch 1112 - lr: 0.07108 - Train loss: 0.09448 - Test loss: 0.05348\n",
      "Epoch 1113 - lr: 0.07106 - Train loss: 0.09425 - Test loss: 0.05342\n",
      "Epoch 1114 - lr: 0.07103 - Train loss: 0.09402 - Test loss: 0.05335\n",
      "Epoch 1115 - lr: 0.07101 - Train loss: 0.09379 - Test loss: 0.05328\n",
      "Epoch 1116 - lr: 0.07099 - Train loss: 0.09355 - Test loss: 0.05320\n",
      "Epoch 1117 - lr: 0.07097 - Train loss: 0.09332 - Test loss: 0.05313\n",
      "Epoch 1118 - lr: 0.07095 - Train loss: 0.09308 - Test loss: 0.05306\n",
      "Epoch 1119 - lr: 0.07093 - Train loss: 0.09284 - Test loss: 0.05299\n",
      "Epoch 1120 - lr: 0.07090 - Train loss: 0.09260 - Test loss: 0.05291\n",
      "Epoch 1121 - lr: 0.07088 - Train loss: 0.09236 - Test loss: 0.05284\n",
      "Epoch 1122 - lr: 0.07086 - Train loss: 0.09212 - Test loss: 0.05276\n",
      "Epoch 1123 - lr: 0.07084 - Train loss: 0.09187 - Test loss: 0.05269\n",
      "Epoch 1124 - lr: 0.07082 - Train loss: 0.09162 - Test loss: 0.05261\n",
      "Epoch 1125 - lr: 0.07079 - Train loss: 0.09138 - Test loss: 0.05253\n",
      "Epoch 1126 - lr: 0.07077 - Train loss: 0.09113 - Test loss: 0.05246\n",
      "Epoch 1127 - lr: 0.07075 - Train loss: 0.09087 - Test loss: 0.05238\n",
      "Epoch 1128 - lr: 0.07073 - Train loss: 0.09062 - Test loss: 0.05230\n",
      "Epoch 1129 - lr: 0.07071 - Train loss: 0.09036 - Test loss: 0.05222\n",
      "Epoch 1130 - lr: 0.07069 - Train loss: 0.09011 - Test loss: 0.05213\n",
      "Epoch 1131 - lr: 0.07066 - Train loss: 0.08985 - Test loss: 0.05205\n",
      "Epoch 1132 - lr: 0.07064 - Train loss: 0.08959 - Test loss: 0.05197\n",
      "Epoch 1133 - lr: 0.07062 - Train loss: 0.08933 - Test loss: 0.05188\n",
      "Epoch 1134 - lr: 0.07060 - Train loss: 0.08906 - Test loss: 0.05180\n",
      "Epoch 1135 - lr: 0.07058 - Train loss: 0.08880 - Test loss: 0.05171\n",
      "Epoch 1136 - lr: 0.07056 - Train loss: 0.08853 - Test loss: 0.05163\n",
      "Epoch 1137 - lr: 0.07053 - Train loss: 0.08826 - Test loss: 0.05154\n",
      "Epoch 1138 - lr: 0.07051 - Train loss: 0.08799 - Test loss: 0.05145\n",
      "Epoch 1139 - lr: 0.07049 - Train loss: 0.08772 - Test loss: 0.05136\n",
      "Epoch 1140 - lr: 0.07047 - Train loss: 0.08744 - Test loss: 0.05127\n",
      "Epoch 1141 - lr: 0.07045 - Train loss: 0.08717 - Test loss: 0.05118\n",
      "Epoch 1142 - lr: 0.07043 - Train loss: 0.08689 - Test loss: 0.05109\n",
      "Epoch 1143 - lr: 0.07040 - Train loss: 0.08661 - Test loss: 0.05100\n",
      "Epoch 1144 - lr: 0.07038 - Train loss: 0.08633 - Test loss: 0.05091\n",
      "Epoch 1145 - lr: 0.07036 - Train loss: 0.08605 - Test loss: 0.05081\n",
      "Epoch 1146 - lr: 0.07034 - Train loss: 0.08576 - Test loss: 0.05072\n",
      "Epoch 1147 - lr: 0.07032 - Train loss: 0.08547 - Test loss: 0.05062\n",
      "Epoch 1148 - lr: 0.07030 - Train loss: 0.08518 - Test loss: 0.05052\n",
      "Epoch 1149 - lr: 0.07027 - Train loss: 0.08489 - Test loss: 0.05043\n",
      "Epoch 1150 - lr: 0.07025 - Train loss: 0.08460 - Test loss: 0.05033\n",
      "Epoch 1151 - lr: 0.07023 - Train loss: 0.08431 - Test loss: 0.05023\n",
      "Epoch 1152 - lr: 0.07021 - Train loss: 0.08401 - Test loss: 0.05013\n",
      "Epoch 1153 - lr: 0.07019 - Train loss: 0.08371 - Test loss: 0.05003\n",
      "Epoch 1154 - lr: 0.07017 - Train loss: 0.08342 - Test loss: 0.04993\n",
      "Epoch 1155 - lr: 0.07015 - Train loss: 0.08311 - Test loss: 0.04982\n",
      "Epoch 1156 - lr: 0.07012 - Train loss: 0.08281 - Test loss: 0.04972\n",
      "Epoch 1157 - lr: 0.07010 - Train loss: 0.08251 - Test loss: 0.04962\n",
      "Epoch 1158 - lr: 0.07008 - Train loss: 0.08220 - Test loss: 0.04951\n",
      "Epoch 1159 - lr: 0.07006 - Train loss: 0.08189 - Test loss: 0.04940\n",
      "Epoch 1160 - lr: 0.07004 - Train loss: 0.08158 - Test loss: 0.04930\n",
      "Epoch 1161 - lr: 0.07002 - Train loss: 0.08127 - Test loss: 0.04919\n",
      "Epoch 1162 - lr: 0.06999 - Train loss: 0.08096 - Test loss: 0.04908\n",
      "Epoch 1163 - lr: 0.06997 - Train loss: 0.08064 - Test loss: 0.04897\n",
      "Epoch 1164 - lr: 0.06995 - Train loss: 0.08032 - Test loss: 0.04886\n",
      "Epoch 1165 - lr: 0.06993 - Train loss: 0.08000 - Test loss: 0.04875\n",
      "Epoch 1166 - lr: 0.06991 - Train loss: 0.07968 - Test loss: 0.04864\n",
      "Epoch 1167 - lr: 0.06989 - Train loss: 0.07936 - Test loss: 0.04852\n",
      "Epoch 1168 - lr: 0.06987 - Train loss: 0.07904 - Test loss: 0.04841\n",
      "Epoch 1169 - lr: 0.06984 - Train loss: 0.07871 - Test loss: 0.04830\n",
      "Epoch 1170 - lr: 0.06982 - Train loss: 0.07838 - Test loss: 0.04818\n",
      "Epoch 1171 - lr: 0.06980 - Train loss: 0.07805 - Test loss: 0.04807\n",
      "Epoch 1172 - lr: 0.06978 - Train loss: 0.07772 - Test loss: 0.04795\n",
      "Epoch 1173 - lr: 0.06976 - Train loss: 0.07739 - Test loss: 0.04783\n",
      "Epoch 1174 - lr: 0.06974 - Train loss: 0.07705 - Test loss: 0.04771\n",
      "Epoch 1175 - lr: 0.06972 - Train loss: 0.07672 - Test loss: 0.04759\n",
      "Epoch 1176 - lr: 0.06969 - Train loss: 0.07638 - Test loss: 0.04747\n",
      "Epoch 1177 - lr: 0.06967 - Train loss: 0.07604 - Test loss: 0.04735\n",
      "Epoch 1178 - lr: 0.06965 - Train loss: 0.07570 - Test loss: 0.04723\n",
      "Epoch 1179 - lr: 0.06963 - Train loss: 0.07535 - Test loss: 0.04711\n",
      "Epoch 1180 - lr: 0.06961 - Train loss: 0.07501 - Test loss: 0.04699\n",
      "Epoch 1181 - lr: 0.06959 - Train loss: 0.07466 - Test loss: 0.04686\n",
      "Epoch 1182 - lr: 0.06957 - Train loss: 0.07432 - Test loss: 0.04674\n",
      "Epoch 1183 - lr: 0.06955 - Train loss: 0.07397 - Test loss: 0.04661\n",
      "Epoch 1184 - lr: 0.06952 - Train loss: 0.07362 - Test loss: 0.04649\n",
      "Epoch 1185 - lr: 0.06950 - Train loss: 0.07326 - Test loss: 0.04636\n",
      "Epoch 1186 - lr: 0.06948 - Train loss: 0.07291 - Test loss: 0.04624\n",
      "Epoch 1187 - lr: 0.06946 - Train loss: 0.07255 - Test loss: 0.04611\n",
      "Epoch 1188 - lr: 0.06944 - Train loss: 0.07220 - Test loss: 0.04598\n",
      "Epoch 1189 - lr: 0.06942 - Train loss: 0.07184 - Test loss: 0.04585\n",
      "Epoch 1190 - lr: 0.06940 - Train loss: 0.07148 - Test loss: 0.04572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1191 - lr: 0.06937 - Train loss: 0.07112 - Test loss: 0.04559\n",
      "Epoch 1192 - lr: 0.06935 - Train loss: 0.07076 - Test loss: 0.04546\n",
      "Epoch 1193 - lr: 0.06933 - Train loss: 0.07039 - Test loss: 0.04533\n",
      "Epoch 1194 - lr: 0.06931 - Train loss: 0.07003 - Test loss: 0.04520\n",
      "Epoch 1195 - lr: 0.06929 - Train loss: 0.06966 - Test loss: 0.04507\n",
      "Epoch 1196 - lr: 0.06927 - Train loss: 0.06929 - Test loss: 0.04494\n",
      "Epoch 1197 - lr: 0.06925 - Train loss: 0.06892 - Test loss: 0.04480\n",
      "Epoch 1198 - lr: 0.06923 - Train loss: 0.06855 - Test loss: 0.04467\n",
      "Epoch 1199 - lr: 0.06920 - Train loss: 0.06818 - Test loss: 0.04454\n",
      "Epoch 1200 - lr: 0.06918 - Train loss: 0.06781 - Test loss: 0.04440\n",
      "Epoch 1201 - lr: 0.06916 - Train loss: 0.06744 - Test loss: 0.04427\n",
      "Epoch 1202 - lr: 0.06914 - Train loss: 0.06706 - Test loss: 0.04414\n",
      "Epoch 1203 - lr: 0.06912 - Train loss: 0.06669 - Test loss: 0.04400\n",
      "Epoch 1204 - lr: 0.06910 - Train loss: 0.06631 - Test loss: 0.04387\n",
      "Epoch 1205 - lr: 0.06908 - Train loss: 0.06593 - Test loss: 0.04373\n",
      "Epoch 1206 - lr: 0.06906 - Train loss: 0.06556 - Test loss: 0.04360\n",
      "Epoch 1207 - lr: 0.06903 - Train loss: 0.06518 - Test loss: 0.04346\n",
      "Epoch 1208 - lr: 0.06901 - Train loss: 0.06480 - Test loss: 0.04333\n",
      "Epoch 1209 - lr: 0.06899 - Train loss: 0.06442 - Test loss: 0.04319\n",
      "Epoch 1210 - lr: 0.06897 - Train loss: 0.06404 - Test loss: 0.04306\n",
      "Epoch 1211 - lr: 0.06895 - Train loss: 0.06365 - Test loss: 0.04292\n",
      "Epoch 1212 - lr: 0.06893 - Train loss: 0.06327 - Test loss: 0.04278\n",
      "Epoch 1213 - lr: 0.06891 - Train loss: 0.06289 - Test loss: 0.04265\n",
      "Epoch 1214 - lr: 0.06889 - Train loss: 0.06251 - Test loss: 0.04251\n",
      "Epoch 1215 - lr: 0.06887 - Train loss: 0.06212 - Test loss: 0.04238\n",
      "Epoch 1216 - lr: 0.06884 - Train loss: 0.06174 - Test loss: 0.04224\n",
      "Epoch 1217 - lr: 0.06882 - Train loss: 0.06135 - Test loss: 0.04211\n",
      "Epoch 1218 - lr: 0.06880 - Train loss: 0.06097 - Test loss: 0.04197\n",
      "Epoch 1219 - lr: 0.06878 - Train loss: 0.06058 - Test loss: 0.04184\n",
      "Epoch 1220 - lr: 0.06876 - Train loss: 0.06019 - Test loss: 0.04170\n",
      "Epoch 1221 - lr: 0.06874 - Train loss: 0.05981 - Test loss: 0.04157\n",
      "Epoch 1222 - lr: 0.06872 - Train loss: 0.05942 - Test loss: 0.04143\n",
      "Epoch 1223 - lr: 0.06870 - Train loss: 0.05904 - Test loss: 0.04130\n",
      "Epoch 1224 - lr: 0.06868 - Train loss: 0.05865 - Test loss: 0.04116\n",
      "Epoch 1225 - lr: 0.06865 - Train loss: 0.05826 - Test loss: 0.04103\n",
      "Epoch 1226 - lr: 0.06863 - Train loss: 0.05788 - Test loss: 0.04090\n",
      "Epoch 1227 - lr: 0.06861 - Train loss: 0.05749 - Test loss: 0.04077\n",
      "Epoch 1228 - lr: 0.06859 - Train loss: 0.05711 - Test loss: 0.04063\n",
      "Epoch 1229 - lr: 0.06857 - Train loss: 0.05672 - Test loss: 0.04050\n",
      "Epoch 1230 - lr: 0.06855 - Train loss: 0.05634 - Test loss: 0.04037\n",
      "Epoch 1231 - lr: 0.06853 - Train loss: 0.05595 - Test loss: 0.04024\n",
      "Epoch 1232 - lr: 0.06851 - Train loss: 0.05557 - Test loss: 0.04011\n",
      "Epoch 1233 - lr: 0.06849 - Train loss: 0.05518 - Test loss: 0.03998\n",
      "Epoch 1234 - lr: 0.06846 - Train loss: 0.05480 - Test loss: 0.03985\n",
      "Epoch 1235 - lr: 0.06844 - Train loss: 0.05442 - Test loss: 0.03973\n",
      "Epoch 1236 - lr: 0.06842 - Train loss: 0.05403 - Test loss: 0.03960\n",
      "Epoch 1237 - lr: 0.06840 - Train loss: 0.05365 - Test loss: 0.03947\n",
      "Epoch 1238 - lr: 0.06838 - Train loss: 0.05327 - Test loss: 0.03935\n",
      "Epoch 1239 - lr: 0.06836 - Train loss: 0.05289 - Test loss: 0.03922\n",
      "Epoch 1240 - lr: 0.06834 - Train loss: 0.05251 - Test loss: 0.03910\n",
      "Epoch 1241 - lr: 0.06832 - Train loss: 0.05214 - Test loss: 0.03898\n",
      "Epoch 1242 - lr: 0.06830 - Train loss: 0.05176 - Test loss: 0.03885\n",
      "Epoch 1243 - lr: 0.06828 - Train loss: 0.05138 - Test loss: 0.03873\n",
      "Epoch 1244 - lr: 0.06825 - Train loss: 0.05101 - Test loss: 0.03861\n",
      "Epoch 1245 - lr: 0.06823 - Train loss: 0.05064 - Test loss: 0.03849\n",
      "Epoch 1246 - lr: 0.06821 - Train loss: 0.05027 - Test loss: 0.03838\n",
      "Epoch 1247 - lr: 0.06819 - Train loss: 0.04990 - Test loss: 0.03826\n",
      "Epoch 1248 - lr: 0.06817 - Train loss: 0.04953 - Test loss: 0.03814\n",
      "Epoch 1249 - lr: 0.06815 - Train loss: 0.04916 - Test loss: 0.03803\n",
      "Epoch 1250 - lr: 0.06813 - Train loss: 0.04879 - Test loss: 0.03792\n",
      "Epoch 1251 - lr: 0.06811 - Train loss: 0.04843 - Test loss: 0.03780\n",
      "Epoch 1252 - lr: 0.06809 - Train loss: 0.04807 - Test loss: 0.03769\n",
      "Epoch 1253 - lr: 0.06807 - Train loss: 0.04770 - Test loss: 0.03758\n",
      "Epoch 1254 - lr: 0.06805 - Train loss: 0.04734 - Test loss: 0.03747\n",
      "Epoch 1255 - lr: 0.06802 - Train loss: 0.04699 - Test loss: 0.03736\n",
      "Epoch 1256 - lr: 0.06800 - Train loss: 0.04663 - Test loss: 0.03726\n",
      "Epoch 1257 - lr: 0.06798 - Train loss: 0.04628 - Test loss: 0.03715\n",
      "Epoch 1258 - lr: 0.06796 - Train loss: 0.04593 - Test loss: 0.03705\n",
      "Epoch 1259 - lr: 0.06794 - Train loss: 0.04558 - Test loss: 0.03695\n",
      "Epoch 1260 - lr: 0.06792 - Train loss: 0.04523 - Test loss: 0.03684\n",
      "Epoch 1261 - lr: 0.06790 - Train loss: 0.04488 - Test loss: 0.03674\n",
      "Epoch 1262 - lr: 0.06788 - Train loss: 0.04454 - Test loss: 0.03665\n",
      "Epoch 1263 - lr: 0.06786 - Train loss: 0.04420 - Test loss: 0.03655\n",
      "Epoch 1264 - lr: 0.06784 - Train loss: 0.04386 - Test loss: 0.03645\n",
      "Epoch 1265 - lr: 0.06782 - Train loss: 0.04352 - Test loss: 0.03636\n",
      "Epoch 1266 - lr: 0.06780 - Train loss: 0.04319 - Test loss: 0.03627\n",
      "Epoch 1267 - lr: 0.06777 - Train loss: 0.04286 - Test loss: 0.03617\n",
      "Epoch 1268 - lr: 0.06775 - Train loss: 0.04253 - Test loss: 0.03608\n",
      "Epoch 1269 - lr: 0.06773 - Train loss: 0.04220 - Test loss: 0.03599\n",
      "Epoch 1270 - lr: 0.06771 - Train loss: 0.04188 - Test loss: 0.03591\n",
      "Epoch 1271 - lr: 0.06769 - Train loss: 0.04156 - Test loss: 0.03582\n",
      "Epoch 1272 - lr: 0.06767 - Train loss: 0.04124 - Test loss: 0.03574\n",
      "Epoch 1273 - lr: 0.06765 - Train loss: 0.04092 - Test loss: 0.03565\n",
      "Epoch 1274 - lr: 0.06763 - Train loss: 0.04061 - Test loss: 0.03557\n",
      "Epoch 1275 - lr: 0.06761 - Train loss: 0.04030 - Test loss: 0.03549\n",
      "Epoch 1276 - lr: 0.06759 - Train loss: 0.03999 - Test loss: 0.03541\n",
      "Epoch 1277 - lr: 0.06757 - Train loss: 0.03969 - Test loss: 0.03534\n",
      "Epoch 1278 - lr: 0.06755 - Train loss: 0.03939 - Test loss: 0.03526\n",
      "Epoch 1279 - lr: 0.06753 - Train loss: 0.03909 - Test loss: 0.03519\n",
      "Epoch 1280 - lr: 0.06750 - Train loss: 0.03879 - Test loss: 0.03512\n",
      "Epoch 1281 - lr: 0.06748 - Train loss: 0.03850 - Test loss: 0.03504\n",
      "Epoch 1282 - lr: 0.06746 - Train loss: 0.03821 - Test loss: 0.03498\n",
      "Epoch 1283 - lr: 0.06744 - Train loss: 0.03792 - Test loss: 0.03491\n",
      "Epoch 1284 - lr: 0.06742 - Train loss: 0.03763 - Test loss: 0.03484\n",
      "Epoch 1285 - lr: 0.06740 - Train loss: 0.03735 - Test loss: 0.03478\n",
      "Epoch 1286 - lr: 0.06738 - Train loss: 0.03707 - Test loss: 0.03471\n",
      "Epoch 1287 - lr: 0.06736 - Train loss: 0.03680 - Test loss: 0.03465\n",
      "Epoch 1288 - lr: 0.06734 - Train loss: 0.03653 - Test loss: 0.03459\n",
      "Epoch 1289 - lr: 0.06732 - Train loss: 0.03626 - Test loss: 0.03453\n",
      "Epoch 1290 - lr: 0.06730 - Train loss: 0.03599 - Test loss: 0.03447\n",
      "Epoch 1291 - lr: 0.06728 - Train loss: 0.03573 - Test loss: 0.03442\n",
      "Epoch 1292 - lr: 0.06726 - Train loss: 0.03547 - Test loss: 0.03436\n",
      "Epoch 1293 - lr: 0.06724 - Train loss: 0.03521 - Test loss: 0.03431\n",
      "Epoch 1294 - lr: 0.06722 - Train loss: 0.03496 - Test loss: 0.03426\n",
      "Epoch 1295 - lr: 0.06719 - Train loss: 0.03471 - Test loss: 0.03421\n",
      "Epoch 1296 - lr: 0.06717 - Train loss: 0.03446 - Test loss: 0.03416\n",
      "Epoch 1297 - lr: 0.06715 - Train loss: 0.03422 - Test loss: 0.03411\n",
      "Epoch 1298 - lr: 0.06713 - Train loss: 0.03397 - Test loss: 0.03407\n",
      "Epoch 1299 - lr: 0.06711 - Train loss: 0.03374 - Test loss: 0.03402\n",
      "Epoch 1300 - lr: 0.06709 - Train loss: 0.03350 - Test loss: 0.03398\n",
      "Epoch 1301 - lr: 0.06707 - Train loss: 0.03327 - Test loss: 0.03394\n",
      "Epoch 1302 - lr: 0.06705 - Train loss: 0.03304 - Test loss: 0.03390\n",
      "Epoch 1303 - lr: 0.06703 - Train loss: 0.03282 - Test loss: 0.03386\n",
      "Epoch 1304 - lr: 0.06701 - Train loss: 0.03259 - Test loss: 0.03383\n",
      "Epoch 1305 - lr: 0.06699 - Train loss: 0.03237 - Test loss: 0.03379\n",
      "Epoch 1306 - lr: 0.06697 - Train loss: 0.03216 - Test loss: 0.03376\n",
      "Epoch 1307 - lr: 0.06695 - Train loss: 0.03194 - Test loss: 0.03372\n",
      "Epoch 1308 - lr: 0.06693 - Train loss: 0.03173 - Test loss: 0.03369\n",
      "Epoch 1309 - lr: 0.06691 - Train loss: 0.03153 - Test loss: 0.03366\n",
      "Epoch 1310 - lr: 0.06689 - Train loss: 0.03132 - Test loss: 0.03363\n",
      "Epoch 1311 - lr: 0.06687 - Train loss: 0.03112 - Test loss: 0.03360\n",
      "Epoch 1312 - lr: 0.06684 - Train loss: 0.03092 - Test loss: 0.03358\n",
      "Epoch 1313 - lr: 0.06682 - Train loss: 0.03073 - Test loss: 0.03355\n",
      "Epoch 1314 - lr: 0.06680 - Train loss: 0.03054 - Test loss: 0.03353\n",
      "Epoch 1315 - lr: 0.06678 - Train loss: 0.03035 - Test loss: 0.03351\n",
      "Epoch 1316 - lr: 0.06676 - Train loss: 0.03016 - Test loss: 0.03348\n",
      "Epoch 1317 - lr: 0.06674 - Train loss: 0.02998 - Test loss: 0.03346\n",
      "Epoch 1318 - lr: 0.06672 - Train loss: 0.02980 - Test loss: 0.03345\n",
      "Epoch 1319 - lr: 0.06670 - Train loss: 0.02962 - Test loss: 0.03343\n",
      "Epoch 1320 - lr: 0.06668 - Train loss: 0.02945 - Test loss: 0.03341\n",
      "Epoch 1321 - lr: 0.06666 - Train loss: 0.02927 - Test loss: 0.03340\n",
      "Epoch 1322 - lr: 0.06664 - Train loss: 0.02911 - Test loss: 0.03338\n",
      "Epoch 1323 - lr: 0.06662 - Train loss: 0.02894 - Test loss: 0.03337\n",
      "Epoch 1324 - lr: 0.06660 - Train loss: 0.02878 - Test loss: 0.03336\n",
      "Epoch 1325 - lr: 0.06658 - Train loss: 0.02861 - Test loss: 0.03335\n",
      "Epoch 1326 - lr: 0.06656 - Train loss: 0.02846 - Test loss: 0.03334\n",
      "Epoch 1327 - lr: 0.06654 - Train loss: 0.02830 - Test loss: 0.03333\n",
      "Epoch 1328 - lr: 0.06652 - Train loss: 0.02815 - Test loss: 0.03332\n",
      "Epoch 1329 - lr: 0.06650 - Train loss: 0.02800 - Test loss: 0.03332\n",
      "Epoch 1330 - lr: 0.06648 - Train loss: 0.02785 - Test loss: 0.03331\n",
      "Epoch 1331 - lr: 0.06646 - Train loss: 0.02771 - Test loss: 0.03331\n",
      "Epoch 1332 - lr: 0.06644 - Train loss: 0.02756 - Test loss: 0.03331\n",
      "Epoch 1333 - lr: 0.06642 - Train loss: 0.02742 - Test loss: 0.03330\n",
      "Epoch 1334 - lr: 0.06639 - Train loss: 0.02729 - Test loss: 0.03330\n",
      "Epoch 1335 - lr: 0.06637 - Train loss: 0.02715 - Test loss: 0.03330\n",
      "Epoch 1336 - lr: 0.06635 - Train loss: 0.02702 - Test loss: 0.03330\n",
      "Epoch 1337 - lr: 0.06633 - Train loss: 0.02689 - Test loss: 0.03331\n",
      "Epoch 1338 - lr: 0.06631 - Train loss: 0.02676 - Test loss: 0.03331\n",
      "Epoch 1339 - lr: 0.06629 - Train loss: 0.02664 - Test loss: 0.03331\n",
      "Epoch 1340 - lr: 0.06627 - Train loss: 0.02651 - Test loss: 0.03332\n",
      "Epoch 1341 - lr: 0.06625 - Train loss: 0.02639 - Test loss: 0.03333\n",
      "Epoch 1342 - lr: 0.06623 - Train loss: 0.02628 - Test loss: 0.03333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1343 - lr: 0.06621 - Train loss: 0.02616 - Test loss: 0.03334\n",
      "Epoch 1344 - lr: 0.06619 - Train loss: 0.02604 - Test loss: 0.03335\n",
      "Epoch 1345 - lr: 0.06617 - Train loss: 0.02593 - Test loss: 0.03336\n",
      "Epoch 1346 - lr: 0.06615 - Train loss: 0.02582 - Test loss: 0.03337\n",
      "Epoch 1347 - lr: 0.06613 - Train loss: 0.02572 - Test loss: 0.03338\n",
      "Epoch 1348 - lr: 0.06611 - Train loss: 0.02561 - Test loss: 0.03339\n",
      "Epoch 1349 - lr: 0.06609 - Train loss: 0.02551 - Test loss: 0.03341\n",
      "Epoch 1350 - lr: 0.06607 - Train loss: 0.02541 - Test loss: 0.03342\n",
      "Epoch 1351 - lr: 0.06605 - Train loss: 0.02531 - Test loss: 0.03344\n",
      "Epoch 1352 - lr: 0.06603 - Train loss: 0.02521 - Test loss: 0.03345\n",
      "Epoch 1353 - lr: 0.06601 - Train loss: 0.02511 - Test loss: 0.03347\n",
      "Epoch 1354 - lr: 0.06599 - Train loss: 0.02502 - Test loss: 0.03349\n",
      "Epoch 1355 - lr: 0.06597 - Train loss: 0.02493 - Test loss: 0.03351\n",
      "Epoch 1356 - lr: 0.06595 - Train loss: 0.02484 - Test loss: 0.03352\n",
      "Epoch 1357 - lr: 0.06593 - Train loss: 0.02475 - Test loss: 0.03354\n",
      "Epoch 1358 - lr: 0.06591 - Train loss: 0.02466 - Test loss: 0.03356\n",
      "Epoch 1359 - lr: 0.06589 - Train loss: 0.02458 - Test loss: 0.03359\n",
      "Epoch 1360 - lr: 0.06587 - Train loss: 0.02450 - Test loss: 0.03361\n",
      "Epoch 1361 - lr: 0.06585 - Train loss: 0.02441 - Test loss: 0.03363\n",
      "Epoch 1362 - lr: 0.06583 - Train loss: 0.02434 - Test loss: 0.03365\n",
      "Epoch 1363 - lr: 0.06581 - Train loss: 0.02426 - Test loss: 0.03368\n",
      "Epoch 1364 - lr: 0.06579 - Train loss: 0.02418 - Test loss: 0.03370\n",
      "Epoch 1365 - lr: 0.06577 - Train loss: 0.02411 - Test loss: 0.03373\n",
      "Epoch 1366 - lr: 0.06575 - Train loss: 0.02403 - Test loss: 0.03376\n",
      "Epoch 1367 - lr: 0.06573 - Train loss: 0.02396 - Test loss: 0.03378\n",
      "Epoch 1368 - lr: 0.06571 - Train loss: 0.02389 - Test loss: 0.03381\n",
      "Epoch 1369 - lr: 0.06569 - Train loss: 0.02382 - Test loss: 0.03384\n",
      "Epoch 1370 - lr: 0.06566 - Train loss: 0.02376 - Test loss: 0.03387\n",
      "Epoch 1371 - lr: 0.06564 - Train loss: 0.02369 - Test loss: 0.03390\n",
      "Epoch 1372 - lr: 0.06562 - Train loss: 0.02363 - Test loss: 0.03393\n",
      "Epoch 1373 - lr: 0.06560 - Train loss: 0.02357 - Test loss: 0.03396\n",
      "Epoch 1374 - lr: 0.06558 - Train loss: 0.02350 - Test loss: 0.03399\n",
      "Epoch 1375 - lr: 0.06556 - Train loss: 0.02344 - Test loss: 0.03402\n",
      "Epoch 1376 - lr: 0.06554 - Train loss: 0.02339 - Test loss: 0.03405\n",
      "Epoch 1377 - lr: 0.06552 - Train loss: 0.02333 - Test loss: 0.03409\n",
      "Epoch 1378 - lr: 0.06550 - Train loss: 0.02327 - Test loss: 0.03412\n",
      "Epoch 1379 - lr: 0.06548 - Train loss: 0.02322 - Test loss: 0.03415\n",
      "Epoch 1380 - lr: 0.06546 - Train loss: 0.02316 - Test loss: 0.03419\n",
      "Epoch 1381 - lr: 0.06544 - Train loss: 0.02311 - Test loss: 0.03422\n",
      "Epoch 1382 - lr: 0.06542 - Train loss: 0.02306 - Test loss: 0.03426\n",
      "Epoch 1383 - lr: 0.06540 - Train loss: 0.02301 - Test loss: 0.03430\n",
      "Epoch 1384 - lr: 0.06538 - Train loss: 0.02296 - Test loss: 0.03433\n",
      "Epoch 1385 - lr: 0.06536 - Train loss: 0.02291 - Test loss: 0.03437\n",
      "Epoch 1386 - lr: 0.06534 - Train loss: 0.02287 - Test loss: 0.03441\n",
      "Epoch 1387 - lr: 0.06532 - Train loss: 0.02282 - Test loss: 0.03445\n",
      "Epoch 1388 - lr: 0.06530 - Train loss: 0.02278 - Test loss: 0.03449\n",
      "Epoch 1389 - lr: 0.06528 - Train loss: 0.02273 - Test loss: 0.03453\n",
      "Epoch 1390 - lr: 0.06526 - Train loss: 0.02269 - Test loss: 0.03457\n",
      "Epoch 1391 - lr: 0.06524 - Train loss: 0.02265 - Test loss: 0.03461\n",
      "Epoch 1392 - lr: 0.06522 - Train loss: 0.02261 - Test loss: 0.03465\n",
      "Epoch 1393 - lr: 0.06520 - Train loss: 0.02257 - Test loss: 0.03469\n",
      "Epoch 1394 - lr: 0.06518 - Train loss: 0.02253 - Test loss: 0.03473\n",
      "Epoch 1395 - lr: 0.06516 - Train loss: 0.02249 - Test loss: 0.03477\n",
      "Epoch 1396 - lr: 0.06514 - Train loss: 0.02246 - Test loss: 0.03482\n",
      "Epoch 1397 - lr: 0.06512 - Train loss: 0.02242 - Test loss: 0.03486\n",
      "Epoch 1398 - lr: 0.06510 - Train loss: 0.02239 - Test loss: 0.03490\n",
      "Epoch 1399 - lr: 0.06508 - Train loss: 0.02235 - Test loss: 0.03495\n",
      "Epoch 1400 - lr: 0.06506 - Train loss: 0.02232 - Test loss: 0.03499\n",
      "Epoch 1401 - lr: 0.06504 - Train loss: 0.02229 - Test loss: 0.03503\n",
      "Epoch 1402 - lr: 0.06502 - Train loss: 0.02226 - Test loss: 0.03508\n",
      "Epoch 1403 - lr: 0.06500 - Train loss: 0.02223 - Test loss: 0.03513\n",
      "Epoch 1404 - lr: 0.06498 - Train loss: 0.02220 - Test loss: 0.03517\n",
      "Epoch 1405 - lr: 0.06496 - Train loss: 0.02217 - Test loss: 0.03522\n",
      "Epoch 1406 - lr: 0.06494 - Train loss: 0.02214 - Test loss: 0.03526\n",
      "Epoch 1407 - lr: 0.06492 - Train loss: 0.02211 - Test loss: 0.03531\n",
      "Epoch 1408 - lr: 0.06490 - Train loss: 0.02208 - Test loss: 0.03536\n",
      "Epoch 1409 - lr: 0.06488 - Train loss: 0.02206 - Test loss: 0.03541\n",
      "Epoch 1410 - lr: 0.06486 - Train loss: 0.02203 - Test loss: 0.03545\n",
      "Epoch 1411 - lr: 0.06484 - Train loss: 0.02201 - Test loss: 0.03550\n",
      "Epoch 1412 - lr: 0.06482 - Train loss: 0.02198 - Test loss: 0.03555\n",
      "Epoch 1413 - lr: 0.06480 - Train loss: 0.02196 - Test loss: 0.03560\n",
      "Epoch 1414 - lr: 0.06478 - Train loss: 0.02194 - Test loss: 0.03565\n",
      "Epoch 1415 - lr: 0.06476 - Train loss: 0.02192 - Test loss: 0.03570\n",
      "Epoch 1416 - lr: 0.06474 - Train loss: 0.02189 - Test loss: 0.03575\n",
      "Epoch 1417 - lr: 0.06472 - Train loss: 0.02187 - Test loss: 0.03580\n",
      "Epoch 1418 - lr: 0.06470 - Train loss: 0.02185 - Test loss: 0.03585\n",
      "Epoch 1419 - lr: 0.06468 - Train loss: 0.02183 - Test loss: 0.03590\n",
      "Epoch 1420 - lr: 0.06466 - Train loss: 0.02181 - Test loss: 0.03595\n",
      "Epoch 1421 - lr: 0.06464 - Train loss: 0.02180 - Test loss: 0.03600\n",
      "Epoch 1422 - lr: 0.06462 - Train loss: 0.02178 - Test loss: 0.03606\n",
      "Epoch 1423 - lr: 0.06461 - Train loss: 0.02176 - Test loss: 0.03611\n",
      "Epoch 1424 - lr: 0.06459 - Train loss: 0.02174 - Test loss: 0.03616\n",
      "Epoch 1425 - lr: 0.06457 - Train loss: 0.02173 - Test loss: 0.03621\n",
      "Epoch 1426 - lr: 0.06455 - Train loss: 0.02171 - Test loss: 0.03627\n",
      "Epoch 1427 - lr: 0.06453 - Train loss: 0.02169 - Test loss: 0.03632\n",
      "Epoch 1428 - lr: 0.06451 - Train loss: 0.02168 - Test loss: 0.03637\n",
      "Epoch 1429 - lr: 0.06449 - Train loss: 0.02167 - Test loss: 0.03643\n",
      "Epoch 1430 - lr: 0.06447 - Train loss: 0.02165 - Test loss: 0.03648\n",
      "Epoch 1431 - lr: 0.06445 - Train loss: 0.02164 - Test loss: 0.03653\n",
      "Epoch 1432 - lr: 0.06443 - Train loss: 0.02162 - Test loss: 0.03659\n",
      "Epoch 1433 - lr: 0.06441 - Train loss: 0.02161 - Test loss: 0.03664\n",
      "Epoch 1434 - lr: 0.06439 - Train loss: 0.02160 - Test loss: 0.03670\n",
      "Epoch 1435 - lr: 0.06437 - Train loss: 0.02159 - Test loss: 0.03675\n",
      "Epoch 1436 - lr: 0.06435 - Train loss: 0.02158 - Test loss: 0.03681\n",
      "Epoch 1437 - lr: 0.06433 - Train loss: 0.02156 - Test loss: 0.03686\n",
      "Epoch 1438 - lr: 0.06431 - Train loss: 0.02155 - Test loss: 0.03692\n",
      "Epoch 1439 - lr: 0.06429 - Train loss: 0.02154 - Test loss: 0.03698\n",
      "Epoch 1440 - lr: 0.06427 - Train loss: 0.02153 - Test loss: 0.03703\n",
      "Epoch 1441 - lr: 0.06425 - Train loss: 0.02152 - Test loss: 0.03709\n",
      "Epoch 1442 - lr: 0.06423 - Train loss: 0.02152 - Test loss: 0.03714\n",
      "Epoch 1443 - lr: 0.06421 - Train loss: 0.02151 - Test loss: 0.03720\n",
      "Epoch 1444 - lr: 0.06419 - Train loss: 0.02150 - Test loss: 0.03726\n",
      "Epoch 1445 - lr: 0.06417 - Train loss: 0.02149 - Test loss: 0.03732\n",
      "Epoch 1446 - lr: 0.06415 - Train loss: 0.02148 - Test loss: 0.03737\n",
      "Epoch 1447 - lr: 0.06413 - Train loss: 0.02147 - Test loss: 0.03743\n",
      "Epoch 1448 - lr: 0.06411 - Train loss: 0.02147 - Test loss: 0.03749\n",
      "Epoch 1449 - lr: 0.06409 - Train loss: 0.02146 - Test loss: 0.03755\n",
      "Epoch 1450 - lr: 0.06407 - Train loss: 0.02145 - Test loss: 0.03760\n",
      "Epoch 1451 - lr: 0.06405 - Train loss: 0.02145 - Test loss: 0.03766\n",
      "Epoch 1452 - lr: 0.06403 - Train loss: 0.02144 - Test loss: 0.03772\n",
      "Epoch 1453 - lr: 0.06401 - Train loss: 0.02144 - Test loss: 0.03778\n",
      "Epoch 1454 - lr: 0.06399 - Train loss: 0.02143 - Test loss: 0.03784\n",
      "Epoch 1455 - lr: 0.06397 - Train loss: 0.02143 - Test loss: 0.03790\n",
      "Epoch 1456 - lr: 0.06395 - Train loss: 0.02142 - Test loss: 0.03795\n",
      "Epoch 1457 - lr: 0.06393 - Train loss: 0.02142 - Test loss: 0.03801\n",
      "Epoch 1458 - lr: 0.06391 - Train loss: 0.02141 - Test loss: 0.03807\n",
      "Epoch 1459 - lr: 0.06389 - Train loss: 0.02141 - Test loss: 0.03813\n",
      "Epoch 1460 - lr: 0.06388 - Train loss: 0.02141 - Test loss: 0.03819\n",
      "Epoch 1461 - lr: 0.06386 - Train loss: 0.02140 - Test loss: 0.03825\n",
      "Epoch 1462 - lr: 0.06384 - Train loss: 0.02140 - Test loss: 0.03831\n",
      "Epoch 1463 - lr: 0.06382 - Train loss: 0.02140 - Test loss: 0.03837\n",
      "Epoch 1464 - lr: 0.06380 - Train loss: 0.02139 - Test loss: 0.03843\n",
      "Epoch 1465 - lr: 0.06378 - Train loss: 0.02139 - Test loss: 0.03849\n",
      "Epoch 1466 - lr: 0.06376 - Train loss: 0.02139 - Test loss: 0.03855\n",
      "Epoch 1467 - lr: 0.06374 - Train loss: 0.02139 - Test loss: 0.03861\n",
      "Epoch 1468 - lr: 0.06372 - Train loss: 0.02138 - Test loss: 0.03867\n",
      "Epoch 1469 - lr: 0.06370 - Train loss: 0.02138 - Test loss: 0.03873\n",
      "Epoch 1470 - lr: 0.06368 - Train loss: 0.02138 - Test loss: 0.03879\n",
      "Epoch 1471 - lr: 0.06366 - Train loss: 0.02138 - Test loss: 0.03885\n",
      "Epoch 1472 - lr: 0.06364 - Train loss: 0.02138 - Test loss: 0.03891\n",
      "Epoch 1473 - lr: 0.06362 - Train loss: 0.02138 - Test loss: 0.03897\n",
      "Epoch 1474 - lr: 0.06360 - Train loss: 0.02138 - Test loss: 0.03903\n",
      "Epoch 1475 - lr: 0.06358 - Train loss: 0.02138 - Test loss: 0.03910\n",
      "Epoch 1476 - lr: 0.06356 - Train loss: 0.02138 - Test loss: 0.03916\n",
      "Epoch 1477 - lr: 0.06354 - Train loss: 0.02138 - Test loss: 0.03922\n",
      "Epoch 1478 - lr: 0.06352 - Train loss: 0.02138 - Test loss: 0.03928\n",
      "Epoch 1479 - lr: 0.06350 - Train loss: 0.02138 - Test loss: 0.03934\n",
      "Epoch 1480 - lr: 0.06348 - Train loss: 0.02138 - Test loss: 0.03940\n",
      "Epoch 1481 - lr: 0.06346 - Train loss: 0.02138 - Test loss: 0.03946\n",
      "Epoch 1482 - lr: 0.06345 - Train loss: 0.02138 - Test loss: 0.03953\n",
      "Epoch 1483 - lr: 0.06343 - Train loss: 0.02138 - Test loss: 0.03959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1484 - lr: 0.06341 - Train loss: 0.02138 - Test loss: 0.03965\n",
      "Epoch 1485 - lr: 0.06339 - Train loss: 0.02138 - Test loss: 0.03971\n",
      "Epoch 1486 - lr: 0.06337 - Train loss: 0.02138 - Test loss: 0.03977\n",
      "Epoch 1487 - lr: 0.06335 - Train loss: 0.02138 - Test loss: 0.03983\n",
      "Epoch 1488 - lr: 0.06333 - Train loss: 0.02138 - Test loss: 0.03990\n",
      "Epoch 1489 - lr: 0.06331 - Train loss: 0.02139 - Test loss: 0.03996\n",
      "Epoch 1490 - lr: 0.06329 - Train loss: 0.02139 - Test loss: 0.04002\n",
      "Epoch 1491 - lr: 0.06327 - Train loss: 0.02139 - Test loss: 0.04008\n",
      "Epoch 1492 - lr: 0.06325 - Train loss: 0.02139 - Test loss: 0.04014\n",
      "Epoch 1493 - lr: 0.06323 - Train loss: 0.02139 - Test loss: 0.04021\n",
      "Epoch 1494 - lr: 0.06321 - Train loss: 0.02140 - Test loss: 0.04027\n",
      "Epoch 1495 - lr: 0.06319 - Train loss: 0.02140 - Test loss: 0.04033\n",
      "Epoch 1496 - lr: 0.06317 - Train loss: 0.02140 - Test loss: 0.04039\n",
      "Epoch 1497 - lr: 0.06315 - Train loss: 0.02140 - Test loss: 0.04045\n",
      "Epoch 1498 - lr: 0.06313 - Train loss: 0.02141 - Test loss: 0.04052\n",
      "Epoch 1499 - lr: 0.06312 - Train loss: 0.02141 - Test loss: 0.04058\n",
      "Epoch 1500 - lr: 0.06310 - Train loss: 0.02141 - Test loss: 0.04064\n",
      "Epoch 1501 - lr: 0.06308 - Train loss: 0.02142 - Test loss: 0.04070\n",
      "Epoch 1502 - lr: 0.06306 - Train loss: 0.02142 - Test loss: 0.04077\n",
      "Epoch 1503 - lr: 0.06304 - Train loss: 0.02142 - Test loss: 0.04083\n",
      "Epoch 1504 - lr: 0.06302 - Train loss: 0.02143 - Test loss: 0.04089\n",
      "Epoch 1505 - lr: 0.06300 - Train loss: 0.02143 - Test loss: 0.04095\n",
      "Epoch 1506 - lr: 0.06298 - Train loss: 0.02143 - Test loss: 0.04101\n",
      "Epoch 1507 - lr: 0.06296 - Train loss: 0.02144 - Test loss: 0.04108\n",
      "Epoch 1508 - lr: 0.06294 - Train loss: 0.02144 - Test loss: 0.04114\n",
      "Epoch 1509 - lr: 0.06292 - Train loss: 0.02145 - Test loss: 0.04120\n",
      "Epoch 1510 - lr: 0.06290 - Train loss: 0.02145 - Test loss: 0.04126\n",
      "Epoch 1511 - lr: 0.06288 - Train loss: 0.02145 - Test loss: 0.04133\n",
      "Epoch 1512 - lr: 0.06286 - Train loss: 0.02146 - Test loss: 0.04139\n",
      "Epoch 1513 - lr: 0.06284 - Train loss: 0.02146 - Test loss: 0.04145\n",
      "Epoch 1514 - lr: 0.06283 - Train loss: 0.02147 - Test loss: 0.04151\n",
      "Epoch 1515 - lr: 0.06281 - Train loss: 0.02147 - Test loss: 0.04158\n",
      "Epoch 1516 - lr: 0.06279 - Train loss: 0.02148 - Test loss: 0.04164\n",
      "Epoch 1517 - lr: 0.06277 - Train loss: 0.02148 - Test loss: 0.04170\n",
      "Epoch 1518 - lr: 0.06275 - Train loss: 0.02148 - Test loss: 0.04176\n",
      "Epoch 1519 - lr: 0.06273 - Train loss: 0.02149 - Test loss: 0.04182\n",
      "Epoch 1520 - lr: 0.06271 - Train loss: 0.02149 - Test loss: 0.04189\n",
      "Epoch 1521 - lr: 0.06269 - Train loss: 0.02150 - Test loss: 0.04195\n",
      "Epoch 1522 - lr: 0.06267 - Train loss: 0.02150 - Test loss: 0.04201\n",
      "Epoch 1523 - lr: 0.06265 - Train loss: 0.02151 - Test loss: 0.04207\n",
      "Epoch 1524 - lr: 0.06263 - Train loss: 0.02151 - Test loss: 0.04213\n",
      "Epoch 1525 - lr: 0.06261 - Train loss: 0.02152 - Test loss: 0.04220\n",
      "Epoch 1526 - lr: 0.06259 - Train loss: 0.02152 - Test loss: 0.04226\n",
      "Epoch 1527 - lr: 0.06257 - Train loss: 0.02153 - Test loss: 0.04232\n",
      "Epoch 1528 - lr: 0.06256 - Train loss: 0.02154 - Test loss: 0.04238\n",
      "Epoch 1529 - lr: 0.06254 - Train loss: 0.02154 - Test loss: 0.04244\n",
      "Epoch 1530 - lr: 0.06252 - Train loss: 0.02155 - Test loss: 0.04251\n",
      "Epoch 1531 - lr: 0.06250 - Train loss: 0.02155 - Test loss: 0.04257\n",
      "Epoch 1532 - lr: 0.06248 - Train loss: 0.02156 - Test loss: 0.04263\n",
      "Epoch 1533 - lr: 0.06246 - Train loss: 0.02156 - Test loss: 0.04269\n",
      "Epoch 1534 - lr: 0.06244 - Train loss: 0.02157 - Test loss: 0.04275\n",
      "Epoch 1535 - lr: 0.06242 - Train loss: 0.02157 - Test loss: 0.04282\n",
      "Epoch 1536 - lr: 0.06240 - Train loss: 0.02158 - Test loss: 0.04288\n",
      "Epoch 1537 - lr: 0.06238 - Train loss: 0.02159 - Test loss: 0.04294\n",
      "Epoch 1538 - lr: 0.06236 - Train loss: 0.02159 - Test loss: 0.04300\n",
      "Epoch 1539 - lr: 0.06234 - Train loss: 0.02160 - Test loss: 0.04306\n",
      "Epoch 1540 - lr: 0.06233 - Train loss: 0.02160 - Test loss: 0.04312\n",
      "Epoch 1541 - lr: 0.06231 - Train loss: 0.02161 - Test loss: 0.04318\n",
      "Epoch 1542 - lr: 0.06229 - Train loss: 0.02162 - Test loss: 0.04324\n",
      "Epoch 1543 - lr: 0.06227 - Train loss: 0.02162 - Test loss: 0.04331\n",
      "Epoch 1544 - lr: 0.06225 - Train loss: 0.02163 - Test loss: 0.04337\n",
      "Epoch 1545 - lr: 0.06223 - Train loss: 0.02163 - Test loss: 0.04343\n",
      "Epoch 1546 - lr: 0.06221 - Train loss: 0.02164 - Test loss: 0.04349\n",
      "Epoch 1547 - lr: 0.06219 - Train loss: 0.02165 - Test loss: 0.04355\n",
      "Epoch 1548 - lr: 0.06217 - Train loss: 0.02165 - Test loss: 0.04361\n",
      "Epoch 1549 - lr: 0.06215 - Train loss: 0.02166 - Test loss: 0.04367\n",
      "Epoch 1550 - lr: 0.06213 - Train loss: 0.02166 - Test loss: 0.04373\n",
      "Epoch 1551 - lr: 0.06212 - Train loss: 0.02167 - Test loss: 0.04379\n",
      "Epoch 1552 - lr: 0.06210 - Train loss: 0.02168 - Test loss: 0.04385\n",
      "Epoch 1553 - lr: 0.06208 - Train loss: 0.02168 - Test loss: 0.04391\n",
      "Epoch 1554 - lr: 0.06206 - Train loss: 0.02169 - Test loss: 0.04397\n",
      "Epoch 1555 - lr: 0.06204 - Train loss: 0.02170 - Test loss: 0.04403\n",
      "Epoch 1556 - lr: 0.06202 - Train loss: 0.02170 - Test loss: 0.04409\n",
      "Epoch 1557 - lr: 0.06200 - Train loss: 0.02171 - Test loss: 0.04415\n",
      "Epoch 1558 - lr: 0.06198 - Train loss: 0.02172 - Test loss: 0.04421\n",
      "Epoch 1559 - lr: 0.06196 - Train loss: 0.02172 - Test loss: 0.04427\n",
      "Epoch 1560 - lr: 0.06194 - Train loss: 0.02173 - Test loss: 0.04433\n",
      "Epoch 1561 - lr: 0.06193 - Train loss: 0.02174 - Test loss: 0.04439\n",
      "Epoch 1562 - lr: 0.06191 - Train loss: 0.02174 - Test loss: 0.04445\n",
      "Epoch 1563 - lr: 0.06189 - Train loss: 0.02175 - Test loss: 0.04451\n",
      "Epoch 1564 - lr: 0.06187 - Train loss: 0.02176 - Test loss: 0.04457\n",
      "Epoch 1565 - lr: 0.06185 - Train loss: 0.02176 - Test loss: 0.04463\n",
      "Epoch 1566 - lr: 0.06183 - Train loss: 0.02177 - Test loss: 0.04469\n",
      "Epoch 1567 - lr: 0.06181 - Train loss: 0.02178 - Test loss: 0.04475\n",
      "Epoch 1568 - lr: 0.06179 - Train loss: 0.02178 - Test loss: 0.04481\n",
      "Epoch 1569 - lr: 0.06177 - Train loss: 0.02179 - Test loss: 0.04487\n",
      "Epoch 1570 - lr: 0.06175 - Train loss: 0.02180 - Test loss: 0.04492\n",
      "Epoch 1571 - lr: 0.06174 - Train loss: 0.02180 - Test loss: 0.04498\n",
      "Epoch 1572 - lr: 0.06172 - Train loss: 0.02181 - Test loss: 0.04504\n",
      "Epoch 1573 - lr: 0.06170 - Train loss: 0.02182 - Test loss: 0.04510\n",
      "Epoch 1574 - lr: 0.06168 - Train loss: 0.02182 - Test loss: 0.04516\n",
      "Epoch 1575 - lr: 0.06166 - Train loss: 0.02183 - Test loss: 0.04522\n",
      "Epoch 1576 - lr: 0.06164 - Train loss: 0.02184 - Test loss: 0.04527\n",
      "Epoch 1577 - lr: 0.06162 - Train loss: 0.02185 - Test loss: 0.04533\n",
      "Epoch 1578 - lr: 0.06160 - Train loss: 0.02185 - Test loss: 0.04539\n",
      "Epoch 1579 - lr: 0.06158 - Train loss: 0.02186 - Test loss: 0.04545\n",
      "Epoch 1580 - lr: 0.06156 - Train loss: 0.02187 - Test loss: 0.04550\n",
      "Epoch 1581 - lr: 0.06155 - Train loss: 0.02187 - Test loss: 0.04556\n",
      "Epoch 1582 - lr: 0.06153 - Train loss: 0.02188 - Test loss: 0.04562\n",
      "Epoch 1583 - lr: 0.06151 - Train loss: 0.02189 - Test loss: 0.04568\n",
      "Epoch 1584 - lr: 0.06149 - Train loss: 0.02189 - Test loss: 0.04573\n",
      "Epoch 1585 - lr: 0.06147 - Train loss: 0.02190 - Test loss: 0.04579\n",
      "Epoch 1586 - lr: 0.06145 - Train loss: 0.02191 - Test loss: 0.04585\n",
      "Epoch 1587 - lr: 0.06143 - Train loss: 0.02191 - Test loss: 0.04590\n",
      "Epoch 1588 - lr: 0.06141 - Train loss: 0.02192 - Test loss: 0.04596\n",
      "Epoch 1589 - lr: 0.06140 - Train loss: 0.02193 - Test loss: 0.04602\n",
      "Epoch 1590 - lr: 0.06138 - Train loss: 0.02194 - Test loss: 0.04607\n",
      "Epoch 1591 - lr: 0.06136 - Train loss: 0.02194 - Test loss: 0.04613\n",
      "Epoch 1592 - lr: 0.06134 - Train loss: 0.02195 - Test loss: 0.04618\n",
      "Epoch 1593 - lr: 0.06132 - Train loss: 0.02196 - Test loss: 0.04624\n",
      "Epoch 1594 - lr: 0.06130 - Train loss: 0.02196 - Test loss: 0.04629\n",
      "Epoch 1595 - lr: 0.06128 - Train loss: 0.02197 - Test loss: 0.04635\n",
      "Epoch 1596 - lr: 0.06126 - Train loss: 0.02198 - Test loss: 0.04641\n",
      "Epoch 1597 - lr: 0.06124 - Train loss: 0.02199 - Test loss: 0.04646\n",
      "Epoch 1598 - lr: 0.06123 - Train loss: 0.02199 - Test loss: 0.04651\n",
      "Epoch 1599 - lr: 0.06121 - Train loss: 0.02200 - Test loss: 0.04657\n",
      "Epoch 1600 - lr: 0.06119 - Train loss: 0.02201 - Test loss: 0.04662\n",
      "Epoch 1601 - lr: 0.06117 - Train loss: 0.02201 - Test loss: 0.04668\n",
      "Epoch 1602 - lr: 0.06115 - Train loss: 0.02202 - Test loss: 0.04673\n",
      "Epoch 1603 - lr: 0.06113 - Train loss: 0.02203 - Test loss: 0.04679\n",
      "Epoch 1604 - lr: 0.06111 - Train loss: 0.02203 - Test loss: 0.04684\n",
      "Epoch 1605 - lr: 0.06109 - Train loss: 0.02204 - Test loss: 0.04689\n",
      "Epoch 1606 - lr: 0.06108 - Train loss: 0.02205 - Test loss: 0.04695\n",
      "Epoch 1607 - lr: 0.06106 - Train loss: 0.02206 - Test loss: 0.04700\n",
      "Epoch 1608 - lr: 0.06104 - Train loss: 0.02206 - Test loss: 0.04705\n",
      "Epoch 1609 - lr: 0.06102 - Train loss: 0.02207 - Test loss: 0.04711\n",
      "Epoch 1610 - lr: 0.06100 - Train loss: 0.02208 - Test loss: 0.04716\n",
      "Epoch 1611 - lr: 0.06098 - Train loss: 0.02208 - Test loss: 0.04721\n",
      "Epoch 1612 - lr: 0.06096 - Train loss: 0.02209 - Test loss: 0.04727\n",
      "Epoch 1613 - lr: 0.06094 - Train loss: 0.02210 - Test loss: 0.04732\n",
      "Epoch 1614 - lr: 0.06093 - Train loss: 0.02210 - Test loss: 0.04737\n",
      "Epoch 1615 - lr: 0.06091 - Train loss: 0.02211 - Test loss: 0.04742\n",
      "Epoch 1616 - lr: 0.06089 - Train loss: 0.02212 - Test loss: 0.04747\n",
      "Epoch 1617 - lr: 0.06087 - Train loss: 0.02213 - Test loss: 0.04753\n",
      "Epoch 1618 - lr: 0.06085 - Train loss: 0.02213 - Test loss: 0.04758\n",
      "Epoch 1619 - lr: 0.06083 - Train loss: 0.02214 - Test loss: 0.04763\n",
      "Epoch 1620 - lr: 0.06081 - Train loss: 0.02215 - Test loss: 0.04768\n",
      "Epoch 1621 - lr: 0.06079 - Train loss: 0.02215 - Test loss: 0.04773\n",
      "Epoch 1622 - lr: 0.06078 - Train loss: 0.02216 - Test loss: 0.04778\n",
      "Epoch 1623 - lr: 0.06076 - Train loss: 0.02217 - Test loss: 0.04783\n",
      "Epoch 1624 - lr: 0.06074 - Train loss: 0.02217 - Test loss: 0.04788\n",
      "Epoch 1625 - lr: 0.06072 - Train loss: 0.02218 - Test loss: 0.04793\n",
      "Epoch 1626 - lr: 0.06070 - Train loss: 0.02219 - Test loss: 0.04798\n",
      "Epoch 1627 - lr: 0.06068 - Train loss: 0.02219 - Test loss: 0.04803\n",
      "Epoch 1628 - lr: 0.06066 - Train loss: 0.02220 - Test loss: 0.04808\n",
      "Epoch 1629 - lr: 0.06065 - Train loss: 0.02221 - Test loss: 0.04813\n",
      "Epoch 1630 - lr: 0.06063 - Train loss: 0.02221 - Test loss: 0.04818\n",
      "Epoch 1631 - lr: 0.06061 - Train loss: 0.02222 - Test loss: 0.04823\n",
      "Epoch 1632 - lr: 0.06059 - Train loss: 0.02223 - Test loss: 0.04828\n",
      "Epoch 1633 - lr: 0.06057 - Train loss: 0.02223 - Test loss: 0.04832\n",
      "Epoch 1634 - lr: 0.06055 - Train loss: 0.02224 - Test loss: 0.04837\n",
      "Epoch 1635 - lr: 0.06053 - Train loss: 0.02225 - Test loss: 0.04842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1636 - lr: 0.06052 - Train loss: 0.02225 - Test loss: 0.04847\n",
      "Epoch 1637 - lr: 0.06050 - Train loss: 0.02226 - Test loss: 0.04852\n",
      "Epoch 1638 - lr: 0.06048 - Train loss: 0.02227 - Test loss: 0.04856\n",
      "Epoch 1639 - lr: 0.06046 - Train loss: 0.02227 - Test loss: 0.04861\n",
      "Epoch 1640 - lr: 0.06044 - Train loss: 0.02228 - Test loss: 0.04866\n",
      "Epoch 1641 - lr: 0.06042 - Train loss: 0.02229 - Test loss: 0.04870\n",
      "Epoch 1642 - lr: 0.06040 - Train loss: 0.02229 - Test loss: 0.04875\n",
      "Epoch 1643 - lr: 0.06039 - Train loss: 0.02230 - Test loss: 0.04880\n",
      "Epoch 1644 - lr: 0.06037 - Train loss: 0.02231 - Test loss: 0.04884\n",
      "Epoch 1645 - lr: 0.06035 - Train loss: 0.02231 - Test loss: 0.04889\n",
      "Epoch 1646 - lr: 0.06033 - Train loss: 0.02232 - Test loss: 0.04893\n",
      "Epoch 1647 - lr: 0.06031 - Train loss: 0.02233 - Test loss: 0.04898\n",
      "Epoch 1648 - lr: 0.06029 - Train loss: 0.02233 - Test loss: 0.04903\n",
      "Epoch 1649 - lr: 0.06027 - Train loss: 0.02234 - Test loss: 0.04907\n",
      "Epoch 1650 - lr: 0.06026 - Train loss: 0.02234 - Test loss: 0.04912\n",
      "Epoch 1651 - lr: 0.06024 - Train loss: 0.02235 - Test loss: 0.04916\n",
      "Epoch 1652 - lr: 0.06022 - Train loss: 0.02236 - Test loss: 0.04920\n",
      "Epoch 1653 - lr: 0.06020 - Train loss: 0.02236 - Test loss: 0.04925\n",
      "Epoch 1654 - lr: 0.06018 - Train loss: 0.02237 - Test loss: 0.04929\n",
      "Epoch 1655 - lr: 0.06016 - Train loss: 0.02238 - Test loss: 0.04934\n",
      "Epoch 1656 - lr: 0.06015 - Train loss: 0.02238 - Test loss: 0.04938\n",
      "Epoch 1657 - lr: 0.06013 - Train loss: 0.02239 - Test loss: 0.04942\n",
      "Epoch 1658 - lr: 0.06011 - Train loss: 0.02239 - Test loss: 0.04946\n",
      "Epoch 1659 - lr: 0.06009 - Train loss: 0.02240 - Test loss: 0.04951\n",
      "Epoch 1660 - lr: 0.06007 - Train loss: 0.02241 - Test loss: 0.04955\n",
      "Epoch 1661 - lr: 0.06005 - Train loss: 0.02241 - Test loss: 0.04959\n",
      "Epoch 1662 - lr: 0.06003 - Train loss: 0.02242 - Test loss: 0.04963\n",
      "Epoch 1663 - lr: 0.06002 - Train loss: 0.02242 - Test loss: 0.04968\n",
      "Epoch 1664 - lr: 0.06000 - Train loss: 0.02243 - Test loss: 0.04972\n",
      "Epoch 1665 - lr: 0.05998 - Train loss: 0.02244 - Test loss: 0.04976\n",
      "Epoch 1666 - lr: 0.05996 - Train loss: 0.02244 - Test loss: 0.04980\n",
      "Epoch 1667 - lr: 0.05994 - Train loss: 0.02245 - Test loss: 0.04984\n",
      "Epoch 1668 - lr: 0.05992 - Train loss: 0.02245 - Test loss: 0.04988\n",
      "Epoch 1669 - lr: 0.05991 - Train loss: 0.02246 - Test loss: 0.04992\n",
      "Epoch 1670 - lr: 0.05989 - Train loss: 0.02246 - Test loss: 0.04996\n",
      "Epoch 1671 - lr: 0.05987 - Train loss: 0.02247 - Test loss: 0.05000\n",
      "Epoch 1672 - lr: 0.05985 - Train loss: 0.02248 - Test loss: 0.05004\n",
      "Epoch 1673 - lr: 0.05983 - Train loss: 0.02248 - Test loss: 0.05008\n",
      "Epoch 1674 - lr: 0.05981 - Train loss: 0.02249 - Test loss: 0.05012\n",
      "Epoch 1675 - lr: 0.05980 - Train loss: 0.02249 - Test loss: 0.05016\n",
      "Epoch 1676 - lr: 0.05978 - Train loss: 0.02250 - Test loss: 0.05020\n",
      "Epoch 1677 - lr: 0.05976 - Train loss: 0.02250 - Test loss: 0.05023\n",
      "Epoch 1678 - lr: 0.05974 - Train loss: 0.02251 - Test loss: 0.05027\n",
      "Epoch 1679 - lr: 0.05972 - Train loss: 0.02251 - Test loss: 0.05031\n",
      "Epoch 1680 - lr: 0.05970 - Train loss: 0.02252 - Test loss: 0.05035\n",
      "Epoch 1681 - lr: 0.05969 - Train loss: 0.02252 - Test loss: 0.05038\n",
      "Epoch 1682 - lr: 0.05967 - Train loss: 0.02253 - Test loss: 0.05042\n",
      "Epoch 1683 - lr: 0.05965 - Train loss: 0.02253 - Test loss: 0.05046\n",
      "Epoch 1684 - lr: 0.05963 - Train loss: 0.02254 - Test loss: 0.05049\n",
      "Epoch 1685 - lr: 0.05961 - Train loss: 0.02254 - Test loss: 0.05053\n",
      "Epoch 1686 - lr: 0.05959 - Train loss: 0.02255 - Test loss: 0.05057\n",
      "Epoch 1687 - lr: 0.05958 - Train loss: 0.02255 - Test loss: 0.05060\n",
      "Epoch 1688 - lr: 0.05956 - Train loss: 0.02256 - Test loss: 0.05064\n",
      "Epoch 1689 - lr: 0.05954 - Train loss: 0.02256 - Test loss: 0.05067\n",
      "Epoch 1690 - lr: 0.05952 - Train loss: 0.02257 - Test loss: 0.05071\n",
      "Epoch 1691 - lr: 0.05950 - Train loss: 0.02257 - Test loss: 0.05074\n",
      "Epoch 1692 - lr: 0.05948 - Train loss: 0.02258 - Test loss: 0.05078\n",
      "Epoch 1693 - lr: 0.05947 - Train loss: 0.02258 - Test loss: 0.05081\n",
      "Epoch 1694 - lr: 0.05945 - Train loss: 0.02259 - Test loss: 0.05084\n",
      "Epoch 1695 - lr: 0.05943 - Train loss: 0.02259 - Test loss: 0.05088\n",
      "Epoch 1696 - lr: 0.05941 - Train loss: 0.02260 - Test loss: 0.05091\n",
      "Epoch 1697 - lr: 0.05939 - Train loss: 0.02260 - Test loss: 0.05094\n",
      "Epoch 1698 - lr: 0.05937 - Train loss: 0.02261 - Test loss: 0.05098\n",
      "Epoch 1699 - lr: 0.05936 - Train loss: 0.02261 - Test loss: 0.05101\n",
      "Epoch 1700 - lr: 0.05934 - Train loss: 0.02262 - Test loss: 0.05104\n",
      "Epoch 1701 - lr: 0.05932 - Train loss: 0.02262 - Test loss: 0.05107\n",
      "Epoch 1702 - lr: 0.05930 - Train loss: 0.02262 - Test loss: 0.05111\n",
      "Epoch 1703 - lr: 0.05928 - Train loss: 0.02263 - Test loss: 0.05114\n",
      "Epoch 1704 - lr: 0.05927 - Train loss: 0.02263 - Test loss: 0.05117\n",
      "Epoch 1705 - lr: 0.05925 - Train loss: 0.02264 - Test loss: 0.05120\n",
      "Epoch 1706 - lr: 0.05923 - Train loss: 0.02264 - Test loss: 0.05123\n",
      "Epoch 1707 - lr: 0.05921 - Train loss: 0.02264 - Test loss: 0.05126\n",
      "Epoch 1708 - lr: 0.05919 - Train loss: 0.02265 - Test loss: 0.05129\n",
      "Epoch 1709 - lr: 0.05917 - Train loss: 0.02265 - Test loss: 0.05132\n",
      "Epoch 1710 - lr: 0.05916 - Train loss: 0.02266 - Test loss: 0.05135\n",
      "Epoch 1711 - lr: 0.05914 - Train loss: 0.02266 - Test loss: 0.05138\n",
      "Epoch 1712 - lr: 0.05912 - Train loss: 0.02266 - Test loss: 0.05141\n",
      "Epoch 1713 - lr: 0.05910 - Train loss: 0.02267 - Test loss: 0.05144\n",
      "Epoch 1714 - lr: 0.05908 - Train loss: 0.02267 - Test loss: 0.05146\n",
      "Epoch 1715 - lr: 0.05907 - Train loss: 0.02268 - Test loss: 0.05149\n",
      "Epoch 1716 - lr: 0.05905 - Train loss: 0.02268 - Test loss: 0.05152\n",
      "Epoch 1717 - lr: 0.05903 - Train loss: 0.02268 - Test loss: 0.05155\n",
      "Epoch 1718 - lr: 0.05901 - Train loss: 0.02269 - Test loss: 0.05157\n",
      "Epoch 1719 - lr: 0.05899 - Train loss: 0.02269 - Test loss: 0.05160\n",
      "Epoch 1720 - lr: 0.05897 - Train loss: 0.02269 - Test loss: 0.05163\n",
      "Epoch 1721 - lr: 0.05896 - Train loss: 0.02270 - Test loss: 0.05165\n",
      "Epoch 1722 - lr: 0.05894 - Train loss: 0.02270 - Test loss: 0.05168\n",
      "Epoch 1723 - lr: 0.05892 - Train loss: 0.02270 - Test loss: 0.05171\n",
      "Epoch 1724 - lr: 0.05890 - Train loss: 0.02271 - Test loss: 0.05173\n",
      "Epoch 1725 - lr: 0.05888 - Train loss: 0.02271 - Test loss: 0.05176\n",
      "Epoch 1726 - lr: 0.05887 - Train loss: 0.02271 - Test loss: 0.05178\n",
      "Epoch 1727 - lr: 0.05885 - Train loss: 0.02271 - Test loss: 0.05181\n",
      "Epoch 1728 - lr: 0.05883 - Train loss: 0.02272 - Test loss: 0.05183\n",
      "Epoch 1729 - lr: 0.05881 - Train loss: 0.02272 - Test loss: 0.05186\n",
      "Epoch 1730 - lr: 0.05879 - Train loss: 0.02272 - Test loss: 0.05188\n",
      "Epoch 1731 - lr: 0.05878 - Train loss: 0.02273 - Test loss: 0.05190\n",
      "Epoch 1732 - lr: 0.05876 - Train loss: 0.02273 - Test loss: 0.05193\n",
      "Epoch 1733 - lr: 0.05874 - Train loss: 0.02273 - Test loss: 0.05195\n",
      "Epoch 1734 - lr: 0.05872 - Train loss: 0.02273 - Test loss: 0.05197\n",
      "Epoch 1735 - lr: 0.05870 - Train loss: 0.02274 - Test loss: 0.05199\n",
      "Epoch 1736 - lr: 0.05869 - Train loss: 0.02274 - Test loss: 0.05202\n",
      "Epoch 1737 - lr: 0.05867 - Train loss: 0.02274 - Test loss: 0.05204\n",
      "Epoch 1738 - lr: 0.05865 - Train loss: 0.02274 - Test loss: 0.05206\n",
      "Epoch 1739 - lr: 0.05863 - Train loss: 0.02275 - Test loss: 0.05208\n",
      "Epoch 1740 - lr: 0.05861 - Train loss: 0.02275 - Test loss: 0.05210\n",
      "Epoch 1741 - lr: 0.05860 - Train loss: 0.02275 - Test loss: 0.05212\n",
      "Epoch 1742 - lr: 0.05858 - Train loss: 0.02275 - Test loss: 0.05214\n",
      "Epoch 1743 - lr: 0.05856 - Train loss: 0.02275 - Test loss: 0.05216\n",
      "Epoch 1744 - lr: 0.05854 - Train loss: 0.02276 - Test loss: 0.05218\n",
      "Epoch 1745 - lr: 0.05852 - Train loss: 0.02276 - Test loss: 0.05220\n",
      "Epoch 1746 - lr: 0.05851 - Train loss: 0.02276 - Test loss: 0.05222\n",
      "Epoch 1747 - lr: 0.05849 - Train loss: 0.02276 - Test loss: 0.05224\n",
      "Epoch 1748 - lr: 0.05847 - Train loss: 0.02276 - Test loss: 0.05226\n",
      "Epoch 1749 - lr: 0.05845 - Train loss: 0.02277 - Test loss: 0.05228\n",
      "Epoch 1750 - lr: 0.05843 - Train loss: 0.02277 - Test loss: 0.05230\n",
      "Epoch 1751 - lr: 0.05842 - Train loss: 0.02277 - Test loss: 0.05232\n",
      "Epoch 1752 - lr: 0.05840 - Train loss: 0.02277 - Test loss: 0.05233\n",
      "Epoch 1753 - lr: 0.05838 - Train loss: 0.02277 - Test loss: 0.05235\n",
      "Epoch 1754 - lr: 0.05836 - Train loss: 0.02277 - Test loss: 0.05237\n",
      "Epoch 1755 - lr: 0.05834 - Train loss: 0.02277 - Test loss: 0.05239\n",
      "Epoch 1756 - lr: 0.05833 - Train loss: 0.02277 - Test loss: 0.05240\n",
      "Epoch 1757 - lr: 0.05831 - Train loss: 0.02278 - Test loss: 0.05242\n",
      "Epoch 1758 - lr: 0.05829 - Train loss: 0.02278 - Test loss: 0.05243\n",
      "Epoch 1759 - lr: 0.05827 - Train loss: 0.02278 - Test loss: 0.05245\n",
      "Epoch 1760 - lr: 0.05826 - Train loss: 0.02278 - Test loss: 0.05247\n",
      "Epoch 1761 - lr: 0.05824 - Train loss: 0.02278 - Test loss: 0.05248\n",
      "Epoch 1762 - lr: 0.05822 - Train loss: 0.02278 - Test loss: 0.05250\n",
      "Epoch 1763 - lr: 0.05820 - Train loss: 0.02278 - Test loss: 0.05251\n",
      "Epoch 1764 - lr: 0.05818 - Train loss: 0.02278 - Test loss: 0.05252\n",
      "Epoch 1765 - lr: 0.05817 - Train loss: 0.02278 - Test loss: 0.05254\n",
      "Epoch 1766 - lr: 0.05815 - Train loss: 0.02278 - Test loss: 0.05255\n",
      "Epoch 1767 - lr: 0.05813 - Train loss: 0.02278 - Test loss: 0.05257\n",
      "Epoch 1768 - lr: 0.05811 - Train loss: 0.02278 - Test loss: 0.05258\n",
      "Epoch 1769 - lr: 0.05809 - Train loss: 0.02278 - Test loss: 0.05259\n",
      "Epoch 1770 - lr: 0.05808 - Train loss: 0.02278 - Test loss: 0.05261\n",
      "Epoch 1771 - lr: 0.05806 - Train loss: 0.02278 - Test loss: 0.05262\n",
      "Epoch 1772 - lr: 0.05804 - Train loss: 0.02278 - Test loss: 0.05263\n",
      "Epoch 1773 - lr: 0.05802 - Train loss: 0.02278 - Test loss: 0.05264\n",
      "Epoch 1774 - lr: 0.05801 - Train loss: 0.02278 - Test loss: 0.05265\n",
      "Epoch 1775 - lr: 0.05799 - Train loss: 0.02278 - Test loss: 0.05266\n",
      "Epoch 1776 - lr: 0.05797 - Train loss: 0.02278 - Test loss: 0.05268\n",
      "Epoch 1777 - lr: 0.05795 - Train loss: 0.02278 - Test loss: 0.05269\n",
      "Epoch 1778 - lr: 0.05793 - Train loss: 0.02278 - Test loss: 0.05270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1779 - lr: 0.05792 - Train loss: 0.02278 - Test loss: 0.05271\n",
      "Epoch 1780 - lr: 0.05790 - Train loss: 0.02278 - Test loss: 0.05272\n",
      "Epoch 1781 - lr: 0.05788 - Train loss: 0.02278 - Test loss: 0.05273\n",
      "Epoch 1782 - lr: 0.05786 - Train loss: 0.02278 - Test loss: 0.05274\n",
      "Epoch 1783 - lr: 0.05785 - Train loss: 0.02278 - Test loss: 0.05275\n",
      "Epoch 1784 - lr: 0.05783 - Train loss: 0.02278 - Test loss: 0.05275\n",
      "Epoch 1785 - lr: 0.05781 - Train loss: 0.02278 - Test loss: 0.05276\n",
      "Epoch 1786 - lr: 0.05779 - Train loss: 0.02278 - Test loss: 0.05277\n",
      "Epoch 1787 - lr: 0.05777 - Train loss: 0.02278 - Test loss: 0.05278\n",
      "Epoch 1788 - lr: 0.05776 - Train loss: 0.02278 - Test loss: 0.05279\n",
      "Epoch 1789 - lr: 0.05774 - Train loss: 0.02278 - Test loss: 0.05280\n",
      "Epoch 1790 - lr: 0.05772 - Train loss: 0.02278 - Test loss: 0.05280\n",
      "Epoch 1791 - lr: 0.05770 - Train loss: 0.02277 - Test loss: 0.05281\n",
      "Epoch 1792 - lr: 0.05769 - Train loss: 0.02277 - Test loss: 0.05282\n",
      "Epoch 1793 - lr: 0.05767 - Train loss: 0.02277 - Test loss: 0.05282\n",
      "Epoch 1794 - lr: 0.05765 - Train loss: 0.02277 - Test loss: 0.05283\n",
      "Epoch 1795 - lr: 0.05763 - Train loss: 0.02277 - Test loss: 0.05284\n",
      "Epoch 1796 - lr: 0.05761 - Train loss: 0.02277 - Test loss: 0.05284\n",
      "Epoch 1797 - lr: 0.05760 - Train loss: 0.02277 - Test loss: 0.05285\n",
      "Epoch 1798 - lr: 0.05758 - Train loss: 0.02276 - Test loss: 0.05285\n",
      "Epoch 1799 - lr: 0.05756 - Train loss: 0.02276 - Test loss: 0.05286\n",
      "Epoch 1800 - lr: 0.05754 - Train loss: 0.02276 - Test loss: 0.05286\n",
      "Epoch 1801 - lr: 0.05753 - Train loss: 0.02276 - Test loss: 0.05287\n",
      "Epoch 1802 - lr: 0.05751 - Train loss: 0.02276 - Test loss: 0.05287\n",
      "Epoch 1803 - lr: 0.05749 - Train loss: 0.02276 - Test loss: 0.05288\n",
      "Epoch 1804 - lr: 0.05747 - Train loss: 0.02275 - Test loss: 0.05288\n",
      "Epoch 1805 - lr: 0.05746 - Train loss: 0.02275 - Test loss: 0.05288\n",
      "Epoch 1806 - lr: 0.05744 - Train loss: 0.02275 - Test loss: 0.05289\n",
      "Epoch 1807 - lr: 0.05742 - Train loss: 0.02275 - Test loss: 0.05289\n",
      "Epoch 1808 - lr: 0.05740 - Train loss: 0.02274 - Test loss: 0.05289\n",
      "Epoch 1809 - lr: 0.05739 - Train loss: 0.02274 - Test loss: 0.05289\n",
      "Epoch 1810 - lr: 0.05737 - Train loss: 0.02274 - Test loss: 0.05290\n",
      "Epoch 1811 - lr: 0.05735 - Train loss: 0.02274 - Test loss: 0.05290\n",
      "Epoch 1812 - lr: 0.05733 - Train loss: 0.02273 - Test loss: 0.05290\n",
      "Epoch 1813 - lr: 0.05731 - Train loss: 0.02273 - Test loss: 0.05290\n",
      "Epoch 1814 - lr: 0.05730 - Train loss: 0.02273 - Test loss: 0.05290\n",
      "Epoch 1815 - lr: 0.05728 - Train loss: 0.02273 - Test loss: 0.05290\n",
      "Epoch 1816 - lr: 0.05726 - Train loss: 0.02272 - Test loss: 0.05291\n",
      "Epoch 1817 - lr: 0.05724 - Train loss: 0.02272 - Test loss: 0.05291\n",
      "Epoch 1818 - lr: 0.05723 - Train loss: 0.02272 - Test loss: 0.05291\n",
      "Epoch 1819 - lr: 0.05721 - Train loss: 0.02271 - Test loss: 0.05291\n",
      "Epoch 1820 - lr: 0.05719 - Train loss: 0.02271 - Test loss: 0.05291\n",
      "Epoch 1821 - lr: 0.05717 - Train loss: 0.02271 - Test loss: 0.05291\n",
      "Epoch 1822 - lr: 0.05716 - Train loss: 0.02270 - Test loss: 0.05291\n",
      "Epoch 1823 - lr: 0.05714 - Train loss: 0.02270 - Test loss: 0.05291\n",
      "Epoch 1824 - lr: 0.05712 - Train loss: 0.02270 - Test loss: 0.05290\n",
      "Epoch 1825 - lr: 0.05710 - Train loss: 0.02269 - Test loss: 0.05290\n",
      "Epoch 1826 - lr: 0.05709 - Train loss: 0.02269 - Test loss: 0.05290\n",
      "Epoch 1827 - lr: 0.05707 - Train loss: 0.02269 - Test loss: 0.05290\n",
      "Epoch 1828 - lr: 0.05705 - Train loss: 0.02268 - Test loss: 0.05290\n",
      "Epoch 1829 - lr: 0.05703 - Train loss: 0.02268 - Test loss: 0.05290\n",
      "Epoch 1830 - lr: 0.05702 - Train loss: 0.02268 - Test loss: 0.05289\n",
      "Epoch 1831 - lr: 0.05700 - Train loss: 0.02267 - Test loss: 0.05289\n",
      "Epoch 1832 - lr: 0.05698 - Train loss: 0.02267 - Test loss: 0.05289\n",
      "Epoch 1833 - lr: 0.05696 - Train loss: 0.02266 - Test loss: 0.05289\n",
      "Epoch 1834 - lr: 0.05695 - Train loss: 0.02266 - Test loss: 0.05288\n",
      "Epoch 1835 - lr: 0.05693 - Train loss: 0.02265 - Test loss: 0.05288\n",
      "Epoch 1836 - lr: 0.05691 - Train loss: 0.02265 - Test loss: 0.05288\n",
      "Epoch 1837 - lr: 0.05689 - Train loss: 0.02265 - Test loss: 0.05287\n",
      "Epoch 1838 - lr: 0.05688 - Train loss: 0.02264 - Test loss: 0.05287\n",
      "Epoch 1839 - lr: 0.05686 - Train loss: 0.02264 - Test loss: 0.05286\n",
      "Epoch 1840 - lr: 0.05684 - Train loss: 0.02263 - Test loss: 0.05286\n",
      "Epoch 1841 - lr: 0.05682 - Train loss: 0.02263 - Test loss: 0.05285\n",
      "Epoch 1842 - lr: 0.05681 - Train loss: 0.02262 - Test loss: 0.05285\n",
      "Epoch 1843 - lr: 0.05679 - Train loss: 0.02262 - Test loss: 0.05285\n",
      "Epoch 1844 - lr: 0.05677 - Train loss: 0.02261 - Test loss: 0.05284\n",
      "Epoch 1845 - lr: 0.05675 - Train loss: 0.02261 - Test loss: 0.05283\n",
      "Epoch 1846 - lr: 0.05674 - Train loss: 0.02260 - Test loss: 0.05283\n",
      "Epoch 1847 - lr: 0.05672 - Train loss: 0.02260 - Test loss: 0.05282\n",
      "Epoch 1848 - lr: 0.05670 - Train loss: 0.02259 - Test loss: 0.05282\n",
      "Epoch 1849 - lr: 0.05668 - Train loss: 0.02259 - Test loss: 0.05281\n",
      "Epoch 1850 - lr: 0.05667 - Train loss: 0.02258 - Test loss: 0.05280\n",
      "Epoch 1851 - lr: 0.05665 - Train loss: 0.02258 - Test loss: 0.05280\n",
      "Epoch 1852 - lr: 0.05663 - Train loss: 0.02257 - Test loss: 0.05279\n",
      "Epoch 1853 - lr: 0.05662 - Train loss: 0.02257 - Test loss: 0.05278\n",
      "Epoch 1854 - lr: 0.05660 - Train loss: 0.02256 - Test loss: 0.05278\n",
      "Epoch 1855 - lr: 0.05658 - Train loss: 0.02256 - Test loss: 0.05277\n",
      "Epoch 1856 - lr: 0.05656 - Train loss: 0.02255 - Test loss: 0.05276\n",
      "Epoch 1857 - lr: 0.05655 - Train loss: 0.02255 - Test loss: 0.05275\n",
      "Epoch 1858 - lr: 0.05653 - Train loss: 0.02254 - Test loss: 0.05275\n",
      "Epoch 1859 - lr: 0.05651 - Train loss: 0.02253 - Test loss: 0.05274\n",
      "Epoch 1860 - lr: 0.05649 - Train loss: 0.02253 - Test loss: 0.05273\n",
      "Epoch 1861 - lr: 0.05648 - Train loss: 0.02252 - Test loss: 0.05272\n",
      "Epoch 1862 - lr: 0.05646 - Train loss: 0.02252 - Test loss: 0.05271\n",
      "Epoch 1863 - lr: 0.05644 - Train loss: 0.02251 - Test loss: 0.05270\n",
      "Epoch 1864 - lr: 0.05642 - Train loss: 0.02251 - Test loss: 0.05270\n",
      "Epoch 1865 - lr: 0.05641 - Train loss: 0.02250 - Test loss: 0.05269\n",
      "Epoch 1866 - lr: 0.05639 - Train loss: 0.02249 - Test loss: 0.05268\n",
      "Epoch 1867 - lr: 0.05637 - Train loss: 0.02249 - Test loss: 0.05267\n",
      "Epoch 1868 - lr: 0.05636 - Train loss: 0.02248 - Test loss: 0.05266\n",
      "Epoch 1869 - lr: 0.05634 - Train loss: 0.02247 - Test loss: 0.05265\n",
      "Epoch 1870 - lr: 0.05632 - Train loss: 0.02247 - Test loss: 0.05264\n",
      "Epoch 1871 - lr: 0.05630 - Train loss: 0.02246 - Test loss: 0.05263\n",
      "Epoch 1872 - lr: 0.05629 - Train loss: 0.02245 - Test loss: 0.05262\n",
      "Epoch 1873 - lr: 0.05627 - Train loss: 0.02245 - Test loss: 0.05261\n",
      "Epoch 1874 - lr: 0.05625 - Train loss: 0.02244 - Test loss: 0.05260\n",
      "Epoch 1875 - lr: 0.05623 - Train loss: 0.02244 - Test loss: 0.05259\n",
      "Epoch 1876 - lr: 0.05622 - Train loss: 0.02243 - Test loss: 0.05257\n",
      "Epoch 1877 - lr: 0.05620 - Train loss: 0.02242 - Test loss: 0.05256\n",
      "Epoch 1878 - lr: 0.05618 - Train loss: 0.02241 - Test loss: 0.05255\n",
      "Epoch 1879 - lr: 0.05617 - Train loss: 0.02241 - Test loss: 0.05254\n",
      "Epoch 1880 - lr: 0.05615 - Train loss: 0.02240 - Test loss: 0.05253\n",
      "Epoch 1881 - lr: 0.05613 - Train loss: 0.02239 - Test loss: 0.05252\n",
      "Epoch 1882 - lr: 0.05611 - Train loss: 0.02239 - Test loss: 0.05251\n",
      "Epoch 1883 - lr: 0.05610 - Train loss: 0.02238 - Test loss: 0.05249\n",
      "Epoch 1884 - lr: 0.05608 - Train loss: 0.02237 - Test loss: 0.05248\n",
      "Epoch 1885 - lr: 0.05606 - Train loss: 0.02237 - Test loss: 0.05247\n",
      "Epoch 1886 - lr: 0.05604 - Train loss: 0.02236 - Test loss: 0.05246\n",
      "Epoch 1887 - lr: 0.05603 - Train loss: 0.02235 - Test loss: 0.05244\n",
      "Epoch 1888 - lr: 0.05601 - Train loss: 0.02234 - Test loss: 0.05243\n",
      "Epoch 1889 - lr: 0.05599 - Train loss: 0.02234 - Test loss: 0.05242\n",
      "Epoch 1890 - lr: 0.05598 - Train loss: 0.02233 - Test loss: 0.05241\n",
      "Epoch 1891 - lr: 0.05596 - Train loss: 0.02232 - Test loss: 0.05239\n",
      "Epoch 1892 - lr: 0.05594 - Train loss: 0.02231 - Test loss: 0.05238\n",
      "Epoch 1893 - lr: 0.05592 - Train loss: 0.02231 - Test loss: 0.05237\n",
      "Epoch 1894 - lr: 0.05591 - Train loss: 0.02230 - Test loss: 0.05235\n",
      "Epoch 1895 - lr: 0.05589 - Train loss: 0.02229 - Test loss: 0.05234\n",
      "Epoch 1896 - lr: 0.05587 - Train loss: 0.02228 - Test loss: 0.05232\n",
      "Epoch 1897 - lr: 0.05586 - Train loss: 0.02227 - Test loss: 0.05231\n",
      "Epoch 1898 - lr: 0.05584 - Train loss: 0.02227 - Test loss: 0.05230\n",
      "Epoch 1899 - lr: 0.05582 - Train loss: 0.02226 - Test loss: 0.05228\n",
      "Epoch 1900 - lr: 0.05580 - Train loss: 0.02225 - Test loss: 0.05227\n",
      "Epoch 1901 - lr: 0.05579 - Train loss: 0.02224 - Test loss: 0.05225\n",
      "Epoch 1902 - lr: 0.05577 - Train loss: 0.02223 - Test loss: 0.05224\n",
      "Epoch 1903 - lr: 0.05575 - Train loss: 0.02223 - Test loss: 0.05222\n",
      "Epoch 1904 - lr: 0.05574 - Train loss: 0.02222 - Test loss: 0.05221\n",
      "Epoch 1905 - lr: 0.05572 - Train loss: 0.02221 - Test loss: 0.05220\n",
      "Epoch 1906 - lr: 0.05570 - Train loss: 0.02220 - Test loss: 0.05218\n",
      "Epoch 1907 - lr: 0.05568 - Train loss: 0.02219 - Test loss: 0.05217\n",
      "Epoch 1908 - lr: 0.05567 - Train loss: 0.02218 - Test loss: 0.05215\n",
      "Epoch 1909 - lr: 0.05565 - Train loss: 0.02218 - Test loss: 0.05213\n",
      "Epoch 1910 - lr: 0.05563 - Train loss: 0.02217 - Test loss: 0.05212\n",
      "Epoch 1911 - lr: 0.05562 - Train loss: 0.02216 - Test loss: 0.05210\n",
      "Epoch 1912 - lr: 0.05560 - Train loss: 0.02215 - Test loss: 0.05209\n",
      "Epoch 1913 - lr: 0.05558 - Train loss: 0.02214 - Test loss: 0.05207\n",
      "Epoch 1914 - lr: 0.05556 - Train loss: 0.02213 - Test loss: 0.05206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1915 - lr: 0.05555 - Train loss: 0.02213 - Test loss: 0.05204\n",
      "Epoch 1916 - lr: 0.05553 - Train loss: 0.02212 - Test loss: 0.05202\n",
      "Epoch 1917 - lr: 0.05551 - Train loss: 0.02211 - Test loss: 0.05201\n",
      "Epoch 1918 - lr: 0.05550 - Train loss: 0.02210 - Test loss: 0.05199\n",
      "Epoch 1919 - lr: 0.05548 - Train loss: 0.02209 - Test loss: 0.05198\n",
      "Epoch 1920 - lr: 0.05546 - Train loss: 0.02208 - Test loss: 0.05196\n",
      "Epoch 1921 - lr: 0.05545 - Train loss: 0.02207 - Test loss: 0.05194\n",
      "Epoch 1922 - lr: 0.05543 - Train loss: 0.02206 - Test loss: 0.05193\n",
      "Epoch 1923 - lr: 0.05541 - Train loss: 0.02205 - Test loss: 0.05191\n",
      "Epoch 1924 - lr: 0.05539 - Train loss: 0.02205 - Test loss: 0.05189\n",
      "Epoch 1925 - lr: 0.05538 - Train loss: 0.02204 - Test loss: 0.05188\n",
      "Epoch 1926 - lr: 0.05536 - Train loss: 0.02203 - Test loss: 0.05186\n",
      "Epoch 1927 - lr: 0.05534 - Train loss: 0.02202 - Test loss: 0.05184\n",
      "Epoch 1928 - lr: 0.05533 - Train loss: 0.02201 - Test loss: 0.05182\n",
      "Epoch 1929 - lr: 0.05531 - Train loss: 0.02200 - Test loss: 0.05181\n",
      "Epoch 1930 - lr: 0.05529 - Train loss: 0.02199 - Test loss: 0.05179\n",
      "Epoch 1931 - lr: 0.05528 - Train loss: 0.02198 - Test loss: 0.05177\n",
      "Epoch 1932 - lr: 0.05526 - Train loss: 0.02197 - Test loss: 0.05176\n",
      "Epoch 1933 - lr: 0.05524 - Train loss: 0.02196 - Test loss: 0.05174\n",
      "Epoch 1934 - lr: 0.05522 - Train loss: 0.02195 - Test loss: 0.05172\n",
      "Epoch 1935 - lr: 0.05521 - Train loss: 0.02194 - Test loss: 0.05170\n",
      "Epoch 1936 - lr: 0.05519 - Train loss: 0.02193 - Test loss: 0.05168\n",
      "Epoch 1937 - lr: 0.05517 - Train loss: 0.02192 - Test loss: 0.05167\n",
      "Epoch 1938 - lr: 0.05516 - Train loss: 0.02191 - Test loss: 0.05165\n",
      "Epoch 1939 - lr: 0.05514 - Train loss: 0.02190 - Test loss: 0.05163\n",
      "Epoch 1940 - lr: 0.05512 - Train loss: 0.02190 - Test loss: 0.05161\n",
      "Epoch 1941 - lr: 0.05511 - Train loss: 0.02189 - Test loss: 0.05159\n",
      "Epoch 1942 - lr: 0.05509 - Train loss: 0.02188 - Test loss: 0.05158\n",
      "Epoch 1943 - lr: 0.05507 - Train loss: 0.02187 - Test loss: 0.05156\n",
      "Epoch 1944 - lr: 0.05506 - Train loss: 0.02186 - Test loss: 0.05154\n",
      "Epoch 1945 - lr: 0.05504 - Train loss: 0.02185 - Test loss: 0.05152\n",
      "Epoch 1946 - lr: 0.05502 - Train loss: 0.02184 - Test loss: 0.05150\n",
      "Epoch 1947 - lr: 0.05500 - Train loss: 0.02183 - Test loss: 0.05148\n",
      "Epoch 1948 - lr: 0.05499 - Train loss: 0.02182 - Test loss: 0.05147\n",
      "Epoch 1949 - lr: 0.05497 - Train loss: 0.02181 - Test loss: 0.05145\n",
      "Epoch 1950 - lr: 0.05495 - Train loss: 0.02180 - Test loss: 0.05143\n",
      "Epoch 1951 - lr: 0.05494 - Train loss: 0.02179 - Test loss: 0.05141\n",
      "Epoch 1952 - lr: 0.05492 - Train loss: 0.02178 - Test loss: 0.05139\n",
      "Epoch 1953 - lr: 0.05490 - Train loss: 0.02177 - Test loss: 0.05137\n",
      "Epoch 1954 - lr: 0.05489 - Train loss: 0.02176 - Test loss: 0.05135\n",
      "Epoch 1955 - lr: 0.05487 - Train loss: 0.02175 - Test loss: 0.05133\n",
      "Epoch 1956 - lr: 0.05485 - Train loss: 0.02174 - Test loss: 0.05132\n",
      "Epoch 1957 - lr: 0.05484 - Train loss: 0.02173 - Test loss: 0.05130\n",
      "Epoch 1958 - lr: 0.05482 - Train loss: 0.02172 - Test loss: 0.05128\n",
      "Epoch 1959 - lr: 0.05480 - Train loss: 0.02171 - Test loss: 0.05126\n",
      "Epoch 1960 - lr: 0.05479 - Train loss: 0.02170 - Test loss: 0.05124\n",
      "Epoch 1961 - lr: 0.05477 - Train loss: 0.02169 - Test loss: 0.05122\n",
      "Epoch 1962 - lr: 0.05475 - Train loss: 0.02168 - Test loss: 0.05120\n",
      "Epoch 1963 - lr: 0.05474 - Train loss: 0.02167 - Test loss: 0.05118\n",
      "Epoch 1964 - lr: 0.05472 - Train loss: 0.02165 - Test loss: 0.05116\n",
      "Epoch 1965 - lr: 0.05470 - Train loss: 0.02164 - Test loss: 0.05114\n",
      "Epoch 1966 - lr: 0.05468 - Train loss: 0.02163 - Test loss: 0.05112\n",
      "Epoch 1967 - lr: 0.05467 - Train loss: 0.02162 - Test loss: 0.05110\n",
      "Epoch 1968 - lr: 0.05465 - Train loss: 0.02161 - Test loss: 0.05108\n",
      "Epoch 1969 - lr: 0.05463 - Train loss: 0.02160 - Test loss: 0.05106\n",
      "Epoch 1970 - lr: 0.05462 - Train loss: 0.02159 - Test loss: 0.05104\n",
      "Epoch 1971 - lr: 0.05460 - Train loss: 0.02158 - Test loss: 0.05103\n",
      "Epoch 1972 - lr: 0.05458 - Train loss: 0.02157 - Test loss: 0.05101\n",
      "Epoch 1973 - lr: 0.05457 - Train loss: 0.02156 - Test loss: 0.05099\n",
      "Epoch 1974 - lr: 0.05455 - Train loss: 0.02155 - Test loss: 0.05097\n",
      "Epoch 1975 - lr: 0.05453 - Train loss: 0.02154 - Test loss: 0.05095\n",
      "Epoch 1976 - lr: 0.05452 - Train loss: 0.02153 - Test loss: 0.05093\n",
      "Epoch 1977 - lr: 0.05450 - Train loss: 0.02152 - Test loss: 0.05091\n",
      "Epoch 1978 - lr: 0.05448 - Train loss: 0.02151 - Test loss: 0.05089\n",
      "Epoch 1979 - lr: 0.05447 - Train loss: 0.02150 - Test loss: 0.05087\n",
      "Epoch 1980 - lr: 0.05445 - Train loss: 0.02149 - Test loss: 0.05085\n",
      "Epoch 1981 - lr: 0.05443 - Train loss: 0.02148 - Test loss: 0.05083\n",
      "Epoch 1982 - lr: 0.05442 - Train loss: 0.02147 - Test loss: 0.05081\n",
      "Epoch 1983 - lr: 0.05440 - Train loss: 0.02145 - Test loss: 0.05079\n",
      "Epoch 1984 - lr: 0.05438 - Train loss: 0.02144 - Test loss: 0.05077\n",
      "Epoch 1985 - lr: 0.05437 - Train loss: 0.02143 - Test loss: 0.05075\n",
      "Epoch 1986 - lr: 0.05435 - Train loss: 0.02142 - Test loss: 0.05073\n",
      "Epoch 1987 - lr: 0.05433 - Train loss: 0.02141 - Test loss: 0.05071\n",
      "Epoch 1988 - lr: 0.05432 - Train loss: 0.02140 - Test loss: 0.05069\n",
      "Epoch 1989 - lr: 0.05430 - Train loss: 0.02139 - Test loss: 0.05067\n",
      "Epoch 1990 - lr: 0.05428 - Train loss: 0.02138 - Test loss: 0.05065\n",
      "Epoch 1991 - lr: 0.05427 - Train loss: 0.02137 - Test loss: 0.05063\n",
      "Epoch 1992 - lr: 0.05425 - Train loss: 0.02136 - Test loss: 0.05061\n",
      "Epoch 1993 - lr: 0.05423 - Train loss: 0.02135 - Test loss: 0.05059\n",
      "Epoch 1994 - lr: 0.05422 - Train loss: 0.02134 - Test loss: 0.05057\n",
      "Epoch 1995 - lr: 0.05420 - Train loss: 0.02133 - Test loss: 0.05055\n",
      "Epoch 1996 - lr: 0.05418 - Train loss: 0.02131 - Test loss: 0.05053\n",
      "Epoch 1997 - lr: 0.05417 - Train loss: 0.02130 - Test loss: 0.05051\n",
      "Epoch 1998 - lr: 0.05415 - Train loss: 0.02129 - Test loss: 0.05049\n",
      "Epoch 1999 - lr: 0.05413 - Train loss: 0.02128 - Test loss: 0.05047\n",
      "Epoch 2000 - lr: 0.05412 - Train loss: 0.02127 - Test loss: 0.05045\n",
      "Epoch 2001 - lr: 0.05410 - Train loss: 0.02126 - Test loss: 0.05043\n",
      "Epoch 2002 - lr: 0.05408 - Train loss: 0.02125 - Test loss: 0.05041\n",
      "Epoch 2003 - lr: 0.05407 - Train loss: 0.02124 - Test loss: 0.05039\n",
      "Epoch 2004 - lr: 0.05405 - Train loss: 0.02123 - Test loss: 0.05036\n",
      "Epoch 2005 - lr: 0.05403 - Train loss: 0.02122 - Test loss: 0.05034\n",
      "Epoch 2006 - lr: 0.05402 - Train loss: 0.02120 - Test loss: 0.05032\n",
      "Epoch 2007 - lr: 0.05400 - Train loss: 0.02119 - Test loss: 0.05030\n",
      "Epoch 2008 - lr: 0.05398 - Train loss: 0.02118 - Test loss: 0.05028\n",
      "Epoch 2009 - lr: 0.05397 - Train loss: 0.02117 - Test loss: 0.05026\n",
      "Epoch 2010 - lr: 0.05395 - Train loss: 0.02116 - Test loss: 0.05024\n",
      "Epoch 2011 - lr: 0.05393 - Train loss: 0.02115 - Test loss: 0.05022\n",
      "Epoch 2012 - lr: 0.05392 - Train loss: 0.02114 - Test loss: 0.05020\n",
      "Epoch 2013 - lr: 0.05390 - Train loss: 0.02113 - Test loss: 0.05018\n",
      "Epoch 2014 - lr: 0.05388 - Train loss: 0.02112 - Test loss: 0.05016\n",
      "Epoch 2015 - lr: 0.05387 - Train loss: 0.02111 - Test loss: 0.05014\n",
      "Epoch 2016 - lr: 0.05385 - Train loss: 0.02109 - Test loss: 0.05012\n",
      "Epoch 2017 - lr: 0.05384 - Train loss: 0.02108 - Test loss: 0.05010\n",
      "Epoch 2018 - lr: 0.05382 - Train loss: 0.02107 - Test loss: 0.05008\n",
      "Epoch 2019 - lr: 0.05380 - Train loss: 0.02106 - Test loss: 0.05006\n",
      "Epoch 2020 - lr: 0.05379 - Train loss: 0.02105 - Test loss: 0.05004\n",
      "Epoch 2021 - lr: 0.05377 - Train loss: 0.02104 - Test loss: 0.05002\n",
      "Epoch 2022 - lr: 0.05375 - Train loss: 0.02103 - Test loss: 0.05000\n",
      "Epoch 2023 - lr: 0.05374 - Train loss: 0.02102 - Test loss: 0.04998\n",
      "Epoch 2024 - lr: 0.05372 - Train loss: 0.02101 - Test loss: 0.04996\n",
      "Epoch 2025 - lr: 0.05370 - Train loss: 0.02099 - Test loss: 0.04994\n",
      "Epoch 2026 - lr: 0.05369 - Train loss: 0.02098 - Test loss: 0.04992\n",
      "Epoch 2027 - lr: 0.05367 - Train loss: 0.02097 - Test loss: 0.04990\n",
      "Epoch 2028 - lr: 0.05365 - Train loss: 0.02096 - Test loss: 0.04988\n",
      "Epoch 2029 - lr: 0.05364 - Train loss: 0.02095 - Test loss: 0.04986\n",
      "Epoch 2030 - lr: 0.05362 - Train loss: 0.02094 - Test loss: 0.04984\n",
      "Epoch 2031 - lr: 0.05360 - Train loss: 0.02093 - Test loss: 0.04982\n",
      "Epoch 2032 - lr: 0.05359 - Train loss: 0.02092 - Test loss: 0.04980\n",
      "Epoch 2033 - lr: 0.05357 - Train loss: 0.02091 - Test loss: 0.04978\n",
      "Epoch 2034 - lr: 0.05355 - Train loss: 0.02089 - Test loss: 0.04976\n",
      "Epoch 2035 - lr: 0.05354 - Train loss: 0.02088 - Test loss: 0.04974\n",
      "Epoch 2036 - lr: 0.05352 - Train loss: 0.02087 - Test loss: 0.04972\n",
      "Epoch 2037 - lr: 0.05351 - Train loss: 0.02086 - Test loss: 0.04970\n",
      "Epoch 2038 - lr: 0.05349 - Train loss: 0.02085 - Test loss: 0.04968\n",
      "Epoch 2039 - lr: 0.05347 - Train loss: 0.02084 - Test loss: 0.04966\n",
      "Epoch 2040 - lr: 0.05346 - Train loss: 0.02083 - Test loss: 0.04964\n",
      "Epoch 2041 - lr: 0.05344 - Train loss: 0.02082 - Test loss: 0.04962\n",
      "Epoch 2042 - lr: 0.05342 - Train loss: 0.02081 - Test loss: 0.04960\n",
      "Epoch 2043 - lr: 0.05341 - Train loss: 0.02079 - Test loss: 0.04958\n",
      "Epoch 2044 - lr: 0.05339 - Train loss: 0.02078 - Test loss: 0.04956\n",
      "Epoch 2045 - lr: 0.05337 - Train loss: 0.02077 - Test loss: 0.04954\n",
      "Epoch 2046 - lr: 0.05336 - Train loss: 0.02076 - Test loss: 0.04952\n",
      "Epoch 2047 - lr: 0.05334 - Train loss: 0.02075 - Test loss: 0.04950\n",
      "Epoch 2048 - lr: 0.05333 - Train loss: 0.02074 - Test loss: 0.04948\n",
      "Epoch 2049 - lr: 0.05331 - Train loss: 0.02073 - Test loss: 0.04946\n",
      "Epoch 2050 - lr: 0.05329 - Train loss: 0.02072 - Test loss: 0.04944\n",
      "Epoch 2051 - lr: 0.05328 - Train loss: 0.02071 - Test loss: 0.04942\n",
      "Epoch 2052 - lr: 0.05326 - Train loss: 0.02069 - Test loss: 0.04940\n",
      "Epoch 2053 - lr: 0.05324 - Train loss: 0.02068 - Test loss: 0.04938\n",
      "Epoch 2054 - lr: 0.05323 - Train loss: 0.02067 - Test loss: 0.04936\n",
      "Epoch 2055 - lr: 0.05321 - Train loss: 0.02066 - Test loss: 0.04934\n",
      "Epoch 2056 - lr: 0.05319 - Train loss: 0.02065 - Test loss: 0.04932\n",
      "Epoch 2057 - lr: 0.05318 - Train loss: 0.02064 - Test loss: 0.04930\n",
      "Epoch 2058 - lr: 0.05316 - Train loss: 0.02063 - Test loss: 0.04928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2059 - lr: 0.05315 - Train loss: 0.02062 - Test loss: 0.04926\n",
      "Epoch 2060 - lr: 0.05313 - Train loss: 0.02061 - Test loss: 0.04924\n",
      "Epoch 2061 - lr: 0.05311 - Train loss: 0.02060 - Test loss: 0.04923\n",
      "Epoch 2062 - lr: 0.05310 - Train loss: 0.02058 - Test loss: 0.04921\n",
      "Epoch 2063 - lr: 0.05308 - Train loss: 0.02057 - Test loss: 0.04919\n",
      "Epoch 2064 - lr: 0.05306 - Train loss: 0.02056 - Test loss: 0.04917\n",
      "Epoch 2065 - lr: 0.05305 - Train loss: 0.02055 - Test loss: 0.04915\n",
      "Epoch 2066 - lr: 0.05303 - Train loss: 0.02054 - Test loss: 0.04913\n",
      "Epoch 2067 - lr: 0.05302 - Train loss: 0.02053 - Test loss: 0.04911\n",
      "Epoch 2068 - lr: 0.05300 - Train loss: 0.02052 - Test loss: 0.04909\n",
      "Epoch 2069 - lr: 0.05298 - Train loss: 0.02051 - Test loss: 0.04907\n",
      "Epoch 2070 - lr: 0.05297 - Train loss: 0.02050 - Test loss: 0.04905\n",
      "Epoch 2071 - lr: 0.05295 - Train loss: 0.02049 - Test loss: 0.04903\n",
      "Epoch 2072 - lr: 0.05293 - Train loss: 0.02047 - Test loss: 0.04901\n",
      "Epoch 2073 - lr: 0.05292 - Train loss: 0.02046 - Test loss: 0.04899\n",
      "Epoch 2074 - lr: 0.05290 - Train loss: 0.02045 - Test loss: 0.04897\n",
      "Epoch 2075 - lr: 0.05289 - Train loss: 0.02044 - Test loss: 0.04896\n",
      "Epoch 2076 - lr: 0.05287 - Train loss: 0.02043 - Test loss: 0.04894\n",
      "Epoch 2077 - lr: 0.05285 - Train loss: 0.02042 - Test loss: 0.04892\n",
      "Epoch 2078 - lr: 0.05284 - Train loss: 0.02041 - Test loss: 0.04890\n",
      "Epoch 2079 - lr: 0.05282 - Train loss: 0.02040 - Test loss: 0.04888\n",
      "Epoch 2080 - lr: 0.05280 - Train loss: 0.02039 - Test loss: 0.04886\n",
      "Epoch 2081 - lr: 0.05279 - Train loss: 0.02038 - Test loss: 0.04884\n",
      "Epoch 2082 - lr: 0.05277 - Train loss: 0.02037 - Test loss: 0.04882\n",
      "Epoch 2083 - lr: 0.05276 - Train loss: 0.02035 - Test loss: 0.04880\n",
      "Epoch 2084 - lr: 0.05274 - Train loss: 0.02034 - Test loss: 0.04879\n",
      "Epoch 2085 - lr: 0.05272 - Train loss: 0.02033 - Test loss: 0.04877\n",
      "Epoch 2086 - lr: 0.05271 - Train loss: 0.02032 - Test loss: 0.04875\n",
      "Epoch 2087 - lr: 0.05269 - Train loss: 0.02031 - Test loss: 0.04873\n",
      "Epoch 2088 - lr: 0.05267 - Train loss: 0.02030 - Test loss: 0.04871\n",
      "Epoch 2089 - lr: 0.05266 - Train loss: 0.02029 - Test loss: 0.04869\n",
      "Epoch 2090 - lr: 0.05264 - Train loss: 0.02028 - Test loss: 0.04867\n",
      "Epoch 2091 - lr: 0.05263 - Train loss: 0.02027 - Test loss: 0.04866\n",
      "Epoch 2092 - lr: 0.05261 - Train loss: 0.02026 - Test loss: 0.04864\n",
      "Epoch 2093 - lr: 0.05259 - Train loss: 0.02025 - Test loss: 0.04862\n",
      "Epoch 2094 - lr: 0.05258 - Train loss: 0.02024 - Test loss: 0.04860\n",
      "Epoch 2095 - lr: 0.05256 - Train loss: 0.02023 - Test loss: 0.04858\n",
      "Epoch 2096 - lr: 0.05255 - Train loss: 0.02021 - Test loss: 0.04856\n",
      "Epoch 2097 - lr: 0.05253 - Train loss: 0.02020 - Test loss: 0.04854\n",
      "Epoch 2098 - lr: 0.05251 - Train loss: 0.02019 - Test loss: 0.04853\n",
      "Epoch 2099 - lr: 0.05250 - Train loss: 0.02018 - Test loss: 0.04851\n",
      "Epoch 2100 - lr: 0.05248 - Train loss: 0.02017 - Test loss: 0.04849\n",
      "Epoch 2101 - lr: 0.05246 - Train loss: 0.02016 - Test loss: 0.04847\n",
      "Epoch 2102 - lr: 0.05245 - Train loss: 0.02015 - Test loss: 0.04845\n",
      "Epoch 2103 - lr: 0.05243 - Train loss: 0.02014 - Test loss: 0.04844\n",
      "Epoch 2104 - lr: 0.05242 - Train loss: 0.02013 - Test loss: 0.04842\n",
      "Epoch 2105 - lr: 0.05240 - Train loss: 0.02012 - Test loss: 0.04840\n",
      "Epoch 2106 - lr: 0.05238 - Train loss: 0.02011 - Test loss: 0.04838\n",
      "Epoch 2107 - lr: 0.05237 - Train loss: 0.02010 - Test loss: 0.04836\n",
      "Epoch 2108 - lr: 0.05235 - Train loss: 0.02009 - Test loss: 0.04835\n",
      "Epoch 2109 - lr: 0.05234 - Train loss: 0.02008 - Test loss: 0.04833\n",
      "Epoch 2110 - lr: 0.05232 - Train loss: 0.02007 - Test loss: 0.04831\n",
      "Epoch 2111 - lr: 0.05230 - Train loss: 0.02006 - Test loss: 0.04829\n",
      "Epoch 2112 - lr: 0.05229 - Train loss: 0.02004 - Test loss: 0.04827\n",
      "Epoch 2113 - lr: 0.05227 - Train loss: 0.02003 - Test loss: 0.04826\n",
      "Epoch 2114 - lr: 0.05226 - Train loss: 0.02002 - Test loss: 0.04824\n",
      "Epoch 2115 - lr: 0.05224 - Train loss: 0.02001 - Test loss: 0.04822\n",
      "Epoch 2116 - lr: 0.05222 - Train loss: 0.02000 - Test loss: 0.04820\n",
      "Epoch 2117 - lr: 0.05221 - Train loss: 0.01999 - Test loss: 0.04819\n",
      "Epoch 2118 - lr: 0.05219 - Train loss: 0.01998 - Test loss: 0.04817\n",
      "Epoch 2119 - lr: 0.05218 - Train loss: 0.01997 - Test loss: 0.04815\n",
      "Epoch 2120 - lr: 0.05216 - Train loss: 0.01996 - Test loss: 0.04813\n",
      "Epoch 2121 - lr: 0.05214 - Train loss: 0.01995 - Test loss: 0.04812\n",
      "Epoch 2122 - lr: 0.05213 - Train loss: 0.01994 - Test loss: 0.04810\n",
      "Epoch 2123 - lr: 0.05211 - Train loss: 0.01993 - Test loss: 0.04808\n",
      "Epoch 2124 - lr: 0.05210 - Train loss: 0.01992 - Test loss: 0.04807\n",
      "Epoch 2125 - lr: 0.05208 - Train loss: 0.01991 - Test loss: 0.04805\n",
      "Epoch 2126 - lr: 0.05206 - Train loss: 0.01990 - Test loss: 0.04803\n",
      "Epoch 2127 - lr: 0.05205 - Train loss: 0.01989 - Test loss: 0.04801\n",
      "Epoch 2128 - lr: 0.05203 - Train loss: 0.01988 - Test loss: 0.04800\n",
      "Epoch 2129 - lr: 0.05202 - Train loss: 0.01987 - Test loss: 0.04798\n",
      "Epoch 2130 - lr: 0.05200 - Train loss: 0.01986 - Test loss: 0.04796\n",
      "Epoch 2131 - lr: 0.05198 - Train loss: 0.01985 - Test loss: 0.04795\n",
      "Epoch 2132 - lr: 0.05197 - Train loss: 0.01984 - Test loss: 0.04793\n",
      "Epoch 2133 - lr: 0.05195 - Train loss: 0.01983 - Test loss: 0.04791\n",
      "Epoch 2134 - lr: 0.05194 - Train loss: 0.01982 - Test loss: 0.04789\n",
      "Epoch 2135 - lr: 0.05192 - Train loss: 0.01981 - Test loss: 0.04788\n",
      "Epoch 2136 - lr: 0.05190 - Train loss: 0.01980 - Test loss: 0.04786\n",
      "Epoch 2137 - lr: 0.05189 - Train loss: 0.01979 - Test loss: 0.04784\n",
      "Epoch 2138 - lr: 0.05187 - Train loss: 0.01978 - Test loss: 0.04783\n",
      "Epoch 2139 - lr: 0.05186 - Train loss: 0.01977 - Test loss: 0.04781\n",
      "Epoch 2140 - lr: 0.05184 - Train loss: 0.01976 - Test loss: 0.04779\n",
      "Epoch 2141 - lr: 0.05182 - Train loss: 0.01975 - Test loss: 0.04778\n",
      "Epoch 2142 - lr: 0.05181 - Train loss: 0.01974 - Test loss: 0.04776\n",
      "Epoch 2143 - lr: 0.05179 - Train loss: 0.01973 - Test loss: 0.04775\n",
      "Epoch 2144 - lr: 0.05178 - Train loss: 0.01972 - Test loss: 0.04773\n",
      "Epoch 2145 - lr: 0.05176 - Train loss: 0.01971 - Test loss: 0.04771\n",
      "Epoch 2146 - lr: 0.05174 - Train loss: 0.01970 - Test loss: 0.04770\n",
      "Epoch 2147 - lr: 0.05173 - Train loss: 0.01969 - Test loss: 0.04768\n",
      "Epoch 2148 - lr: 0.05171 - Train loss: 0.01968 - Test loss: 0.04766\n",
      "Epoch 2149 - lr: 0.05170 - Train loss: 0.01967 - Test loss: 0.04765\n",
      "Epoch 2150 - lr: 0.05168 - Train loss: 0.01966 - Test loss: 0.04763\n",
      "Epoch 2151 - lr: 0.05167 - Train loss: 0.01965 - Test loss: 0.04761\n",
      "Epoch 2152 - lr: 0.05165 - Train loss: 0.01964 - Test loss: 0.04760\n",
      "Epoch 2153 - lr: 0.05163 - Train loss: 0.01963 - Test loss: 0.04758\n",
      "Epoch 2154 - lr: 0.05162 - Train loss: 0.01962 - Test loss: 0.04757\n",
      "Epoch 2155 - lr: 0.05160 - Train loss: 0.01961 - Test loss: 0.04755\n",
      "Epoch 2156 - lr: 0.05159 - Train loss: 0.01960 - Test loss: 0.04753\n",
      "Epoch 2157 - lr: 0.05157 - Train loss: 0.01959 - Test loss: 0.04752\n",
      "Epoch 2158 - lr: 0.05155 - Train loss: 0.01958 - Test loss: 0.04750\n",
      "Epoch 2159 - lr: 0.05154 - Train loss: 0.01957 - Test loss: 0.04749\n",
      "Epoch 2160 - lr: 0.05152 - Train loss: 0.01956 - Test loss: 0.04747\n",
      "Epoch 2161 - lr: 0.05151 - Train loss: 0.01955 - Test loss: 0.04746\n",
      "Epoch 2162 - lr: 0.05149 - Train loss: 0.01954 - Test loss: 0.04744\n",
      "Epoch 2163 - lr: 0.05148 - Train loss: 0.01953 - Test loss: 0.04742\n",
      "Epoch 2164 - lr: 0.05146 - Train loss: 0.01952 - Test loss: 0.04741\n",
      "Epoch 2165 - lr: 0.05144 - Train loss: 0.01951 - Test loss: 0.04739\n",
      "Epoch 2166 - lr: 0.05143 - Train loss: 0.01950 - Test loss: 0.04738\n",
      "Epoch 2167 - lr: 0.05141 - Train loss: 0.01949 - Test loss: 0.04736\n",
      "Epoch 2168 - lr: 0.05140 - Train loss: 0.01948 - Test loss: 0.04735\n",
      "Epoch 2169 - lr: 0.05138 - Train loss: 0.01947 - Test loss: 0.04733\n",
      "Epoch 2170 - lr: 0.05136 - Train loss: 0.01946 - Test loss: 0.04732\n",
      "Epoch 2171 - lr: 0.05135 - Train loss: 0.01945 - Test loss: 0.04730\n",
      "Epoch 2172 - lr: 0.05133 - Train loss: 0.01944 - Test loss: 0.04729\n",
      "Epoch 2173 - lr: 0.05132 - Train loss: 0.01943 - Test loss: 0.04727\n",
      "Epoch 2174 - lr: 0.05130 - Train loss: 0.01942 - Test loss: 0.04725\n",
      "Epoch 2175 - lr: 0.05129 - Train loss: 0.01941 - Test loss: 0.04724\n",
      "Epoch 2176 - lr: 0.05127 - Train loss: 0.01940 - Test loss: 0.04722\n",
      "Epoch 2177 - lr: 0.05125 - Train loss: 0.01939 - Test loss: 0.04721\n",
      "Epoch 2178 - lr: 0.05124 - Train loss: 0.01938 - Test loss: 0.04719\n",
      "Epoch 2179 - lr: 0.05122 - Train loss: 0.01937 - Test loss: 0.04718\n",
      "Epoch 2180 - lr: 0.05121 - Train loss: 0.01937 - Test loss: 0.04716\n",
      "Epoch 2181 - lr: 0.05119 - Train loss: 0.01936 - Test loss: 0.04715\n",
      "Epoch 2182 - lr: 0.05118 - Train loss: 0.01935 - Test loss: 0.04713\n",
      "Epoch 2183 - lr: 0.05116 - Train loss: 0.01934 - Test loss: 0.04712\n",
      "Epoch 2184 - lr: 0.05114 - Train loss: 0.01933 - Test loss: 0.04711\n",
      "Epoch 2185 - lr: 0.05113 - Train loss: 0.01932 - Test loss: 0.04709\n",
      "Epoch 2186 - lr: 0.05111 - Train loss: 0.01931 - Test loss: 0.04708\n",
      "Epoch 2187 - lr: 0.05110 - Train loss: 0.01930 - Test loss: 0.04706\n",
      "Epoch 2188 - lr: 0.05108 - Train loss: 0.01929 - Test loss: 0.04705\n",
      "Epoch 2189 - lr: 0.05107 - Train loss: 0.01928 - Test loss: 0.04703\n",
      "Epoch 2190 - lr: 0.05105 - Train loss: 0.01927 - Test loss: 0.04702\n",
      "Epoch 2191 - lr: 0.05103 - Train loss: 0.01926 - Test loss: 0.04700\n",
      "Epoch 2192 - lr: 0.05102 - Train loss: 0.01925 - Test loss: 0.04699\n",
      "Epoch 2193 - lr: 0.05100 - Train loss: 0.01924 - Test loss: 0.04697\n",
      "Epoch 2194 - lr: 0.05099 - Train loss: 0.01924 - Test loss: 0.04696\n",
      "Epoch 2195 - lr: 0.05097 - Train loss: 0.01923 - Test loss: 0.04695\n",
      "Epoch 2196 - lr: 0.05096 - Train loss: 0.01922 - Test loss: 0.04693\n",
      "Epoch 2197 - lr: 0.05094 - Train loss: 0.01921 - Test loss: 0.04692\n",
      "Epoch 2198 - lr: 0.05093 - Train loss: 0.01920 - Test loss: 0.04690\n",
      "Epoch 2199 - lr: 0.05091 - Train loss: 0.01919 - Test loss: 0.04689\n",
      "Epoch 2200 - lr: 0.05089 - Train loss: 0.01918 - Test loss: 0.04687\n",
      "Epoch 2201 - lr: 0.05088 - Train loss: 0.01917 - Test loss: 0.04686\n",
      "Epoch 2202 - lr: 0.05086 - Train loss: 0.01916 - Test loss: 0.04685\n",
      "Epoch 2203 - lr: 0.05085 - Train loss: 0.01915 - Test loss: 0.04683\n",
      "Epoch 2204 - lr: 0.05083 - Train loss: 0.01914 - Test loss: 0.04682\n",
      "Epoch 2205 - lr: 0.05082 - Train loss: 0.01913 - Test loss: 0.04680\n",
      "Epoch 2206 - lr: 0.05080 - Train loss: 0.01913 - Test loss: 0.04679\n",
      "Epoch 2207 - lr: 0.05078 - Train loss: 0.01912 - Test loss: 0.04678\n",
      "Epoch 2208 - lr: 0.05077 - Train loss: 0.01911 - Test loss: 0.04676\n",
      "Epoch 2209 - lr: 0.05075 - Train loss: 0.01910 - Test loss: 0.04675\n",
      "Epoch 2210 - lr: 0.05074 - Train loss: 0.01909 - Test loss: 0.04673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2211 - lr: 0.05072 - Train loss: 0.01908 - Test loss: 0.04672\n",
      "Epoch 2212 - lr: 0.05071 - Train loss: 0.01907 - Test loss: 0.04671\n",
      "Epoch 2213 - lr: 0.05069 - Train loss: 0.01906 - Test loss: 0.04669\n",
      "Epoch 2214 - lr: 0.05068 - Train loss: 0.01905 - Test loss: 0.04668\n",
      "Epoch 2215 - lr: 0.05066 - Train loss: 0.01905 - Test loss: 0.04667\n",
      "Epoch 2216 - lr: 0.05064 - Train loss: 0.01904 - Test loss: 0.04665\n",
      "Epoch 2217 - lr: 0.05063 - Train loss: 0.01903 - Test loss: 0.04664\n",
      "Epoch 2218 - lr: 0.05061 - Train loss: 0.01902 - Test loss: 0.04663\n",
      "Epoch 2219 - lr: 0.05060 - Train loss: 0.01901 - Test loss: 0.04661\n",
      "Epoch 2220 - lr: 0.05058 - Train loss: 0.01900 - Test loss: 0.04660\n",
      "Epoch 2221 - lr: 0.05057 - Train loss: 0.01899 - Test loss: 0.04659\n",
      "Epoch 2222 - lr: 0.05055 - Train loss: 0.01898 - Test loss: 0.04657\n",
      "Epoch 2223 - lr: 0.05054 - Train loss: 0.01898 - Test loss: 0.04656\n",
      "Epoch 2224 - lr: 0.05052 - Train loss: 0.01897 - Test loss: 0.04655\n",
      "Epoch 2225 - lr: 0.05050 - Train loss: 0.01896 - Test loss: 0.04653\n",
      "Epoch 2226 - lr: 0.05049 - Train loss: 0.01895 - Test loss: 0.04652\n",
      "Epoch 2227 - lr: 0.05047 - Train loss: 0.01894 - Test loss: 0.04651\n",
      "Epoch 2228 - lr: 0.05046 - Train loss: 0.01893 - Test loss: 0.04649\n",
      "Epoch 2229 - lr: 0.05044 - Train loss: 0.01892 - Test loss: 0.04648\n",
      "Epoch 2230 - lr: 0.05043 - Train loss: 0.01891 - Test loss: 0.04647\n",
      "Epoch 2231 - lr: 0.05041 - Train loss: 0.01891 - Test loss: 0.04645\n",
      "Epoch 2232 - lr: 0.05040 - Train loss: 0.01890 - Test loss: 0.04644\n",
      "Epoch 2233 - lr: 0.05038 - Train loss: 0.01889 - Test loss: 0.04643\n",
      "Epoch 2234 - lr: 0.05037 - Train loss: 0.01888 - Test loss: 0.04642\n",
      "Epoch 2235 - lr: 0.05035 - Train loss: 0.01887 - Test loss: 0.04640\n",
      "Epoch 2236 - lr: 0.05033 - Train loss: 0.01886 - Test loss: 0.04639\n",
      "Epoch 2237 - lr: 0.05032 - Train loss: 0.01885 - Test loss: 0.04638\n",
      "Epoch 2238 - lr: 0.05030 - Train loss: 0.01885 - Test loss: 0.04637\n",
      "Epoch 2239 - lr: 0.05029 - Train loss: 0.01884 - Test loss: 0.04635\n",
      "Epoch 2240 - lr: 0.05027 - Train loss: 0.01883 - Test loss: 0.04634\n",
      "Epoch 2241 - lr: 0.05026 - Train loss: 0.01882 - Test loss: 0.04633\n",
      "Epoch 2242 - lr: 0.05024 - Train loss: 0.01881 - Test loss: 0.04631\n",
      "Epoch 2243 - lr: 0.05023 - Train loss: 0.01880 - Test loss: 0.04630\n",
      "Epoch 2244 - lr: 0.05021 - Train loss: 0.01880 - Test loss: 0.04629\n",
      "Epoch 2245 - lr: 0.05020 - Train loss: 0.01879 - Test loss: 0.04628\n",
      "Epoch 2246 - lr: 0.05018 - Train loss: 0.01878 - Test loss: 0.04627\n",
      "Epoch 2247 - lr: 0.05016 - Train loss: 0.01877 - Test loss: 0.04625\n",
      "Epoch 2248 - lr: 0.05015 - Train loss: 0.01876 - Test loss: 0.04624\n",
      "Epoch 2249 - lr: 0.05013 - Train loss: 0.01875 - Test loss: 0.04623\n",
      "Epoch 2250 - lr: 0.05012 - Train loss: 0.01875 - Test loss: 0.04622\n",
      "Epoch 2251 - lr: 0.05010 - Train loss: 0.01874 - Test loss: 0.04620\n",
      "Epoch 2252 - lr: 0.05009 - Train loss: 0.01873 - Test loss: 0.04619\n",
      "Epoch 2253 - lr: 0.05007 - Train loss: 0.01872 - Test loss: 0.04618\n",
      "Epoch 2254 - lr: 0.05006 - Train loss: 0.01871 - Test loss: 0.04617\n",
      "Epoch 2255 - lr: 0.05004 - Train loss: 0.01870 - Test loss: 0.04616\n",
      "Epoch 2256 - lr: 0.05003 - Train loss: 0.01870 - Test loss: 0.04614\n",
      "Epoch 2257 - lr: 0.05001 - Train loss: 0.01869 - Test loss: 0.04613\n",
      "Epoch 2258 - lr: 0.05000 - Train loss: 0.01868 - Test loss: 0.04612\n",
      "Epoch 2259 - lr: 0.04998 - Train loss: 0.01867 - Test loss: 0.04611\n",
      "Epoch 2260 - lr: 0.04997 - Train loss: 0.01866 - Test loss: 0.04610\n",
      "Epoch 2261 - lr: 0.04995 - Train loss: 0.01866 - Test loss: 0.04608\n",
      "Epoch 2262 - lr: 0.04993 - Train loss: 0.01865 - Test loss: 0.04607\n",
      "Epoch 2263 - lr: 0.04992 - Train loss: 0.01864 - Test loss: 0.04606\n",
      "Epoch 2264 - lr: 0.04990 - Train loss: 0.01863 - Test loss: 0.04605\n",
      "Epoch 2265 - lr: 0.04989 - Train loss: 0.01862 - Test loss: 0.04604\n",
      "Epoch 2266 - lr: 0.04987 - Train loss: 0.01862 - Test loss: 0.04602\n",
      "Epoch 2267 - lr: 0.04986 - Train loss: 0.01861 - Test loss: 0.04601\n",
      "Epoch 2268 - lr: 0.04984 - Train loss: 0.01860 - Test loss: 0.04600\n",
      "Epoch 2269 - lr: 0.04983 - Train loss: 0.01859 - Test loss: 0.04599\n",
      "Epoch 2270 - lr: 0.04981 - Train loss: 0.01858 - Test loss: 0.04598\n",
      "Epoch 2271 - lr: 0.04980 - Train loss: 0.01858 - Test loss: 0.04597\n",
      "Epoch 2272 - lr: 0.04978 - Train loss: 0.01857 - Test loss: 0.04595\n",
      "Epoch 2273 - lr: 0.04977 - Train loss: 0.01856 - Test loss: 0.04594\n",
      "Epoch 2274 - lr: 0.04975 - Train loss: 0.01855 - Test loss: 0.04593\n",
      "Epoch 2275 - lr: 0.04974 - Train loss: 0.01854 - Test loss: 0.04592\n",
      "Epoch 2276 - lr: 0.04972 - Train loss: 0.01854 - Test loss: 0.04591\n",
      "Epoch 2277 - lr: 0.04970 - Train loss: 0.01853 - Test loss: 0.04590\n",
      "Epoch 2278 - lr: 0.04969 - Train loss: 0.01852 - Test loss: 0.04589\n",
      "Epoch 2279 - lr: 0.04967 - Train loss: 0.01851 - Test loss: 0.04588\n",
      "Epoch 2280 - lr: 0.04966 - Train loss: 0.01850 - Test loss: 0.04586\n",
      "Epoch 2281 - lr: 0.04964 - Train loss: 0.01850 - Test loss: 0.04585\n",
      "Epoch 2282 - lr: 0.04963 - Train loss: 0.01849 - Test loss: 0.04584\n",
      "Epoch 2283 - lr: 0.04961 - Train loss: 0.01848 - Test loss: 0.04583\n",
      "Epoch 2284 - lr: 0.04960 - Train loss: 0.01847 - Test loss: 0.04582\n",
      "Epoch 2285 - lr: 0.04958 - Train loss: 0.01847 - Test loss: 0.04581\n",
      "Epoch 2286 - lr: 0.04957 - Train loss: 0.01846 - Test loss: 0.04580\n",
      "Epoch 2287 - lr: 0.04955 - Train loss: 0.01845 - Test loss: 0.04579\n",
      "Epoch 2288 - lr: 0.04954 - Train loss: 0.01844 - Test loss: 0.04578\n",
      "Epoch 2289 - lr: 0.04952 - Train loss: 0.01843 - Test loss: 0.04576\n",
      "Epoch 2290 - lr: 0.04951 - Train loss: 0.01843 - Test loss: 0.04575\n",
      "Epoch 2291 - lr: 0.04949 - Train loss: 0.01842 - Test loss: 0.04574\n",
      "Epoch 2292 - lr: 0.04948 - Train loss: 0.01841 - Test loss: 0.04573\n",
      "Epoch 2293 - lr: 0.04946 - Train loss: 0.01840 - Test loss: 0.04572\n",
      "Epoch 2294 - lr: 0.04945 - Train loss: 0.01840 - Test loss: 0.04571\n",
      "Epoch 2295 - lr: 0.04943 - Train loss: 0.01839 - Test loss: 0.04570\n",
      "Epoch 2296 - lr: 0.04942 - Train loss: 0.01838 - Test loss: 0.04569\n",
      "Epoch 2297 - lr: 0.04940 - Train loss: 0.01837 - Test loss: 0.04568\n",
      "Epoch 2298 - lr: 0.04939 - Train loss: 0.01837 - Test loss: 0.04567\n",
      "Epoch 2299 - lr: 0.04937 - Train loss: 0.01836 - Test loss: 0.04566\n",
      "Epoch 2300 - lr: 0.04936 - Train loss: 0.01835 - Test loss: 0.04565\n",
      "Epoch 2301 - lr: 0.04934 - Train loss: 0.01834 - Test loss: 0.04564\n",
      "Epoch 2302 - lr: 0.04932 - Train loss: 0.01834 - Test loss: 0.04562\n",
      "Epoch 2303 - lr: 0.04931 - Train loss: 0.01833 - Test loss: 0.04561\n",
      "Epoch 2304 - lr: 0.04929 - Train loss: 0.01832 - Test loss: 0.04560\n",
      "Epoch 2305 - lr: 0.04928 - Train loss: 0.01831 - Test loss: 0.04559\n",
      "Epoch 2306 - lr: 0.04926 - Train loss: 0.01831 - Test loss: 0.04558\n",
      "Epoch 2307 - lr: 0.04925 - Train loss: 0.01830 - Test loss: 0.04557\n",
      "Epoch 2308 - lr: 0.04923 - Train loss: 0.01829 - Test loss: 0.04556\n",
      "Epoch 2309 - lr: 0.04922 - Train loss: 0.01828 - Test loss: 0.04555\n",
      "Epoch 2310 - lr: 0.04920 - Train loss: 0.01828 - Test loss: 0.04554\n",
      "Epoch 2311 - lr: 0.04919 - Train loss: 0.01827 - Test loss: 0.04553\n",
      "Epoch 2312 - lr: 0.04917 - Train loss: 0.01826 - Test loss: 0.04552\n",
      "Epoch 2313 - lr: 0.04916 - Train loss: 0.01825 - Test loss: 0.04551\n",
      "Epoch 2314 - lr: 0.04914 - Train loss: 0.01825 - Test loss: 0.04550\n",
      "Epoch 2315 - lr: 0.04913 - Train loss: 0.01824 - Test loss: 0.04549\n",
      "Epoch 2316 - lr: 0.04911 - Train loss: 0.01823 - Test loss: 0.04548\n",
      "Epoch 2317 - lr: 0.04910 - Train loss: 0.01823 - Test loss: 0.04547\n",
      "Epoch 2318 - lr: 0.04908 - Train loss: 0.01822 - Test loss: 0.04546\n",
      "Epoch 2319 - lr: 0.04907 - Train loss: 0.01821 - Test loss: 0.04545\n",
      "Epoch 2320 - lr: 0.04905 - Train loss: 0.01820 - Test loss: 0.04544\n",
      "Epoch 2321 - lr: 0.04904 - Train loss: 0.01820 - Test loss: 0.04543\n",
      "Epoch 2322 - lr: 0.04902 - Train loss: 0.01819 - Test loss: 0.04542\n",
      "Epoch 2323 - lr: 0.04901 - Train loss: 0.01818 - Test loss: 0.04541\n",
      "Epoch 2324 - lr: 0.04899 - Train loss: 0.01817 - Test loss: 0.04540\n",
      "Epoch 2325 - lr: 0.04898 - Train loss: 0.01817 - Test loss: 0.04539\n",
      "Epoch 2326 - lr: 0.04896 - Train loss: 0.01816 - Test loss: 0.04538\n",
      "Epoch 2327 - lr: 0.04895 - Train loss: 0.01815 - Test loss: 0.04537\n",
      "Epoch 2328 - lr: 0.04893 - Train loss: 0.01815 - Test loss: 0.04536\n",
      "Epoch 2329 - lr: 0.04892 - Train loss: 0.01814 - Test loss: 0.04535\n",
      "Epoch 2330 - lr: 0.04890 - Train loss: 0.01813 - Test loss: 0.04534\n",
      "Epoch 2331 - lr: 0.04889 - Train loss: 0.01812 - Test loss: 0.04533\n",
      "Epoch 2332 - lr: 0.04887 - Train loss: 0.01812 - Test loss: 0.04532\n",
      "Epoch 2333 - lr: 0.04886 - Train loss: 0.01811 - Test loss: 0.04531\n",
      "Epoch 2334 - lr: 0.04884 - Train loss: 0.01810 - Test loss: 0.04530\n",
      "Epoch 2335 - lr: 0.04883 - Train loss: 0.01810 - Test loss: 0.04529\n",
      "Epoch 2336 - lr: 0.04881 - Train loss: 0.01809 - Test loss: 0.04528\n",
      "Epoch 2337 - lr: 0.04880 - Train loss: 0.01808 - Test loss: 0.04527\n",
      "Epoch 2338 - lr: 0.04878 - Train loss: 0.01807 - Test loss: 0.04526\n",
      "Epoch 2339 - lr: 0.04877 - Train loss: 0.01807 - Test loss: 0.04526\n",
      "Epoch 2340 - lr: 0.04875 - Train loss: 0.01806 - Test loss: 0.04525\n",
      "Epoch 2341 - lr: 0.04874 - Train loss: 0.01805 - Test loss: 0.04524\n",
      "Epoch 2342 - lr: 0.04872 - Train loss: 0.01805 - Test loss: 0.04523\n",
      "Epoch 2343 - lr: 0.04871 - Train loss: 0.01804 - Test loss: 0.04522\n",
      "Epoch 2344 - lr: 0.04869 - Train loss: 0.01803 - Test loss: 0.04521\n",
      "Epoch 2345 - lr: 0.04868 - Train loss: 0.01803 - Test loss: 0.04520\n",
      "Epoch 2346 - lr: 0.04866 - Train loss: 0.01802 - Test loss: 0.04519\n",
      "Epoch 2347 - lr: 0.04865 - Train loss: 0.01801 - Test loss: 0.04518\n",
      "Epoch 2348 - lr: 0.04863 - Train loss: 0.01801 - Test loss: 0.04517\n",
      "Epoch 2349 - lr: 0.04862 - Train loss: 0.01800 - Test loss: 0.04516\n",
      "Epoch 2350 - lr: 0.04860 - Train loss: 0.01799 - Test loss: 0.04515\n",
      "Epoch 2351 - lr: 0.04859 - Train loss: 0.01798 - Test loss: 0.04514\n",
      "Epoch 2352 - lr: 0.04857 - Train loss: 0.01798 - Test loss: 0.04513\n",
      "Epoch 2353 - lr: 0.04856 - Train loss: 0.01797 - Test loss: 0.04513\n",
      "Epoch 2354 - lr: 0.04854 - Train loss: 0.01796 - Test loss: 0.04512\n",
      "Epoch 2355 - lr: 0.04853 - Train loss: 0.01796 - Test loss: 0.04511\n",
      "Epoch 2356 - lr: 0.04851 - Train loss: 0.01795 - Test loss: 0.04510\n",
      "Epoch 2357 - lr: 0.04850 - Train loss: 0.01794 - Test loss: 0.04509\n",
      "Epoch 2358 - lr: 0.04848 - Train loss: 0.01794 - Test loss: 0.04508\n",
      "Epoch 2359 - lr: 0.04847 - Train loss: 0.01793 - Test loss: 0.04507\n",
      "Epoch 2360 - lr: 0.04845 - Train loss: 0.01792 - Test loss: 0.04506\n",
      "Epoch 2361 - lr: 0.04844 - Train loss: 0.01792 - Test loss: 0.04505\n",
      "Epoch 2362 - lr: 0.04842 - Train loss: 0.01791 - Test loss: 0.04504\n",
      "Epoch 2363 - lr: 0.04841 - Train loss: 0.01790 - Test loss: 0.04504\n",
      "Epoch 2364 - lr: 0.04839 - Train loss: 0.01790 - Test loss: 0.04503\n",
      "Epoch 2365 - lr: 0.04838 - Train loss: 0.01789 - Test loss: 0.04502\n",
      "Epoch 2366 - lr: 0.04837 - Train loss: 0.01788 - Test loss: 0.04501\n",
      "Epoch 2367 - lr: 0.04835 - Train loss: 0.01788 - Test loss: 0.04500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2368 - lr: 0.04834 - Train loss: 0.01787 - Test loss: 0.04499\n",
      "Epoch 2369 - lr: 0.04832 - Train loss: 0.01786 - Test loss: 0.04498\n",
      "Epoch 2370 - lr: 0.04831 - Train loss: 0.01786 - Test loss: 0.04497\n",
      "Epoch 2371 - lr: 0.04829 - Train loss: 0.01785 - Test loss: 0.04497\n",
      "Epoch 2372 - lr: 0.04828 - Train loss: 0.01784 - Test loss: 0.04496\n",
      "Epoch 2373 - lr: 0.04826 - Train loss: 0.01784 - Test loss: 0.04495\n",
      "Epoch 2374 - lr: 0.04825 - Train loss: 0.01783 - Test loss: 0.04494\n",
      "Epoch 2375 - lr: 0.04823 - Train loss: 0.01782 - Test loss: 0.04493\n",
      "Epoch 2376 - lr: 0.04822 - Train loss: 0.01782 - Test loss: 0.04492\n",
      "Epoch 2377 - lr: 0.04820 - Train loss: 0.01781 - Test loss: 0.04491\n",
      "Epoch 2378 - lr: 0.04819 - Train loss: 0.01780 - Test loss: 0.04491\n",
      "Epoch 2379 - lr: 0.04817 - Train loss: 0.01780 - Test loss: 0.04490\n",
      "Epoch 2380 - lr: 0.04816 - Train loss: 0.01779 - Test loss: 0.04489\n",
      "Epoch 2381 - lr: 0.04814 - Train loss: 0.01778 - Test loss: 0.04488\n",
      "Epoch 2382 - lr: 0.04813 - Train loss: 0.01778 - Test loss: 0.04487\n",
      "Epoch 2383 - lr: 0.04811 - Train loss: 0.01777 - Test loss: 0.04486\n",
      "Epoch 2384 - lr: 0.04810 - Train loss: 0.01777 - Test loss: 0.04485\n",
      "Epoch 2385 - lr: 0.04808 - Train loss: 0.01776 - Test loss: 0.04485\n",
      "Epoch 2386 - lr: 0.04807 - Train loss: 0.01775 - Test loss: 0.04484\n",
      "Epoch 2387 - lr: 0.04805 - Train loss: 0.01775 - Test loss: 0.04483\n",
      "Epoch 2388 - lr: 0.04804 - Train loss: 0.01774 - Test loss: 0.04482\n",
      "Epoch 2389 - lr: 0.04802 - Train loss: 0.01773 - Test loss: 0.04481\n",
      "Epoch 2390 - lr: 0.04801 - Train loss: 0.01773 - Test loss: 0.04481\n",
      "Epoch 2391 - lr: 0.04800 - Train loss: 0.01772 - Test loss: 0.04480\n",
      "Epoch 2392 - lr: 0.04798 - Train loss: 0.01771 - Test loss: 0.04479\n",
      "Epoch 2393 - lr: 0.04797 - Train loss: 0.01771 - Test loss: 0.04478\n",
      "Epoch 2394 - lr: 0.04795 - Train loss: 0.01770 - Test loss: 0.04477\n",
      "Epoch 2395 - lr: 0.04794 - Train loss: 0.01769 - Test loss: 0.04476\n",
      "Epoch 2396 - lr: 0.04792 - Train loss: 0.01769 - Test loss: 0.04476\n",
      "Epoch 2397 - lr: 0.04791 - Train loss: 0.01768 - Test loss: 0.04475\n",
      "Epoch 2398 - lr: 0.04789 - Train loss: 0.01768 - Test loss: 0.04474\n",
      "Epoch 2399 - lr: 0.04788 - Train loss: 0.01767 - Test loss: 0.04473\n",
      "Epoch 2400 - lr: 0.04786 - Train loss: 0.01766 - Test loss: 0.04472\n",
      "Epoch 2401 - lr: 0.04785 - Train loss: 0.01766 - Test loss: 0.04472\n",
      "Epoch 2402 - lr: 0.04783 - Train loss: 0.01765 - Test loss: 0.04471\n",
      "Epoch 2403 - lr: 0.04782 - Train loss: 0.01764 - Test loss: 0.04470\n",
      "Epoch 2404 - lr: 0.04780 - Train loss: 0.01764 - Test loss: 0.04469\n",
      "Epoch 2405 - lr: 0.04779 - Train loss: 0.01763 - Test loss: 0.04468\n",
      "Epoch 2406 - lr: 0.04777 - Train loss: 0.01763 - Test loss: 0.04468\n",
      "Epoch 2407 - lr: 0.04776 - Train loss: 0.01762 - Test loss: 0.04467\n",
      "Epoch 2408 - lr: 0.04775 - Train loss: 0.01761 - Test loss: 0.04466\n",
      "Epoch 2409 - lr: 0.04773 - Train loss: 0.01761 - Test loss: 0.04465\n",
      "Epoch 2410 - lr: 0.04772 - Train loss: 0.01760 - Test loss: 0.04465\n",
      "Epoch 2411 - lr: 0.04770 - Train loss: 0.01759 - Test loss: 0.04464\n",
      "Epoch 2412 - lr: 0.04769 - Train loss: 0.01759 - Test loss: 0.04463\n",
      "Epoch 2413 - lr: 0.04767 - Train loss: 0.01758 - Test loss: 0.04462\n",
      "Epoch 2414 - lr: 0.04766 - Train loss: 0.01758 - Test loss: 0.04461\n",
      "Epoch 2415 - lr: 0.04764 - Train loss: 0.01757 - Test loss: 0.04461\n",
      "Epoch 2416 - lr: 0.04763 - Train loss: 0.01756 - Test loss: 0.04460\n",
      "Epoch 2417 - lr: 0.04761 - Train loss: 0.01756 - Test loss: 0.04459\n",
      "Epoch 2418 - lr: 0.04760 - Train loss: 0.01755 - Test loss: 0.04458\n",
      "Epoch 2419 - lr: 0.04758 - Train loss: 0.01755 - Test loss: 0.04458\n",
      "Epoch 2420 - lr: 0.04757 - Train loss: 0.01754 - Test loss: 0.04457\n",
      "Epoch 2421 - lr: 0.04756 - Train loss: 0.01753 - Test loss: 0.04456\n",
      "Epoch 2422 - lr: 0.04754 - Train loss: 0.01753 - Test loss: 0.04455\n",
      "Epoch 2423 - lr: 0.04753 - Train loss: 0.01752 - Test loss: 0.04455\n",
      "Epoch 2424 - lr: 0.04751 - Train loss: 0.01752 - Test loss: 0.04454\n",
      "Epoch 2425 - lr: 0.04750 - Train loss: 0.01751 - Test loss: 0.04453\n",
      "Epoch 2426 - lr: 0.04748 - Train loss: 0.01750 - Test loss: 0.04452\n",
      "Epoch 2427 - lr: 0.04747 - Train loss: 0.01750 - Test loss: 0.04452\n",
      "Epoch 2428 - lr: 0.04745 - Train loss: 0.01749 - Test loss: 0.04451\n",
      "Epoch 2429 - lr: 0.04744 - Train loss: 0.01749 - Test loss: 0.04450\n",
      "Epoch 2430 - lr: 0.04742 - Train loss: 0.01748 - Test loss: 0.04449\n",
      "Epoch 2431 - lr: 0.04741 - Train loss: 0.01747 - Test loss: 0.04449\n",
      "Epoch 2432 - lr: 0.04740 - Train loss: 0.01747 - Test loss: 0.04448\n",
      "Epoch 2433 - lr: 0.04738 - Train loss: 0.01746 - Test loss: 0.04447\n",
      "Epoch 2434 - lr: 0.04737 - Train loss: 0.01746 - Test loss: 0.04446\n",
      "Epoch 2435 - lr: 0.04735 - Train loss: 0.01745 - Test loss: 0.04446\n",
      "Epoch 2436 - lr: 0.04734 - Train loss: 0.01744 - Test loss: 0.04445\n",
      "Epoch 2437 - lr: 0.04732 - Train loss: 0.01744 - Test loss: 0.04444\n",
      "Epoch 2438 - lr: 0.04731 - Train loss: 0.01743 - Test loss: 0.04444\n",
      "Epoch 2439 - lr: 0.04729 - Train loss: 0.01743 - Test loss: 0.04443\n",
      "Epoch 2440 - lr: 0.04728 - Train loss: 0.01742 - Test loss: 0.04442\n",
      "Epoch 2441 - lr: 0.04726 - Train loss: 0.01741 - Test loss: 0.04441\n",
      "Epoch 2442 - lr: 0.04725 - Train loss: 0.01741 - Test loss: 0.04441\n",
      "Epoch 2443 - lr: 0.04724 - Train loss: 0.01740 - Test loss: 0.04440\n",
      "Epoch 2444 - lr: 0.04722 - Train loss: 0.01740 - Test loss: 0.04439\n",
      "Epoch 2445 - lr: 0.04721 - Train loss: 0.01739 - Test loss: 0.04439\n",
      "Epoch 2446 - lr: 0.04719 - Train loss: 0.01739 - Test loss: 0.04438\n",
      "Epoch 2447 - lr: 0.04718 - Train loss: 0.01738 - Test loss: 0.04437\n",
      "Epoch 2448 - lr: 0.04716 - Train loss: 0.01737 - Test loss: 0.04436\n",
      "Epoch 2449 - lr: 0.04715 - Train loss: 0.01737 - Test loss: 0.04436\n",
      "Epoch 2450 - lr: 0.04713 - Train loss: 0.01736 - Test loss: 0.04435\n",
      "Epoch 2451 - lr: 0.04712 - Train loss: 0.01736 - Test loss: 0.04434\n",
      "Epoch 2452 - lr: 0.04710 - Train loss: 0.01735 - Test loss: 0.04434\n",
      "Epoch 2453 - lr: 0.04709 - Train loss: 0.01734 - Test loss: 0.04433\n",
      "Epoch 2454 - lr: 0.04708 - Train loss: 0.01734 - Test loss: 0.04432\n",
      "Epoch 2455 - lr: 0.04706 - Train loss: 0.01733 - Test loss: 0.04432\n",
      "Epoch 2456 - lr: 0.04705 - Train loss: 0.01733 - Test loss: 0.04431\n",
      "Epoch 2457 - lr: 0.04703 - Train loss: 0.01732 - Test loss: 0.04430\n",
      "Epoch 2458 - lr: 0.04702 - Train loss: 0.01732 - Test loss: 0.04429\n",
      "Epoch 2459 - lr: 0.04700 - Train loss: 0.01731 - Test loss: 0.04429\n",
      "Epoch 2460 - lr: 0.04699 - Train loss: 0.01730 - Test loss: 0.04428\n",
      "Epoch 2461 - lr: 0.04697 - Train loss: 0.01730 - Test loss: 0.04427\n",
      "Epoch 2462 - lr: 0.04696 - Train loss: 0.01729 - Test loss: 0.04427\n",
      "Epoch 2463 - lr: 0.04695 - Train loss: 0.01729 - Test loss: 0.04426\n",
      "Epoch 2464 - lr: 0.04693 - Train loss: 0.01728 - Test loss: 0.04425\n",
      "Epoch 2465 - lr: 0.04692 - Train loss: 0.01728 - Test loss: 0.04425\n",
      "Epoch 2466 - lr: 0.04690 - Train loss: 0.01727 - Test loss: 0.04424\n",
      "Epoch 2467 - lr: 0.04689 - Train loss: 0.01726 - Test loss: 0.04423\n",
      "Epoch 2468 - lr: 0.04687 - Train loss: 0.01726 - Test loss: 0.04423\n",
      "Epoch 2469 - lr: 0.04686 - Train loss: 0.01725 - Test loss: 0.04422\n",
      "Epoch 2470 - lr: 0.04685 - Train loss: 0.01725 - Test loss: 0.04421\n",
      "Epoch 2471 - lr: 0.04683 - Train loss: 0.01724 - Test loss: 0.04421\n",
      "Epoch 2472 - lr: 0.04682 - Train loss: 0.01724 - Test loss: 0.04420\n",
      "Epoch 2473 - lr: 0.04680 - Train loss: 0.01723 - Test loss: 0.04419\n",
      "Epoch 2474 - lr: 0.04679 - Train loss: 0.01723 - Test loss: 0.04419\n",
      "Epoch 2475 - lr: 0.04677 - Train loss: 0.01722 - Test loss: 0.04418\n",
      "Epoch 2476 - lr: 0.04676 - Train loss: 0.01721 - Test loss: 0.04417\n",
      "Epoch 2477 - lr: 0.04674 - Train loss: 0.01721 - Test loss: 0.04417\n",
      "Epoch 2478 - lr: 0.04673 - Train loss: 0.01720 - Test loss: 0.04416\n",
      "Epoch 2479 - lr: 0.04672 - Train loss: 0.01720 - Test loss: 0.04415\n",
      "Epoch 2480 - lr: 0.04670 - Train loss: 0.01719 - Test loss: 0.04415\n",
      "Epoch 2481 - lr: 0.04669 - Train loss: 0.01719 - Test loss: 0.04414\n",
      "Epoch 2482 - lr: 0.04667 - Train loss: 0.01718 - Test loss: 0.04414\n",
      "Epoch 2483 - lr: 0.04666 - Train loss: 0.01718 - Test loss: 0.04413\n",
      "Epoch 2484 - lr: 0.04664 - Train loss: 0.01717 - Test loss: 0.04412\n",
      "Epoch 2485 - lr: 0.04663 - Train loss: 0.01716 - Test loss: 0.04412\n",
      "Epoch 2486 - lr: 0.04662 - Train loss: 0.01716 - Test loss: 0.04411\n",
      "Epoch 2487 - lr: 0.04660 - Train loss: 0.01715 - Test loss: 0.04410\n",
      "Epoch 2488 - lr: 0.04659 - Train loss: 0.01715 - Test loss: 0.04410\n",
      "Epoch 2489 - lr: 0.04657 - Train loss: 0.01714 - Test loss: 0.04409\n",
      "Epoch 2490 - lr: 0.04656 - Train loss: 0.01714 - Test loss: 0.04408\n",
      "Epoch 2491 - lr: 0.04654 - Train loss: 0.01713 - Test loss: 0.04408\n",
      "Epoch 2492 - lr: 0.04653 - Train loss: 0.01713 - Test loss: 0.04407\n",
      "Epoch 2493 - lr: 0.04652 - Train loss: 0.01712 - Test loss: 0.04407\n",
      "Epoch 2494 - lr: 0.04650 - Train loss: 0.01712 - Test loss: 0.04406\n",
      "Epoch 2495 - lr: 0.04649 - Train loss: 0.01711 - Test loss: 0.04405\n",
      "Epoch 2496 - lr: 0.04647 - Train loss: 0.01711 - Test loss: 0.04405\n",
      "Epoch 2497 - lr: 0.04646 - Train loss: 0.01710 - Test loss: 0.04404\n",
      "Epoch 2498 - lr: 0.04644 - Train loss: 0.01709 - Test loss: 0.04403\n",
      "Epoch 2499 - lr: 0.04643 - Train loss: 0.01709 - Test loss: 0.04403\n",
      "Epoch 2500 - lr: 0.04642 - Train loss: 0.01708 - Test loss: 0.04402\n",
      "Epoch 2501 - lr: 0.04640 - Train loss: 0.01708 - Test loss: 0.04402\n",
      "Epoch 2502 - lr: 0.04639 - Train loss: 0.01707 - Test loss: 0.04401\n",
      "Epoch 2503 - lr: 0.04637 - Train loss: 0.01707 - Test loss: 0.04400\n",
      "Epoch 2504 - lr: 0.04636 - Train loss: 0.01706 - Test loss: 0.04400\n",
      "Epoch 2505 - lr: 0.04634 - Train loss: 0.01706 - Test loss: 0.04399\n",
      "Epoch 2506 - lr: 0.04633 - Train loss: 0.01705 - Test loss: 0.04399\n",
      "Epoch 2507 - lr: 0.04632 - Train loss: 0.01705 - Test loss: 0.04398\n",
      "Epoch 2508 - lr: 0.04630 - Train loss: 0.01704 - Test loss: 0.04397\n",
      "Epoch 2509 - lr: 0.04629 - Train loss: 0.01704 - Test loss: 0.04397\n",
      "Epoch 2510 - lr: 0.04627 - Train loss: 0.01703 - Test loss: 0.04396\n",
      "Epoch 2511 - lr: 0.04626 - Train loss: 0.01703 - Test loss: 0.04396\n",
      "Epoch 2512 - lr: 0.04625 - Train loss: 0.01702 - Test loss: 0.04395\n",
      "Epoch 2513 - lr: 0.04623 - Train loss: 0.01701 - Test loss: 0.04394\n",
      "Epoch 2514 - lr: 0.04622 - Train loss: 0.01701 - Test loss: 0.04394\n",
      "Epoch 2515 - lr: 0.04620 - Train loss: 0.01700 - Test loss: 0.04393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2516 - lr: 0.04619 - Train loss: 0.01700 - Test loss: 0.04393\n",
      "Epoch 2517 - lr: 0.04617 - Train loss: 0.01699 - Test loss: 0.04392\n",
      "Epoch 2518 - lr: 0.04616 - Train loss: 0.01699 - Test loss: 0.04391\n",
      "Epoch 2519 - lr: 0.04615 - Train loss: 0.01698 - Test loss: 0.04391\n",
      "Epoch 2520 - lr: 0.04613 - Train loss: 0.01698 - Test loss: 0.04390\n",
      "Epoch 2521 - lr: 0.04612 - Train loss: 0.01697 - Test loss: 0.04390\n",
      "Epoch 2522 - lr: 0.04610 - Train loss: 0.01697 - Test loss: 0.04389\n",
      "Epoch 2523 - lr: 0.04609 - Train loss: 0.01696 - Test loss: 0.04388\n",
      "Epoch 2524 - lr: 0.04608 - Train loss: 0.01696 - Test loss: 0.04388\n",
      "Epoch 2525 - lr: 0.04606 - Train loss: 0.01695 - Test loss: 0.04387\n",
      "Epoch 2526 - lr: 0.04605 - Train loss: 0.01695 - Test loss: 0.04387\n",
      "Epoch 2527 - lr: 0.04603 - Train loss: 0.01694 - Test loss: 0.04386\n",
      "Epoch 2528 - lr: 0.04602 - Train loss: 0.01694 - Test loss: 0.04386\n",
      "Epoch 2529 - lr: 0.04600 - Train loss: 0.01693 - Test loss: 0.04385\n",
      "Epoch 2530 - lr: 0.04599 - Train loss: 0.01693 - Test loss: 0.04384\n",
      "Epoch 2531 - lr: 0.04598 - Train loss: 0.01692 - Test loss: 0.04384\n",
      "Epoch 2532 - lr: 0.04596 - Train loss: 0.01692 - Test loss: 0.04383\n",
      "Epoch 2533 - lr: 0.04595 - Train loss: 0.01691 - Test loss: 0.04383\n",
      "Epoch 2534 - lr: 0.04593 - Train loss: 0.01691 - Test loss: 0.04382\n",
      "Epoch 2535 - lr: 0.04592 - Train loss: 0.01690 - Test loss: 0.04382\n",
      "Epoch 2536 - lr: 0.04591 - Train loss: 0.01690 - Test loss: 0.04381\n",
      "Epoch 2537 - lr: 0.04589 - Train loss: 0.01689 - Test loss: 0.04380\n",
      "Epoch 2538 - lr: 0.04588 - Train loss: 0.01689 - Test loss: 0.04380\n",
      "Epoch 2539 - lr: 0.04586 - Train loss: 0.01688 - Test loss: 0.04379\n",
      "Epoch 2540 - lr: 0.04585 - Train loss: 0.01688 - Test loss: 0.04379\n",
      "Epoch 2541 - lr: 0.04584 - Train loss: 0.01687 - Test loss: 0.04378\n",
      "Epoch 2542 - lr: 0.04582 - Train loss: 0.01687 - Test loss: 0.04378\n",
      "Epoch 2543 - lr: 0.04581 - Train loss: 0.01686 - Test loss: 0.04377\n",
      "Epoch 2544 - lr: 0.04579 - Train loss: 0.01686 - Test loss: 0.04377\n",
      "Epoch 2545 - lr: 0.04578 - Train loss: 0.01685 - Test loss: 0.04376\n",
      "Epoch 2546 - lr: 0.04576 - Train loss: 0.01685 - Test loss: 0.04375\n",
      "Epoch 2547 - lr: 0.04575 - Train loss: 0.01684 - Test loss: 0.04375\n",
      "Epoch 2548 - lr: 0.04574 - Train loss: 0.01684 - Test loss: 0.04374\n",
      "Epoch 2549 - lr: 0.04572 - Train loss: 0.01683 - Test loss: 0.04374\n",
      "Epoch 2550 - lr: 0.04571 - Train loss: 0.01683 - Test loss: 0.04373\n",
      "Epoch 2551 - lr: 0.04569 - Train loss: 0.01682 - Test loss: 0.04373\n",
      "Epoch 2552 - lr: 0.04568 - Train loss: 0.01682 - Test loss: 0.04372\n",
      "Epoch 2553 - lr: 0.04567 - Train loss: 0.01681 - Test loss: 0.04372\n",
      "Epoch 2554 - lr: 0.04565 - Train loss: 0.01681 - Test loss: 0.04371\n",
      "Epoch 2555 - lr: 0.04564 - Train loss: 0.01680 - Test loss: 0.04371\n",
      "Epoch 2556 - lr: 0.04562 - Train loss: 0.01680 - Test loss: 0.04370\n",
      "Epoch 2557 - lr: 0.04561 - Train loss: 0.01679 - Test loss: 0.04369\n",
      "Epoch 2558 - lr: 0.04560 - Train loss: 0.01679 - Test loss: 0.04369\n",
      "Epoch 2559 - lr: 0.04558 - Train loss: 0.01678 - Test loss: 0.04368\n",
      "Epoch 2560 - lr: 0.04557 - Train loss: 0.01678 - Test loss: 0.04368\n",
      "Epoch 2561 - lr: 0.04555 - Train loss: 0.01677 - Test loss: 0.04367\n",
      "Epoch 2562 - lr: 0.04554 - Train loss: 0.01677 - Test loss: 0.04367\n",
      "Epoch 2563 - lr: 0.04553 - Train loss: 0.01676 - Test loss: 0.04366\n",
      "Epoch 2564 - lr: 0.04551 - Train loss: 0.01676 - Test loss: 0.04366\n",
      "Epoch 2565 - lr: 0.04550 - Train loss: 0.01675 - Test loss: 0.04365\n",
      "Epoch 2566 - lr: 0.04548 - Train loss: 0.01675 - Test loss: 0.04365\n",
      "Epoch 2567 - lr: 0.04547 - Train loss: 0.01674 - Test loss: 0.04364\n",
      "Epoch 2568 - lr: 0.04546 - Train loss: 0.01674 - Test loss: 0.04364\n",
      "Epoch 2569 - lr: 0.04544 - Train loss: 0.01673 - Test loss: 0.04363\n",
      "Epoch 2570 - lr: 0.04543 - Train loss: 0.01673 - Test loss: 0.04363\n",
      "Epoch 2571 - lr: 0.04542 - Train loss: 0.01672 - Test loss: 0.04362\n",
      "Epoch 2572 - lr: 0.04540 - Train loss: 0.01672 - Test loss: 0.04362\n",
      "Epoch 2573 - lr: 0.04539 - Train loss: 0.01671 - Test loss: 0.04361\n",
      "Epoch 2574 - lr: 0.04537 - Train loss: 0.01671 - Test loss: 0.04360\n",
      "Epoch 2575 - lr: 0.04536 - Train loss: 0.01670 - Test loss: 0.04360\n",
      "Epoch 2576 - lr: 0.04535 - Train loss: 0.01670 - Test loss: 0.04359\n",
      "Epoch 2577 - lr: 0.04533 - Train loss: 0.01669 - Test loss: 0.04359\n",
      "Epoch 2578 - lr: 0.04532 - Train loss: 0.01669 - Test loss: 0.04358\n",
      "Epoch 2579 - lr: 0.04530 - Train loss: 0.01668 - Test loss: 0.04358\n",
      "Epoch 2580 - lr: 0.04529 - Train loss: 0.01668 - Test loss: 0.04357\n",
      "Epoch 2581 - lr: 0.04528 - Train loss: 0.01667 - Test loss: 0.04357\n",
      "Epoch 2582 - lr: 0.04526 - Train loss: 0.01667 - Test loss: 0.04356\n",
      "Epoch 2583 - lr: 0.04525 - Train loss: 0.01666 - Test loss: 0.04356\n",
      "Epoch 2584 - lr: 0.04523 - Train loss: 0.01666 - Test loss: 0.04355\n",
      "Epoch 2585 - lr: 0.04522 - Train loss: 0.01665 - Test loss: 0.04355\n",
      "Epoch 2586 - lr: 0.04521 - Train loss: 0.01665 - Test loss: 0.04354\n",
      "Epoch 2587 - lr: 0.04519 - Train loss: 0.01665 - Test loss: 0.04354\n",
      "Epoch 2588 - lr: 0.04518 - Train loss: 0.01664 - Test loss: 0.04353\n",
      "Epoch 2589 - lr: 0.04516 - Train loss: 0.01664 - Test loss: 0.04353\n",
      "Epoch 2590 - lr: 0.04515 - Train loss: 0.01663 - Test loss: 0.04352\n",
      "Epoch 2591 - lr: 0.04514 - Train loss: 0.01663 - Test loss: 0.04352\n",
      "Epoch 2592 - lr: 0.04512 - Train loss: 0.01662 - Test loss: 0.04351\n",
      "Epoch 2593 - lr: 0.04511 - Train loss: 0.01662 - Test loss: 0.04351\n",
      "Epoch 2594 - lr: 0.04510 - Train loss: 0.01661 - Test loss: 0.04350\n",
      "Epoch 2595 - lr: 0.04508 - Train loss: 0.01661 - Test loss: 0.04350\n",
      "Epoch 2596 - lr: 0.04507 - Train loss: 0.01660 - Test loss: 0.04349\n",
      "Epoch 2597 - lr: 0.04505 - Train loss: 0.01660 - Test loss: 0.04349\n",
      "Epoch 2598 - lr: 0.04504 - Train loss: 0.01659 - Test loss: 0.04348\n",
      "Epoch 2599 - lr: 0.04503 - Train loss: 0.01659 - Test loss: 0.04348\n",
      "Epoch 2600 - lr: 0.04501 - Train loss: 0.01658 - Test loss: 0.04347\n",
      "Epoch 2601 - lr: 0.04500 - Train loss: 0.01658 - Test loss: 0.04347\n",
      "Epoch 2602 - lr: 0.04498 - Train loss: 0.01657 - Test loss: 0.04346\n",
      "Epoch 2603 - lr: 0.04497 - Train loss: 0.01657 - Test loss: 0.04346\n",
      "Epoch 2604 - lr: 0.04496 - Train loss: 0.01657 - Test loss: 0.04346\n",
      "Epoch 2605 - lr: 0.04494 - Train loss: 0.01656 - Test loss: 0.04345\n",
      "Epoch 2606 - lr: 0.04493 - Train loss: 0.01656 - Test loss: 0.04345\n",
      "Epoch 2607 - lr: 0.04492 - Train loss: 0.01655 - Test loss: 0.04344\n",
      "Epoch 2608 - lr: 0.04490 - Train loss: 0.01655 - Test loss: 0.04344\n",
      "Epoch 2609 - lr: 0.04489 - Train loss: 0.01654 - Test loss: 0.04343\n",
      "Epoch 2610 - lr: 0.04487 - Train loss: 0.01654 - Test loss: 0.04343\n",
      "Epoch 2611 - lr: 0.04486 - Train loss: 0.01653 - Test loss: 0.04342\n",
      "Epoch 2612 - lr: 0.04485 - Train loss: 0.01653 - Test loss: 0.04342\n",
      "Epoch 2613 - lr: 0.04483 - Train loss: 0.01652 - Test loss: 0.04341\n",
      "Epoch 2614 - lr: 0.04482 - Train loss: 0.01652 - Test loss: 0.04341\n",
      "Epoch 2615 - lr: 0.04481 - Train loss: 0.01651 - Test loss: 0.04340\n",
      "Epoch 2616 - lr: 0.04479 - Train loss: 0.01651 - Test loss: 0.04340\n",
      "Epoch 2617 - lr: 0.04478 - Train loss: 0.01651 - Test loss: 0.04339\n",
      "Epoch 2618 - lr: 0.04476 - Train loss: 0.01650 - Test loss: 0.04339\n",
      "Epoch 2619 - lr: 0.04475 - Train loss: 0.01650 - Test loss: 0.04338\n",
      "Epoch 2620 - lr: 0.04474 - Train loss: 0.01649 - Test loss: 0.04338\n",
      "Epoch 2621 - lr: 0.04472 - Train loss: 0.01649 - Test loss: 0.04337\n",
      "Epoch 2622 - lr: 0.04471 - Train loss: 0.01648 - Test loss: 0.04337\n",
      "Epoch 2623 - lr: 0.04470 - Train loss: 0.01648 - Test loss: 0.04337\n",
      "Epoch 2624 - lr: 0.04468 - Train loss: 0.01647 - Test loss: 0.04336\n",
      "Epoch 2625 - lr: 0.04467 - Train loss: 0.01647 - Test loss: 0.04336\n",
      "Epoch 2626 - lr: 0.04465 - Train loss: 0.01646 - Test loss: 0.04335\n",
      "Epoch 2627 - lr: 0.04464 - Train loss: 0.01646 - Test loss: 0.04335\n",
      "Epoch 2628 - lr: 0.04463 - Train loss: 0.01646 - Test loss: 0.04334\n",
      "Epoch 2629 - lr: 0.04461 - Train loss: 0.01645 - Test loss: 0.04334\n",
      "Epoch 2630 - lr: 0.04460 - Train loss: 0.01645 - Test loss: 0.04333\n",
      "Epoch 2631 - lr: 0.04459 - Train loss: 0.01644 - Test loss: 0.04333\n",
      "Epoch 2632 - lr: 0.04457 - Train loss: 0.01644 - Test loss: 0.04332\n",
      "Epoch 2633 - lr: 0.04456 - Train loss: 0.01643 - Test loss: 0.04332\n",
      "Epoch 2634 - lr: 0.04455 - Train loss: 0.01643 - Test loss: 0.04332\n",
      "Epoch 2635 - lr: 0.04453 - Train loss: 0.01642 - Test loss: 0.04331\n",
      "Epoch 2636 - lr: 0.04452 - Train loss: 0.01642 - Test loss: 0.04331\n",
      "Epoch 2637 - lr: 0.04450 - Train loss: 0.01642 - Test loss: 0.04330\n",
      "Epoch 2638 - lr: 0.04449 - Train loss: 0.01641 - Test loss: 0.04330\n",
      "Epoch 2639 - lr: 0.04448 - Train loss: 0.01641 - Test loss: 0.04329\n",
      "Epoch 2640 - lr: 0.04446 - Train loss: 0.01640 - Test loss: 0.04329\n",
      "Epoch 2641 - lr: 0.04445 - Train loss: 0.01640 - Test loss: 0.04328\n",
      "Epoch 2642 - lr: 0.04444 - Train loss: 0.01639 - Test loss: 0.04328\n",
      "Epoch 2643 - lr: 0.04442 - Train loss: 0.01639 - Test loss: 0.04327\n",
      "Epoch 2644 - lr: 0.04441 - Train loss: 0.01638 - Test loss: 0.04327\n",
      "Epoch 2645 - lr: 0.04439 - Train loss: 0.01638 - Test loss: 0.04327\n",
      "Epoch 2646 - lr: 0.04438 - Train loss: 0.01638 - Test loss: 0.04326\n",
      "Epoch 2647 - lr: 0.04437 - Train loss: 0.01637 - Test loss: 0.04326\n",
      "Epoch 2648 - lr: 0.04435 - Train loss: 0.01637 - Test loss: 0.04325\n",
      "Epoch 2649 - lr: 0.04434 - Train loss: 0.01636 - Test loss: 0.04325\n",
      "Epoch 2650 - lr: 0.04433 - Train loss: 0.01636 - Test loss: 0.04324\n",
      "Epoch 2651 - lr: 0.04431 - Train loss: 0.01635 - Test loss: 0.04324\n",
      "Epoch 2652 - lr: 0.04430 - Train loss: 0.01635 - Test loss: 0.04324\n",
      "Epoch 2653 - lr: 0.04429 - Train loss: 0.01634 - Test loss: 0.04323\n",
      "Epoch 2654 - lr: 0.04427 - Train loss: 0.01634 - Test loss: 0.04323\n",
      "Epoch 2655 - lr: 0.04426 - Train loss: 0.01634 - Test loss: 0.04322\n",
      "Epoch 2656 - lr: 0.04425 - Train loss: 0.01633 - Test loss: 0.04322\n",
      "Epoch 2657 - lr: 0.04423 - Train loss: 0.01633 - Test loss: 0.04321\n",
      "Epoch 2658 - lr: 0.04422 - Train loss: 0.01632 - Test loss: 0.04321\n",
      "Epoch 2659 - lr: 0.04420 - Train loss: 0.01632 - Test loss: 0.04321\n",
      "Epoch 2660 - lr: 0.04419 - Train loss: 0.01631 - Test loss: 0.04320\n",
      "Epoch 2661 - lr: 0.04418 - Train loss: 0.01631 - Test loss: 0.04320\n",
      "Epoch 2662 - lr: 0.04416 - Train loss: 0.01631 - Test loss: 0.04319\n",
      "Epoch 2663 - lr: 0.04415 - Train loss: 0.01630 - Test loss: 0.04319\n",
      "Epoch 2664 - lr: 0.04414 - Train loss: 0.01630 - Test loss: 0.04318\n",
      "Epoch 2665 - lr: 0.04412 - Train loss: 0.01629 - Test loss: 0.04318\n",
      "Epoch 2666 - lr: 0.04411 - Train loss: 0.01629 - Test loss: 0.04318\n",
      "Epoch 2667 - lr: 0.04410 - Train loss: 0.01628 - Test loss: 0.04317\n",
      "Epoch 2668 - lr: 0.04408 - Train loss: 0.01628 - Test loss: 0.04317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2669 - lr: 0.04407 - Train loss: 0.01628 - Test loss: 0.04316\n",
      "Epoch 2670 - lr: 0.04406 - Train loss: 0.01627 - Test loss: 0.04316\n",
      "Epoch 2671 - lr: 0.04404 - Train loss: 0.01627 - Test loss: 0.04315\n",
      "Epoch 2672 - lr: 0.04403 - Train loss: 0.01626 - Test loss: 0.04315\n",
      "Epoch 2673 - lr: 0.04401 - Train loss: 0.01626 - Test loss: 0.04315\n",
      "Epoch 2674 - lr: 0.04400 - Train loss: 0.01625 - Test loss: 0.04314\n",
      "Epoch 2675 - lr: 0.04399 - Train loss: 0.01625 - Test loss: 0.04314\n",
      "Epoch 2676 - lr: 0.04397 - Train loss: 0.01625 - Test loss: 0.04313\n",
      "Epoch 2677 - lr: 0.04396 - Train loss: 0.01624 - Test loss: 0.04313\n",
      "Epoch 2678 - lr: 0.04395 - Train loss: 0.01624 - Test loss: 0.04313\n",
      "Epoch 2679 - lr: 0.04393 - Train loss: 0.01623 - Test loss: 0.04312\n",
      "Epoch 2680 - lr: 0.04392 - Train loss: 0.01623 - Test loss: 0.04312\n",
      "Epoch 2681 - lr: 0.04391 - Train loss: 0.01622 - Test loss: 0.04311\n",
      "Epoch 2682 - lr: 0.04389 - Train loss: 0.01622 - Test loss: 0.04311\n",
      "Epoch 2683 - lr: 0.04388 - Train loss: 0.01622 - Test loss: 0.04310\n",
      "Epoch 2684 - lr: 0.04387 - Train loss: 0.01621 - Test loss: 0.04310\n",
      "Epoch 2685 - lr: 0.04385 - Train loss: 0.01621 - Test loss: 0.04310\n",
      "Epoch 2686 - lr: 0.04384 - Train loss: 0.01620 - Test loss: 0.04309\n",
      "Epoch 2687 - lr: 0.04383 - Train loss: 0.01620 - Test loss: 0.04309\n",
      "Epoch 2688 - lr: 0.04381 - Train loss: 0.01619 - Test loss: 0.04308\n",
      "Epoch 2689 - lr: 0.04380 - Train loss: 0.01619 - Test loss: 0.04308\n",
      "Epoch 2690 - lr: 0.04379 - Train loss: 0.01619 - Test loss: 0.04308\n",
      "Epoch 2691 - lr: 0.04377 - Train loss: 0.01618 - Test loss: 0.04307\n",
      "Epoch 2692 - lr: 0.04376 - Train loss: 0.01618 - Test loss: 0.04307\n",
      "Epoch 2693 - lr: 0.04375 - Train loss: 0.01617 - Test loss: 0.04306\n",
      "Epoch 2694 - lr: 0.04373 - Train loss: 0.01617 - Test loss: 0.04306\n",
      "Epoch 2695 - lr: 0.04372 - Train loss: 0.01616 - Test loss: 0.04306\n",
      "Epoch 2696 - lr: 0.04371 - Train loss: 0.01616 - Test loss: 0.04305\n",
      "Epoch 2697 - lr: 0.04369 - Train loss: 0.01616 - Test loss: 0.04305\n",
      "Epoch 2698 - lr: 0.04368 - Train loss: 0.01615 - Test loss: 0.04304\n",
      "Epoch 2699 - lr: 0.04366 - Train loss: 0.01615 - Test loss: 0.04304\n",
      "Epoch 2700 - lr: 0.04365 - Train loss: 0.01614 - Test loss: 0.04304\n",
      "Epoch 2701 - lr: 0.04364 - Train loss: 0.01614 - Test loss: 0.04303\n",
      "Epoch 2702 - lr: 0.04362 - Train loss: 0.01614 - Test loss: 0.04303\n",
      "Epoch 2703 - lr: 0.04361 - Train loss: 0.01613 - Test loss: 0.04303\n",
      "Epoch 2704 - lr: 0.04360 - Train loss: 0.01613 - Test loss: 0.04302\n",
      "Epoch 2705 - lr: 0.04358 - Train loss: 0.01612 - Test loss: 0.04302\n",
      "Epoch 2706 - lr: 0.04357 - Train loss: 0.01612 - Test loss: 0.04301\n",
      "Epoch 2707 - lr: 0.04356 - Train loss: 0.01612 - Test loss: 0.04301\n",
      "Epoch 2708 - lr: 0.04354 - Train loss: 0.01611 - Test loss: 0.04301\n",
      "Epoch 2709 - lr: 0.04353 - Train loss: 0.01611 - Test loss: 0.04300\n",
      "Epoch 2710 - lr: 0.04352 - Train loss: 0.01610 - Test loss: 0.04300\n",
      "Epoch 2711 - lr: 0.04350 - Train loss: 0.01610 - Test loss: 0.04299\n",
      "Epoch 2712 - lr: 0.04349 - Train loss: 0.01609 - Test loss: 0.04299\n",
      "Epoch 2713 - lr: 0.04348 - Train loss: 0.01609 - Test loss: 0.04299\n",
      "Epoch 2714 - lr: 0.04346 - Train loss: 0.01609 - Test loss: 0.04298\n",
      "Epoch 2715 - lr: 0.04345 - Train loss: 0.01608 - Test loss: 0.04298\n",
      "Epoch 2716 - lr: 0.04344 - Train loss: 0.01608 - Test loss: 0.04297\n",
      "Epoch 2717 - lr: 0.04342 - Train loss: 0.01607 - Test loss: 0.04297\n",
      "Epoch 2718 - lr: 0.04341 - Train loss: 0.01607 - Test loss: 0.04297\n",
      "Epoch 2719 - lr: 0.04340 - Train loss: 0.01607 - Test loss: 0.04296\n",
      "Epoch 2720 - lr: 0.04338 - Train loss: 0.01606 - Test loss: 0.04296\n",
      "Epoch 2721 - lr: 0.04337 - Train loss: 0.01606 - Test loss: 0.04296\n",
      "Epoch 2722 - lr: 0.04336 - Train loss: 0.01605 - Test loss: 0.04295\n",
      "Epoch 2723 - lr: 0.04334 - Train loss: 0.01605 - Test loss: 0.04295\n",
      "Epoch 2724 - lr: 0.04333 - Train loss: 0.01605 - Test loss: 0.04294\n",
      "Epoch 2725 - lr: 0.04332 - Train loss: 0.01604 - Test loss: 0.04294\n",
      "Epoch 2726 - lr: 0.04330 - Train loss: 0.01604 - Test loss: 0.04294\n",
      "Epoch 2727 - lr: 0.04329 - Train loss: 0.01603 - Test loss: 0.04293\n",
      "Epoch 2728 - lr: 0.04328 - Train loss: 0.01603 - Test loss: 0.04293\n",
      "Epoch 2729 - lr: 0.04326 - Train loss: 0.01603 - Test loss: 0.04293\n",
      "Epoch 2730 - lr: 0.04325 - Train loss: 0.01602 - Test loss: 0.04292\n",
      "Epoch 2731 - lr: 0.04324 - Train loss: 0.01602 - Test loss: 0.04292\n",
      "Epoch 2732 - lr: 0.04322 - Train loss: 0.01601 - Test loss: 0.04291\n",
      "Epoch 2733 - lr: 0.04321 - Train loss: 0.01601 - Test loss: 0.04291\n",
      "Epoch 2734 - lr: 0.04320 - Train loss: 0.01601 - Test loss: 0.04291\n",
      "Epoch 2735 - lr: 0.04319 - Train loss: 0.01600 - Test loss: 0.04290\n",
      "Epoch 2736 - lr: 0.04317 - Train loss: 0.01600 - Test loss: 0.04290\n",
      "Epoch 2737 - lr: 0.04316 - Train loss: 0.01599 - Test loss: 0.04290\n",
      "Epoch 2738 - lr: 0.04315 - Train loss: 0.01599 - Test loss: 0.04289\n",
      "Epoch 2739 - lr: 0.04313 - Train loss: 0.01599 - Test loss: 0.04289\n",
      "Epoch 2740 - lr: 0.04312 - Train loss: 0.01598 - Test loss: 0.04289\n",
      "Epoch 2741 - lr: 0.04311 - Train loss: 0.01598 - Test loss: 0.04288\n",
      "Epoch 2742 - lr: 0.04309 - Train loss: 0.01597 - Test loss: 0.04288\n",
      "Epoch 2743 - lr: 0.04308 - Train loss: 0.01597 - Test loss: 0.04287\n",
      "Epoch 2744 - lr: 0.04307 - Train loss: 0.01597 - Test loss: 0.04287\n",
      "Epoch 2745 - lr: 0.04305 - Train loss: 0.01596 - Test loss: 0.04287\n",
      "Epoch 2746 - lr: 0.04304 - Train loss: 0.01596 - Test loss: 0.04286\n",
      "Epoch 2747 - lr: 0.04303 - Train loss: 0.01595 - Test loss: 0.04286\n",
      "Epoch 2748 - lr: 0.04301 - Train loss: 0.01595 - Test loss: 0.04286\n",
      "Epoch 2749 - lr: 0.04300 - Train loss: 0.01595 - Test loss: 0.04285\n",
      "Epoch 2750 - lr: 0.04299 - Train loss: 0.01594 - Test loss: 0.04285\n",
      "Epoch 2751 - lr: 0.04297 - Train loss: 0.01594 - Test loss: 0.04285\n",
      "Epoch 2752 - lr: 0.04296 - Train loss: 0.01593 - Test loss: 0.04284\n",
      "Epoch 2753 - lr: 0.04295 - Train loss: 0.01593 - Test loss: 0.04284\n",
      "Epoch 2754 - lr: 0.04293 - Train loss: 0.01593 - Test loss: 0.04283\n",
      "Epoch 2755 - lr: 0.04292 - Train loss: 0.01592 - Test loss: 0.04283\n",
      "Epoch 2756 - lr: 0.04291 - Train loss: 0.01592 - Test loss: 0.04283\n",
      "Epoch 2757 - lr: 0.04289 - Train loss: 0.01591 - Test loss: 0.04282\n",
      "Epoch 2758 - lr: 0.04288 - Train loss: 0.01591 - Test loss: 0.04282\n",
      "Epoch 2759 - lr: 0.04287 - Train loss: 0.01591 - Test loss: 0.04282\n",
      "Epoch 2760 - lr: 0.04285 - Train loss: 0.01590 - Test loss: 0.04281\n",
      "Epoch 2761 - lr: 0.04284 - Train loss: 0.01590 - Test loss: 0.04281\n",
      "Epoch 2762 - lr: 0.04283 - Train loss: 0.01590 - Test loss: 0.04281\n",
      "Epoch 2763 - lr: 0.04282 - Train loss: 0.01589 - Test loss: 0.04280\n",
      "Epoch 2764 - lr: 0.04280 - Train loss: 0.01589 - Test loss: 0.04280\n",
      "Epoch 2765 - lr: 0.04279 - Train loss: 0.01588 - Test loss: 0.04280\n",
      "Epoch 2766 - lr: 0.04278 - Train loss: 0.01588 - Test loss: 0.04279\n",
      "Epoch 2767 - lr: 0.04276 - Train loss: 0.01588 - Test loss: 0.04279\n",
      "Epoch 2768 - lr: 0.04275 - Train loss: 0.01587 - Test loss: 0.04279\n",
      "Epoch 2769 - lr: 0.04274 - Train loss: 0.01587 - Test loss: 0.04278\n",
      "Epoch 2770 - lr: 0.04272 - Train loss: 0.01586 - Test loss: 0.04278\n",
      "Epoch 2771 - lr: 0.04271 - Train loss: 0.01586 - Test loss: 0.04278\n",
      "Epoch 2772 - lr: 0.04270 - Train loss: 0.01586 - Test loss: 0.04277\n",
      "Epoch 2773 - lr: 0.04268 - Train loss: 0.01585 - Test loss: 0.04277\n",
      "Epoch 2774 - lr: 0.04267 - Train loss: 0.01585 - Test loss: 0.04277\n",
      "Epoch 2775 - lr: 0.04266 - Train loss: 0.01584 - Test loss: 0.04276\n",
      "Epoch 2776 - lr: 0.04264 - Train loss: 0.01584 - Test loss: 0.04276\n",
      "Epoch 2777 - lr: 0.04263 - Train loss: 0.01584 - Test loss: 0.04276\n",
      "Epoch 2778 - lr: 0.04262 - Train loss: 0.01583 - Test loss: 0.04275\n",
      "Epoch 2779 - lr: 0.04261 - Train loss: 0.01583 - Test loss: 0.04275\n",
      "Epoch 2780 - lr: 0.04259 - Train loss: 0.01583 - Test loss: 0.04274\n",
      "Epoch 2781 - lr: 0.04258 - Train loss: 0.01582 - Test loss: 0.04274\n",
      "Epoch 2782 - lr: 0.04257 - Train loss: 0.01582 - Test loss: 0.04274\n",
      "Epoch 2783 - lr: 0.04255 - Train loss: 0.01581 - Test loss: 0.04273\n",
      "Epoch 2784 - lr: 0.04254 - Train loss: 0.01581 - Test loss: 0.04273\n",
      "Epoch 2785 - lr: 0.04253 - Train loss: 0.01581 - Test loss: 0.04273\n",
      "Epoch 2786 - lr: 0.04251 - Train loss: 0.01580 - Test loss: 0.04272\n",
      "Epoch 2787 - lr: 0.04250 - Train loss: 0.01580 - Test loss: 0.04272\n",
      "Epoch 2788 - lr: 0.04249 - Train loss: 0.01580 - Test loss: 0.04272\n",
      "Epoch 2789 - lr: 0.04247 - Train loss: 0.01579 - Test loss: 0.04271\n",
      "Epoch 2790 - lr: 0.04246 - Train loss: 0.01579 - Test loss: 0.04271\n",
      "Epoch 2791 - lr: 0.04245 - Train loss: 0.01578 - Test loss: 0.04271\n",
      "Epoch 2792 - lr: 0.04244 - Train loss: 0.01578 - Test loss: 0.04270\n",
      "Epoch 2793 - lr: 0.04242 - Train loss: 0.01578 - Test loss: 0.04270\n",
      "Epoch 2794 - lr: 0.04241 - Train loss: 0.01577 - Test loss: 0.04270\n",
      "Epoch 2795 - lr: 0.04240 - Train loss: 0.01577 - Test loss: 0.04269\n",
      "Epoch 2796 - lr: 0.04238 - Train loss: 0.01577 - Test loss: 0.04269\n",
      "Epoch 2797 - lr: 0.04237 - Train loss: 0.01576 - Test loss: 0.04269\n",
      "Epoch 2798 - lr: 0.04236 - Train loss: 0.01576 - Test loss: 0.04269\n",
      "Epoch 2799 - lr: 0.04234 - Train loss: 0.01575 - Test loss: 0.04268\n",
      "Epoch 2800 - lr: 0.04233 - Train loss: 0.01575 - Test loss: 0.04268\n",
      "Epoch 2801 - lr: 0.04232 - Train loss: 0.01575 - Test loss: 0.04268\n",
      "Epoch 2802 - lr: 0.04231 - Train loss: 0.01574 - Test loss: 0.04267\n",
      "Epoch 2803 - lr: 0.04229 - Train loss: 0.01574 - Test loss: 0.04267\n",
      "Epoch 2804 - lr: 0.04228 - Train loss: 0.01574 - Test loss: 0.04267\n",
      "Epoch 2805 - lr: 0.04227 - Train loss: 0.01573 - Test loss: 0.04266\n",
      "Epoch 2806 - lr: 0.04225 - Train loss: 0.01573 - Test loss: 0.04266\n",
      "Epoch 2807 - lr: 0.04224 - Train loss: 0.01572 - Test loss: 0.04266\n",
      "Epoch 2808 - lr: 0.04223 - Train loss: 0.01572 - Test loss: 0.04265\n",
      "Epoch 2809 - lr: 0.04221 - Train loss: 0.01572 - Test loss: 0.04265\n",
      "Epoch 2810 - lr: 0.04220 - Train loss: 0.01571 - Test loss: 0.04265\n",
      "Epoch 2811 - lr: 0.04219 - Train loss: 0.01571 - Test loss: 0.04264\n",
      "Epoch 2812 - lr: 0.04218 - Train loss: 0.01571 - Test loss: 0.04264\n",
      "Epoch 2813 - lr: 0.04216 - Train loss: 0.01570 - Test loss: 0.04264\n",
      "Epoch 2814 - lr: 0.04215 - Train loss: 0.01570 - Test loss: 0.04263\n",
      "Epoch 2815 - lr: 0.04214 - Train loss: 0.01569 - Test loss: 0.04263\n",
      "Epoch 2816 - lr: 0.04212 - Train loss: 0.01569 - Test loss: 0.04263\n",
      "Epoch 2817 - lr: 0.04211 - Train loss: 0.01569 - Test loss: 0.04262\n",
      "Epoch 2818 - lr: 0.04210 - Train loss: 0.01568 - Test loss: 0.04262\n",
      "Epoch 2819 - lr: 0.04209 - Train loss: 0.01568 - Test loss: 0.04262\n",
      "Epoch 2820 - lr: 0.04207 - Train loss: 0.01568 - Test loss: 0.04261\n",
      "Epoch 2821 - lr: 0.04206 - Train loss: 0.01567 - Test loss: 0.04261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2822 - lr: 0.04205 - Train loss: 0.01567 - Test loss: 0.04261\n",
      "Epoch 2823 - lr: 0.04203 - Train loss: 0.01566 - Test loss: 0.04261\n",
      "Epoch 2824 - lr: 0.04202 - Train loss: 0.01566 - Test loss: 0.04260\n",
      "Epoch 2825 - lr: 0.04201 - Train loss: 0.01566 - Test loss: 0.04260\n",
      "Epoch 2826 - lr: 0.04200 - Train loss: 0.01565 - Test loss: 0.04260\n",
      "Epoch 2827 - lr: 0.04198 - Train loss: 0.01565 - Test loss: 0.04259\n",
      "Epoch 2828 - lr: 0.04197 - Train loss: 0.01565 - Test loss: 0.04259\n",
      "Epoch 2829 - lr: 0.04196 - Train loss: 0.01564 - Test loss: 0.04259\n",
      "Epoch 2830 - lr: 0.04194 - Train loss: 0.01564 - Test loss: 0.04258\n",
      "Epoch 2831 - lr: 0.04193 - Train loss: 0.01564 - Test loss: 0.04258\n",
      "Epoch 2832 - lr: 0.04192 - Train loss: 0.01563 - Test loss: 0.04258\n",
      "Epoch 2833 - lr: 0.04191 - Train loss: 0.01563 - Test loss: 0.04257\n",
      "Epoch 2834 - lr: 0.04189 - Train loss: 0.01562 - Test loss: 0.04257\n",
      "Epoch 2835 - lr: 0.04188 - Train loss: 0.01562 - Test loss: 0.04257\n",
      "Epoch 2836 - lr: 0.04187 - Train loss: 0.01562 - Test loss: 0.04256\n",
      "Epoch 2837 - lr: 0.04185 - Train loss: 0.01561 - Test loss: 0.04256\n",
      "Epoch 2838 - lr: 0.04184 - Train loss: 0.01561 - Test loss: 0.04256\n",
      "Epoch 2839 - lr: 0.04183 - Train loss: 0.01561 - Test loss: 0.04256\n",
      "Epoch 2840 - lr: 0.04182 - Train loss: 0.01560 - Test loss: 0.04255\n",
      "Epoch 2841 - lr: 0.04180 - Train loss: 0.01560 - Test loss: 0.04255\n",
      "Epoch 2842 - lr: 0.04179 - Train loss: 0.01560 - Test loss: 0.04255\n",
      "Epoch 2843 - lr: 0.04178 - Train loss: 0.01559 - Test loss: 0.04254\n",
      "Epoch 2844 - lr: 0.04176 - Train loss: 0.01559 - Test loss: 0.04254\n",
      "Epoch 2845 - lr: 0.04175 - Train loss: 0.01559 - Test loss: 0.04254\n",
      "Epoch 2846 - lr: 0.04174 - Train loss: 0.01558 - Test loss: 0.04253\n",
      "Epoch 2847 - lr: 0.04173 - Train loss: 0.01558 - Test loss: 0.04253\n",
      "Epoch 2848 - lr: 0.04171 - Train loss: 0.01557 - Test loss: 0.04253\n",
      "Epoch 2849 - lr: 0.04170 - Train loss: 0.01557 - Test loss: 0.04253\n",
      "Epoch 2850 - lr: 0.04169 - Train loss: 0.01557 - Test loss: 0.04252\n",
      "Epoch 2851 - lr: 0.04167 - Train loss: 0.01556 - Test loss: 0.04252\n",
      "Epoch 2852 - lr: 0.04166 - Train loss: 0.01556 - Test loss: 0.04252\n",
      "Epoch 2853 - lr: 0.04165 - Train loss: 0.01556 - Test loss: 0.04251\n",
      "Epoch 2854 - lr: 0.04164 - Train loss: 0.01555 - Test loss: 0.04251\n",
      "Epoch 2855 - lr: 0.04162 - Train loss: 0.01555 - Test loss: 0.04251\n",
      "Epoch 2856 - lr: 0.04161 - Train loss: 0.01555 - Test loss: 0.04250\n",
      "Epoch 2857 - lr: 0.04160 - Train loss: 0.01554 - Test loss: 0.04250\n",
      "Epoch 2858 - lr: 0.04158 - Train loss: 0.01554 - Test loss: 0.04250\n",
      "Epoch 2859 - lr: 0.04157 - Train loss: 0.01554 - Test loss: 0.04250\n",
      "Epoch 2860 - lr: 0.04156 - Train loss: 0.01553 - Test loss: 0.04249\n",
      "Epoch 2861 - lr: 0.04155 - Train loss: 0.01553 - Test loss: 0.04249\n",
      "Epoch 2862 - lr: 0.04153 - Train loss: 0.01552 - Test loss: 0.04249\n",
      "Epoch 2863 - lr: 0.04152 - Train loss: 0.01552 - Test loss: 0.04248\n",
      "Epoch 2864 - lr: 0.04151 - Train loss: 0.01552 - Test loss: 0.04248\n",
      "Epoch 2865 - lr: 0.04150 - Train loss: 0.01551 - Test loss: 0.04248\n",
      "Epoch 2866 - lr: 0.04148 - Train loss: 0.01551 - Test loss: 0.04248\n",
      "Epoch 2867 - lr: 0.04147 - Train loss: 0.01551 - Test loss: 0.04247\n",
      "Epoch 2868 - lr: 0.04146 - Train loss: 0.01550 - Test loss: 0.04247\n",
      "Epoch 2869 - lr: 0.04144 - Train loss: 0.01550 - Test loss: 0.04247\n",
      "Epoch 2870 - lr: 0.04143 - Train loss: 0.01550 - Test loss: 0.04246\n",
      "Epoch 2871 - lr: 0.04142 - Train loss: 0.01549 - Test loss: 0.04246\n",
      "Epoch 2872 - lr: 0.04141 - Train loss: 0.01549 - Test loss: 0.04246\n",
      "Epoch 2873 - lr: 0.04139 - Train loss: 0.01549 - Test loss: 0.04246\n",
      "Epoch 2874 - lr: 0.04138 - Train loss: 0.01548 - Test loss: 0.04245\n",
      "Epoch 2875 - lr: 0.04137 - Train loss: 0.01548 - Test loss: 0.04245\n",
      "Epoch 2876 - lr: 0.04136 - Train loss: 0.01548 - Test loss: 0.04245\n",
      "Epoch 2877 - lr: 0.04134 - Train loss: 0.01547 - Test loss: 0.04244\n",
      "Epoch 2878 - lr: 0.04133 - Train loss: 0.01547 - Test loss: 0.04244\n",
      "Epoch 2879 - lr: 0.04132 - Train loss: 0.01546 - Test loss: 0.04244\n",
      "Epoch 2880 - lr: 0.04130 - Train loss: 0.01546 - Test loss: 0.04244\n",
      "Epoch 2881 - lr: 0.04129 - Train loss: 0.01546 - Test loss: 0.04243\n",
      "Epoch 2882 - lr: 0.04128 - Train loss: 0.01545 - Test loss: 0.04243\n",
      "Epoch 2883 - lr: 0.04127 - Train loss: 0.01545 - Test loss: 0.04243\n",
      "Epoch 2884 - lr: 0.04125 - Train loss: 0.01545 - Test loss: 0.04242\n",
      "Epoch 2885 - lr: 0.04124 - Train loss: 0.01544 - Test loss: 0.04242\n",
      "Epoch 2886 - lr: 0.04123 - Train loss: 0.01544 - Test loss: 0.04242\n",
      "Epoch 2887 - lr: 0.04122 - Train loss: 0.01544 - Test loss: 0.04242\n",
      "Epoch 2888 - lr: 0.04120 - Train loss: 0.01543 - Test loss: 0.04241\n",
      "Epoch 2889 - lr: 0.04119 - Train loss: 0.01543 - Test loss: 0.04241\n",
      "Epoch 2890 - lr: 0.04118 - Train loss: 0.01543 - Test loss: 0.04241\n",
      "Epoch 2891 - lr: 0.04117 - Train loss: 0.01542 - Test loss: 0.04240\n",
      "Epoch 2892 - lr: 0.04115 - Train loss: 0.01542 - Test loss: 0.04240\n",
      "Epoch 2893 - lr: 0.04114 - Train loss: 0.01542 - Test loss: 0.04240\n",
      "Epoch 2894 - lr: 0.04113 - Train loss: 0.01541 - Test loss: 0.04240\n",
      "Epoch 2895 - lr: 0.04111 - Train loss: 0.01541 - Test loss: 0.04239\n",
      "Epoch 2896 - lr: 0.04110 - Train loss: 0.01541 - Test loss: 0.04239\n",
      "Epoch 2897 - lr: 0.04109 - Train loss: 0.01540 - Test loss: 0.04239\n",
      "Epoch 2898 - lr: 0.04108 - Train loss: 0.01540 - Test loss: 0.04238\n",
      "Epoch 2899 - lr: 0.04106 - Train loss: 0.01540 - Test loss: 0.04238\n",
      "Epoch 2900 - lr: 0.04105 - Train loss: 0.01539 - Test loss: 0.04238\n",
      "Epoch 2901 - lr: 0.04104 - Train loss: 0.01539 - Test loss: 0.04238\n",
      "Epoch 2902 - lr: 0.04103 - Train loss: 0.01539 - Test loss: 0.04237\n",
      "Epoch 2903 - lr: 0.04101 - Train loss: 0.01538 - Test loss: 0.04237\n",
      "Epoch 2904 - lr: 0.04100 - Train loss: 0.01538 - Test loss: 0.04237\n",
      "Epoch 2905 - lr: 0.04099 - Train loss: 0.01537 - Test loss: 0.04237\n",
      "Epoch 2906 - lr: 0.04098 - Train loss: 0.01537 - Test loss: 0.04236\n",
      "Epoch 2907 - lr: 0.04096 - Train loss: 0.01537 - Test loss: 0.04236\n",
      "Epoch 2908 - lr: 0.04095 - Train loss: 0.01536 - Test loss: 0.04236\n",
      "Epoch 2909 - lr: 0.04094 - Train loss: 0.01536 - Test loss: 0.04235\n",
      "Epoch 2910 - lr: 0.04093 - Train loss: 0.01536 - Test loss: 0.04235\n",
      "Epoch 2911 - lr: 0.04091 - Train loss: 0.01535 - Test loss: 0.04235\n",
      "Epoch 2912 - lr: 0.04090 - Train loss: 0.01535 - Test loss: 0.04235\n",
      "Epoch 2913 - lr: 0.04089 - Train loss: 0.01535 - Test loss: 0.04234\n",
      "Epoch 2914 - lr: 0.04088 - Train loss: 0.01534 - Test loss: 0.04234\n",
      "Epoch 2915 - lr: 0.04086 - Train loss: 0.01534 - Test loss: 0.04234\n",
      "Epoch 2916 - lr: 0.04085 - Train loss: 0.01534 - Test loss: 0.04234\n",
      "Epoch 2917 - lr: 0.04084 - Train loss: 0.01533 - Test loss: 0.04233\n",
      "Epoch 2918 - lr: 0.04083 - Train loss: 0.01533 - Test loss: 0.04233\n",
      "Epoch 2919 - lr: 0.04081 - Train loss: 0.01533 - Test loss: 0.04233\n",
      "Epoch 2920 - lr: 0.04080 - Train loss: 0.01532 - Test loss: 0.04233\n",
      "Epoch 2921 - lr: 0.04079 - Train loss: 0.01532 - Test loss: 0.04232\n",
      "Epoch 2922 - lr: 0.04078 - Train loss: 0.01532 - Test loss: 0.04232\n",
      "Epoch 2923 - lr: 0.04076 - Train loss: 0.01531 - Test loss: 0.04232\n",
      "Epoch 2924 - lr: 0.04075 - Train loss: 0.01531 - Test loss: 0.04231\n",
      "Epoch 2925 - lr: 0.04074 - Train loss: 0.01531 - Test loss: 0.04231\n",
      "Epoch 2926 - lr: 0.04073 - Train loss: 0.01530 - Test loss: 0.04231\n",
      "Epoch 2927 - lr: 0.04071 - Train loss: 0.01530 - Test loss: 0.04231\n",
      "Epoch 2928 - lr: 0.04070 - Train loss: 0.01530 - Test loss: 0.04230\n",
      "Epoch 2929 - lr: 0.04069 - Train loss: 0.01529 - Test loss: 0.04230\n",
      "Epoch 2930 - lr: 0.04068 - Train loss: 0.01529 - Test loss: 0.04230\n",
      "Epoch 2931 - lr: 0.04066 - Train loss: 0.01529 - Test loss: 0.04230\n",
      "Epoch 2932 - lr: 0.04065 - Train loss: 0.01528 - Test loss: 0.04229\n",
      "Epoch 2933 - lr: 0.04064 - Train loss: 0.01528 - Test loss: 0.04229\n",
      "Epoch 2934 - lr: 0.04063 - Train loss: 0.01528 - Test loss: 0.04229\n",
      "Epoch 2935 - lr: 0.04061 - Train loss: 0.01527 - Test loss: 0.04229\n",
      "Epoch 2936 - lr: 0.04060 - Train loss: 0.01527 - Test loss: 0.04228\n",
      "Epoch 2937 - lr: 0.04059 - Train loss: 0.01527 - Test loss: 0.04228\n",
      "Epoch 2938 - lr: 0.04058 - Train loss: 0.01526 - Test loss: 0.04228\n",
      "Epoch 2939 - lr: 0.04056 - Train loss: 0.01526 - Test loss: 0.04228\n",
      "Epoch 2940 - lr: 0.04055 - Train loss: 0.01526 - Test loss: 0.04227\n",
      "Epoch 2941 - lr: 0.04054 - Train loss: 0.01525 - Test loss: 0.04227\n",
      "Epoch 2942 - lr: 0.04053 - Train loss: 0.01525 - Test loss: 0.04227\n",
      "Epoch 2943 - lr: 0.04051 - Train loss: 0.01525 - Test loss: 0.04227\n",
      "Epoch 2944 - lr: 0.04050 - Train loss: 0.01524 - Test loss: 0.04226\n",
      "Epoch 2945 - lr: 0.04049 - Train loss: 0.01524 - Test loss: 0.04226\n",
      "Epoch 2946 - lr: 0.04048 - Train loss: 0.01524 - Test loss: 0.04226\n",
      "Epoch 2947 - lr: 0.04046 - Train loss: 0.01523 - Test loss: 0.04226\n",
      "Epoch 2948 - lr: 0.04045 - Train loss: 0.01523 - Test loss: 0.04225\n",
      "Epoch 2949 - lr: 0.04044 - Train loss: 0.01523 - Test loss: 0.04225\n",
      "Epoch 2950 - lr: 0.04043 - Train loss: 0.01522 - Test loss: 0.04225\n",
      "Epoch 2951 - lr: 0.04041 - Train loss: 0.01522 - Test loss: 0.04225\n",
      "Epoch 2952 - lr: 0.04040 - Train loss: 0.01522 - Test loss: 0.04224\n",
      "Epoch 2953 - lr: 0.04039 - Train loss: 0.01521 - Test loss: 0.04224\n",
      "Epoch 2954 - lr: 0.04038 - Train loss: 0.01521 - Test loss: 0.04224\n",
      "Epoch 2955 - lr: 0.04036 - Train loss: 0.01521 - Test loss: 0.04224\n",
      "Epoch 2956 - lr: 0.04035 - Train loss: 0.01520 - Test loss: 0.04223\n",
      "Epoch 2957 - lr: 0.04034 - Train loss: 0.01520 - Test loss: 0.04223\n",
      "Epoch 2958 - lr: 0.04033 - Train loss: 0.01520 - Test loss: 0.04223\n",
      "Epoch 2959 - lr: 0.04032 - Train loss: 0.01519 - Test loss: 0.04223\n",
      "Epoch 2960 - lr: 0.04030 - Train loss: 0.01519 - Test loss: 0.04222\n",
      "Epoch 2961 - lr: 0.04029 - Train loss: 0.01519 - Test loss: 0.04222\n",
      "Epoch 2962 - lr: 0.04028 - Train loss: 0.01518 - Test loss: 0.04222\n",
      "Epoch 2963 - lr: 0.04027 - Train loss: 0.01518 - Test loss: 0.04222\n",
      "Epoch 2964 - lr: 0.04025 - Train loss: 0.01518 - Test loss: 0.04221\n",
      "Epoch 2965 - lr: 0.04024 - Train loss: 0.01517 - Test loss: 0.04221\n",
      "Epoch 2966 - lr: 0.04023 - Train loss: 0.01517 - Test loss: 0.04221\n",
      "Epoch 2967 - lr: 0.04022 - Train loss: 0.01517 - Test loss: 0.04221\n",
      "Epoch 2968 - lr: 0.04020 - Train loss: 0.01517 - Test loss: 0.04220\n",
      "Epoch 2969 - lr: 0.04019 - Train loss: 0.01516 - Test loss: 0.04220\n",
      "Epoch 2970 - lr: 0.04018 - Train loss: 0.01516 - Test loss: 0.04220\n",
      "Epoch 2971 - lr: 0.04017 - Train loss: 0.01516 - Test loss: 0.04220\n",
      "Epoch 2972 - lr: 0.04015 - Train loss: 0.01515 - Test loss: 0.04219\n",
      "Epoch 2973 - lr: 0.04014 - Train loss: 0.01515 - Test loss: 0.04219\n",
      "Epoch 2974 - lr: 0.04013 - Train loss: 0.01515 - Test loss: 0.04219\n",
      "Epoch 2975 - lr: 0.04012 - Train loss: 0.01514 - Test loss: 0.04219\n",
      "Epoch 2976 - lr: 0.04011 - Train loss: 0.01514 - Test loss: 0.04218\n",
      "Epoch 2977 - lr: 0.04009 - Train loss: 0.01514 - Test loss: 0.04218\n",
      "Epoch 2978 - lr: 0.04008 - Train loss: 0.01513 - Test loss: 0.04218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2979 - lr: 0.04007 - Train loss: 0.01513 - Test loss: 0.04218\n",
      "Epoch 2980 - lr: 0.04006 - Train loss: 0.01513 - Test loss: 0.04217\n",
      "Epoch 2981 - lr: 0.04004 - Train loss: 0.01512 - Test loss: 0.04217\n",
      "Epoch 2982 - lr: 0.04003 - Train loss: 0.01512 - Test loss: 0.04217\n",
      "Epoch 2983 - lr: 0.04002 - Train loss: 0.01512 - Test loss: 0.04217\n",
      "Epoch 2984 - lr: 0.04001 - Train loss: 0.01511 - Test loss: 0.04216\n",
      "Epoch 2985 - lr: 0.03999 - Train loss: 0.01511 - Test loss: 0.04216\n",
      "Epoch 2986 - lr: 0.03998 - Train loss: 0.01511 - Test loss: 0.04216\n",
      "Epoch 2987 - lr: 0.03997 - Train loss: 0.01510 - Test loss: 0.04216\n",
      "Epoch 2988 - lr: 0.03996 - Train loss: 0.01510 - Test loss: 0.04215\n",
      "Epoch 2989 - lr: 0.03995 - Train loss: 0.01510 - Test loss: 0.04215\n",
      "Epoch 2990 - lr: 0.03993 - Train loss: 0.01509 - Test loss: 0.04215\n",
      "Epoch 2991 - lr: 0.03992 - Train loss: 0.01509 - Test loss: 0.04215\n",
      "Epoch 2992 - lr: 0.03991 - Train loss: 0.01509 - Test loss: 0.04214\n",
      "Epoch 2993 - lr: 0.03990 - Train loss: 0.01508 - Test loss: 0.04214\n",
      "Epoch 2994 - lr: 0.03988 - Train loss: 0.01508 - Test loss: 0.04214\n",
      "Epoch 2995 - lr: 0.03987 - Train loss: 0.01508 - Test loss: 0.04214\n",
      "Epoch 2996 - lr: 0.03986 - Train loss: 0.01508 - Test loss: 0.04214\n",
      "Epoch 2997 - lr: 0.03985 - Train loss: 0.01507 - Test loss: 0.04213\n",
      "Epoch 2998 - lr: 0.03984 - Train loss: 0.01507 - Test loss: 0.04213\n",
      "Epoch 2999 - lr: 0.03982 - Train loss: 0.01507 - Test loss: 0.04213\n",
      "Epoch 3000 - lr: 0.03981 - Train loss: 0.01506 - Test loss: 0.04213\n",
      "Epoch 3001 - lr: 0.03980 - Train loss: 0.01506 - Test loss: 0.04212\n",
      "Epoch 3002 - lr: 0.03979 - Train loss: 0.01506 - Test loss: 0.04212\n",
      "Epoch 3003 - lr: 0.03977 - Train loss: 0.01505 - Test loss: 0.04212\n",
      "Epoch 3004 - lr: 0.03976 - Train loss: 0.01505 - Test loss: 0.04212\n",
      "Epoch 3005 - lr: 0.03975 - Train loss: 0.01505 - Test loss: 0.04211\n",
      "Epoch 3006 - lr: 0.03974 - Train loss: 0.01504 - Test loss: 0.04211\n",
      "Epoch 3007 - lr: 0.03973 - Train loss: 0.01504 - Test loss: 0.04211\n",
      "Epoch 3008 - lr: 0.03971 - Train loss: 0.01504 - Test loss: 0.04211\n",
      "Epoch 3009 - lr: 0.03970 - Train loss: 0.01503 - Test loss: 0.04210\n",
      "Epoch 3010 - lr: 0.03969 - Train loss: 0.01503 - Test loss: 0.04210\n",
      "Epoch 3011 - lr: 0.03968 - Train loss: 0.01503 - Test loss: 0.04210\n",
      "Epoch 3012 - lr: 0.03966 - Train loss: 0.01502 - Test loss: 0.04210\n",
      "Epoch 3013 - lr: 0.03965 - Train loss: 0.01502 - Test loss: 0.04210\n",
      "Epoch 3014 - lr: 0.03964 - Train loss: 0.01502 - Test loss: 0.04209\n",
      "Epoch 3015 - lr: 0.03963 - Train loss: 0.01502 - Test loss: 0.04209\n",
      "Epoch 3016 - lr: 0.03962 - Train loss: 0.01501 - Test loss: 0.04209\n",
      "Epoch 3017 - lr: 0.03960 - Train loss: 0.01501 - Test loss: 0.04209\n",
      "Epoch 3018 - lr: 0.03959 - Train loss: 0.01501 - Test loss: 0.04208\n",
      "Epoch 3019 - lr: 0.03958 - Train loss: 0.01500 - Test loss: 0.04208\n",
      "Epoch 3020 - lr: 0.03957 - Train loss: 0.01500 - Test loss: 0.04208\n",
      "Epoch 3021 - lr: 0.03955 - Train loss: 0.01500 - Test loss: 0.04208\n",
      "Epoch 3022 - lr: 0.03954 - Train loss: 0.01499 - Test loss: 0.04208\n",
      "Epoch 3023 - lr: 0.03953 - Train loss: 0.01499 - Test loss: 0.04207\n",
      "Epoch 3024 - lr: 0.03952 - Train loss: 0.01499 - Test loss: 0.04207\n",
      "Epoch 3025 - lr: 0.03951 - Train loss: 0.01498 - Test loss: 0.04207\n",
      "Epoch 3026 - lr: 0.03949 - Train loss: 0.01498 - Test loss: 0.04207\n",
      "Epoch 3027 - lr: 0.03948 - Train loss: 0.01498 - Test loss: 0.04206\n",
      "Epoch 3028 - lr: 0.03947 - Train loss: 0.01497 - Test loss: 0.04206\n",
      "Epoch 3029 - lr: 0.03946 - Train loss: 0.01497 - Test loss: 0.04206\n",
      "Epoch 3030 - lr: 0.03945 - Train loss: 0.01497 - Test loss: 0.04206\n",
      "Epoch 3031 - lr: 0.03943 - Train loss: 0.01497 - Test loss: 0.04205\n",
      "Epoch 3032 - lr: 0.03942 - Train loss: 0.01496 - Test loss: 0.04205\n",
      "Epoch 3033 - lr: 0.03941 - Train loss: 0.01496 - Test loss: 0.04205\n",
      "Epoch 3034 - lr: 0.03940 - Train loss: 0.01496 - Test loss: 0.04205\n",
      "Epoch 3035 - lr: 0.03939 - Train loss: 0.01495 - Test loss: 0.04205\n",
      "Epoch 3036 - lr: 0.03937 - Train loss: 0.01495 - Test loss: 0.04204\n",
      "Epoch 3037 - lr: 0.03936 - Train loss: 0.01495 - Test loss: 0.04204\n",
      "Epoch 3038 - lr: 0.03935 - Train loss: 0.01494 - Test loss: 0.04204\n",
      "Epoch 3039 - lr: 0.03934 - Train loss: 0.01494 - Test loss: 0.04204\n",
      "Epoch 3040 - lr: 0.03932 - Train loss: 0.01494 - Test loss: 0.04203\n",
      "Epoch 3041 - lr: 0.03931 - Train loss: 0.01493 - Test loss: 0.04203\n",
      "Epoch 3042 - lr: 0.03930 - Train loss: 0.01493 - Test loss: 0.04203\n",
      "Epoch 3043 - lr: 0.03929 - Train loss: 0.01493 - Test loss: 0.04203\n",
      "Epoch 3044 - lr: 0.03928 - Train loss: 0.01493 - Test loss: 0.04203\n",
      "Epoch 3045 - lr: 0.03926 - Train loss: 0.01492 - Test loss: 0.04202\n",
      "Epoch 3046 - lr: 0.03925 - Train loss: 0.01492 - Test loss: 0.04202\n",
      "Epoch 3047 - lr: 0.03924 - Train loss: 0.01492 - Test loss: 0.04202\n",
      "Epoch 3048 - lr: 0.03923 - Train loss: 0.01491 - Test loss: 0.04202\n",
      "Epoch 3049 - lr: 0.03922 - Train loss: 0.01491 - Test loss: 0.04201\n",
      "Epoch 3050 - lr: 0.03920 - Train loss: 0.01491 - Test loss: 0.04201\n",
      "Epoch 3051 - lr: 0.03919 - Train loss: 0.01490 - Test loss: 0.04201\n",
      "Epoch 3052 - lr: 0.03918 - Train loss: 0.01490 - Test loss: 0.04201\n",
      "Epoch 3053 - lr: 0.03917 - Train loss: 0.01490 - Test loss: 0.04201\n",
      "Epoch 3054 - lr: 0.03916 - Train loss: 0.01489 - Test loss: 0.04200\n",
      "Epoch 3055 - lr: 0.03914 - Train loss: 0.01489 - Test loss: 0.04200\n",
      "Epoch 3056 - lr: 0.03913 - Train loss: 0.01489 - Test loss: 0.04200\n",
      "Epoch 3057 - lr: 0.03912 - Train loss: 0.01489 - Test loss: 0.04200\n",
      "Epoch 3058 - lr: 0.03911 - Train loss: 0.01488 - Test loss: 0.04200\n",
      "Epoch 3059 - lr: 0.03910 - Train loss: 0.01488 - Test loss: 0.04199\n",
      "Epoch 3060 - lr: 0.03908 - Train loss: 0.01488 - Test loss: 0.04199\n",
      "Epoch 3061 - lr: 0.03907 - Train loss: 0.01487 - Test loss: 0.04199\n",
      "Epoch 3062 - lr: 0.03906 - Train loss: 0.01487 - Test loss: 0.04199\n",
      "Epoch 3063 - lr: 0.03905 - Train loss: 0.01487 - Test loss: 0.04198\n",
      "Epoch 3064 - lr: 0.03904 - Train loss: 0.01486 - Test loss: 0.04198\n",
      "Epoch 3065 - lr: 0.03902 - Train loss: 0.01486 - Test loss: 0.04198\n",
      "Epoch 3066 - lr: 0.03901 - Train loss: 0.01486 - Test loss: 0.04198\n",
      "Epoch 3067 - lr: 0.03900 - Train loss: 0.01486 - Test loss: 0.04198\n",
      "Epoch 3068 - lr: 0.03899 - Train loss: 0.01485 - Test loss: 0.04197\n",
      "Epoch 3069 - lr: 0.03898 - Train loss: 0.01485 - Test loss: 0.04197\n",
      "Epoch 3070 - lr: 0.03896 - Train loss: 0.01485 - Test loss: 0.04197\n",
      "Epoch 3071 - lr: 0.03895 - Train loss: 0.01484 - Test loss: 0.04197\n",
      "Epoch 3072 - lr: 0.03894 - Train loss: 0.01484 - Test loss: 0.04197\n",
      "Epoch 3073 - lr: 0.03893 - Train loss: 0.01484 - Test loss: 0.04196\n",
      "Epoch 3074 - lr: 0.03892 - Train loss: 0.01483 - Test loss: 0.04196\n",
      "Epoch 3075 - lr: 0.03890 - Train loss: 0.01483 - Test loss: 0.04196\n",
      "Epoch 3076 - lr: 0.03889 - Train loss: 0.01483 - Test loss: 0.04196\n",
      "Epoch 3077 - lr: 0.03888 - Train loss: 0.01483 - Test loss: 0.04196\n",
      "Epoch 3078 - lr: 0.03887 - Train loss: 0.01482 - Test loss: 0.04195\n",
      "Epoch 3079 - lr: 0.03886 - Train loss: 0.01482 - Test loss: 0.04195\n",
      "Epoch 3080 - lr: 0.03884 - Train loss: 0.01482 - Test loss: 0.04195\n",
      "Epoch 3081 - lr: 0.03883 - Train loss: 0.01481 - Test loss: 0.04195\n",
      "Epoch 3082 - lr: 0.03882 - Train loss: 0.01481 - Test loss: 0.04194\n",
      "Epoch 3083 - lr: 0.03881 - Train loss: 0.01481 - Test loss: 0.04194\n",
      "Epoch 3084 - lr: 0.03880 - Train loss: 0.01480 - Test loss: 0.04194\n",
      "Epoch 3085 - lr: 0.03879 - Train loss: 0.01480 - Test loss: 0.04194\n",
      "Epoch 3086 - lr: 0.03877 - Train loss: 0.01480 - Test loss: 0.04194\n",
      "Epoch 3087 - lr: 0.03876 - Train loss: 0.01480 - Test loss: 0.04193\n",
      "Epoch 3088 - lr: 0.03875 - Train loss: 0.01479 - Test loss: 0.04193\n",
      "Epoch 3089 - lr: 0.03874 - Train loss: 0.01479 - Test loss: 0.04193\n",
      "Epoch 3090 - lr: 0.03873 - Train loss: 0.01479 - Test loss: 0.04193\n",
      "Epoch 3091 - lr: 0.03871 - Train loss: 0.01478 - Test loss: 0.04193\n",
      "Epoch 3092 - lr: 0.03870 - Train loss: 0.01478 - Test loss: 0.04192\n",
      "Epoch 3093 - lr: 0.03869 - Train loss: 0.01478 - Test loss: 0.04192\n",
      "Epoch 3094 - lr: 0.03868 - Train loss: 0.01477 - Test loss: 0.04192\n",
      "Epoch 3095 - lr: 0.03867 - Train loss: 0.01477 - Test loss: 0.04192\n",
      "Epoch 3096 - lr: 0.03865 - Train loss: 0.01477 - Test loss: 0.04192\n",
      "Epoch 3097 - lr: 0.03864 - Train loss: 0.01477 - Test loss: 0.04191\n",
      "Epoch 3098 - lr: 0.03863 - Train loss: 0.01476 - Test loss: 0.04191\n",
      "Epoch 3099 - lr: 0.03862 - Train loss: 0.01476 - Test loss: 0.04191\n",
      "Epoch 3100 - lr: 0.03861 - Train loss: 0.01476 - Test loss: 0.04191\n",
      "Epoch 3101 - lr: 0.03860 - Train loss: 0.01475 - Test loss: 0.04191\n",
      "Epoch 3102 - lr: 0.03858 - Train loss: 0.01475 - Test loss: 0.04190\n",
      "Epoch 3103 - lr: 0.03857 - Train loss: 0.01475 - Test loss: 0.04190\n",
      "Epoch 3104 - lr: 0.03856 - Train loss: 0.01474 - Test loss: 0.04190\n",
      "Epoch 3105 - lr: 0.03855 - Train loss: 0.01474 - Test loss: 0.04190\n",
      "Epoch 3106 - lr: 0.03854 - Train loss: 0.01474 - Test loss: 0.04190\n",
      "Epoch 3107 - lr: 0.03852 - Train loss: 0.01474 - Test loss: 0.04189\n",
      "Epoch 3108 - lr: 0.03851 - Train loss: 0.01473 - Test loss: 0.04189\n",
      "Epoch 3109 - lr: 0.03850 - Train loss: 0.01473 - Test loss: 0.04189\n",
      "Epoch 3110 - lr: 0.03849 - Train loss: 0.01473 - Test loss: 0.04189\n",
      "Epoch 3111 - lr: 0.03848 - Train loss: 0.01472 - Test loss: 0.04189\n",
      "Epoch 3112 - lr: 0.03847 - Train loss: 0.01472 - Test loss: 0.04188\n",
      "Epoch 3113 - lr: 0.03845 - Train loss: 0.01472 - Test loss: 0.04188\n",
      "Epoch 3114 - lr: 0.03844 - Train loss: 0.01472 - Test loss: 0.04188\n",
      "Epoch 3115 - lr: 0.03843 - Train loss: 0.01471 - Test loss: 0.04188\n",
      "Epoch 3116 - lr: 0.03842 - Train loss: 0.01471 - Test loss: 0.04188\n",
      "Epoch 3117 - lr: 0.03841 - Train loss: 0.01471 - Test loss: 0.04187\n",
      "Epoch 3118 - lr: 0.03839 - Train loss: 0.01470 - Test loss: 0.04187\n",
      "Epoch 3119 - lr: 0.03838 - Train loss: 0.01470 - Test loss: 0.04187\n",
      "Epoch 3120 - lr: 0.03837 - Train loss: 0.01470 - Test loss: 0.04187\n",
      "Epoch 3121 - lr: 0.03836 - Train loss: 0.01469 - Test loss: 0.04187\n",
      "Epoch 3122 - lr: 0.03835 - Train loss: 0.01469 - Test loss: 0.04186\n",
      "Epoch 3123 - lr: 0.03834 - Train loss: 0.01469 - Test loss: 0.04186\n",
      "Epoch 3124 - lr: 0.03832 - Train loss: 0.01469 - Test loss: 0.04186\n",
      "Epoch 3125 - lr: 0.03831 - Train loss: 0.01468 - Test loss: 0.04186\n",
      "Epoch 3126 - lr: 0.03830 - Train loss: 0.01468 - Test loss: 0.04186\n",
      "Epoch 3127 - lr: 0.03829 - Train loss: 0.01468 - Test loss: 0.04185\n",
      "Epoch 3128 - lr: 0.03828 - Train loss: 0.01467 - Test loss: 0.04185\n",
      "Epoch 3129 - lr: 0.03826 - Train loss: 0.01467 - Test loss: 0.04185\n",
      "Epoch 3130 - lr: 0.03825 - Train loss: 0.01467 - Test loss: 0.04185\n",
      "Epoch 3131 - lr: 0.03824 - Train loss: 0.01467 - Test loss: 0.04185\n",
      "Epoch 3132 - lr: 0.03823 - Train loss: 0.01466 - Test loss: 0.04184\n",
      "Epoch 3133 - lr: 0.03822 - Train loss: 0.01466 - Test loss: 0.04184\n",
      "Epoch 3134 - lr: 0.03821 - Train loss: 0.01466 - Test loss: 0.04184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3135 - lr: 0.03819 - Train loss: 0.01465 - Test loss: 0.04184\n",
      "Epoch 3136 - lr: 0.03818 - Train loss: 0.01465 - Test loss: 0.04184\n",
      "Epoch 3137 - lr: 0.03817 - Train loss: 0.01465 - Test loss: 0.04183\n",
      "Epoch 3138 - lr: 0.03816 - Train loss: 0.01465 - Test loss: 0.04183\n",
      "Epoch 3139 - lr: 0.03815 - Train loss: 0.01464 - Test loss: 0.04183\n",
      "Epoch 3140 - lr: 0.03814 - Train loss: 0.01464 - Test loss: 0.04183\n",
      "Epoch 3141 - lr: 0.03812 - Train loss: 0.01464 - Test loss: 0.04183\n",
      "Epoch 3142 - lr: 0.03811 - Train loss: 0.01463 - Test loss: 0.04183\n",
      "Epoch 3143 - lr: 0.03810 - Train loss: 0.01463 - Test loss: 0.04182\n",
      "Epoch 3144 - lr: 0.03809 - Train loss: 0.01463 - Test loss: 0.04182\n",
      "Epoch 3145 - lr: 0.03808 - Train loss: 0.01463 - Test loss: 0.04182\n",
      "Epoch 3146 - lr: 0.03807 - Train loss: 0.01462 - Test loss: 0.04182\n",
      "Epoch 3147 - lr: 0.03805 - Train loss: 0.01462 - Test loss: 0.04182\n",
      "Epoch 3148 - lr: 0.03804 - Train loss: 0.01462 - Test loss: 0.04181\n",
      "Epoch 3149 - lr: 0.03803 - Train loss: 0.01461 - Test loss: 0.04181\n",
      "Epoch 3150 - lr: 0.03802 - Train loss: 0.01461 - Test loss: 0.04181\n",
      "Epoch 3151 - lr: 0.03801 - Train loss: 0.01461 - Test loss: 0.04181\n",
      "Epoch 3152 - lr: 0.03800 - Train loss: 0.01461 - Test loss: 0.04181\n",
      "Epoch 3153 - lr: 0.03798 - Train loss: 0.01460 - Test loss: 0.04180\n",
      "Epoch 3154 - lr: 0.03797 - Train loss: 0.01460 - Test loss: 0.04180\n",
      "Epoch 3155 - lr: 0.03796 - Train loss: 0.01460 - Test loss: 0.04180\n",
      "Epoch 3156 - lr: 0.03795 - Train loss: 0.01459 - Test loss: 0.04180\n",
      "Epoch 3157 - lr: 0.03794 - Train loss: 0.01459 - Test loss: 0.04180\n",
      "Epoch 3158 - lr: 0.03793 - Train loss: 0.01459 - Test loss: 0.04180\n",
      "Epoch 3159 - lr: 0.03791 - Train loss: 0.01459 - Test loss: 0.04179\n",
      "Epoch 3160 - lr: 0.03790 - Train loss: 0.01458 - Test loss: 0.04179\n",
      "Epoch 3161 - lr: 0.03789 - Train loss: 0.01458 - Test loss: 0.04179\n",
      "Epoch 3162 - lr: 0.03788 - Train loss: 0.01458 - Test loss: 0.04179\n",
      "Epoch 3163 - lr: 0.03787 - Train loss: 0.01457 - Test loss: 0.04179\n",
      "Epoch 3164 - lr: 0.03786 - Train loss: 0.01457 - Test loss: 0.04178\n",
      "Epoch 3165 - lr: 0.03784 - Train loss: 0.01457 - Test loss: 0.04178\n",
      "Epoch 3166 - lr: 0.03783 - Train loss: 0.01457 - Test loss: 0.04178\n",
      "Epoch 3167 - lr: 0.03782 - Train loss: 0.01456 - Test loss: 0.04178\n",
      "Epoch 3168 - lr: 0.03781 - Train loss: 0.01456 - Test loss: 0.04178\n",
      "Epoch 3169 - lr: 0.03780 - Train loss: 0.01456 - Test loss: 0.04177\n",
      "Epoch 3170 - lr: 0.03779 - Train loss: 0.01455 - Test loss: 0.04177\n",
      "Epoch 3171 - lr: 0.03777 - Train loss: 0.01455 - Test loss: 0.04177\n",
      "Epoch 3172 - lr: 0.03776 - Train loss: 0.01455 - Test loss: 0.04177\n",
      "Epoch 3173 - lr: 0.03775 - Train loss: 0.01455 - Test loss: 0.04177\n",
      "Epoch 3174 - lr: 0.03774 - Train loss: 0.01454 - Test loss: 0.04177\n",
      "Epoch 3175 - lr: 0.03773 - Train loss: 0.01454 - Test loss: 0.04176\n",
      "Epoch 3176 - lr: 0.03772 - Train loss: 0.01454 - Test loss: 0.04176\n",
      "Epoch 3177 - lr: 0.03771 - Train loss: 0.01453 - Test loss: 0.04176\n",
      "Epoch 3178 - lr: 0.03769 - Train loss: 0.01453 - Test loss: 0.04176\n",
      "Epoch 3179 - lr: 0.03768 - Train loss: 0.01453 - Test loss: 0.04176\n",
      "Epoch 3180 - lr: 0.03767 - Train loss: 0.01453 - Test loss: 0.04175\n",
      "Epoch 3181 - lr: 0.03766 - Train loss: 0.01452 - Test loss: 0.04175\n",
      "Epoch 3182 - lr: 0.03765 - Train loss: 0.01452 - Test loss: 0.04175\n",
      "Epoch 3183 - lr: 0.03764 - Train loss: 0.01452 - Test loss: 0.04175\n",
      "Epoch 3184 - lr: 0.03762 - Train loss: 0.01451 - Test loss: 0.04175\n",
      "Epoch 3185 - lr: 0.03761 - Train loss: 0.01451 - Test loss: 0.04175\n",
      "Epoch 3186 - lr: 0.03760 - Train loss: 0.01451 - Test loss: 0.04174\n",
      "Epoch 3187 - lr: 0.03759 - Train loss: 0.01451 - Test loss: 0.04174\n",
      "Epoch 3188 - lr: 0.03758 - Train loss: 0.01450 - Test loss: 0.04174\n",
      "Epoch 3189 - lr: 0.03757 - Train loss: 0.01450 - Test loss: 0.04174\n",
      "Epoch 3190 - lr: 0.03755 - Train loss: 0.01450 - Test loss: 0.04174\n",
      "Epoch 3191 - lr: 0.03754 - Train loss: 0.01450 - Test loss: 0.04173\n",
      "Epoch 3192 - lr: 0.03753 - Train loss: 0.01449 - Test loss: 0.04173\n",
      "Epoch 3193 - lr: 0.03752 - Train loss: 0.01449 - Test loss: 0.04173\n",
      "Epoch 3194 - lr: 0.03751 - Train loss: 0.01449 - Test loss: 0.04173\n",
      "Epoch 3195 - lr: 0.03750 - Train loss: 0.01448 - Test loss: 0.04173\n",
      "Epoch 3196 - lr: 0.03749 - Train loss: 0.01448 - Test loss: 0.04173\n",
      "Epoch 3197 - lr: 0.03747 - Train loss: 0.01448 - Test loss: 0.04172\n",
      "Epoch 3198 - lr: 0.03746 - Train loss: 0.01448 - Test loss: 0.04172\n",
      "Epoch 3199 - lr: 0.03745 - Train loss: 0.01447 - Test loss: 0.04172\n",
      "Epoch 3200 - lr: 0.03744 - Train loss: 0.01447 - Test loss: 0.04172\n",
      "Epoch 3201 - lr: 0.03743 - Train loss: 0.01447 - Test loss: 0.04172\n",
      "Epoch 3202 - lr: 0.03742 - Train loss: 0.01446 - Test loss: 0.04172\n",
      "Epoch 3203 - lr: 0.03741 - Train loss: 0.01446 - Test loss: 0.04171\n",
      "Epoch 3204 - lr: 0.03739 - Train loss: 0.01446 - Test loss: 0.04171\n",
      "Epoch 3205 - lr: 0.03738 - Train loss: 0.01446 - Test loss: 0.04171\n",
      "Epoch 3206 - lr: 0.03737 - Train loss: 0.01445 - Test loss: 0.04171\n",
      "Epoch 3207 - lr: 0.03736 - Train loss: 0.01445 - Test loss: 0.04171\n",
      "Epoch 3208 - lr: 0.03735 - Train loss: 0.01445 - Test loss: 0.04170\n",
      "Epoch 3209 - lr: 0.03734 - Train loss: 0.01445 - Test loss: 0.04170\n",
      "Epoch 3210 - lr: 0.03733 - Train loss: 0.01444 - Test loss: 0.04170\n",
      "Epoch 3211 - lr: 0.03731 - Train loss: 0.01444 - Test loss: 0.04170\n",
      "Epoch 3212 - lr: 0.03730 - Train loss: 0.01444 - Test loss: 0.04170\n",
      "Epoch 3213 - lr: 0.03729 - Train loss: 0.01443 - Test loss: 0.04170\n",
      "Epoch 3214 - lr: 0.03728 - Train loss: 0.01443 - Test loss: 0.04169\n",
      "Epoch 3215 - lr: 0.03727 - Train loss: 0.01443 - Test loss: 0.04169\n",
      "Epoch 3216 - lr: 0.03726 - Train loss: 0.01443 - Test loss: 0.04169\n",
      "Epoch 3217 - lr: 0.03724 - Train loss: 0.01442 - Test loss: 0.04169\n",
      "Epoch 3218 - lr: 0.03723 - Train loss: 0.01442 - Test loss: 0.04169\n",
      "Epoch 3219 - lr: 0.03722 - Train loss: 0.01442 - Test loss: 0.04169\n",
      "Epoch 3220 - lr: 0.03721 - Train loss: 0.01442 - Test loss: 0.04168\n",
      "Epoch 3221 - lr: 0.03720 - Train loss: 0.01441 - Test loss: 0.04168\n",
      "Epoch 3222 - lr: 0.03719 - Train loss: 0.01441 - Test loss: 0.04168\n",
      "Epoch 3223 - lr: 0.03718 - Train loss: 0.01441 - Test loss: 0.04168\n",
      "Epoch 3224 - lr: 0.03716 - Train loss: 0.01440 - Test loss: 0.04168\n",
      "Epoch 3225 - lr: 0.03715 - Train loss: 0.01440 - Test loss: 0.04168\n",
      "Epoch 3226 - lr: 0.03714 - Train loss: 0.01440 - Test loss: 0.04167\n",
      "Epoch 3227 - lr: 0.03713 - Train loss: 0.01440 - Test loss: 0.04167\n",
      "Epoch 3228 - lr: 0.03712 - Train loss: 0.01439 - Test loss: 0.04167\n",
      "Epoch 3229 - lr: 0.03711 - Train loss: 0.01439 - Test loss: 0.04167\n",
      "Epoch 3230 - lr: 0.03710 - Train loss: 0.01439 - Test loss: 0.04167\n",
      "Epoch 3231 - lr: 0.03709 - Train loss: 0.01438 - Test loss: 0.04167\n",
      "Epoch 3232 - lr: 0.03707 - Train loss: 0.01438 - Test loss: 0.04166\n",
      "Epoch 3233 - lr: 0.03706 - Train loss: 0.01438 - Test loss: 0.04166\n",
      "Epoch 3234 - lr: 0.03705 - Train loss: 0.01438 - Test loss: 0.04166\n",
      "Epoch 3235 - lr: 0.03704 - Train loss: 0.01437 - Test loss: 0.04166\n",
      "Epoch 3236 - lr: 0.03703 - Train loss: 0.01437 - Test loss: 0.04166\n",
      "Epoch 3237 - lr: 0.03702 - Train loss: 0.01437 - Test loss: 0.04166\n",
      "Epoch 3238 - lr: 0.03701 - Train loss: 0.01437 - Test loss: 0.04165\n",
      "Epoch 3239 - lr: 0.03699 - Train loss: 0.01436 - Test loss: 0.04165\n",
      "Epoch 3240 - lr: 0.03698 - Train loss: 0.01436 - Test loss: 0.04165\n",
      "Epoch 3241 - lr: 0.03697 - Train loss: 0.01436 - Test loss: 0.04165\n",
      "Epoch 3242 - lr: 0.03696 - Train loss: 0.01436 - Test loss: 0.04165\n",
      "Epoch 3243 - lr: 0.03695 - Train loss: 0.01435 - Test loss: 0.04165\n",
      "Epoch 3244 - lr: 0.03694 - Train loss: 0.01435 - Test loss: 0.04164\n",
      "Epoch 3245 - lr: 0.03693 - Train loss: 0.01435 - Test loss: 0.04164\n",
      "Epoch 3246 - lr: 0.03691 - Train loss: 0.01434 - Test loss: 0.04164\n",
      "Epoch 3247 - lr: 0.03690 - Train loss: 0.01434 - Test loss: 0.04164\n",
      "Epoch 3248 - lr: 0.03689 - Train loss: 0.01434 - Test loss: 0.04164\n",
      "Epoch 3249 - lr: 0.03688 - Train loss: 0.01434 - Test loss: 0.04164\n",
      "Epoch 3250 - lr: 0.03687 - Train loss: 0.01433 - Test loss: 0.04163\n",
      "Epoch 3251 - lr: 0.03686 - Train loss: 0.01433 - Test loss: 0.04163\n",
      "Epoch 3252 - lr: 0.03685 - Train loss: 0.01433 - Test loss: 0.04163\n",
      "Epoch 3253 - lr: 0.03684 - Train loss: 0.01433 - Test loss: 0.04163\n",
      "Epoch 3254 - lr: 0.03682 - Train loss: 0.01432 - Test loss: 0.04163\n",
      "Epoch 3255 - lr: 0.03681 - Train loss: 0.01432 - Test loss: 0.04163\n",
      "Epoch 3256 - lr: 0.03680 - Train loss: 0.01432 - Test loss: 0.04162\n",
      "Epoch 3257 - lr: 0.03679 - Train loss: 0.01431 - Test loss: 0.04162\n",
      "Epoch 3258 - lr: 0.03678 - Train loss: 0.01431 - Test loss: 0.04162\n",
      "Epoch 3259 - lr: 0.03677 - Train loss: 0.01431 - Test loss: 0.04162\n",
      "Epoch 3260 - lr: 0.03676 - Train loss: 0.01431 - Test loss: 0.04162\n",
      "Epoch 3261 - lr: 0.03675 - Train loss: 0.01430 - Test loss: 0.04162\n",
      "Epoch 3262 - lr: 0.03673 - Train loss: 0.01430 - Test loss: 0.04161\n",
      "Epoch 3263 - lr: 0.03672 - Train loss: 0.01430 - Test loss: 0.04161\n",
      "Epoch 3264 - lr: 0.03671 - Train loss: 0.01430 - Test loss: 0.04161\n",
      "Epoch 3265 - lr: 0.03670 - Train loss: 0.01429 - Test loss: 0.04161\n",
      "Epoch 3266 - lr: 0.03669 - Train loss: 0.01429 - Test loss: 0.04161\n",
      "Epoch 3267 - lr: 0.03668 - Train loss: 0.01429 - Test loss: 0.04161\n",
      "Epoch 3268 - lr: 0.03667 - Train loss: 0.01429 - Test loss: 0.04160\n",
      "Epoch 3269 - lr: 0.03666 - Train loss: 0.01428 - Test loss: 0.04160\n",
      "Epoch 3270 - lr: 0.03664 - Train loss: 0.01428 - Test loss: 0.04160\n",
      "Epoch 3271 - lr: 0.03663 - Train loss: 0.01428 - Test loss: 0.04160\n",
      "Epoch 3272 - lr: 0.03662 - Train loss: 0.01427 - Test loss: 0.04160\n",
      "Epoch 3273 - lr: 0.03661 - Train loss: 0.01427 - Test loss: 0.04160\n",
      "Epoch 3274 - lr: 0.03660 - Train loss: 0.01427 - Test loss: 0.04159\n",
      "Epoch 3275 - lr: 0.03659 - Train loss: 0.01427 - Test loss: 0.04159\n",
      "Epoch 3276 - lr: 0.03658 - Train loss: 0.01426 - Test loss: 0.04159\n",
      "Epoch 3277 - lr: 0.03657 - Train loss: 0.01426 - Test loss: 0.04159\n",
      "Epoch 3278 - lr: 0.03655 - Train loss: 0.01426 - Test loss: 0.04159\n",
      "Epoch 3279 - lr: 0.03654 - Train loss: 0.01426 - Test loss: 0.04159\n",
      "Epoch 3280 - lr: 0.03653 - Train loss: 0.01425 - Test loss: 0.04159\n",
      "Epoch 3281 - lr: 0.03652 - Train loss: 0.01425 - Test loss: 0.04158\n",
      "Epoch 3282 - lr: 0.03651 - Train loss: 0.01425 - Test loss: 0.04158\n",
      "Epoch 3283 - lr: 0.03650 - Train loss: 0.01425 - Test loss: 0.04158\n",
      "Epoch 3284 - lr: 0.03649 - Train loss: 0.01424 - Test loss: 0.04158\n",
      "Epoch 3285 - lr: 0.03648 - Train loss: 0.01424 - Test loss: 0.04158\n",
      "Epoch 3286 - lr: 0.03646 - Train loss: 0.01424 - Test loss: 0.04158\n",
      "Epoch 3287 - lr: 0.03645 - Train loss: 0.01423 - Test loss: 0.04157\n",
      "Epoch 3288 - lr: 0.03644 - Train loss: 0.01423 - Test loss: 0.04157\n",
      "Epoch 3289 - lr: 0.03643 - Train loss: 0.01423 - Test loss: 0.04157\n",
      "Epoch 3290 - lr: 0.03642 - Train loss: 0.01423 - Test loss: 0.04157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3291 - lr: 0.03641 - Train loss: 0.01422 - Test loss: 0.04157\n",
      "Epoch 3292 - lr: 0.03640 - Train loss: 0.01422 - Test loss: 0.04157\n",
      "Epoch 3293 - lr: 0.03639 - Train loss: 0.01422 - Test loss: 0.04156\n",
      "Epoch 3294 - lr: 0.03637 - Train loss: 0.01422 - Test loss: 0.04156\n",
      "Epoch 3295 - lr: 0.03636 - Train loss: 0.01421 - Test loss: 0.04156\n",
      "Epoch 3296 - lr: 0.03635 - Train loss: 0.01421 - Test loss: 0.04156\n",
      "Epoch 3297 - lr: 0.03634 - Train loss: 0.01421 - Test loss: 0.04156\n",
      "Epoch 3298 - lr: 0.03633 - Train loss: 0.01421 - Test loss: 0.04156\n",
      "Epoch 3299 - lr: 0.03632 - Train loss: 0.01420 - Test loss: 0.04156\n",
      "Epoch 3300 - lr: 0.03631 - Train loss: 0.01420 - Test loss: 0.04155\n",
      "Epoch 3301 - lr: 0.03630 - Train loss: 0.01420 - Test loss: 0.04155\n",
      "Epoch 3302 - lr: 0.03629 - Train loss: 0.01420 - Test loss: 0.04155\n",
      "Epoch 3303 - lr: 0.03627 - Train loss: 0.01419 - Test loss: 0.04155\n",
      "Epoch 3304 - lr: 0.03626 - Train loss: 0.01419 - Test loss: 0.04155\n",
      "Epoch 3305 - lr: 0.03625 - Train loss: 0.01419 - Test loss: 0.04155\n",
      "Epoch 3306 - lr: 0.03624 - Train loss: 0.01418 - Test loss: 0.04154\n",
      "Epoch 3307 - lr: 0.03623 - Train loss: 0.01418 - Test loss: 0.04154\n",
      "Epoch 3308 - lr: 0.03622 - Train loss: 0.01418 - Test loss: 0.04154\n",
      "Epoch 3309 - lr: 0.03621 - Train loss: 0.01418 - Test loss: 0.04154\n",
      "Epoch 3310 - lr: 0.03620 - Train loss: 0.01417 - Test loss: 0.04154\n",
      "Epoch 3311 - lr: 0.03619 - Train loss: 0.01417 - Test loss: 0.04154\n",
      "Epoch 3312 - lr: 0.03617 - Train loss: 0.01417 - Test loss: 0.04154\n",
      "Epoch 3313 - lr: 0.03616 - Train loss: 0.01417 - Test loss: 0.04153\n",
      "Epoch 3314 - lr: 0.03615 - Train loss: 0.01416 - Test loss: 0.04153\n",
      "Epoch 3315 - lr: 0.03614 - Train loss: 0.01416 - Test loss: 0.04153\n",
      "Epoch 3316 - lr: 0.03613 - Train loss: 0.01416 - Test loss: 0.04153\n",
      "Epoch 3317 - lr: 0.03612 - Train loss: 0.01416 - Test loss: 0.04153\n",
      "Epoch 3318 - lr: 0.03611 - Train loss: 0.01415 - Test loss: 0.04153\n",
      "Epoch 3319 - lr: 0.03610 - Train loss: 0.01415 - Test loss: 0.04152\n",
      "Epoch 3320 - lr: 0.03609 - Train loss: 0.01415 - Test loss: 0.04152\n",
      "Epoch 3321 - lr: 0.03607 - Train loss: 0.01415 - Test loss: 0.04152\n",
      "Epoch 3322 - lr: 0.03606 - Train loss: 0.01414 - Test loss: 0.04152\n",
      "Epoch 3323 - lr: 0.03605 - Train loss: 0.01414 - Test loss: 0.04152\n",
      "Epoch 3324 - lr: 0.03604 - Train loss: 0.01414 - Test loss: 0.04152\n",
      "Epoch 3325 - lr: 0.03603 - Train loss: 0.01414 - Test loss: 0.04152\n",
      "Epoch 3326 - lr: 0.03602 - Train loss: 0.01413 - Test loss: 0.04151\n",
      "Epoch 3327 - lr: 0.03601 - Train loss: 0.01413 - Test loss: 0.04151\n",
      "Epoch 3328 - lr: 0.03600 - Train loss: 0.01413 - Test loss: 0.04151\n",
      "Epoch 3329 - lr: 0.03599 - Train loss: 0.01413 - Test loss: 0.04151\n",
      "Epoch 3330 - lr: 0.03597 - Train loss: 0.01412 - Test loss: 0.04151\n",
      "Epoch 3331 - lr: 0.03596 - Train loss: 0.01412 - Test loss: 0.04151\n",
      "Epoch 3332 - lr: 0.03595 - Train loss: 0.01412 - Test loss: 0.04151\n",
      "Epoch 3333 - lr: 0.03594 - Train loss: 0.01411 - Test loss: 0.04150\n",
      "Epoch 3334 - lr: 0.03593 - Train loss: 0.01411 - Test loss: 0.04150\n",
      "Epoch 3335 - lr: 0.03592 - Train loss: 0.01411 - Test loss: 0.04150\n",
      "Epoch 3336 - lr: 0.03591 - Train loss: 0.01411 - Test loss: 0.04150\n",
      "Epoch 3337 - lr: 0.03590 - Train loss: 0.01410 - Test loss: 0.04150\n",
      "Epoch 3338 - lr: 0.03589 - Train loss: 0.01410 - Test loss: 0.04150\n",
      "Epoch 3339 - lr: 0.03588 - Train loss: 0.01410 - Test loss: 0.04150\n",
      "Epoch 3340 - lr: 0.03586 - Train loss: 0.01410 - Test loss: 0.04149\n",
      "Epoch 3341 - lr: 0.03585 - Train loss: 0.01409 - Test loss: 0.04149\n",
      "Epoch 3342 - lr: 0.03584 - Train loss: 0.01409 - Test loss: 0.04149\n",
      "Epoch 3343 - lr: 0.03583 - Train loss: 0.01409 - Test loss: 0.04149\n",
      "Epoch 3344 - lr: 0.03582 - Train loss: 0.01409 - Test loss: 0.04149\n",
      "Epoch 3345 - lr: 0.03581 - Train loss: 0.01408 - Test loss: 0.04149\n",
      "Epoch 3346 - lr: 0.03580 - Train loss: 0.01408 - Test loss: 0.04148\n",
      "Epoch 3347 - lr: 0.03579 - Train loss: 0.01408 - Test loss: 0.04148\n",
      "Epoch 3348 - lr: 0.03578 - Train loss: 0.01408 - Test loss: 0.04148\n",
      "Epoch 3349 - lr: 0.03577 - Train loss: 0.01407 - Test loss: 0.04148\n",
      "Epoch 3350 - lr: 0.03575 - Train loss: 0.01407 - Test loss: 0.04148\n",
      "Epoch 3351 - lr: 0.03574 - Train loss: 0.01407 - Test loss: 0.04148\n",
      "Epoch 3352 - lr: 0.03573 - Train loss: 0.01407 - Test loss: 0.04148\n",
      "Epoch 3353 - lr: 0.03572 - Train loss: 0.01406 - Test loss: 0.04147\n",
      "Epoch 3354 - lr: 0.03571 - Train loss: 0.01406 - Test loss: 0.04147\n",
      "Epoch 3355 - lr: 0.03570 - Train loss: 0.01406 - Test loss: 0.04147\n",
      "Epoch 3356 - lr: 0.03569 - Train loss: 0.01406 - Test loss: 0.04147\n",
      "Epoch 3357 - lr: 0.03568 - Train loss: 0.01405 - Test loss: 0.04147\n",
      "Epoch 3358 - lr: 0.03567 - Train loss: 0.01405 - Test loss: 0.04147\n",
      "Epoch 3359 - lr: 0.03566 - Train loss: 0.01405 - Test loss: 0.04147\n",
      "Epoch 3360 - lr: 0.03565 - Train loss: 0.01405 - Test loss: 0.04146\n",
      "Epoch 3361 - lr: 0.03563 - Train loss: 0.01404 - Test loss: 0.04146\n",
      "Epoch 3362 - lr: 0.03562 - Train loss: 0.01404 - Test loss: 0.04146\n",
      "Epoch 3363 - lr: 0.03561 - Train loss: 0.01404 - Test loss: 0.04146\n",
      "Epoch 3364 - lr: 0.03560 - Train loss: 0.01404 - Test loss: 0.04146\n",
      "Epoch 3365 - lr: 0.03559 - Train loss: 0.01403 - Test loss: 0.04146\n",
      "Epoch 3366 - lr: 0.03558 - Train loss: 0.01403 - Test loss: 0.04146\n",
      "Epoch 3367 - lr: 0.03557 - Train loss: 0.01403 - Test loss: 0.04145\n",
      "Epoch 3368 - lr: 0.03556 - Train loss: 0.01403 - Test loss: 0.04145\n",
      "Epoch 3369 - lr: 0.03555 - Train loss: 0.01402 - Test loss: 0.04145\n",
      "Epoch 3370 - lr: 0.03554 - Train loss: 0.01402 - Test loss: 0.04145\n",
      "Epoch 3371 - lr: 0.03552 - Train loss: 0.01402 - Test loss: 0.04145\n",
      "Epoch 3372 - lr: 0.03551 - Train loss: 0.01402 - Test loss: 0.04145\n",
      "Epoch 3373 - lr: 0.03550 - Train loss: 0.01401 - Test loss: 0.04145\n",
      "Epoch 3374 - lr: 0.03549 - Train loss: 0.01401 - Test loss: 0.04144\n",
      "Epoch 3375 - lr: 0.03548 - Train loss: 0.01401 - Test loss: 0.04144\n",
      "Epoch 3376 - lr: 0.03547 - Train loss: 0.01401 - Test loss: 0.04144\n",
      "Epoch 3377 - lr: 0.03546 - Train loss: 0.01400 - Test loss: 0.04144\n",
      "Epoch 3378 - lr: 0.03545 - Train loss: 0.01400 - Test loss: 0.04144\n",
      "Epoch 3379 - lr: 0.03544 - Train loss: 0.01400 - Test loss: 0.04144\n",
      "Epoch 3380 - lr: 0.03543 - Train loss: 0.01400 - Test loss: 0.04144\n",
      "Epoch 3381 - lr: 0.03542 - Train loss: 0.01399 - Test loss: 0.04144\n",
      "Epoch 3382 - lr: 0.03541 - Train loss: 0.01399 - Test loss: 0.04143\n",
      "Epoch 3383 - lr: 0.03539 - Train loss: 0.01399 - Test loss: 0.04143\n",
      "Epoch 3384 - lr: 0.03538 - Train loss: 0.01399 - Test loss: 0.04143\n",
      "Epoch 3385 - lr: 0.03537 - Train loss: 0.01398 - Test loss: 0.04143\n",
      "Epoch 3386 - lr: 0.03536 - Train loss: 0.01398 - Test loss: 0.04143\n",
      "Epoch 3387 - lr: 0.03535 - Train loss: 0.01398 - Test loss: 0.04143\n",
      "Epoch 3388 - lr: 0.03534 - Train loss: 0.01398 - Test loss: 0.04143\n",
      "Epoch 3389 - lr: 0.03533 - Train loss: 0.01397 - Test loss: 0.04142\n",
      "Epoch 3390 - lr: 0.03532 - Train loss: 0.01397 - Test loss: 0.04142\n",
      "Epoch 3391 - lr: 0.03531 - Train loss: 0.01397 - Test loss: 0.04142\n",
      "Epoch 3392 - lr: 0.03530 - Train loss: 0.01397 - Test loss: 0.04142\n",
      "Epoch 3393 - lr: 0.03529 - Train loss: 0.01396 - Test loss: 0.04142\n",
      "Epoch 3394 - lr: 0.03527 - Train loss: 0.01396 - Test loss: 0.04142\n",
      "Epoch 3395 - lr: 0.03526 - Train loss: 0.01396 - Test loss: 0.04142\n",
      "Epoch 3396 - lr: 0.03525 - Train loss: 0.01396 - Test loss: 0.04141\n",
      "Epoch 3397 - lr: 0.03524 - Train loss: 0.01395 - Test loss: 0.04141\n",
      "Epoch 3398 - lr: 0.03523 - Train loss: 0.01395 - Test loss: 0.04141\n",
      "Epoch 3399 - lr: 0.03522 - Train loss: 0.01395 - Test loss: 0.04141\n",
      "Epoch 3400 - lr: 0.03521 - Train loss: 0.01395 - Test loss: 0.04141\n",
      "Epoch 3401 - lr: 0.03520 - Train loss: 0.01394 - Test loss: 0.04141\n",
      "Epoch 3402 - lr: 0.03519 - Train loss: 0.01394 - Test loss: 0.04141\n",
      "Epoch 3403 - lr: 0.03518 - Train loss: 0.01394 - Test loss: 0.04141\n",
      "Epoch 3404 - lr: 0.03517 - Train loss: 0.01394 - Test loss: 0.04140\n",
      "Epoch 3405 - lr: 0.03516 - Train loss: 0.01393 - Test loss: 0.04140\n",
      "Epoch 3406 - lr: 0.03515 - Train loss: 0.01393 - Test loss: 0.04140\n",
      "Epoch 3407 - lr: 0.03513 - Train loss: 0.01393 - Test loss: 0.04140\n",
      "Epoch 3408 - lr: 0.03512 - Train loss: 0.01393 - Test loss: 0.04140\n",
      "Epoch 3409 - lr: 0.03511 - Train loss: 0.01392 - Test loss: 0.04140\n",
      "Epoch 3410 - lr: 0.03510 - Train loss: 0.01392 - Test loss: 0.04140\n",
      "Epoch 3411 - lr: 0.03509 - Train loss: 0.01392 - Test loss: 0.04139\n",
      "Epoch 3412 - lr: 0.03508 - Train loss: 0.01392 - Test loss: 0.04139\n",
      "Epoch 3413 - lr: 0.03507 - Train loss: 0.01391 - Test loss: 0.04139\n",
      "Epoch 3414 - lr: 0.03506 - Train loss: 0.01391 - Test loss: 0.04139\n",
      "Epoch 3415 - lr: 0.03505 - Train loss: 0.01391 - Test loss: 0.04139\n",
      "Epoch 3416 - lr: 0.03504 - Train loss: 0.01391 - Test loss: 0.04139\n",
      "Epoch 3417 - lr: 0.03503 - Train loss: 0.01390 - Test loss: 0.04139\n",
      "Epoch 3418 - lr: 0.03502 - Train loss: 0.01390 - Test loss: 0.04138\n",
      "Epoch 3419 - lr: 0.03501 - Train loss: 0.01390 - Test loss: 0.04138\n",
      "Epoch 3420 - lr: 0.03499 - Train loss: 0.01390 - Test loss: 0.04138\n",
      "Epoch 3421 - lr: 0.03498 - Train loss: 0.01389 - Test loss: 0.04138\n",
      "Epoch 3422 - lr: 0.03497 - Train loss: 0.01389 - Test loss: 0.04138\n",
      "Epoch 3423 - lr: 0.03496 - Train loss: 0.01389 - Test loss: 0.04138\n",
      "Epoch 3424 - lr: 0.03495 - Train loss: 0.01389 - Test loss: 0.04138\n",
      "Epoch 3425 - lr: 0.03494 - Train loss: 0.01388 - Test loss: 0.04138\n",
      "Epoch 3426 - lr: 0.03493 - Train loss: 0.01388 - Test loss: 0.04137\n",
      "Epoch 3427 - lr: 0.03492 - Train loss: 0.01388 - Test loss: 0.04137\n",
      "Epoch 3428 - lr: 0.03491 - Train loss: 0.01388 - Test loss: 0.04137\n",
      "Epoch 3429 - lr: 0.03490 - Train loss: 0.01387 - Test loss: 0.04137\n",
      "Epoch 3430 - lr: 0.03489 - Train loss: 0.01387 - Test loss: 0.04137\n",
      "Epoch 3431 - lr: 0.03488 - Train loss: 0.01387 - Test loss: 0.04137\n",
      "Epoch 3432 - lr: 0.03487 - Train loss: 0.01387 - Test loss: 0.04137\n",
      "Epoch 3433 - lr: 0.03486 - Train loss: 0.01386 - Test loss: 0.04137\n",
      "Epoch 3434 - lr: 0.03484 - Train loss: 0.01386 - Test loss: 0.04136\n",
      "Epoch 3435 - lr: 0.03483 - Train loss: 0.01386 - Test loss: 0.04136\n",
      "Epoch 3436 - lr: 0.03482 - Train loss: 0.01386 - Test loss: 0.04136\n",
      "Epoch 3437 - lr: 0.03481 - Train loss: 0.01385 - Test loss: 0.04136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3438 - lr: 0.03480 - Train loss: 0.01385 - Test loss: 0.04136\n",
      "Epoch 3439 - lr: 0.03479 - Train loss: 0.01385 - Test loss: 0.04136\n",
      "Epoch 3440 - lr: 0.03478 - Train loss: 0.01385 - Test loss: 0.04136\n",
      "Epoch 3441 - lr: 0.03477 - Train loss: 0.01384 - Test loss: 0.04135\n",
      "Epoch 3442 - lr: 0.03476 - Train loss: 0.01384 - Test loss: 0.04135\n",
      "Epoch 3443 - lr: 0.03475 - Train loss: 0.01384 - Test loss: 0.04135\n",
      "Epoch 3444 - lr: 0.03474 - Train loss: 0.01384 - Test loss: 0.04135\n",
      "Epoch 3445 - lr: 0.03473 - Train loss: 0.01383 - Test loss: 0.04135\n",
      "Epoch 3446 - lr: 0.03472 - Train loss: 0.01383 - Test loss: 0.04135\n",
      "Epoch 3447 - lr: 0.03471 - Train loss: 0.01383 - Test loss: 0.04135\n",
      "Epoch 3448 - lr: 0.03469 - Train loss: 0.01383 - Test loss: 0.04135\n",
      "Epoch 3449 - lr: 0.03468 - Train loss: 0.01383 - Test loss: 0.04134\n",
      "Epoch 3450 - lr: 0.03467 - Train loss: 0.01382 - Test loss: 0.04134\n",
      "Epoch 3451 - lr: 0.03466 - Train loss: 0.01382 - Test loss: 0.04134\n",
      "Epoch 3452 - lr: 0.03465 - Train loss: 0.01382 - Test loss: 0.04134\n",
      "Epoch 3453 - lr: 0.03464 - Train loss: 0.01382 - Test loss: 0.04134\n",
      "Epoch 3454 - lr: 0.03463 - Train loss: 0.01381 - Test loss: 0.04134\n",
      "Epoch 3455 - lr: 0.03462 - Train loss: 0.01381 - Test loss: 0.04134\n",
      "Epoch 3456 - lr: 0.03461 - Train loss: 0.01381 - Test loss: 0.04134\n",
      "Epoch 3457 - lr: 0.03460 - Train loss: 0.01381 - Test loss: 0.04133\n",
      "Epoch 3458 - lr: 0.03459 - Train loss: 0.01380 - Test loss: 0.04133\n",
      "Epoch 3459 - lr: 0.03458 - Train loss: 0.01380 - Test loss: 0.04133\n",
      "Epoch 3460 - lr: 0.03457 - Train loss: 0.01380 - Test loss: 0.04133\n",
      "Epoch 3461 - lr: 0.03456 - Train loss: 0.01380 - Test loss: 0.04133\n",
      "Epoch 3462 - lr: 0.03455 - Train loss: 0.01379 - Test loss: 0.04133\n",
      "Epoch 3463 - lr: 0.03454 - Train loss: 0.01379 - Test loss: 0.04133\n",
      "Epoch 3464 - lr: 0.03452 - Train loss: 0.01379 - Test loss: 0.04133\n",
      "Epoch 3465 - lr: 0.03451 - Train loss: 0.01379 - Test loss: 0.04132\n",
      "Epoch 3466 - lr: 0.03450 - Train loss: 0.01378 - Test loss: 0.04132\n",
      "Epoch 3467 - lr: 0.03449 - Train loss: 0.01378 - Test loss: 0.04132\n",
      "Epoch 3468 - lr: 0.03448 - Train loss: 0.01378 - Test loss: 0.04132\n",
      "Epoch 3469 - lr: 0.03447 - Train loss: 0.01378 - Test loss: 0.04132\n",
      "Epoch 3470 - lr: 0.03446 - Train loss: 0.01377 - Test loss: 0.04132\n",
      "Epoch 3471 - lr: 0.03445 - Train loss: 0.01377 - Test loss: 0.04132\n",
      "Epoch 3472 - lr: 0.03444 - Train loss: 0.01377 - Test loss: 0.04132\n",
      "Epoch 3473 - lr: 0.03443 - Train loss: 0.01377 - Test loss: 0.04131\n",
      "Epoch 3474 - lr: 0.03442 - Train loss: 0.01376 - Test loss: 0.04131\n",
      "Epoch 3475 - lr: 0.03441 - Train loss: 0.01376 - Test loss: 0.04131\n",
      "Epoch 3476 - lr: 0.03440 - Train loss: 0.01376 - Test loss: 0.04131\n",
      "Epoch 3477 - lr: 0.03439 - Train loss: 0.01376 - Test loss: 0.04131\n",
      "Epoch 3478 - lr: 0.03438 - Train loss: 0.01376 - Test loss: 0.04131\n",
      "Epoch 3479 - lr: 0.03437 - Train loss: 0.01375 - Test loss: 0.04131\n",
      "Epoch 3480 - lr: 0.03436 - Train loss: 0.01375 - Test loss: 0.04131\n",
      "Epoch 3481 - lr: 0.03435 - Train loss: 0.01375 - Test loss: 0.04130\n",
      "Epoch 3482 - lr: 0.03433 - Train loss: 0.01375 - Test loss: 0.04130\n",
      "Epoch 3483 - lr: 0.03432 - Train loss: 0.01374 - Test loss: 0.04130\n",
      "Epoch 3484 - lr: 0.03431 - Train loss: 0.01374 - Test loss: 0.04130\n",
      "Epoch 3485 - lr: 0.03430 - Train loss: 0.01374 - Test loss: 0.04130\n",
      "Epoch 3486 - lr: 0.03429 - Train loss: 0.01374 - Test loss: 0.04130\n",
      "Epoch 3487 - lr: 0.03428 - Train loss: 0.01373 - Test loss: 0.04130\n",
      "Epoch 3488 - lr: 0.03427 - Train loss: 0.01373 - Test loss: 0.04130\n",
      "Epoch 3489 - lr: 0.03426 - Train loss: 0.01373 - Test loss: 0.04129\n",
      "Epoch 3490 - lr: 0.03425 - Train loss: 0.01373 - Test loss: 0.04129\n",
      "Epoch 3491 - lr: 0.03424 - Train loss: 0.01372 - Test loss: 0.04129\n",
      "Epoch 3492 - lr: 0.03423 - Train loss: 0.01372 - Test loss: 0.04129\n",
      "Epoch 3493 - lr: 0.03422 - Train loss: 0.01372 - Test loss: 0.04129\n",
      "Epoch 3494 - lr: 0.03421 - Train loss: 0.01372 - Test loss: 0.04129\n",
      "Epoch 3495 - lr: 0.03420 - Train loss: 0.01371 - Test loss: 0.04129\n",
      "Epoch 3496 - lr: 0.03419 - Train loss: 0.01371 - Test loss: 0.04129\n",
      "Epoch 3497 - lr: 0.03418 - Train loss: 0.01371 - Test loss: 0.04129\n",
      "Epoch 3498 - lr: 0.03417 - Train loss: 0.01371 - Test loss: 0.04128\n",
      "Epoch 3499 - lr: 0.03416 - Train loss: 0.01371 - Test loss: 0.04128\n",
      "Epoch 3500 - lr: 0.03415 - Train loss: 0.01370 - Test loss: 0.04128\n",
      "Epoch 3501 - lr: 0.03414 - Train loss: 0.01370 - Test loss: 0.04128\n",
      "Epoch 3502 - lr: 0.03412 - Train loss: 0.01370 - Test loss: 0.04128\n",
      "Epoch 3503 - lr: 0.03411 - Train loss: 0.01370 - Test loss: 0.04128\n",
      "Epoch 3504 - lr: 0.03410 - Train loss: 0.01369 - Test loss: 0.04128\n",
      "Epoch 3505 - lr: 0.03409 - Train loss: 0.01369 - Test loss: 0.04128\n",
      "Epoch 3506 - lr: 0.03408 - Train loss: 0.01369 - Test loss: 0.04127\n",
      "Epoch 3507 - lr: 0.03407 - Train loss: 0.01369 - Test loss: 0.04127\n",
      "Epoch 3508 - lr: 0.03406 - Train loss: 0.01368 - Test loss: 0.04127\n",
      "Epoch 3509 - lr: 0.03405 - Train loss: 0.01368 - Test loss: 0.04127\n",
      "Epoch 3510 - lr: 0.03404 - Train loss: 0.01368 - Test loss: 0.04127\n",
      "Epoch 3511 - lr: 0.03403 - Train loss: 0.01368 - Test loss: 0.04127\n",
      "Epoch 3512 - lr: 0.03402 - Train loss: 0.01367 - Test loss: 0.04127\n",
      "Epoch 3513 - lr: 0.03401 - Train loss: 0.01367 - Test loss: 0.04127\n",
      "Epoch 3514 - lr: 0.03400 - Train loss: 0.01367 - Test loss: 0.04127\n",
      "Epoch 3515 - lr: 0.03399 - Train loss: 0.01367 - Test loss: 0.04126\n",
      "Epoch 3516 - lr: 0.03398 - Train loss: 0.01367 - Test loss: 0.04126\n",
      "Epoch 3517 - lr: 0.03397 - Train loss: 0.01366 - Test loss: 0.04126\n",
      "Epoch 3518 - lr: 0.03396 - Train loss: 0.01366 - Test loss: 0.04126\n",
      "Epoch 3519 - lr: 0.03395 - Train loss: 0.01366 - Test loss: 0.04126\n",
      "Epoch 3520 - lr: 0.03394 - Train loss: 0.01366 - Test loss: 0.04126\n",
      "Epoch 3521 - lr: 0.03393 - Train loss: 0.01365 - Test loss: 0.04126\n",
      "Epoch 3522 - lr: 0.03392 - Train loss: 0.01365 - Test loss: 0.04126\n",
      "Epoch 3523 - lr: 0.03391 - Train loss: 0.01365 - Test loss: 0.04125\n",
      "Epoch 3524 - lr: 0.03389 - Train loss: 0.01365 - Test loss: 0.04125\n",
      "Epoch 3525 - lr: 0.03388 - Train loss: 0.01364 - Test loss: 0.04125\n",
      "Epoch 3526 - lr: 0.03387 - Train loss: 0.01364 - Test loss: 0.04125\n",
      "Epoch 3527 - lr: 0.03386 - Train loss: 0.01364 - Test loss: 0.04125\n",
      "Epoch 3528 - lr: 0.03385 - Train loss: 0.01364 - Test loss: 0.04125\n",
      "Epoch 3529 - lr: 0.03384 - Train loss: 0.01363 - Test loss: 0.04125\n",
      "Epoch 3530 - lr: 0.03383 - Train loss: 0.01363 - Test loss: 0.04125\n",
      "Epoch 3531 - lr: 0.03382 - Train loss: 0.01363 - Test loss: 0.04125\n",
      "Epoch 3532 - lr: 0.03381 - Train loss: 0.01363 - Test loss: 0.04124\n",
      "Epoch 3533 - lr: 0.03380 - Train loss: 0.01363 - Test loss: 0.04124\n",
      "Epoch 3534 - lr: 0.03379 - Train loss: 0.01362 - Test loss: 0.04124\n",
      "Epoch 3535 - lr: 0.03378 - Train loss: 0.01362 - Test loss: 0.04124\n",
      "Epoch 3536 - lr: 0.03377 - Train loss: 0.01362 - Test loss: 0.04124\n",
      "Epoch 3537 - lr: 0.03376 - Train loss: 0.01362 - Test loss: 0.04124\n",
      "Epoch 3538 - lr: 0.03375 - Train loss: 0.01361 - Test loss: 0.04124\n",
      "Epoch 3539 - lr: 0.03374 - Train loss: 0.01361 - Test loss: 0.04124\n",
      "Epoch 3540 - lr: 0.03373 - Train loss: 0.01361 - Test loss: 0.04124\n",
      "Epoch 3541 - lr: 0.03372 - Train loss: 0.01361 - Test loss: 0.04123\n",
      "Epoch 3542 - lr: 0.03371 - Train loss: 0.01360 - Test loss: 0.04123\n",
      "Epoch 3543 - lr: 0.03370 - Train loss: 0.01360 - Test loss: 0.04123\n",
      "Epoch 3544 - lr: 0.03369 - Train loss: 0.01360 - Test loss: 0.04123\n",
      "Epoch 3545 - lr: 0.03368 - Train loss: 0.01360 - Test loss: 0.04123\n",
      "Epoch 3546 - lr: 0.03367 - Train loss: 0.01360 - Test loss: 0.04123\n",
      "Epoch 3547 - lr: 0.03366 - Train loss: 0.01359 - Test loss: 0.04123\n",
      "Epoch 3548 - lr: 0.03365 - Train loss: 0.01359 - Test loss: 0.04123\n",
      "Epoch 3549 - lr: 0.03364 - Train loss: 0.01359 - Test loss: 0.04122\n",
      "Epoch 3550 - lr: 0.03363 - Train loss: 0.01359 - Test loss: 0.04122\n",
      "Epoch 3551 - lr: 0.03362 - Train loss: 0.01358 - Test loss: 0.04122\n",
      "Epoch 3552 - lr: 0.03360 - Train loss: 0.01358 - Test loss: 0.04122\n",
      "Epoch 3553 - lr: 0.03359 - Train loss: 0.01358 - Test loss: 0.04122\n",
      "Epoch 3554 - lr: 0.03358 - Train loss: 0.01358 - Test loss: 0.04122\n",
      "Epoch 3555 - lr: 0.03357 - Train loss: 0.01357 - Test loss: 0.04122\n",
      "Epoch 3556 - lr: 0.03356 - Train loss: 0.01357 - Test loss: 0.04122\n",
      "Epoch 3557 - lr: 0.03355 - Train loss: 0.01357 - Test loss: 0.04122\n",
      "Epoch 3558 - lr: 0.03354 - Train loss: 0.01357 - Test loss: 0.04121\n",
      "Epoch 3559 - lr: 0.03353 - Train loss: 0.01357 - Test loss: 0.04121\n",
      "Epoch 3560 - lr: 0.03352 - Train loss: 0.01356 - Test loss: 0.04121\n",
      "Epoch 3561 - lr: 0.03351 - Train loss: 0.01356 - Test loss: 0.04121\n",
      "Epoch 3562 - lr: 0.03350 - Train loss: 0.01356 - Test loss: 0.04121\n",
      "Epoch 3563 - lr: 0.03349 - Train loss: 0.01356 - Test loss: 0.04121\n",
      "Epoch 3564 - lr: 0.03348 - Train loss: 0.01355 - Test loss: 0.04121\n",
      "Epoch 3565 - lr: 0.03347 - Train loss: 0.01355 - Test loss: 0.04121\n",
      "Epoch 3566 - lr: 0.03346 - Train loss: 0.01355 - Test loss: 0.04121\n",
      "Epoch 3567 - lr: 0.03345 - Train loss: 0.01355 - Test loss: 0.04120\n",
      "Epoch 3568 - lr: 0.03344 - Train loss: 0.01354 - Test loss: 0.04120\n",
      "Epoch 3569 - lr: 0.03343 - Train loss: 0.01354 - Test loss: 0.04120\n",
      "Epoch 3570 - lr: 0.03342 - Train loss: 0.01354 - Test loss: 0.04120\n",
      "Epoch 3571 - lr: 0.03341 - Train loss: 0.01354 - Test loss: 0.04120\n",
      "Epoch 3572 - lr: 0.03340 - Train loss: 0.01354 - Test loss: 0.04120\n",
      "Epoch 3573 - lr: 0.03339 - Train loss: 0.01353 - Test loss: 0.04120\n",
      "Epoch 3574 - lr: 0.03338 - Train loss: 0.01353 - Test loss: 0.04120\n",
      "Epoch 3575 - lr: 0.03337 - Train loss: 0.01353 - Test loss: 0.04120\n",
      "Epoch 3576 - lr: 0.03336 - Train loss: 0.01353 - Test loss: 0.04120\n",
      "Epoch 3577 - lr: 0.03335 - Train loss: 0.01352 - Test loss: 0.04119\n",
      "Epoch 3578 - lr: 0.03334 - Train loss: 0.01352 - Test loss: 0.04119\n",
      "Epoch 3579 - lr: 0.03333 - Train loss: 0.01352 - Test loss: 0.04119\n",
      "Epoch 3580 - lr: 0.03332 - Train loss: 0.01352 - Test loss: 0.04119\n",
      "Epoch 3581 - lr: 0.03331 - Train loss: 0.01351 - Test loss: 0.04119\n",
      "Epoch 3582 - lr: 0.03330 - Train loss: 0.01351 - Test loss: 0.04119\n",
      "Epoch 3583 - lr: 0.03329 - Train loss: 0.01351 - Test loss: 0.04119\n",
      "Epoch 3584 - lr: 0.03328 - Train loss: 0.01351 - Test loss: 0.04119\n",
      "Epoch 3585 - lr: 0.03327 - Train loss: 0.01351 - Test loss: 0.04119\n",
      "Epoch 3586 - lr: 0.03326 - Train loss: 0.01350 - Test loss: 0.04118\n",
      "Epoch 3587 - lr: 0.03325 - Train loss: 0.01350 - Test loss: 0.04118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3588 - lr: 0.03324 - Train loss: 0.01350 - Test loss: 0.04118\n",
      "Epoch 3589 - lr: 0.03323 - Train loss: 0.01350 - Test loss: 0.04118\n",
      "Epoch 3590 - lr: 0.03321 - Train loss: 0.01349 - Test loss: 0.04118\n",
      "Epoch 3591 - lr: 0.03320 - Train loss: 0.01349 - Test loss: 0.04118\n",
      "Epoch 3592 - lr: 0.03319 - Train loss: 0.01349 - Test loss: 0.04118\n",
      "Epoch 3593 - lr: 0.03318 - Train loss: 0.01349 - Test loss: 0.04118\n",
      "Epoch 3594 - lr: 0.03317 - Train loss: 0.01349 - Test loss: 0.04118\n",
      "Epoch 3595 - lr: 0.03316 - Train loss: 0.01348 - Test loss: 0.04117\n",
      "Epoch 3596 - lr: 0.03315 - Train loss: 0.01348 - Test loss: 0.04117\n",
      "Epoch 3597 - lr: 0.03314 - Train loss: 0.01348 - Test loss: 0.04117\n",
      "Epoch 3598 - lr: 0.03313 - Train loss: 0.01348 - Test loss: 0.04117\n",
      "Epoch 3599 - lr: 0.03312 - Train loss: 0.01347 - Test loss: 0.04117\n",
      "Epoch 3600 - lr: 0.03311 - Train loss: 0.01347 - Test loss: 0.04117\n",
      "Epoch 3601 - lr: 0.03310 - Train loss: 0.01347 - Test loss: 0.04117\n",
      "Epoch 3602 - lr: 0.03309 - Train loss: 0.01347 - Test loss: 0.04117\n",
      "Epoch 3603 - lr: 0.03308 - Train loss: 0.01346 - Test loss: 0.04117\n",
      "Epoch 3604 - lr: 0.03307 - Train loss: 0.01346 - Test loss: 0.04117\n",
      "Epoch 3605 - lr: 0.03306 - Train loss: 0.01346 - Test loss: 0.04116\n",
      "Epoch 3606 - lr: 0.03305 - Train loss: 0.01346 - Test loss: 0.04116\n",
      "Epoch 3607 - lr: 0.03304 - Train loss: 0.01346 - Test loss: 0.04116\n",
      "Epoch 3608 - lr: 0.03303 - Train loss: 0.01345 - Test loss: 0.04116\n",
      "Epoch 3609 - lr: 0.03302 - Train loss: 0.01345 - Test loss: 0.04116\n",
      "Epoch 3610 - lr: 0.03301 - Train loss: 0.01345 - Test loss: 0.04116\n",
      "Epoch 3611 - lr: 0.03300 - Train loss: 0.01345 - Test loss: 0.04116\n",
      "Epoch 3612 - lr: 0.03299 - Train loss: 0.01344 - Test loss: 0.04116\n",
      "Epoch 3613 - lr: 0.03298 - Train loss: 0.01344 - Test loss: 0.04116\n",
      "Epoch 3614 - lr: 0.03297 - Train loss: 0.01344 - Test loss: 0.04115\n",
      "Epoch 3615 - lr: 0.03296 - Train loss: 0.01344 - Test loss: 0.04115\n",
      "Epoch 3616 - lr: 0.03295 - Train loss: 0.01344 - Test loss: 0.04115\n",
      "Epoch 3617 - lr: 0.03294 - Train loss: 0.01343 - Test loss: 0.04115\n",
      "Epoch 3618 - lr: 0.03293 - Train loss: 0.01343 - Test loss: 0.04115\n",
      "Epoch 3619 - lr: 0.03292 - Train loss: 0.01343 - Test loss: 0.04115\n",
      "Epoch 3620 - lr: 0.03291 - Train loss: 0.01343 - Test loss: 0.04115\n",
      "Epoch 3621 - lr: 0.03290 - Train loss: 0.01342 - Test loss: 0.04115\n",
      "Epoch 3622 - lr: 0.03289 - Train loss: 0.01342 - Test loss: 0.04115\n",
      "Epoch 3623 - lr: 0.03288 - Train loss: 0.01342 - Test loss: 0.04115\n",
      "Epoch 3624 - lr: 0.03287 - Train loss: 0.01342 - Test loss: 0.04114\n",
      "Epoch 3625 - lr: 0.03286 - Train loss: 0.01342 - Test loss: 0.04114\n",
      "Epoch 3626 - lr: 0.03285 - Train loss: 0.01341 - Test loss: 0.04114\n",
      "Epoch 3627 - lr: 0.03284 - Train loss: 0.01341 - Test loss: 0.04114\n",
      "Epoch 3628 - lr: 0.03283 - Train loss: 0.01341 - Test loss: 0.04114\n",
      "Epoch 3629 - lr: 0.03282 - Train loss: 0.01341 - Test loss: 0.04114\n",
      "Epoch 3630 - lr: 0.03281 - Train loss: 0.01340 - Test loss: 0.04114\n",
      "Epoch 3631 - lr: 0.03280 - Train loss: 0.01340 - Test loss: 0.04114\n",
      "Epoch 3632 - lr: 0.03279 - Train loss: 0.01340 - Test loss: 0.04114\n",
      "Epoch 3633 - lr: 0.03278 - Train loss: 0.01340 - Test loss: 0.04114\n",
      "Epoch 3634 - lr: 0.03277 - Train loss: 0.01340 - Test loss: 0.04113\n",
      "Epoch 3635 - lr: 0.03276 - Train loss: 0.01339 - Test loss: 0.04113\n",
      "Epoch 3636 - lr: 0.03275 - Train loss: 0.01339 - Test loss: 0.04113\n",
      "Epoch 3637 - lr: 0.03274 - Train loss: 0.01339 - Test loss: 0.04113\n",
      "Epoch 3638 - lr: 0.03273 - Train loss: 0.01339 - Test loss: 0.04113\n",
      "Epoch 3639 - lr: 0.03272 - Train loss: 0.01338 - Test loss: 0.04113\n",
      "Epoch 3640 - lr: 0.03271 - Train loss: 0.01338 - Test loss: 0.04113\n",
      "Epoch 3641 - lr: 0.03270 - Train loss: 0.01338 - Test loss: 0.04113\n",
      "Epoch 3642 - lr: 0.03269 - Train loss: 0.01338 - Test loss: 0.04113\n",
      "Epoch 3643 - lr: 0.03268 - Train loss: 0.01338 - Test loss: 0.04113\n",
      "Epoch 3644 - lr: 0.03267 - Train loss: 0.01337 - Test loss: 0.04112\n",
      "Epoch 3645 - lr: 0.03266 - Train loss: 0.01337 - Test loss: 0.04112\n",
      "Epoch 3646 - lr: 0.03265 - Train loss: 0.01337 - Test loss: 0.04112\n",
      "Epoch 3647 - lr: 0.03264 - Train loss: 0.01337 - Test loss: 0.04112\n",
      "Epoch 3648 - lr: 0.03263 - Train loss: 0.01336 - Test loss: 0.04112\n",
      "Epoch 3649 - lr: 0.03262 - Train loss: 0.01336 - Test loss: 0.04112\n",
      "Epoch 3650 - lr: 0.03261 - Train loss: 0.01336 - Test loss: 0.04112\n",
      "Epoch 3651 - lr: 0.03260 - Train loss: 0.01336 - Test loss: 0.04112\n",
      "Epoch 3652 - lr: 0.03259 - Train loss: 0.01336 - Test loss: 0.04112\n",
      "Epoch 3653 - lr: 0.03258 - Train loss: 0.01335 - Test loss: 0.04112\n",
      "Epoch 3654 - lr: 0.03257 - Train loss: 0.01335 - Test loss: 0.04111\n",
      "Epoch 3655 - lr: 0.03256 - Train loss: 0.01335 - Test loss: 0.04111\n",
      "Epoch 3656 - lr: 0.03255 - Train loss: 0.01335 - Test loss: 0.04111\n",
      "Epoch 3657 - lr: 0.03254 - Train loss: 0.01334 - Test loss: 0.04111\n",
      "Epoch 3658 - lr: 0.03253 - Train loss: 0.01334 - Test loss: 0.04111\n",
      "Epoch 3659 - lr: 0.03252 - Train loss: 0.01334 - Test loss: 0.04111\n",
      "Epoch 3660 - lr: 0.03251 - Train loss: 0.01334 - Test loss: 0.04111\n",
      "Epoch 3661 - lr: 0.03250 - Train loss: 0.01334 - Test loss: 0.04111\n",
      "Epoch 3662 - lr: 0.03249 - Train loss: 0.01333 - Test loss: 0.04111\n",
      "Epoch 3663 - lr: 0.03248 - Train loss: 0.01333 - Test loss: 0.04111\n",
      "Epoch 3664 - lr: 0.03247 - Train loss: 0.01333 - Test loss: 0.04110\n",
      "Epoch 3665 - lr: 0.03246 - Train loss: 0.01333 - Test loss: 0.04110\n",
      "Epoch 3666 - lr: 0.03245 - Train loss: 0.01332 - Test loss: 0.04110\n",
      "Epoch 3667 - lr: 0.03244 - Train loss: 0.01332 - Test loss: 0.04110\n",
      "Epoch 3668 - lr: 0.03243 - Train loss: 0.01332 - Test loss: 0.04110\n",
      "Epoch 3669 - lr: 0.03242 - Train loss: 0.01332 - Test loss: 0.04110\n",
      "Epoch 3670 - lr: 0.03241 - Train loss: 0.01332 - Test loss: 0.04110\n",
      "Epoch 3671 - lr: 0.03240 - Train loss: 0.01331 - Test loss: 0.04110\n",
      "Epoch 3672 - lr: 0.03239 - Train loss: 0.01331 - Test loss: 0.04110\n",
      "Epoch 3673 - lr: 0.03238 - Train loss: 0.01331 - Test loss: 0.04110\n",
      "Epoch 3674 - lr: 0.03237 - Train loss: 0.01331 - Test loss: 0.04109\n",
      "Epoch 3675 - lr: 0.03236 - Train loss: 0.01330 - Test loss: 0.04109\n",
      "Epoch 3676 - lr: 0.03235 - Train loss: 0.01330 - Test loss: 0.04109\n",
      "Epoch 3677 - lr: 0.03234 - Train loss: 0.01330 - Test loss: 0.04109\n",
      "Epoch 3678 - lr: 0.03233 - Train loss: 0.01330 - Test loss: 0.04109\n",
      "Epoch 3679 - lr: 0.03232 - Train loss: 0.01330 - Test loss: 0.04109\n",
      "Epoch 3680 - lr: 0.03231 - Train loss: 0.01329 - Test loss: 0.04109\n",
      "Epoch 3681 - lr: 0.03230 - Train loss: 0.01329 - Test loss: 0.04109\n",
      "Epoch 3682 - lr: 0.03229 - Train loss: 0.01329 - Test loss: 0.04109\n",
      "Epoch 3683 - lr: 0.03228 - Train loss: 0.01329 - Test loss: 0.04109\n",
      "Epoch 3684 - lr: 0.03227 - Train loss: 0.01329 - Test loss: 0.04108\n",
      "Epoch 3685 - lr: 0.03226 - Train loss: 0.01328 - Test loss: 0.04108\n",
      "Epoch 3686 - lr: 0.03225 - Train loss: 0.01328 - Test loss: 0.04108\n",
      "Epoch 3687 - lr: 0.03224 - Train loss: 0.01328 - Test loss: 0.04108\n",
      "Epoch 3688 - lr: 0.03223 - Train loss: 0.01328 - Test loss: 0.04108\n",
      "Epoch 3689 - lr: 0.03222 - Train loss: 0.01327 - Test loss: 0.04108\n",
      "Epoch 3690 - lr: 0.03221 - Train loss: 0.01327 - Test loss: 0.04108\n",
      "Epoch 3691 - lr: 0.03220 - Train loss: 0.01327 - Test loss: 0.04108\n",
      "Epoch 3692 - lr: 0.03219 - Train loss: 0.01327 - Test loss: 0.04108\n",
      "Epoch 3693 - lr: 0.03218 - Train loss: 0.01327 - Test loss: 0.04108\n",
      "Epoch 3694 - lr: 0.03217 - Train loss: 0.01326 - Test loss: 0.04108\n",
      "Epoch 3695 - lr: 0.03216 - Train loss: 0.01326 - Test loss: 0.04107\n",
      "Epoch 3696 - lr: 0.03215 - Train loss: 0.01326 - Test loss: 0.04107\n",
      "Epoch 3697 - lr: 0.03214 - Train loss: 0.01326 - Test loss: 0.04107\n",
      "Epoch 3698 - lr: 0.03213 - Train loss: 0.01325 - Test loss: 0.04107\n",
      "Epoch 3699 - lr: 0.03212 - Train loss: 0.01325 - Test loss: 0.04107\n",
      "Epoch 3700 - lr: 0.03211 - Train loss: 0.01325 - Test loss: 0.04107\n",
      "Epoch 3701 - lr: 0.03210 - Train loss: 0.01325 - Test loss: 0.04107\n",
      "Epoch 3702 - lr: 0.03209 - Train loss: 0.01325 - Test loss: 0.04107\n",
      "Epoch 3703 - lr: 0.03208 - Train loss: 0.01324 - Test loss: 0.04107\n",
      "Epoch 3704 - lr: 0.03207 - Train loss: 0.01324 - Test loss: 0.04107\n",
      "Epoch 3705 - lr: 0.03206 - Train loss: 0.01324 - Test loss: 0.04107\n",
      "Epoch 3706 - lr: 0.03205 - Train loss: 0.01324 - Test loss: 0.04106\n",
      "Epoch 3707 - lr: 0.03204 - Train loss: 0.01324 - Test loss: 0.04106\n",
      "Epoch 3708 - lr: 0.03203 - Train loss: 0.01323 - Test loss: 0.04106\n",
      "Epoch 3709 - lr: 0.03202 - Train loss: 0.01323 - Test loss: 0.04106\n",
      "Epoch 3710 - lr: 0.03201 - Train loss: 0.01323 - Test loss: 0.04106\n",
      "Epoch 3711 - lr: 0.03200 - Train loss: 0.01323 - Test loss: 0.04106\n",
      "Epoch 3712 - lr: 0.03199 - Train loss: 0.01322 - Test loss: 0.04106\n",
      "Epoch 3713 - lr: 0.03198 - Train loss: 0.01322 - Test loss: 0.04106\n",
      "Epoch 3714 - lr: 0.03197 - Train loss: 0.01322 - Test loss: 0.04106\n",
      "Epoch 3715 - lr: 0.03196 - Train loss: 0.01322 - Test loss: 0.04106\n",
      "Epoch 3716 - lr: 0.03195 - Train loss: 0.01322 - Test loss: 0.04106\n",
      "Epoch 3717 - lr: 0.03194 - Train loss: 0.01321 - Test loss: 0.04105\n",
      "Epoch 3718 - lr: 0.03193 - Train loss: 0.01321 - Test loss: 0.04105\n",
      "Epoch 3719 - lr: 0.03193 - Train loss: 0.01321 - Test loss: 0.04105\n",
      "Epoch 3720 - lr: 0.03192 - Train loss: 0.01321 - Test loss: 0.04105\n",
      "Epoch 3721 - lr: 0.03191 - Train loss: 0.01321 - Test loss: 0.04105\n",
      "Epoch 3722 - lr: 0.03190 - Train loss: 0.01320 - Test loss: 0.04105\n",
      "Epoch 3723 - lr: 0.03189 - Train loss: 0.01320 - Test loss: 0.04105\n",
      "Epoch 3724 - lr: 0.03188 - Train loss: 0.01320 - Test loss: 0.04105\n",
      "Epoch 3725 - lr: 0.03187 - Train loss: 0.01320 - Test loss: 0.04105\n",
      "Epoch 3726 - lr: 0.03186 - Train loss: 0.01319 - Test loss: 0.04105\n",
      "Epoch 3727 - lr: 0.03185 - Train loss: 0.01319 - Test loss: 0.04105\n",
      "Epoch 3728 - lr: 0.03184 - Train loss: 0.01319 - Test loss: 0.04104\n",
      "Epoch 3729 - lr: 0.03183 - Train loss: 0.01319 - Test loss: 0.04104\n",
      "Epoch 3730 - lr: 0.03182 - Train loss: 0.01319 - Test loss: 0.04104\n",
      "Epoch 3731 - lr: 0.03181 - Train loss: 0.01318 - Test loss: 0.04104\n",
      "Epoch 3732 - lr: 0.03180 - Train loss: 0.01318 - Test loss: 0.04104\n",
      "Epoch 3733 - lr: 0.03179 - Train loss: 0.01318 - Test loss: 0.04104\n",
      "Epoch 3734 - lr: 0.03178 - Train loss: 0.01318 - Test loss: 0.04104\n",
      "Epoch 3735 - lr: 0.03177 - Train loss: 0.01318 - Test loss: 0.04104\n",
      "Epoch 3736 - lr: 0.03176 - Train loss: 0.01317 - Test loss: 0.04104\n",
      "Epoch 3737 - lr: 0.03175 - Train loss: 0.01317 - Test loss: 0.04104\n",
      "Epoch 3738 - lr: 0.03174 - Train loss: 0.01317 - Test loss: 0.04104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3739 - lr: 0.03173 - Train loss: 0.01317 - Test loss: 0.04103\n",
      "Epoch 3740 - lr: 0.03172 - Train loss: 0.01316 - Test loss: 0.04103\n",
      "Epoch 3741 - lr: 0.03171 - Train loss: 0.01316 - Test loss: 0.04103\n",
      "Epoch 3742 - lr: 0.03170 - Train loss: 0.01316 - Test loss: 0.04103\n",
      "Epoch 3743 - lr: 0.03169 - Train loss: 0.01316 - Test loss: 0.04103\n",
      "Epoch 3744 - lr: 0.03168 - Train loss: 0.01316 - Test loss: 0.04103\n",
      "Epoch 3745 - lr: 0.03167 - Train loss: 0.01315 - Test loss: 0.04103\n",
      "Epoch 3746 - lr: 0.03166 - Train loss: 0.01315 - Test loss: 0.04103\n",
      "Epoch 3747 - lr: 0.03165 - Train loss: 0.01315 - Test loss: 0.04103\n",
      "Epoch 3748 - lr: 0.03164 - Train loss: 0.01315 - Test loss: 0.04103\n",
      "Epoch 3749 - lr: 0.03163 - Train loss: 0.01315 - Test loss: 0.04103\n",
      "Epoch 3750 - lr: 0.03162 - Train loss: 0.01314 - Test loss: 0.04102\n",
      "Epoch 3751 - lr: 0.03161 - Train loss: 0.01314 - Test loss: 0.04102\n",
      "Epoch 3752 - lr: 0.03160 - Train loss: 0.01314 - Test loss: 0.04102\n",
      "Epoch 3753 - lr: 0.03159 - Train loss: 0.01314 - Test loss: 0.04102\n",
      "Epoch 3754 - lr: 0.03158 - Train loss: 0.01314 - Test loss: 0.04102\n",
      "Epoch 3755 - lr: 0.03157 - Train loss: 0.01313 - Test loss: 0.04102\n",
      "Epoch 3756 - lr: 0.03156 - Train loss: 0.01313 - Test loss: 0.04102\n",
      "Epoch 3757 - lr: 0.03155 - Train loss: 0.01313 - Test loss: 0.04102\n",
      "Epoch 3758 - lr: 0.03155 - Train loss: 0.01313 - Test loss: 0.04102\n",
      "Epoch 3759 - lr: 0.03154 - Train loss: 0.01312 - Test loss: 0.04102\n",
      "Epoch 3760 - lr: 0.03153 - Train loss: 0.01312 - Test loss: 0.04102\n",
      "Epoch 3761 - lr: 0.03152 - Train loss: 0.01312 - Test loss: 0.04101\n",
      "Epoch 3762 - lr: 0.03151 - Train loss: 0.01312 - Test loss: 0.04101\n",
      "Epoch 3763 - lr: 0.03150 - Train loss: 0.01312 - Test loss: 0.04101\n",
      "Epoch 3764 - lr: 0.03149 - Train loss: 0.01311 - Test loss: 0.04101\n",
      "Epoch 3765 - lr: 0.03148 - Train loss: 0.01311 - Test loss: 0.04101\n",
      "Epoch 3766 - lr: 0.03147 - Train loss: 0.01311 - Test loss: 0.04101\n",
      "Epoch 3767 - lr: 0.03146 - Train loss: 0.01311 - Test loss: 0.04101\n",
      "Epoch 3768 - lr: 0.03145 - Train loss: 0.01311 - Test loss: 0.04101\n",
      "Epoch 3769 - lr: 0.03144 - Train loss: 0.01310 - Test loss: 0.04101\n",
      "Epoch 3770 - lr: 0.03143 - Train loss: 0.01310 - Test loss: 0.04101\n",
      "Epoch 3771 - lr: 0.03142 - Train loss: 0.01310 - Test loss: 0.04101\n",
      "Epoch 3772 - lr: 0.03141 - Train loss: 0.01310 - Test loss: 0.04101\n",
      "Epoch 3773 - lr: 0.03140 - Train loss: 0.01310 - Test loss: 0.04100\n",
      "Epoch 3774 - lr: 0.03139 - Train loss: 0.01309 - Test loss: 0.04100\n",
      "Epoch 3775 - lr: 0.03138 - Train loss: 0.01309 - Test loss: 0.04100\n",
      "Epoch 3776 - lr: 0.03137 - Train loss: 0.01309 - Test loss: 0.04100\n",
      "Epoch 3777 - lr: 0.03136 - Train loss: 0.01309 - Test loss: 0.04100\n",
      "Epoch 3778 - lr: 0.03135 - Train loss: 0.01308 - Test loss: 0.04100\n",
      "Epoch 3779 - lr: 0.03134 - Train loss: 0.01308 - Test loss: 0.04100\n",
      "Epoch 3780 - lr: 0.03133 - Train loss: 0.01308 - Test loss: 0.04100\n",
      "Epoch 3781 - lr: 0.03132 - Train loss: 0.01308 - Test loss: 0.04100\n",
      "Epoch 3782 - lr: 0.03131 - Train loss: 0.01308 - Test loss: 0.04100\n",
      "Epoch 3783 - lr: 0.03130 - Train loss: 0.01307 - Test loss: 0.04100\n",
      "Epoch 3784 - lr: 0.03129 - Train loss: 0.01307 - Test loss: 0.04100\n",
      "Epoch 3785 - lr: 0.03128 - Train loss: 0.01307 - Test loss: 0.04099\n",
      "Epoch 3786 - lr: 0.03128 - Train loss: 0.01307 - Test loss: 0.04099\n",
      "Epoch 3787 - lr: 0.03127 - Train loss: 0.01307 - Test loss: 0.04099\n",
      "Epoch 3788 - lr: 0.03126 - Train loss: 0.01306 - Test loss: 0.04099\n",
      "Epoch 3789 - lr: 0.03125 - Train loss: 0.01306 - Test loss: 0.04099\n",
      "Epoch 3790 - lr: 0.03124 - Train loss: 0.01306 - Test loss: 0.04099\n",
      "Epoch 3791 - lr: 0.03123 - Train loss: 0.01306 - Test loss: 0.04099\n",
      "Epoch 3792 - lr: 0.03122 - Train loss: 0.01306 - Test loss: 0.04099\n",
      "Epoch 3793 - lr: 0.03121 - Train loss: 0.01305 - Test loss: 0.04099\n",
      "Epoch 3794 - lr: 0.03120 - Train loss: 0.01305 - Test loss: 0.04099\n",
      "Epoch 3795 - lr: 0.03119 - Train loss: 0.01305 - Test loss: 0.04099\n",
      "Epoch 3796 - lr: 0.03118 - Train loss: 0.01305 - Test loss: 0.04099\n",
      "Epoch 3797 - lr: 0.03117 - Train loss: 0.01305 - Test loss: 0.04098\n",
      "Epoch 3798 - lr: 0.03116 - Train loss: 0.01304 - Test loss: 0.04098\n",
      "Epoch 3799 - lr: 0.03115 - Train loss: 0.01304 - Test loss: 0.04098\n",
      "Epoch 3800 - lr: 0.03114 - Train loss: 0.01304 - Test loss: 0.04098\n",
      "Epoch 3801 - lr: 0.03113 - Train loss: 0.01304 - Test loss: 0.04098\n",
      "Epoch 3802 - lr: 0.03112 - Train loss: 0.01303 - Test loss: 0.04098\n",
      "Epoch 3803 - lr: 0.03111 - Train loss: 0.01303 - Test loss: 0.04098\n",
      "Epoch 3804 - lr: 0.03110 - Train loss: 0.01303 - Test loss: 0.04098\n",
      "Epoch 3805 - lr: 0.03109 - Train loss: 0.01303 - Test loss: 0.04098\n",
      "Epoch 3806 - lr: 0.03108 - Train loss: 0.01303 - Test loss: 0.04098\n",
      "Epoch 3807 - lr: 0.03107 - Train loss: 0.01302 - Test loss: 0.04098\n",
      "Epoch 3808 - lr: 0.03106 - Train loss: 0.01302 - Test loss: 0.04098\n",
      "Epoch 3809 - lr: 0.03106 - Train loss: 0.01302 - Test loss: 0.04097\n",
      "Epoch 3810 - lr: 0.03105 - Train loss: 0.01302 - Test loss: 0.04097\n",
      "Epoch 3811 - lr: 0.03104 - Train loss: 0.01302 - Test loss: 0.04097\n",
      "Epoch 3812 - lr: 0.03103 - Train loss: 0.01301 - Test loss: 0.04097\n",
      "Epoch 3813 - lr: 0.03102 - Train loss: 0.01301 - Test loss: 0.04097\n",
      "Epoch 3814 - lr: 0.03101 - Train loss: 0.01301 - Test loss: 0.04097\n",
      "Epoch 3815 - lr: 0.03100 - Train loss: 0.01301 - Test loss: 0.04097\n",
      "Epoch 3816 - lr: 0.03099 - Train loss: 0.01301 - Test loss: 0.04097\n",
      "Epoch 3817 - lr: 0.03098 - Train loss: 0.01300 - Test loss: 0.04097\n",
      "Epoch 3818 - lr: 0.03097 - Train loss: 0.01300 - Test loss: 0.04097\n",
      "Epoch 3819 - lr: 0.03096 - Train loss: 0.01300 - Test loss: 0.04097\n",
      "Epoch 3820 - lr: 0.03095 - Train loss: 0.01300 - Test loss: 0.04097\n",
      "Epoch 3821 - lr: 0.03094 - Train loss: 0.01300 - Test loss: 0.04096\n",
      "Epoch 3822 - lr: 0.03093 - Train loss: 0.01299 - Test loss: 0.04096\n",
      "Epoch 3823 - lr: 0.03092 - Train loss: 0.01299 - Test loss: 0.04096\n",
      "Epoch 3824 - lr: 0.03091 - Train loss: 0.01299 - Test loss: 0.04096\n",
      "Epoch 3825 - lr: 0.03090 - Train loss: 0.01299 - Test loss: 0.04096\n",
      "Epoch 3826 - lr: 0.03089 - Train loss: 0.01299 - Test loss: 0.04096\n",
      "Epoch 3827 - lr: 0.03088 - Train loss: 0.01298 - Test loss: 0.04096\n",
      "Epoch 3828 - lr: 0.03087 - Train loss: 0.01298 - Test loss: 0.04096\n",
      "Epoch 3829 - lr: 0.03087 - Train loss: 0.01298 - Test loss: 0.04096\n",
      "Epoch 3830 - lr: 0.03086 - Train loss: 0.01298 - Test loss: 0.04096\n",
      "Epoch 3831 - lr: 0.03085 - Train loss: 0.01298 - Test loss: 0.04096\n",
      "Epoch 3832 - lr: 0.03084 - Train loss: 0.01297 - Test loss: 0.04096\n",
      "Epoch 3833 - lr: 0.03083 - Train loss: 0.01297 - Test loss: 0.04096\n",
      "Epoch 3834 - lr: 0.03082 - Train loss: 0.01297 - Test loss: 0.04095\n",
      "Epoch 3835 - lr: 0.03081 - Train loss: 0.01297 - Test loss: 0.04095\n",
      "Epoch 3836 - lr: 0.03080 - Train loss: 0.01296 - Test loss: 0.04095\n",
      "Epoch 3837 - lr: 0.03079 - Train loss: 0.01296 - Test loss: 0.04095\n",
      "Epoch 3838 - lr: 0.03078 - Train loss: 0.01296 - Test loss: 0.04095\n",
      "Epoch 3839 - lr: 0.03077 - Train loss: 0.01296 - Test loss: 0.04095\n",
      "Epoch 3840 - lr: 0.03076 - Train loss: 0.01296 - Test loss: 0.04095\n",
      "Epoch 3841 - lr: 0.03075 - Train loss: 0.01295 - Test loss: 0.04095\n",
      "Epoch 3842 - lr: 0.03074 - Train loss: 0.01295 - Test loss: 0.04095\n",
      "Epoch 3843 - lr: 0.03073 - Train loss: 0.01295 - Test loss: 0.04095\n",
      "Epoch 3844 - lr: 0.03072 - Train loss: 0.01295 - Test loss: 0.04095\n",
      "Epoch 3845 - lr: 0.03071 - Train loss: 0.01295 - Test loss: 0.04095\n",
      "Epoch 3846 - lr: 0.03070 - Train loss: 0.01294 - Test loss: 0.04094\n",
      "Epoch 3847 - lr: 0.03069 - Train loss: 0.01294 - Test loss: 0.04094\n",
      "Epoch 3848 - lr: 0.03069 - Train loss: 0.01294 - Test loss: 0.04094\n",
      "Epoch 3849 - lr: 0.03068 - Train loss: 0.01294 - Test loss: 0.04094\n",
      "Epoch 3850 - lr: 0.03067 - Train loss: 0.01294 - Test loss: 0.04094\n",
      "Epoch 3851 - lr: 0.03066 - Train loss: 0.01293 - Test loss: 0.04094\n",
      "Epoch 3852 - lr: 0.03065 - Train loss: 0.01293 - Test loss: 0.04094\n",
      "Epoch 3853 - lr: 0.03064 - Train loss: 0.01293 - Test loss: 0.04094\n",
      "Epoch 3854 - lr: 0.03063 - Train loss: 0.01293 - Test loss: 0.04094\n",
      "Epoch 3855 - lr: 0.03062 - Train loss: 0.01293 - Test loss: 0.04094\n",
      "Epoch 3856 - lr: 0.03061 - Train loss: 0.01292 - Test loss: 0.04094\n",
      "Epoch 3857 - lr: 0.03060 - Train loss: 0.01292 - Test loss: 0.04094\n",
      "Epoch 3858 - lr: 0.03059 - Train loss: 0.01292 - Test loss: 0.04094\n",
      "Epoch 3859 - lr: 0.03058 - Train loss: 0.01292 - Test loss: 0.04093\n",
      "Epoch 3860 - lr: 0.03057 - Train loss: 0.01292 - Test loss: 0.04093\n",
      "Epoch 3861 - lr: 0.03056 - Train loss: 0.01291 - Test loss: 0.04093\n",
      "Epoch 3862 - lr: 0.03055 - Train loss: 0.01291 - Test loss: 0.04093\n",
      "Epoch 3863 - lr: 0.03054 - Train loss: 0.01291 - Test loss: 0.04093\n",
      "Epoch 3864 - lr: 0.03054 - Train loss: 0.01291 - Test loss: 0.04093\n",
      "Epoch 3865 - lr: 0.03053 - Train loss: 0.01291 - Test loss: 0.04093\n",
      "Epoch 3866 - lr: 0.03052 - Train loss: 0.01290 - Test loss: 0.04093\n",
      "Epoch 3867 - lr: 0.03051 - Train loss: 0.01290 - Test loss: 0.04093\n",
      "Epoch 3868 - lr: 0.03050 - Train loss: 0.01290 - Test loss: 0.04093\n",
      "Epoch 3869 - lr: 0.03049 - Train loss: 0.01290 - Test loss: 0.04093\n",
      "Epoch 3870 - lr: 0.03048 - Train loss: 0.01290 - Test loss: 0.04093\n",
      "Epoch 3871 - lr: 0.03047 - Train loss: 0.01289 - Test loss: 0.04093\n",
      "Epoch 3872 - lr: 0.03046 - Train loss: 0.01289 - Test loss: 0.04092\n",
      "Epoch 3873 - lr: 0.03045 - Train loss: 0.01289 - Test loss: 0.04092\n",
      "Epoch 3874 - lr: 0.03044 - Train loss: 0.01289 - Test loss: 0.04092\n",
      "Epoch 3875 - lr: 0.03043 - Train loss: 0.01289 - Test loss: 0.04092\n",
      "Epoch 3876 - lr: 0.03042 - Train loss: 0.01288 - Test loss: 0.04092\n",
      "Epoch 3877 - lr: 0.03041 - Train loss: 0.01288 - Test loss: 0.04092\n",
      "Epoch 3878 - lr: 0.03040 - Train loss: 0.01288 - Test loss: 0.04092\n",
      "Epoch 3879 - lr: 0.03039 - Train loss: 0.01288 - Test loss: 0.04092\n",
      "Epoch 3880 - lr: 0.03039 - Train loss: 0.01288 - Test loss: 0.04092\n",
      "Epoch 3881 - lr: 0.03038 - Train loss: 0.01287 - Test loss: 0.04092\n",
      "Epoch 3882 - lr: 0.03037 - Train loss: 0.01287 - Test loss: 0.04092\n",
      "Epoch 3883 - lr: 0.03036 - Train loss: 0.01287 - Test loss: 0.04092\n",
      "Epoch 3884 - lr: 0.03035 - Train loss: 0.01287 - Test loss: 0.04092\n",
      "Epoch 3885 - lr: 0.03034 - Train loss: 0.01287 - Test loss: 0.04091\n",
      "Epoch 3886 - lr: 0.03033 - Train loss: 0.01286 - Test loss: 0.04091\n",
      "Epoch 3887 - lr: 0.03032 - Train loss: 0.01286 - Test loss: 0.04091\n",
      "Epoch 3888 - lr: 0.03031 - Train loss: 0.01286 - Test loss: 0.04091\n",
      "Epoch 3889 - lr: 0.03030 - Train loss: 0.01286 - Test loss: 0.04091\n",
      "Epoch 3890 - lr: 0.03029 - Train loss: 0.01286 - Test loss: 0.04091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3891 - lr: 0.03028 - Train loss: 0.01285 - Test loss: 0.04091\n",
      "Epoch 3892 - lr: 0.03027 - Train loss: 0.01285 - Test loss: 0.04091\n",
      "Epoch 3893 - lr: 0.03026 - Train loss: 0.01285 - Test loss: 0.04091\n",
      "Epoch 3894 - lr: 0.03026 - Train loss: 0.01285 - Test loss: 0.04091\n",
      "Epoch 3895 - lr: 0.03025 - Train loss: 0.01285 - Test loss: 0.04091\n",
      "Epoch 3896 - lr: 0.03024 - Train loss: 0.01284 - Test loss: 0.04091\n",
      "Epoch 3897 - lr: 0.03023 - Train loss: 0.01284 - Test loss: 0.04091\n",
      "Epoch 3898 - lr: 0.03022 - Train loss: 0.01284 - Test loss: 0.04091\n",
      "Epoch 3899 - lr: 0.03021 - Train loss: 0.01284 - Test loss: 0.04090\n",
      "Epoch 3900 - lr: 0.03020 - Train loss: 0.01284 - Test loss: 0.04090\n",
      "Epoch 3901 - lr: 0.03019 - Train loss: 0.01283 - Test loss: 0.04090\n",
      "Epoch 3902 - lr: 0.03018 - Train loss: 0.01283 - Test loss: 0.04090\n",
      "Epoch 3903 - lr: 0.03017 - Train loss: 0.01283 - Test loss: 0.04090\n",
      "Epoch 3904 - lr: 0.03016 - Train loss: 0.01283 - Test loss: 0.04090\n",
      "Epoch 3905 - lr: 0.03015 - Train loss: 0.01283 - Test loss: 0.04090\n",
      "Epoch 3906 - lr: 0.03014 - Train loss: 0.01282 - Test loss: 0.04090\n",
      "Epoch 3907 - lr: 0.03013 - Train loss: 0.01282 - Test loss: 0.04090\n",
      "Epoch 3908 - lr: 0.03013 - Train loss: 0.01282 - Test loss: 0.04090\n",
      "Epoch 3909 - lr: 0.03012 - Train loss: 0.01282 - Test loss: 0.04090\n",
      "Epoch 3910 - lr: 0.03011 - Train loss: 0.01282 - Test loss: 0.04090\n",
      "Epoch 3911 - lr: 0.03010 - Train loss: 0.01281 - Test loss: 0.04090\n",
      "Epoch 3912 - lr: 0.03009 - Train loss: 0.01281 - Test loss: 0.04090\n",
      "Epoch 3913 - lr: 0.03008 - Train loss: 0.01281 - Test loss: 0.04089\n",
      "Epoch 3914 - lr: 0.03007 - Train loss: 0.01281 - Test loss: 0.04089\n",
      "Epoch 3915 - lr: 0.03006 - Train loss: 0.01281 - Test loss: 0.04089\n",
      "Epoch 3916 - lr: 0.03005 - Train loss: 0.01280 - Test loss: 0.04089\n",
      "Epoch 3917 - lr: 0.03004 - Train loss: 0.01280 - Test loss: 0.04089\n",
      "Epoch 3918 - lr: 0.03003 - Train loss: 0.01280 - Test loss: 0.04089\n",
      "Epoch 3919 - lr: 0.03002 - Train loss: 0.01280 - Test loss: 0.04089\n",
      "Epoch 3920 - lr: 0.03001 - Train loss: 0.01280 - Test loss: 0.04089\n",
      "Epoch 3921 - lr: 0.03001 - Train loss: 0.01279 - Test loss: 0.04089\n",
      "Epoch 3922 - lr: 0.03000 - Train loss: 0.01279 - Test loss: 0.04089\n",
      "Epoch 3923 - lr: 0.02999 - Train loss: 0.01279 - Test loss: 0.04089\n",
      "Epoch 3924 - lr: 0.02998 - Train loss: 0.01279 - Test loss: 0.04089\n",
      "Epoch 3925 - lr: 0.02997 - Train loss: 0.01279 - Test loss: 0.04089\n",
      "Epoch 3926 - lr: 0.02996 - Train loss: 0.01278 - Test loss: 0.04088\n",
      "Epoch 3927 - lr: 0.02995 - Train loss: 0.01278 - Test loss: 0.04088\n",
      "Epoch 3928 - lr: 0.02994 - Train loss: 0.01278 - Test loss: 0.04088\n",
      "Epoch 3929 - lr: 0.02993 - Train loss: 0.01278 - Test loss: 0.04088\n",
      "Epoch 3930 - lr: 0.02992 - Train loss: 0.01278 - Test loss: 0.04088\n",
      "Epoch 3931 - lr: 0.02991 - Train loss: 0.01277 - Test loss: 0.04088\n",
      "Epoch 3932 - lr: 0.02990 - Train loss: 0.01277 - Test loss: 0.04088\n",
      "Epoch 3933 - lr: 0.02990 - Train loss: 0.01277 - Test loss: 0.04088\n",
      "Epoch 3934 - lr: 0.02989 - Train loss: 0.01277 - Test loss: 0.04088\n",
      "Epoch 3935 - lr: 0.02988 - Train loss: 0.01277 - Test loss: 0.04088\n",
      "Epoch 3936 - lr: 0.02987 - Train loss: 0.01276 - Test loss: 0.04088\n",
      "Epoch 3937 - lr: 0.02986 - Train loss: 0.01276 - Test loss: 0.04088\n",
      "Epoch 3938 - lr: 0.02985 - Train loss: 0.01276 - Test loss: 0.04088\n",
      "Epoch 3939 - lr: 0.02984 - Train loss: 0.01276 - Test loss: 0.04088\n",
      "Epoch 3940 - lr: 0.02983 - Train loss: 0.01276 - Test loss: 0.04088\n",
      "Epoch 3941 - lr: 0.02982 - Train loss: 0.01276 - Test loss: 0.04087\n",
      "Epoch 3942 - lr: 0.02981 - Train loss: 0.01275 - Test loss: 0.04087\n",
      "Epoch 3943 - lr: 0.02980 - Train loss: 0.01275 - Test loss: 0.04087\n",
      "Epoch 3944 - lr: 0.02979 - Train loss: 0.01275 - Test loss: 0.04087\n",
      "Epoch 3945 - lr: 0.02979 - Train loss: 0.01275 - Test loss: 0.04087\n",
      "Epoch 3946 - lr: 0.02978 - Train loss: 0.01275 - Test loss: 0.04087\n",
      "Epoch 3947 - lr: 0.02977 - Train loss: 0.01274 - Test loss: 0.04087\n",
      "Epoch 3948 - lr: 0.02976 - Train loss: 0.01274 - Test loss: 0.04087\n",
      "Epoch 3949 - lr: 0.02975 - Train loss: 0.01274 - Test loss: 0.04087\n",
      "Epoch 3950 - lr: 0.02974 - Train loss: 0.01274 - Test loss: 0.04087\n",
      "Epoch 3951 - lr: 0.02973 - Train loss: 0.01274 - Test loss: 0.04087\n",
      "Epoch 3952 - lr: 0.02972 - Train loss: 0.01273 - Test loss: 0.04087\n",
      "Epoch 3953 - lr: 0.02971 - Train loss: 0.01273 - Test loss: 0.04087\n",
      "Epoch 3954 - lr: 0.02970 - Train loss: 0.01273 - Test loss: 0.04087\n",
      "Epoch 3955 - lr: 0.02969 - Train loss: 0.01273 - Test loss: 0.04086\n",
      "Epoch 3956 - lr: 0.02968 - Train loss: 0.01273 - Test loss: 0.04086\n",
      "Epoch 3957 - lr: 0.02968 - Train loss: 0.01272 - Test loss: 0.04086\n",
      "Epoch 3958 - lr: 0.02967 - Train loss: 0.01272 - Test loss: 0.04086\n",
      "Epoch 3959 - lr: 0.02966 - Train loss: 0.01272 - Test loss: 0.04086\n",
      "Epoch 3960 - lr: 0.02965 - Train loss: 0.01272 - Test loss: 0.04086\n",
      "Epoch 3961 - lr: 0.02964 - Train loss: 0.01272 - Test loss: 0.04086\n",
      "Epoch 3962 - lr: 0.02963 - Train loss: 0.01271 - Test loss: 0.04086\n",
      "Epoch 3963 - lr: 0.02962 - Train loss: 0.01271 - Test loss: 0.04086\n",
      "Epoch 3964 - lr: 0.02961 - Train loss: 0.01271 - Test loss: 0.04086\n",
      "Epoch 3965 - lr: 0.02960 - Train loss: 0.01271 - Test loss: 0.04086\n",
      "Epoch 3966 - lr: 0.02959 - Train loss: 0.01271 - Test loss: 0.04086\n",
      "Epoch 3967 - lr: 0.02958 - Train loss: 0.01270 - Test loss: 0.04086\n",
      "Epoch 3968 - lr: 0.02958 - Train loss: 0.01270 - Test loss: 0.04086\n",
      "Epoch 3969 - lr: 0.02957 - Train loss: 0.01270 - Test loss: 0.04086\n",
      "Epoch 3970 - lr: 0.02956 - Train loss: 0.01270 - Test loss: 0.04085\n",
      "Epoch 3971 - lr: 0.02955 - Train loss: 0.01270 - Test loss: 0.04085\n",
      "Epoch 3972 - lr: 0.02954 - Train loss: 0.01269 - Test loss: 0.04085\n",
      "Epoch 3973 - lr: 0.02953 - Train loss: 0.01269 - Test loss: 0.04085\n",
      "Epoch 3974 - lr: 0.02952 - Train loss: 0.01269 - Test loss: 0.04085\n",
      "Epoch 3975 - lr: 0.02951 - Train loss: 0.01269 - Test loss: 0.04085\n",
      "Epoch 3976 - lr: 0.02950 - Train loss: 0.01269 - Test loss: 0.04085\n",
      "Epoch 3977 - lr: 0.02949 - Train loss: 0.01269 - Test loss: 0.04085\n",
      "Epoch 3978 - lr: 0.02948 - Train loss: 0.01268 - Test loss: 0.04085\n",
      "Epoch 3979 - lr: 0.02948 - Train loss: 0.01268 - Test loss: 0.04085\n",
      "Epoch 3980 - lr: 0.02947 - Train loss: 0.01268 - Test loss: 0.04085\n",
      "Epoch 3981 - lr: 0.02946 - Train loss: 0.01268 - Test loss: 0.04085\n",
      "Epoch 3982 - lr: 0.02945 - Train loss: 0.01268 - Test loss: 0.04085\n",
      "Epoch 3983 - lr: 0.02944 - Train loss: 0.01267 - Test loss: 0.04085\n",
      "Epoch 3984 - lr: 0.02943 - Train loss: 0.01267 - Test loss: 0.04084\n",
      "Epoch 3985 - lr: 0.02942 - Train loss: 0.01267 - Test loss: 0.04084\n",
      "Epoch 3986 - lr: 0.02941 - Train loss: 0.01267 - Test loss: 0.04084\n",
      "Epoch 3987 - lr: 0.02940 - Train loss: 0.01267 - Test loss: 0.04084\n",
      "Epoch 3988 - lr: 0.02939 - Train loss: 0.01266 - Test loss: 0.04084\n",
      "Epoch 3989 - lr: 0.02939 - Train loss: 0.01266 - Test loss: 0.04084\n",
      "Epoch 3990 - lr: 0.02938 - Train loss: 0.01266 - Test loss: 0.04084\n",
      "Epoch 3991 - lr: 0.02937 - Train loss: 0.01266 - Test loss: 0.04084\n",
      "Epoch 3992 - lr: 0.02936 - Train loss: 0.01266 - Test loss: 0.04084\n",
      "Epoch 3993 - lr: 0.02935 - Train loss: 0.01265 - Test loss: 0.04084\n",
      "Epoch 3994 - lr: 0.02934 - Train loss: 0.01265 - Test loss: 0.04084\n",
      "Epoch 3995 - lr: 0.02933 - Train loss: 0.01265 - Test loss: 0.04084\n",
      "Epoch 3996 - lr: 0.02932 - Train loss: 0.01265 - Test loss: 0.04084\n",
      "Epoch 3997 - lr: 0.02931 - Train loss: 0.01265 - Test loss: 0.04084\n",
      "Epoch 3998 - lr: 0.02930 - Train loss: 0.01265 - Test loss: 0.04084\n",
      "Epoch 3999 - lr: 0.02930 - Train loss: 0.01264 - Test loss: 0.04084\n",
      "Epoch 4000 - lr: 0.02929 - Train loss: 0.01264 - Test loss: 0.04083\n",
      "Epoch 4001 - lr: 0.02928 - Train loss: 0.01264 - Test loss: 0.04083\n",
      "Epoch 4002 - lr: 0.02927 - Train loss: 0.01264 - Test loss: 0.04083\n",
      "Epoch 4003 - lr: 0.02926 - Train loss: 0.01264 - Test loss: 0.04083\n",
      "Epoch 4004 - lr: 0.02925 - Train loss: 0.01263 - Test loss: 0.04083\n",
      "Epoch 4005 - lr: 0.02924 - Train loss: 0.01263 - Test loss: 0.04083\n",
      "Epoch 4006 - lr: 0.02923 - Train loss: 0.01263 - Test loss: 0.04083\n",
      "Epoch 4007 - lr: 0.02922 - Train loss: 0.01263 - Test loss: 0.04083\n",
      "Epoch 4008 - lr: 0.02921 - Train loss: 0.01263 - Test loss: 0.04083\n",
      "Epoch 4009 - lr: 0.02921 - Train loss: 0.01262 - Test loss: 0.04083\n",
      "Epoch 4010 - lr: 0.02920 - Train loss: 0.01262 - Test loss: 0.04083\n",
      "Epoch 4011 - lr: 0.02919 - Train loss: 0.01262 - Test loss: 0.04083\n",
      "Epoch 4012 - lr: 0.02918 - Train loss: 0.01262 - Test loss: 0.04083\n",
      "Epoch 4013 - lr: 0.02917 - Train loss: 0.01262 - Test loss: 0.04083\n",
      "Epoch 4014 - lr: 0.02916 - Train loss: 0.01261 - Test loss: 0.04083\n",
      "Epoch 4015 - lr: 0.02915 - Train loss: 0.01261 - Test loss: 0.04082\n",
      "Epoch 4016 - lr: 0.02914 - Train loss: 0.01261 - Test loss: 0.04082\n",
      "Epoch 4017 - lr: 0.02913 - Train loss: 0.01261 - Test loss: 0.04082\n",
      "Epoch 4018 - lr: 0.02913 - Train loss: 0.01261 - Test loss: 0.04082\n",
      "Epoch 4019 - lr: 0.02912 - Train loss: 0.01261 - Test loss: 0.04082\n",
      "Epoch 4020 - lr: 0.02911 - Train loss: 0.01260 - Test loss: 0.04082\n",
      "Epoch 4021 - lr: 0.02910 - Train loss: 0.01260 - Test loss: 0.04082\n",
      "Epoch 4022 - lr: 0.02909 - Train loss: 0.01260 - Test loss: 0.04082\n",
      "Epoch 4023 - lr: 0.02908 - Train loss: 0.01260 - Test loss: 0.04082\n",
      "Epoch 4024 - lr: 0.02907 - Train loss: 0.01260 - Test loss: 0.04082\n",
      "Epoch 4025 - lr: 0.02906 - Train loss: 0.01259 - Test loss: 0.04082\n",
      "Epoch 4026 - lr: 0.02905 - Train loss: 0.01259 - Test loss: 0.04082\n",
      "Epoch 4027 - lr: 0.02904 - Train loss: 0.01259 - Test loss: 0.04082\n",
      "Epoch 4028 - lr: 0.02904 - Train loss: 0.01259 - Test loss: 0.04082\n",
      "Epoch 4029 - lr: 0.02903 - Train loss: 0.01259 - Test loss: 0.04082\n",
      "Epoch 4030 - lr: 0.02902 - Train loss: 0.01258 - Test loss: 0.04082\n",
      "Epoch 4031 - lr: 0.02901 - Train loss: 0.01258 - Test loss: 0.04081\n",
      "Epoch 4032 - lr: 0.02900 - Train loss: 0.01258 - Test loss: 0.04081\n",
      "Epoch 4033 - lr: 0.02899 - Train loss: 0.01258 - Test loss: 0.04081\n",
      "Epoch 4034 - lr: 0.02898 - Train loss: 0.01258 - Test loss: 0.04081\n",
      "Epoch 4035 - lr: 0.02897 - Train loss: 0.01257 - Test loss: 0.04081\n",
      "Epoch 4036 - lr: 0.02896 - Train loss: 0.01257 - Test loss: 0.04081\n",
      "Epoch 4037 - lr: 0.02896 - Train loss: 0.01257 - Test loss: 0.04081\n",
      "Epoch 4038 - lr: 0.02895 - Train loss: 0.01257 - Test loss: 0.04081\n",
      "Epoch 4039 - lr: 0.02894 - Train loss: 0.01257 - Test loss: 0.04081\n",
      "Epoch 4040 - lr: 0.02893 - Train loss: 0.01257 - Test loss: 0.04081\n",
      "Epoch 4041 - lr: 0.02892 - Train loss: 0.01256 - Test loss: 0.04081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4042 - lr: 0.02891 - Train loss: 0.01256 - Test loss: 0.04081\n",
      "Epoch 4043 - lr: 0.02890 - Train loss: 0.01256 - Test loss: 0.04081\n",
      "Epoch 4044 - lr: 0.02889 - Train loss: 0.01256 - Test loss: 0.04081\n",
      "Epoch 4045 - lr: 0.02888 - Train loss: 0.01256 - Test loss: 0.04081\n",
      "Epoch 4046 - lr: 0.02888 - Train loss: 0.01255 - Test loss: 0.04081\n",
      "Epoch 4047 - lr: 0.02887 - Train loss: 0.01255 - Test loss: 0.04080\n",
      "Epoch 4048 - lr: 0.02886 - Train loss: 0.01255 - Test loss: 0.04080\n",
      "Epoch 4049 - lr: 0.02885 - Train loss: 0.01255 - Test loss: 0.04080\n",
      "Epoch 4050 - lr: 0.02884 - Train loss: 0.01255 - Test loss: 0.04080\n",
      "Epoch 4051 - lr: 0.02883 - Train loss: 0.01254 - Test loss: 0.04080\n",
      "Epoch 4052 - lr: 0.02882 - Train loss: 0.01254 - Test loss: 0.04080\n",
      "Epoch 4053 - lr: 0.02881 - Train loss: 0.01254 - Test loss: 0.04080\n",
      "Epoch 4054 - lr: 0.02880 - Train loss: 0.01254 - Test loss: 0.04080\n",
      "Epoch 4055 - lr: 0.02880 - Train loss: 0.01254 - Test loss: 0.04080\n",
      "Epoch 4056 - lr: 0.02879 - Train loss: 0.01254 - Test loss: 0.04080\n",
      "Epoch 4057 - lr: 0.02878 - Train loss: 0.01253 - Test loss: 0.04080\n",
      "Epoch 4058 - lr: 0.02877 - Train loss: 0.01253 - Test loss: 0.04080\n",
      "Epoch 4059 - lr: 0.02876 - Train loss: 0.01253 - Test loss: 0.04080\n",
      "Epoch 4060 - lr: 0.02875 - Train loss: 0.01253 - Test loss: 0.04080\n",
      "Epoch 4061 - lr: 0.02874 - Train loss: 0.01253 - Test loss: 0.04080\n",
      "Epoch 4062 - lr: 0.02873 - Train loss: 0.01252 - Test loss: 0.04080\n",
      "Epoch 4063 - lr: 0.02873 - Train loss: 0.01252 - Test loss: 0.04079\n",
      "Epoch 4064 - lr: 0.02872 - Train loss: 0.01252 - Test loss: 0.04079\n",
      "Epoch 4065 - lr: 0.02871 - Train loss: 0.01252 - Test loss: 0.04079\n",
      "Epoch 4066 - lr: 0.02870 - Train loss: 0.01252 - Test loss: 0.04079\n",
      "Epoch 4067 - lr: 0.02869 - Train loss: 0.01251 - Test loss: 0.04079\n",
      "Epoch 4068 - lr: 0.02868 - Train loss: 0.01251 - Test loss: 0.04079\n",
      "Epoch 4069 - lr: 0.02867 - Train loss: 0.01251 - Test loss: 0.04079\n",
      "Epoch 4070 - lr: 0.02866 - Train loss: 0.01251 - Test loss: 0.04079\n",
      "Epoch 4071 - lr: 0.02865 - Train loss: 0.01251 - Test loss: 0.04079\n",
      "Epoch 4072 - lr: 0.02865 - Train loss: 0.01251 - Test loss: 0.04079\n",
      "Epoch 4073 - lr: 0.02864 - Train loss: 0.01250 - Test loss: 0.04079\n",
      "Epoch 4074 - lr: 0.02863 - Train loss: 0.01250 - Test loss: 0.04079\n",
      "Epoch 4075 - lr: 0.02862 - Train loss: 0.01250 - Test loss: 0.04079\n",
      "Epoch 4076 - lr: 0.02861 - Train loss: 0.01250 - Test loss: 0.04079\n",
      "Epoch 4077 - lr: 0.02860 - Train loss: 0.01250 - Test loss: 0.04079\n",
      "Epoch 4078 - lr: 0.02859 - Train loss: 0.01249 - Test loss: 0.04079\n",
      "Epoch 4079 - lr: 0.02858 - Train loss: 0.01249 - Test loss: 0.04079\n",
      "Epoch 4080 - lr: 0.02858 - Train loss: 0.01249 - Test loss: 0.04078\n",
      "Epoch 4081 - lr: 0.02857 - Train loss: 0.01249 - Test loss: 0.04078\n",
      "Epoch 4082 - lr: 0.02856 - Train loss: 0.01249 - Test loss: 0.04078\n",
      "Epoch 4083 - lr: 0.02855 - Train loss: 0.01249 - Test loss: 0.04078\n",
      "Epoch 4084 - lr: 0.02854 - Train loss: 0.01248 - Test loss: 0.04078\n",
      "Epoch 4085 - lr: 0.02853 - Train loss: 0.01248 - Test loss: 0.04078\n",
      "Epoch 4086 - lr: 0.02852 - Train loss: 0.01248 - Test loss: 0.04078\n",
      "Epoch 4087 - lr: 0.02851 - Train loss: 0.01248 - Test loss: 0.04078\n",
      "Epoch 4088 - lr: 0.02851 - Train loss: 0.01248 - Test loss: 0.04078\n",
      "Epoch 4089 - lr: 0.02850 - Train loss: 0.01247 - Test loss: 0.04078\n",
      "Epoch 4090 - lr: 0.02849 - Train loss: 0.01247 - Test loss: 0.04078\n",
      "Epoch 4091 - lr: 0.02848 - Train loss: 0.01247 - Test loss: 0.04078\n",
      "Epoch 4092 - lr: 0.02847 - Train loss: 0.01247 - Test loss: 0.04078\n",
      "Epoch 4093 - lr: 0.02846 - Train loss: 0.01247 - Test loss: 0.04078\n",
      "Epoch 4094 - lr: 0.02845 - Train loss: 0.01247 - Test loss: 0.04078\n",
      "Epoch 4095 - lr: 0.02844 - Train loss: 0.01246 - Test loss: 0.04078\n",
      "Epoch 4096 - lr: 0.02844 - Train loss: 0.01246 - Test loss: 0.04078\n",
      "Epoch 4097 - lr: 0.02843 - Train loss: 0.01246 - Test loss: 0.04077\n",
      "Epoch 4098 - lr: 0.02842 - Train loss: 0.01246 - Test loss: 0.04077\n",
      "Epoch 4099 - lr: 0.02841 - Train loss: 0.01246 - Test loss: 0.04077\n",
      "Epoch 4100 - lr: 0.02840 - Train loss: 0.01245 - Test loss: 0.04077\n",
      "Epoch 4101 - lr: 0.02839 - Train loss: 0.01245 - Test loss: 0.04077\n",
      "Epoch 4102 - lr: 0.02838 - Train loss: 0.01245 - Test loss: 0.04077\n",
      "Epoch 4103 - lr: 0.02837 - Train loss: 0.01245 - Test loss: 0.04077\n",
      "Epoch 4104 - lr: 0.02837 - Train loss: 0.01245 - Test loss: 0.04077\n",
      "Epoch 4105 - lr: 0.02836 - Train loss: 0.01244 - Test loss: 0.04077\n",
      "Epoch 4106 - lr: 0.02835 - Train loss: 0.01244 - Test loss: 0.04077\n",
      "Epoch 4107 - lr: 0.02834 - Train loss: 0.01244 - Test loss: 0.04077\n",
      "Epoch 4108 - lr: 0.02833 - Train loss: 0.01244 - Test loss: 0.04077\n",
      "Epoch 4109 - lr: 0.02832 - Train loss: 0.01244 - Test loss: 0.04077\n",
      "Epoch 4110 - lr: 0.02831 - Train loss: 0.01244 - Test loss: 0.04077\n",
      "Epoch 4111 - lr: 0.02831 - Train loss: 0.01243 - Test loss: 0.04077\n",
      "Epoch 4112 - lr: 0.02830 - Train loss: 0.01243 - Test loss: 0.04077\n",
      "Epoch 4113 - lr: 0.02829 - Train loss: 0.01243 - Test loss: 0.04077\n",
      "Epoch 4114 - lr: 0.02828 - Train loss: 0.01243 - Test loss: 0.04076\n",
      "Epoch 4115 - lr: 0.02827 - Train loss: 0.01243 - Test loss: 0.04076\n",
      "Epoch 4116 - lr: 0.02826 - Train loss: 0.01242 - Test loss: 0.04076\n",
      "Epoch 4117 - lr: 0.02825 - Train loss: 0.01242 - Test loss: 0.04076\n",
      "Epoch 4118 - lr: 0.02824 - Train loss: 0.01242 - Test loss: 0.04076\n",
      "Epoch 4119 - lr: 0.02824 - Train loss: 0.01242 - Test loss: 0.04076\n",
      "Epoch 4120 - lr: 0.02823 - Train loss: 0.01242 - Test loss: 0.04076\n",
      "Epoch 4121 - lr: 0.02822 - Train loss: 0.01242 - Test loss: 0.04076\n",
      "Epoch 4122 - lr: 0.02821 - Train loss: 0.01241 - Test loss: 0.04076\n",
      "Epoch 4123 - lr: 0.02820 - Train loss: 0.01241 - Test loss: 0.04076\n",
      "Epoch 4124 - lr: 0.02819 - Train loss: 0.01241 - Test loss: 0.04076\n",
      "Epoch 4125 - lr: 0.02818 - Train loss: 0.01241 - Test loss: 0.04076\n",
      "Epoch 4126 - lr: 0.02818 - Train loss: 0.01241 - Test loss: 0.04076\n",
      "Epoch 4127 - lr: 0.02817 - Train loss: 0.01240 - Test loss: 0.04076\n",
      "Epoch 4128 - lr: 0.02816 - Train loss: 0.01240 - Test loss: 0.04076\n",
      "Epoch 4129 - lr: 0.02815 - Train loss: 0.01240 - Test loss: 0.04076\n",
      "Epoch 4130 - lr: 0.02814 - Train loss: 0.01240 - Test loss: 0.04076\n",
      "Epoch 4131 - lr: 0.02813 - Train loss: 0.01240 - Test loss: 0.04076\n",
      "Epoch 4132 - lr: 0.02812 - Train loss: 0.01240 - Test loss: 0.04075\n",
      "Epoch 4133 - lr: 0.02811 - Train loss: 0.01239 - Test loss: 0.04075\n",
      "Epoch 4134 - lr: 0.02811 - Train loss: 0.01239 - Test loss: 0.04075\n",
      "Epoch 4135 - lr: 0.02810 - Train loss: 0.01239 - Test loss: 0.04075\n",
      "Epoch 4136 - lr: 0.02809 - Train loss: 0.01239 - Test loss: 0.04075\n",
      "Epoch 4137 - lr: 0.02808 - Train loss: 0.01239 - Test loss: 0.04075\n",
      "Epoch 4138 - lr: 0.02807 - Train loss: 0.01238 - Test loss: 0.04075\n",
      "Epoch 4139 - lr: 0.02806 - Train loss: 0.01238 - Test loss: 0.04075\n",
      "Epoch 4140 - lr: 0.02805 - Train loss: 0.01238 - Test loss: 0.04075\n",
      "Epoch 4141 - lr: 0.02805 - Train loss: 0.01238 - Test loss: 0.04075\n",
      "Epoch 4142 - lr: 0.02804 - Train loss: 0.01238 - Test loss: 0.04075\n",
      "Epoch 4143 - lr: 0.02803 - Train loss: 0.01238 - Test loss: 0.04075\n",
      "Epoch 4144 - lr: 0.02802 - Train loss: 0.01237 - Test loss: 0.04075\n",
      "Epoch 4145 - lr: 0.02801 - Train loss: 0.01237 - Test loss: 0.04075\n",
      "Epoch 4146 - lr: 0.02800 - Train loss: 0.01237 - Test loss: 0.04075\n",
      "Epoch 4147 - lr: 0.02799 - Train loss: 0.01237 - Test loss: 0.04075\n",
      "Epoch 4148 - lr: 0.02799 - Train loss: 0.01237 - Test loss: 0.04075\n",
      "Epoch 4149 - lr: 0.02798 - Train loss: 0.01236 - Test loss: 0.04074\n",
      "Epoch 4150 - lr: 0.02797 - Train loss: 0.01236 - Test loss: 0.04074\n",
      "Epoch 4151 - lr: 0.02796 - Train loss: 0.01236 - Test loss: 0.04074\n",
      "Epoch 4152 - lr: 0.02795 - Train loss: 0.01236 - Test loss: 0.04074\n",
      "Epoch 4153 - lr: 0.02794 - Train loss: 0.01236 - Test loss: 0.04074\n",
      "Epoch 4154 - lr: 0.02793 - Train loss: 0.01236 - Test loss: 0.04074\n",
      "Epoch 4155 - lr: 0.02793 - Train loss: 0.01235 - Test loss: 0.04074\n",
      "Epoch 4156 - lr: 0.02792 - Train loss: 0.01235 - Test loss: 0.04074\n",
      "Epoch 4157 - lr: 0.02791 - Train loss: 0.01235 - Test loss: 0.04074\n",
      "Epoch 4158 - lr: 0.02790 - Train loss: 0.01235 - Test loss: 0.04074\n",
      "Epoch 4159 - lr: 0.02789 - Train loss: 0.01235 - Test loss: 0.04074\n",
      "Epoch 4160 - lr: 0.02788 - Train loss: 0.01235 - Test loss: 0.04074\n",
      "Epoch 4161 - lr: 0.02787 - Train loss: 0.01234 - Test loss: 0.04074\n",
      "Epoch 4162 - lr: 0.02787 - Train loss: 0.01234 - Test loss: 0.04074\n",
      "Epoch 4163 - lr: 0.02786 - Train loss: 0.01234 - Test loss: 0.04074\n",
      "Epoch 4164 - lr: 0.02785 - Train loss: 0.01234 - Test loss: 0.04074\n",
      "Epoch 4165 - lr: 0.02784 - Train loss: 0.01234 - Test loss: 0.04074\n",
      "Epoch 4166 - lr: 0.02783 - Train loss: 0.01233 - Test loss: 0.04074\n",
      "Epoch 4167 - lr: 0.02782 - Train loss: 0.01233 - Test loss: 0.04074\n",
      "Epoch 4168 - lr: 0.02781 - Train loss: 0.01233 - Test loss: 0.04073\n",
      "Epoch 4169 - lr: 0.02781 - Train loss: 0.01233 - Test loss: 0.04073\n",
      "Epoch 4170 - lr: 0.02780 - Train loss: 0.01233 - Test loss: 0.04073\n",
      "Epoch 4171 - lr: 0.02779 - Train loss: 0.01233 - Test loss: 0.04073\n",
      "Epoch 4172 - lr: 0.02778 - Train loss: 0.01232 - Test loss: 0.04073\n",
      "Epoch 4173 - lr: 0.02777 - Train loss: 0.01232 - Test loss: 0.04073\n",
      "Epoch 4174 - lr: 0.02776 - Train loss: 0.01232 - Test loss: 0.04073\n",
      "Epoch 4175 - lr: 0.02775 - Train loss: 0.01232 - Test loss: 0.04073\n",
      "Epoch 4176 - lr: 0.02775 - Train loss: 0.01232 - Test loss: 0.04073\n",
      "Epoch 4177 - lr: 0.02774 - Train loss: 0.01231 - Test loss: 0.04073\n",
      "Epoch 4178 - lr: 0.02773 - Train loss: 0.01231 - Test loss: 0.04073\n",
      "Epoch 4179 - lr: 0.02772 - Train loss: 0.01231 - Test loss: 0.04073\n",
      "Epoch 4180 - lr: 0.02771 - Train loss: 0.01231 - Test loss: 0.04073\n",
      "Epoch 4181 - lr: 0.02770 - Train loss: 0.01231 - Test loss: 0.04073\n",
      "Epoch 4182 - lr: 0.02769 - Train loss: 0.01231 - Test loss: 0.04073\n",
      "Epoch 4183 - lr: 0.02769 - Train loss: 0.01230 - Test loss: 0.04073\n",
      "Epoch 4184 - lr: 0.02768 - Train loss: 0.01230 - Test loss: 0.04073\n",
      "Epoch 4185 - lr: 0.02767 - Train loss: 0.01230 - Test loss: 0.04073\n",
      "Epoch 4186 - lr: 0.02766 - Train loss: 0.01230 - Test loss: 0.04073\n",
      "Epoch 4187 - lr: 0.02765 - Train loss: 0.01230 - Test loss: 0.04072\n",
      "Epoch 4188 - lr: 0.02764 - Train loss: 0.01230 - Test loss: 0.04072\n",
      "Epoch 4189 - lr: 0.02764 - Train loss: 0.01229 - Test loss: 0.04072\n",
      "Epoch 4190 - lr: 0.02763 - Train loss: 0.01229 - Test loss: 0.04072\n",
      "Epoch 4191 - lr: 0.02762 - Train loss: 0.01229 - Test loss: 0.04072\n",
      "Epoch 4192 - lr: 0.02761 - Train loss: 0.01229 - Test loss: 0.04072\n",
      "Epoch 4193 - lr: 0.02760 - Train loss: 0.01229 - Test loss: 0.04072\n",
      "Epoch 4194 - lr: 0.02759 - Train loss: 0.01228 - Test loss: 0.04072\n",
      "Epoch 4195 - lr: 0.02758 - Train loss: 0.01228 - Test loss: 0.04072\n",
      "Epoch 4196 - lr: 0.02758 - Train loss: 0.01228 - Test loss: 0.04072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4197 - lr: 0.02757 - Train loss: 0.01228 - Test loss: 0.04072\n",
      "Epoch 4198 - lr: 0.02756 - Train loss: 0.01228 - Test loss: 0.04072\n",
      "Epoch 4199 - lr: 0.02755 - Train loss: 0.01228 - Test loss: 0.04072\n",
      "Epoch 4200 - lr: 0.02754 - Train loss: 0.01227 - Test loss: 0.04072\n",
      "Epoch 4201 - lr: 0.02753 - Train loss: 0.01227 - Test loss: 0.04072\n",
      "Epoch 4202 - lr: 0.02753 - Train loss: 0.01227 - Test loss: 0.04072\n",
      "Epoch 4203 - lr: 0.02752 - Train loss: 0.01227 - Test loss: 0.04072\n",
      "Epoch 4204 - lr: 0.02751 - Train loss: 0.01227 - Test loss: 0.04072\n",
      "Epoch 4205 - lr: 0.02750 - Train loss: 0.01227 - Test loss: 0.04072\n",
      "Epoch 4206 - lr: 0.02749 - Train loss: 0.01226 - Test loss: 0.04071\n",
      "Epoch 4207 - lr: 0.02748 - Train loss: 0.01226 - Test loss: 0.04071\n",
      "Epoch 4208 - lr: 0.02747 - Train loss: 0.01226 - Test loss: 0.04071\n",
      "Epoch 4209 - lr: 0.02747 - Train loss: 0.01226 - Test loss: 0.04071\n",
      "Epoch 4210 - lr: 0.02746 - Train loss: 0.01226 - Test loss: 0.04071\n",
      "Epoch 4211 - lr: 0.02745 - Train loss: 0.01225 - Test loss: 0.04071\n",
      "Epoch 4212 - lr: 0.02744 - Train loss: 0.01225 - Test loss: 0.04071\n",
      "Epoch 4213 - lr: 0.02743 - Train loss: 0.01225 - Test loss: 0.04071\n",
      "Epoch 4214 - lr: 0.02742 - Train loss: 0.01225 - Test loss: 0.04071\n",
      "Epoch 4215 - lr: 0.02742 - Train loss: 0.01225 - Test loss: 0.04071\n",
      "Epoch 4216 - lr: 0.02741 - Train loss: 0.01225 - Test loss: 0.04071\n",
      "Epoch 4217 - lr: 0.02740 - Train loss: 0.01224 - Test loss: 0.04071\n",
      "Epoch 4218 - lr: 0.02739 - Train loss: 0.01224 - Test loss: 0.04071\n",
      "Epoch 4219 - lr: 0.02738 - Train loss: 0.01224 - Test loss: 0.04071\n",
      "Epoch 4220 - lr: 0.02737 - Train loss: 0.01224 - Test loss: 0.04071\n",
      "Epoch 4221 - lr: 0.02737 - Train loss: 0.01224 - Test loss: 0.04071\n",
      "Epoch 4222 - lr: 0.02736 - Train loss: 0.01224 - Test loss: 0.04071\n",
      "Epoch 4223 - lr: 0.02735 - Train loss: 0.01223 - Test loss: 0.04071\n",
      "Epoch 4224 - lr: 0.02734 - Train loss: 0.01223 - Test loss: 0.04071\n",
      "Epoch 4225 - lr: 0.02733 - Train loss: 0.01223 - Test loss: 0.04070\n",
      "Epoch 4226 - lr: 0.02732 - Train loss: 0.01223 - Test loss: 0.04070\n",
      "Epoch 4227 - lr: 0.02731 - Train loss: 0.01223 - Test loss: 0.04070\n",
      "Epoch 4228 - lr: 0.02731 - Train loss: 0.01222 - Test loss: 0.04070\n",
      "Epoch 4229 - lr: 0.02730 - Train loss: 0.01222 - Test loss: 0.04070\n",
      "Epoch 4230 - lr: 0.02729 - Train loss: 0.01222 - Test loss: 0.04070\n",
      "Epoch 4231 - lr: 0.02728 - Train loss: 0.01222 - Test loss: 0.04070\n",
      "Epoch 4232 - lr: 0.02727 - Train loss: 0.01222 - Test loss: 0.04070\n",
      "Epoch 4233 - lr: 0.02726 - Train loss: 0.01222 - Test loss: 0.04070\n",
      "Epoch 4234 - lr: 0.02726 - Train loss: 0.01221 - Test loss: 0.04070\n",
      "Epoch 4235 - lr: 0.02725 - Train loss: 0.01221 - Test loss: 0.04070\n",
      "Epoch 4236 - lr: 0.02724 - Train loss: 0.01221 - Test loss: 0.04070\n",
      "Epoch 4237 - lr: 0.02723 - Train loss: 0.01221 - Test loss: 0.04070\n",
      "Epoch 4238 - lr: 0.02722 - Train loss: 0.01221 - Test loss: 0.04070\n",
      "Epoch 4239 - lr: 0.02721 - Train loss: 0.01221 - Test loss: 0.04070\n",
      "Epoch 4240 - lr: 0.02721 - Train loss: 0.01220 - Test loss: 0.04070\n",
      "Epoch 4241 - lr: 0.02720 - Train loss: 0.01220 - Test loss: 0.04070\n",
      "Epoch 4242 - lr: 0.02719 - Train loss: 0.01220 - Test loss: 0.04070\n",
      "Epoch 4243 - lr: 0.02718 - Train loss: 0.01220 - Test loss: 0.04070\n",
      "Epoch 4244 - lr: 0.02717 - Train loss: 0.01220 - Test loss: 0.04070\n",
      "Epoch 4245 - lr: 0.02716 - Train loss: 0.01220 - Test loss: 0.04069\n",
      "Epoch 4246 - lr: 0.02716 - Train loss: 0.01219 - Test loss: 0.04069\n",
      "Epoch 4247 - lr: 0.02715 - Train loss: 0.01219 - Test loss: 0.04069\n",
      "Epoch 4248 - lr: 0.02714 - Train loss: 0.01219 - Test loss: 0.04069\n",
      "Epoch 4249 - lr: 0.02713 - Train loss: 0.01219 - Test loss: 0.04069\n",
      "Epoch 4250 - lr: 0.02712 - Train loss: 0.01219 - Test loss: 0.04069\n",
      "Epoch 4251 - lr: 0.02711 - Train loss: 0.01218 - Test loss: 0.04069\n",
      "Epoch 4252 - lr: 0.02711 - Train loss: 0.01218 - Test loss: 0.04069\n",
      "Epoch 4253 - lr: 0.02710 - Train loss: 0.01218 - Test loss: 0.04069\n",
      "Epoch 4254 - lr: 0.02709 - Train loss: 0.01218 - Test loss: 0.04069\n",
      "Epoch 4255 - lr: 0.02708 - Train loss: 0.01218 - Test loss: 0.04069\n",
      "Epoch 4256 - lr: 0.02707 - Train loss: 0.01218 - Test loss: 0.04069\n",
      "Epoch 4257 - lr: 0.02706 - Train loss: 0.01217 - Test loss: 0.04069\n",
      "Epoch 4258 - lr: 0.02706 - Train loss: 0.01217 - Test loss: 0.04069\n",
      "Epoch 4259 - lr: 0.02705 - Train loss: 0.01217 - Test loss: 0.04069\n",
      "Epoch 4260 - lr: 0.02704 - Train loss: 0.01217 - Test loss: 0.04069\n",
      "Epoch 4261 - lr: 0.02703 - Train loss: 0.01217 - Test loss: 0.04069\n",
      "Epoch 4262 - lr: 0.02702 - Train loss: 0.01217 - Test loss: 0.04069\n",
      "Epoch 4263 - lr: 0.02701 - Train loss: 0.01216 - Test loss: 0.04069\n",
      "Epoch 4264 - lr: 0.02701 - Train loss: 0.01216 - Test loss: 0.04069\n",
      "Epoch 4265 - lr: 0.02700 - Train loss: 0.01216 - Test loss: 0.04069\n",
      "Epoch 4266 - lr: 0.02699 - Train loss: 0.01216 - Test loss: 0.04068\n",
      "Epoch 4267 - lr: 0.02698 - Train loss: 0.01216 - Test loss: 0.04068\n",
      "Epoch 4268 - lr: 0.02697 - Train loss: 0.01216 - Test loss: 0.04068\n",
      "Epoch 4269 - lr: 0.02696 - Train loss: 0.01215 - Test loss: 0.04068\n",
      "Epoch 4270 - lr: 0.02696 - Train loss: 0.01215 - Test loss: 0.04068\n",
      "Epoch 4271 - lr: 0.02695 - Train loss: 0.01215 - Test loss: 0.04068\n",
      "Epoch 4272 - lr: 0.02694 - Train loss: 0.01215 - Test loss: 0.04068\n",
      "Epoch 4273 - lr: 0.02693 - Train loss: 0.01215 - Test loss: 0.04068\n",
      "Epoch 4274 - lr: 0.02692 - Train loss: 0.01215 - Test loss: 0.04068\n",
      "Epoch 4275 - lr: 0.02692 - Train loss: 0.01214 - Test loss: 0.04068\n",
      "Epoch 4276 - lr: 0.02691 - Train loss: 0.01214 - Test loss: 0.04068\n",
      "Epoch 4277 - lr: 0.02690 - Train loss: 0.01214 - Test loss: 0.04068\n",
      "Epoch 4278 - lr: 0.02689 - Train loss: 0.01214 - Test loss: 0.04068\n",
      "Epoch 4279 - lr: 0.02688 - Train loss: 0.01214 - Test loss: 0.04068\n",
      "Epoch 4280 - lr: 0.02687 - Train loss: 0.01214 - Test loss: 0.04068\n",
      "Epoch 4281 - lr: 0.02687 - Train loss: 0.01213 - Test loss: 0.04068\n",
      "Epoch 4282 - lr: 0.02686 - Train loss: 0.01213 - Test loss: 0.04068\n",
      "Epoch 4283 - lr: 0.02685 - Train loss: 0.01213 - Test loss: 0.04068\n",
      "Epoch 4284 - lr: 0.02684 - Train loss: 0.01213 - Test loss: 0.04068\n",
      "Epoch 4285 - lr: 0.02683 - Train loss: 0.01213 - Test loss: 0.04068\n",
      "Epoch 4286 - lr: 0.02682 - Train loss: 0.01212 - Test loss: 0.04068\n",
      "Epoch 4287 - lr: 0.02682 - Train loss: 0.01212 - Test loss: 0.04067\n",
      "Epoch 4288 - lr: 0.02681 - Train loss: 0.01212 - Test loss: 0.04067\n",
      "Epoch 4289 - lr: 0.02680 - Train loss: 0.01212 - Test loss: 0.04067\n",
      "Epoch 4290 - lr: 0.02679 - Train loss: 0.01212 - Test loss: 0.04067\n",
      "Epoch 4291 - lr: 0.02678 - Train loss: 0.01212 - Test loss: 0.04067\n",
      "Epoch 4292 - lr: 0.02678 - Train loss: 0.01211 - Test loss: 0.04067\n",
      "Epoch 4293 - lr: 0.02677 - Train loss: 0.01211 - Test loss: 0.04067\n",
      "Epoch 4294 - lr: 0.02676 - Train loss: 0.01211 - Test loss: 0.04067\n",
      "Epoch 4295 - lr: 0.02675 - Train loss: 0.01211 - Test loss: 0.04067\n",
      "Epoch 4296 - lr: 0.02674 - Train loss: 0.01211 - Test loss: 0.04067\n",
      "Epoch 4297 - lr: 0.02673 - Train loss: 0.01211 - Test loss: 0.04067\n",
      "Epoch 4298 - lr: 0.02673 - Train loss: 0.01210 - Test loss: 0.04067\n",
      "Epoch 4299 - lr: 0.02672 - Train loss: 0.01210 - Test loss: 0.04067\n",
      "Epoch 4300 - lr: 0.02671 - Train loss: 0.01210 - Test loss: 0.04067\n",
      "Epoch 4301 - lr: 0.02670 - Train loss: 0.01210 - Test loss: 0.04067\n",
      "Epoch 4302 - lr: 0.02669 - Train loss: 0.01210 - Test loss: 0.04067\n",
      "Epoch 4303 - lr: 0.02668 - Train loss: 0.01210 - Test loss: 0.04067\n",
      "Epoch 4304 - lr: 0.02668 - Train loss: 0.01209 - Test loss: 0.04067\n",
      "Epoch 4305 - lr: 0.02667 - Train loss: 0.01209 - Test loss: 0.04067\n",
      "Epoch 4306 - lr: 0.02666 - Train loss: 0.01209 - Test loss: 0.04067\n",
      "Epoch 4307 - lr: 0.02665 - Train loss: 0.01209 - Test loss: 0.04067\n",
      "Epoch 4308 - lr: 0.02664 - Train loss: 0.01209 - Test loss: 0.04066\n",
      "Epoch 4309 - lr: 0.02664 - Train loss: 0.01209 - Test loss: 0.04066\n",
      "Epoch 4310 - lr: 0.02663 - Train loss: 0.01208 - Test loss: 0.04066\n",
      "Epoch 4311 - lr: 0.02662 - Train loss: 0.01208 - Test loss: 0.04066\n",
      "Epoch 4312 - lr: 0.02661 - Train loss: 0.01208 - Test loss: 0.04066\n",
      "Epoch 4313 - lr: 0.02660 - Train loss: 0.01208 - Test loss: 0.04066\n",
      "Epoch 4314 - lr: 0.02660 - Train loss: 0.01208 - Test loss: 0.04066\n",
      "Epoch 4315 - lr: 0.02659 - Train loss: 0.01208 - Test loss: 0.04066\n",
      "Epoch 4316 - lr: 0.02658 - Train loss: 0.01207 - Test loss: 0.04066\n",
      "Epoch 4317 - lr: 0.02657 - Train loss: 0.01207 - Test loss: 0.04066\n",
      "Epoch 4318 - lr: 0.02656 - Train loss: 0.01207 - Test loss: 0.04066\n",
      "Epoch 4319 - lr: 0.02655 - Train loss: 0.01207 - Test loss: 0.04066\n",
      "Epoch 4320 - lr: 0.02655 - Train loss: 0.01207 - Test loss: 0.04066\n",
      "Epoch 4321 - lr: 0.02654 - Train loss: 0.01207 - Test loss: 0.04066\n",
      "Epoch 4322 - lr: 0.02653 - Train loss: 0.01206 - Test loss: 0.04066\n",
      "Epoch 4323 - lr: 0.02652 - Train loss: 0.01206 - Test loss: 0.04066\n",
      "Epoch 4324 - lr: 0.02651 - Train loss: 0.01206 - Test loss: 0.04066\n",
      "Epoch 4325 - lr: 0.02651 - Train loss: 0.01206 - Test loss: 0.04066\n",
      "Epoch 4326 - lr: 0.02650 - Train loss: 0.01206 - Test loss: 0.04066\n",
      "Epoch 4327 - lr: 0.02649 - Train loss: 0.01206 - Test loss: 0.04066\n",
      "Epoch 4328 - lr: 0.02648 - Train loss: 0.01205 - Test loss: 0.04066\n",
      "Epoch 4329 - lr: 0.02647 - Train loss: 0.01205 - Test loss: 0.04066\n",
      "Epoch 4330 - lr: 0.02646 - Train loss: 0.01205 - Test loss: 0.04065\n",
      "Epoch 4331 - lr: 0.02646 - Train loss: 0.01205 - Test loss: 0.04065\n",
      "Epoch 4332 - lr: 0.02645 - Train loss: 0.01205 - Test loss: 0.04065\n",
      "Epoch 4333 - lr: 0.02644 - Train loss: 0.01205 - Test loss: 0.04065\n",
      "Epoch 4334 - lr: 0.02643 - Train loss: 0.01204 - Test loss: 0.04065\n",
      "Epoch 4335 - lr: 0.02642 - Train loss: 0.01204 - Test loss: 0.04065\n",
      "Epoch 4336 - lr: 0.02642 - Train loss: 0.01204 - Test loss: 0.04065\n",
      "Epoch 4337 - lr: 0.02641 - Train loss: 0.01204 - Test loss: 0.04065\n",
      "Epoch 4338 - lr: 0.02640 - Train loss: 0.01204 - Test loss: 0.04065\n",
      "Epoch 4339 - lr: 0.02639 - Train loss: 0.01204 - Test loss: 0.04065\n",
      "Epoch 4340 - lr: 0.02638 - Train loss: 0.01203 - Test loss: 0.04065\n",
      "Epoch 4341 - lr: 0.02638 - Train loss: 0.01203 - Test loss: 0.04065\n",
      "Epoch 4342 - lr: 0.02637 - Train loss: 0.01203 - Test loss: 0.04065\n",
      "Epoch 4343 - lr: 0.02636 - Train loss: 0.01203 - Test loss: 0.04065\n",
      "Epoch 4344 - lr: 0.02635 - Train loss: 0.01203 - Test loss: 0.04065\n",
      "Epoch 4345 - lr: 0.02634 - Train loss: 0.01203 - Test loss: 0.04065\n",
      "Epoch 4346 - lr: 0.02634 - Train loss: 0.01202 - Test loss: 0.04065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4347 - lr: 0.02633 - Train loss: 0.01202 - Test loss: 0.04065\n",
      "Epoch 4348 - lr: 0.02632 - Train loss: 0.01202 - Test loss: 0.04065\n",
      "Epoch 4349 - lr: 0.02631 - Train loss: 0.01202 - Test loss: 0.04065\n",
      "Epoch 4350 - lr: 0.02630 - Train loss: 0.01202 - Test loss: 0.04065\n",
      "Epoch 4351 - lr: 0.02629 - Train loss: 0.01202 - Test loss: 0.04065\n",
      "Epoch 4352 - lr: 0.02629 - Train loss: 0.01201 - Test loss: 0.04065\n",
      "Epoch 4353 - lr: 0.02628 - Train loss: 0.01201 - Test loss: 0.04064\n",
      "Epoch 4354 - lr: 0.02627 - Train loss: 0.01201 - Test loss: 0.04064\n",
      "Epoch 4355 - lr: 0.02626 - Train loss: 0.01201 - Test loss: 0.04064\n",
      "Epoch 4356 - lr: 0.02625 - Train loss: 0.01201 - Test loss: 0.04064\n",
      "Epoch 4357 - lr: 0.02625 - Train loss: 0.01201 - Test loss: 0.04064\n",
      "Epoch 4358 - lr: 0.02624 - Train loss: 0.01200 - Test loss: 0.04064\n",
      "Epoch 4359 - lr: 0.02623 - Train loss: 0.01200 - Test loss: 0.04064\n",
      "Epoch 4360 - lr: 0.02622 - Train loss: 0.01200 - Test loss: 0.04064\n",
      "Epoch 4361 - lr: 0.02621 - Train loss: 0.01200 - Test loss: 0.04064\n",
      "Epoch 4362 - lr: 0.02621 - Train loss: 0.01200 - Test loss: 0.04064\n",
      "Epoch 4363 - lr: 0.02620 - Train loss: 0.01200 - Test loss: 0.04064\n",
      "Epoch 4364 - lr: 0.02619 - Train loss: 0.01199 - Test loss: 0.04064\n",
      "Epoch 4365 - lr: 0.02618 - Train loss: 0.01199 - Test loss: 0.04064\n",
      "Epoch 4366 - lr: 0.02617 - Train loss: 0.01199 - Test loss: 0.04064\n",
      "Epoch 4367 - lr: 0.02617 - Train loss: 0.01199 - Test loss: 0.04064\n",
      "Epoch 4368 - lr: 0.02616 - Train loss: 0.01199 - Test loss: 0.04064\n",
      "Epoch 4369 - lr: 0.02615 - Train loss: 0.01199 - Test loss: 0.04064\n",
      "Epoch 4370 - lr: 0.02614 - Train loss: 0.01198 - Test loss: 0.04064\n",
      "Epoch 4371 - lr: 0.02613 - Train loss: 0.01198 - Test loss: 0.04064\n",
      "Epoch 4372 - lr: 0.02613 - Train loss: 0.01198 - Test loss: 0.04064\n",
      "Epoch 4373 - lr: 0.02612 - Train loss: 0.01198 - Test loss: 0.04064\n",
      "Epoch 4374 - lr: 0.02611 - Train loss: 0.01198 - Test loss: 0.04064\n",
      "Epoch 4375 - lr: 0.02610 - Train loss: 0.01198 - Test loss: 0.04064\n",
      "Epoch 4376 - lr: 0.02609 - Train loss: 0.01197 - Test loss: 0.04063\n",
      "Epoch 4377 - lr: 0.02609 - Train loss: 0.01197 - Test loss: 0.04063\n",
      "Epoch 4378 - lr: 0.02608 - Train loss: 0.01197 - Test loss: 0.04063\n",
      "Epoch 4379 - lr: 0.02607 - Train loss: 0.01197 - Test loss: 0.04063\n",
      "Epoch 4380 - lr: 0.02606 - Train loss: 0.01197 - Test loss: 0.04063\n",
      "Epoch 4381 - lr: 0.02605 - Train loss: 0.01197 - Test loss: 0.04063\n",
      "Epoch 4382 - lr: 0.02605 - Train loss: 0.01196 - Test loss: 0.04063\n",
      "Epoch 4383 - lr: 0.02604 - Train loss: 0.01196 - Test loss: 0.04063\n",
      "Epoch 4384 - lr: 0.02603 - Train loss: 0.01196 - Test loss: 0.04063\n",
      "Epoch 4385 - lr: 0.02602 - Train loss: 0.01196 - Test loss: 0.04063\n",
      "Epoch 4386 - lr: 0.02601 - Train loss: 0.01196 - Test loss: 0.04063\n",
      "Epoch 4387 - lr: 0.02601 - Train loss: 0.01196 - Test loss: 0.04063\n",
      "Epoch 4388 - lr: 0.02600 - Train loss: 0.01195 - Test loss: 0.04063\n",
      "Epoch 4389 - lr: 0.02599 - Train loss: 0.01195 - Test loss: 0.04063\n",
      "Epoch 4390 - lr: 0.02598 - Train loss: 0.01195 - Test loss: 0.04063\n",
      "Epoch 4391 - lr: 0.02597 - Train loss: 0.01195 - Test loss: 0.04063\n",
      "Epoch 4392 - lr: 0.02597 - Train loss: 0.01195 - Test loss: 0.04063\n",
      "Epoch 4393 - lr: 0.02596 - Train loss: 0.01195 - Test loss: 0.04063\n",
      "Epoch 4394 - lr: 0.02595 - Train loss: 0.01194 - Test loss: 0.04063\n",
      "Epoch 4395 - lr: 0.02594 - Train loss: 0.01194 - Test loss: 0.04063\n",
      "Epoch 4396 - lr: 0.02593 - Train loss: 0.01194 - Test loss: 0.04063\n",
      "Epoch 4397 - lr: 0.02593 - Train loss: 0.01194 - Test loss: 0.04063\n",
      "Epoch 4398 - lr: 0.02592 - Train loss: 0.01194 - Test loss: 0.04063\n",
      "Epoch 4399 - lr: 0.02591 - Train loss: 0.01194 - Test loss: 0.04063\n",
      "Epoch 4400 - lr: 0.02590 - Train loss: 0.01193 - Test loss: 0.04062\n",
      "Epoch 4401 - lr: 0.02589 - Train loss: 0.01193 - Test loss: 0.04062\n",
      "Epoch 4402 - lr: 0.02589 - Train loss: 0.01193 - Test loss: 0.04062\n",
      "Epoch 4403 - lr: 0.02588 - Train loss: 0.01193 - Test loss: 0.04062\n",
      "Epoch 4404 - lr: 0.02587 - Train loss: 0.01193 - Test loss: 0.04062\n",
      "Epoch 4405 - lr: 0.02586 - Train loss: 0.01193 - Test loss: 0.04062\n",
      "Epoch 4406 - lr: 0.02585 - Train loss: 0.01192 - Test loss: 0.04062\n",
      "Epoch 4407 - lr: 0.02585 - Train loss: 0.01192 - Test loss: 0.04062\n",
      "Epoch 4408 - lr: 0.02584 - Train loss: 0.01192 - Test loss: 0.04062\n",
      "Epoch 4409 - lr: 0.02583 - Train loss: 0.01192 - Test loss: 0.04062\n",
      "Epoch 4410 - lr: 0.02582 - Train loss: 0.01192 - Test loss: 0.04062\n",
      "Epoch 4411 - lr: 0.02581 - Train loss: 0.01192 - Test loss: 0.04062\n",
      "Epoch 4412 - lr: 0.02581 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4413 - lr: 0.02580 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4414 - lr: 0.02579 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4415 - lr: 0.02578 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4416 - lr: 0.02578 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4417 - lr: 0.02577 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4418 - lr: 0.02576 - Train loss: 0.01191 - Test loss: 0.04062\n",
      "Epoch 4419 - lr: 0.02575 - Train loss: 0.01190 - Test loss: 0.04062\n",
      "Epoch 4420 - lr: 0.02574 - Train loss: 0.01190 - Test loss: 0.04062\n",
      "Epoch 4421 - lr: 0.02574 - Train loss: 0.01190 - Test loss: 0.04062\n",
      "Epoch 4422 - lr: 0.02573 - Train loss: 0.01190 - Test loss: 0.04062\n",
      "Epoch 4423 - lr: 0.02572 - Train loss: 0.01190 - Test loss: 0.04062\n",
      "Epoch 4424 - lr: 0.02571 - Train loss: 0.01190 - Test loss: 0.04061\n",
      "Epoch 4425 - lr: 0.02570 - Train loss: 0.01189 - Test loss: 0.04061\n",
      "Epoch 4426 - lr: 0.02570 - Train loss: 0.01189 - Test loss: 0.04061\n",
      "Epoch 4427 - lr: 0.02569 - Train loss: 0.01189 - Test loss: 0.04061\n",
      "Epoch 4428 - lr: 0.02568 - Train loss: 0.01189 - Test loss: 0.04061\n",
      "Epoch 4429 - lr: 0.02567 - Train loss: 0.01189 - Test loss: 0.04061\n",
      "Epoch 4430 - lr: 0.02566 - Train loss: 0.01189 - Test loss: 0.04061\n",
      "Epoch 4431 - lr: 0.02566 - Train loss: 0.01188 - Test loss: 0.04061\n",
      "Epoch 4432 - lr: 0.02565 - Train loss: 0.01188 - Test loss: 0.04061\n",
      "Epoch 4433 - lr: 0.02564 - Train loss: 0.01188 - Test loss: 0.04061\n",
      "Epoch 4434 - lr: 0.02563 - Train loss: 0.01188 - Test loss: 0.04061\n",
      "Epoch 4435 - lr: 0.02563 - Train loss: 0.01188 - Test loss: 0.04061\n",
      "Epoch 4436 - lr: 0.02562 - Train loss: 0.01188 - Test loss: 0.04061\n",
      "Epoch 4437 - lr: 0.02561 - Train loss: 0.01187 - Test loss: 0.04061\n",
      "Epoch 4438 - lr: 0.02560 - Train loss: 0.01187 - Test loss: 0.04061\n",
      "Epoch 4439 - lr: 0.02559 - Train loss: 0.01187 - Test loss: 0.04061\n",
      "Epoch 4440 - lr: 0.02559 - Train loss: 0.01187 - Test loss: 0.04061\n",
      "Epoch 4441 - lr: 0.02558 - Train loss: 0.01187 - Test loss: 0.04061\n",
      "Epoch 4442 - lr: 0.02557 - Train loss: 0.01187 - Test loss: 0.04061\n",
      "Epoch 4443 - lr: 0.02556 - Train loss: 0.01186 - Test loss: 0.04061\n",
      "Epoch 4444 - lr: 0.02555 - Train loss: 0.01186 - Test loss: 0.04061\n",
      "Epoch 4445 - lr: 0.02555 - Train loss: 0.01186 - Test loss: 0.04061\n",
      "Epoch 4446 - lr: 0.02554 - Train loss: 0.01186 - Test loss: 0.04061\n",
      "Epoch 4447 - lr: 0.02553 - Train loss: 0.01186 - Test loss: 0.04061\n",
      "Epoch 4448 - lr: 0.02552 - Train loss: 0.01186 - Test loss: 0.04061\n",
      "Epoch 4449 - lr: 0.02552 - Train loss: 0.01186 - Test loss: 0.04060\n",
      "Epoch 4450 - lr: 0.02551 - Train loss: 0.01185 - Test loss: 0.04060\n",
      "Epoch 4451 - lr: 0.02550 - Train loss: 0.01185 - Test loss: 0.04060\n",
      "Epoch 4452 - lr: 0.02549 - Train loss: 0.01185 - Test loss: 0.04060\n",
      "Epoch 4453 - lr: 0.02548 - Train loss: 0.01185 - Test loss: 0.04060\n",
      "Epoch 4454 - lr: 0.02548 - Train loss: 0.01185 - Test loss: 0.04060\n",
      "Epoch 4455 - lr: 0.02547 - Train loss: 0.01185 - Test loss: 0.04060\n",
      "Epoch 4456 - lr: 0.02546 - Train loss: 0.01184 - Test loss: 0.04060\n",
      "Epoch 4457 - lr: 0.02545 - Train loss: 0.01184 - Test loss: 0.04060\n",
      "Epoch 4458 - lr: 0.02544 - Train loss: 0.01184 - Test loss: 0.04060\n",
      "Epoch 4459 - lr: 0.02544 - Train loss: 0.01184 - Test loss: 0.04060\n",
      "Epoch 4460 - lr: 0.02543 - Train loss: 0.01184 - Test loss: 0.04060\n",
      "Epoch 4461 - lr: 0.02542 - Train loss: 0.01184 - Test loss: 0.04060\n",
      "Epoch 4462 - lr: 0.02541 - Train loss: 0.01183 - Test loss: 0.04060\n",
      "Epoch 4463 - lr: 0.02541 - Train loss: 0.01183 - Test loss: 0.04060\n",
      "Epoch 4464 - lr: 0.02540 - Train loss: 0.01183 - Test loss: 0.04060\n",
      "Epoch 4465 - lr: 0.02539 - Train loss: 0.01183 - Test loss: 0.04060\n",
      "Epoch 4466 - lr: 0.02538 - Train loss: 0.01183 - Test loss: 0.04060\n",
      "Epoch 4467 - lr: 0.02537 - Train loss: 0.01183 - Test loss: 0.04060\n",
      "Epoch 4468 - lr: 0.02537 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4469 - lr: 0.02536 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4470 - lr: 0.02535 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4471 - lr: 0.02534 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4472 - lr: 0.02534 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4473 - lr: 0.02533 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4474 - lr: 0.02532 - Train loss: 0.01182 - Test loss: 0.04060\n",
      "Epoch 4475 - lr: 0.02531 - Train loss: 0.01181 - Test loss: 0.04059\n",
      "Epoch 4476 - lr: 0.02530 - Train loss: 0.01181 - Test loss: 0.04059\n",
      "Epoch 4477 - lr: 0.02530 - Train loss: 0.01181 - Test loss: 0.04059\n",
      "Epoch 4478 - lr: 0.02529 - Train loss: 0.01181 - Test loss: 0.04059\n",
      "Epoch 4479 - lr: 0.02528 - Train loss: 0.01181 - Test loss: 0.04059\n",
      "Epoch 4480 - lr: 0.02527 - Train loss: 0.01181 - Test loss: 0.04059\n",
      "Epoch 4481 - lr: 0.02527 - Train loss: 0.01180 - Test loss: 0.04059\n",
      "Epoch 4482 - lr: 0.02526 - Train loss: 0.01180 - Test loss: 0.04059\n",
      "Epoch 4483 - lr: 0.02525 - Train loss: 0.01180 - Test loss: 0.04059\n",
      "Epoch 4484 - lr: 0.02524 - Train loss: 0.01180 - Test loss: 0.04059\n",
      "Epoch 4485 - lr: 0.02523 - Train loss: 0.01180 - Test loss: 0.04059\n",
      "Epoch 4486 - lr: 0.02523 - Train loss: 0.01180 - Test loss: 0.04059\n",
      "Epoch 4487 - lr: 0.02522 - Train loss: 0.01179 - Test loss: 0.04059\n",
      "Epoch 4488 - lr: 0.02521 - Train loss: 0.01179 - Test loss: 0.04059\n",
      "Epoch 4489 - lr: 0.02520 - Train loss: 0.01179 - Test loss: 0.04059\n",
      "Epoch 4490 - lr: 0.02520 - Train loss: 0.01179 - Test loss: 0.04059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4491 - lr: 0.02519 - Train loss: 0.01179 - Test loss: 0.04059\n",
      "Epoch 4492 - lr: 0.02518 - Train loss: 0.01179 - Test loss: 0.04059\n",
      "Epoch 4493 - lr: 0.02517 - Train loss: 0.01179 - Test loss: 0.04059\n",
      "Epoch 4494 - lr: 0.02517 - Train loss: 0.01178 - Test loss: 0.04059\n",
      "Epoch 4495 - lr: 0.02516 - Train loss: 0.01178 - Test loss: 0.04059\n",
      "Epoch 4496 - lr: 0.02515 - Train loss: 0.01178 - Test loss: 0.04059\n",
      "Epoch 4497 - lr: 0.02514 - Train loss: 0.01178 - Test loss: 0.04059\n",
      "Epoch 4498 - lr: 0.02513 - Train loss: 0.01178 - Test loss: 0.04059\n",
      "Epoch 4499 - lr: 0.02513 - Train loss: 0.01178 - Test loss: 0.04059\n",
      "Epoch 4500 - lr: 0.02512 - Train loss: 0.01177 - Test loss: 0.04059\n",
      "Epoch 4501 - lr: 0.02511 - Train loss: 0.01177 - Test loss: 0.04059\n",
      "Epoch 4502 - lr: 0.02510 - Train loss: 0.01177 - Test loss: 0.04058\n",
      "Epoch 4503 - lr: 0.02510 - Train loss: 0.01177 - Test loss: 0.04058\n",
      "Epoch 4504 - lr: 0.02509 - Train loss: 0.01177 - Test loss: 0.04058\n",
      "Epoch 4505 - lr: 0.02508 - Train loss: 0.01177 - Test loss: 0.04058\n",
      "Epoch 4506 - lr: 0.02507 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4507 - lr: 0.02506 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4508 - lr: 0.02506 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4509 - lr: 0.02505 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4510 - lr: 0.02504 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4511 - lr: 0.02503 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4512 - lr: 0.02503 - Train loss: 0.01176 - Test loss: 0.04058\n",
      "Epoch 4513 - lr: 0.02502 - Train loss: 0.01175 - Test loss: 0.04058\n",
      "Epoch 4514 - lr: 0.02501 - Train loss: 0.01175 - Test loss: 0.04058\n",
      "Epoch 4515 - lr: 0.02500 - Train loss: 0.01175 - Test loss: 0.04058\n",
      "Epoch 4516 - lr: 0.02500 - Train loss: 0.01175 - Test loss: 0.04058\n",
      "Epoch 4517 - lr: 0.02499 - Train loss: 0.01175 - Test loss: 0.04058\n",
      "Epoch 4518 - lr: 0.02498 - Train loss: 0.01175 - Test loss: 0.04058\n",
      "Epoch 4519 - lr: 0.02497 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4520 - lr: 0.02497 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4521 - lr: 0.02496 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4522 - lr: 0.02495 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4523 - lr: 0.02494 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4524 - lr: 0.02493 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4525 - lr: 0.02493 - Train loss: 0.01174 - Test loss: 0.04058\n",
      "Epoch 4526 - lr: 0.02492 - Train loss: 0.01173 - Test loss: 0.04058\n",
      "Epoch 4527 - lr: 0.02491 - Train loss: 0.01173 - Test loss: 0.04058\n",
      "Epoch 4528 - lr: 0.02490 - Train loss: 0.01173 - Test loss: 0.04058\n",
      "Epoch 4529 - lr: 0.02490 - Train loss: 0.01173 - Test loss: 0.04057\n",
      "Epoch 4530 - lr: 0.02489 - Train loss: 0.01173 - Test loss: 0.04057\n",
      "Epoch 4531 - lr: 0.02488 - Train loss: 0.01173 - Test loss: 0.04057\n",
      "Epoch 4532 - lr: 0.02487 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4533 - lr: 0.02487 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4534 - lr: 0.02486 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4535 - lr: 0.02485 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4536 - lr: 0.02484 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4537 - lr: 0.02484 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4538 - lr: 0.02483 - Train loss: 0.01172 - Test loss: 0.04057\n",
      "Epoch 4539 - lr: 0.02482 - Train loss: 0.01171 - Test loss: 0.04057\n",
      "Epoch 4540 - lr: 0.02481 - Train loss: 0.01171 - Test loss: 0.04057\n",
      "Epoch 4541 - lr: 0.02480 - Train loss: 0.01171 - Test loss: 0.04057\n",
      "Epoch 4542 - lr: 0.02480 - Train loss: 0.01171 - Test loss: 0.04057\n",
      "Epoch 4543 - lr: 0.02479 - Train loss: 0.01171 - Test loss: 0.04057\n",
      "Epoch 4544 - lr: 0.02478 - Train loss: 0.01171 - Test loss: 0.04057\n",
      "Epoch 4545 - lr: 0.02477 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4546 - lr: 0.02477 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4547 - lr: 0.02476 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4548 - lr: 0.02475 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4549 - lr: 0.02474 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4550 - lr: 0.02474 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4551 - lr: 0.02473 - Train loss: 0.01170 - Test loss: 0.04057\n",
      "Epoch 4552 - lr: 0.02472 - Train loss: 0.01169 - Test loss: 0.04057\n",
      "Epoch 4553 - lr: 0.02471 - Train loss: 0.01169 - Test loss: 0.04057\n",
      "Epoch 4554 - lr: 0.02471 - Train loss: 0.01169 - Test loss: 0.04057\n",
      "Epoch 4555 - lr: 0.02470 - Train loss: 0.01169 - Test loss: 0.04057\n",
      "Epoch 4556 - lr: 0.02469 - Train loss: 0.01169 - Test loss: 0.04057\n",
      "Epoch 4557 - lr: 0.02468 - Train loss: 0.01169 - Test loss: 0.04056\n",
      "Epoch 4558 - lr: 0.02468 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4559 - lr: 0.02467 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4560 - lr: 0.02466 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4561 - lr: 0.02465 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4562 - lr: 0.02465 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4563 - lr: 0.02464 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4564 - lr: 0.02463 - Train loss: 0.01168 - Test loss: 0.04056\n",
      "Epoch 4565 - lr: 0.02462 - Train loss: 0.01167 - Test loss: 0.04056\n",
      "Epoch 4566 - lr: 0.02462 - Train loss: 0.01167 - Test loss: 0.04056\n",
      "Epoch 4567 - lr: 0.02461 - Train loss: 0.01167 - Test loss: 0.04056\n",
      "Epoch 4568 - lr: 0.02460 - Train loss: 0.01167 - Test loss: 0.04056\n",
      "Epoch 4569 - lr: 0.02459 - Train loss: 0.01167 - Test loss: 0.04056\n",
      "Epoch 4570 - lr: 0.02458 - Train loss: 0.01167 - Test loss: 0.04056\n",
      "Epoch 4571 - lr: 0.02458 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4572 - lr: 0.02457 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4573 - lr: 0.02456 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4574 - lr: 0.02455 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4575 - lr: 0.02455 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4576 - lr: 0.02454 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4577 - lr: 0.02453 - Train loss: 0.01166 - Test loss: 0.04056\n",
      "Epoch 4578 - lr: 0.02452 - Train loss: 0.01165 - Test loss: 0.04056\n",
      "Epoch 4579 - lr: 0.02452 - Train loss: 0.01165 - Test loss: 0.04056\n",
      "Epoch 4580 - lr: 0.02451 - Train loss: 0.01165 - Test loss: 0.04056\n",
      "Epoch 4581 - lr: 0.02450 - Train loss: 0.01165 - Test loss: 0.04056\n",
      "Epoch 4582 - lr: 0.02449 - Train loss: 0.01165 - Test loss: 0.04056\n",
      "Epoch 4583 - lr: 0.02449 - Train loss: 0.01165 - Test loss: 0.04056\n",
      "Epoch 4584 - lr: 0.02448 - Train loss: 0.01164 - Test loss: 0.04056\n",
      "Epoch 4585 - lr: 0.02447 - Train loss: 0.01164 - Test loss: 0.04056\n",
      "Epoch 4586 - lr: 0.02446 - Train loss: 0.01164 - Test loss: 0.04055\n",
      "Epoch 4587 - lr: 0.02446 - Train loss: 0.01164 - Test loss: 0.04055\n",
      "Epoch 4588 - lr: 0.02445 - Train loss: 0.01164 - Test loss: 0.04055\n",
      "Epoch 4589 - lr: 0.02444 - Train loss: 0.01164 - Test loss: 0.04055\n",
      "Epoch 4590 - lr: 0.02443 - Train loss: 0.01164 - Test loss: 0.04055\n",
      "Epoch 4591 - lr: 0.02443 - Train loss: 0.01163 - Test loss: 0.04055\n",
      "Epoch 4592 - lr: 0.02442 - Train loss: 0.01163 - Test loss: 0.04055\n",
      "Epoch 4593 - lr: 0.02441 - Train loss: 0.01163 - Test loss: 0.04055\n",
      "Epoch 4594 - lr: 0.02440 - Train loss: 0.01163 - Test loss: 0.04055\n",
      "Epoch 4595 - lr: 0.02440 - Train loss: 0.01163 - Test loss: 0.04055\n",
      "Epoch 4596 - lr: 0.02439 - Train loss: 0.01163 - Test loss: 0.04055\n",
      "Epoch 4597 - lr: 0.02438 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4598 - lr: 0.02437 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4599 - lr: 0.02437 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4600 - lr: 0.02436 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4601 - lr: 0.02435 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4602 - lr: 0.02434 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4603 - lr: 0.02434 - Train loss: 0.01162 - Test loss: 0.04055\n",
      "Epoch 4604 - lr: 0.02433 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4605 - lr: 0.02432 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4606 - lr: 0.02431 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4607 - lr: 0.02431 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4608 - lr: 0.02430 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4609 - lr: 0.02429 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4610 - lr: 0.02428 - Train loss: 0.01161 - Test loss: 0.04055\n",
      "Epoch 4611 - lr: 0.02428 - Train loss: 0.01160 - Test loss: 0.04055\n",
      "Epoch 4612 - lr: 0.02427 - Train loss: 0.01160 - Test loss: 0.04055\n",
      "Epoch 4613 - lr: 0.02426 - Train loss: 0.01160 - Test loss: 0.04055\n",
      "Epoch 4614 - lr: 0.02425 - Train loss: 0.01160 - Test loss: 0.04055\n",
      "Epoch 4615 - lr: 0.02425 - Train loss: 0.01160 - Test loss: 0.04055\n",
      "Epoch 4616 - lr: 0.02424 - Train loss: 0.01160 - Test loss: 0.04054\n",
      "Epoch 4617 - lr: 0.02423 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4618 - lr: 0.02423 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4619 - lr: 0.02422 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4620 - lr: 0.02421 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4621 - lr: 0.02420 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4622 - lr: 0.02420 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4623 - lr: 0.02419 - Train loss: 0.01159 - Test loss: 0.04054\n",
      "Epoch 4624 - lr: 0.02418 - Train loss: 0.01158 - Test loss: 0.04054\n",
      "Epoch 4625 - lr: 0.02417 - Train loss: 0.01158 - Test loss: 0.04054\n",
      "Epoch 4626 - lr: 0.02417 - Train loss: 0.01158 - Test loss: 0.04054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4627 - lr: 0.02416 - Train loss: 0.01158 - Test loss: 0.04054\n",
      "Epoch 4628 - lr: 0.02415 - Train loss: 0.01158 - Test loss: 0.04054\n",
      "Epoch 4629 - lr: 0.02414 - Train loss: 0.01158 - Test loss: 0.04054\n",
      "Epoch 4630 - lr: 0.02414 - Train loss: 0.01158 - Test loss: 0.04054\n",
      "Epoch 4631 - lr: 0.02413 - Train loss: 0.01157 - Test loss: 0.04054\n",
      "Epoch 4632 - lr: 0.02412 - Train loss: 0.01157 - Test loss: 0.04054\n",
      "Epoch 4633 - lr: 0.02411 - Train loss: 0.01157 - Test loss: 0.04054\n",
      "Epoch 4634 - lr: 0.02411 - Train loss: 0.01157 - Test loss: 0.04054\n",
      "Epoch 4635 - lr: 0.02410 - Train loss: 0.01157 - Test loss: 0.04054\n",
      "Epoch 4636 - lr: 0.02409 - Train loss: 0.01157 - Test loss: 0.04054\n",
      "Epoch 4637 - lr: 0.02408 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4638 - lr: 0.02408 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4639 - lr: 0.02407 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4640 - lr: 0.02406 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4641 - lr: 0.02405 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4642 - lr: 0.02405 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4643 - lr: 0.02404 - Train loss: 0.01156 - Test loss: 0.04054\n",
      "Epoch 4644 - lr: 0.02403 - Train loss: 0.01155 - Test loss: 0.04054\n",
      "Epoch 4645 - lr: 0.02403 - Train loss: 0.01155 - Test loss: 0.04054\n",
      "Epoch 4646 - lr: 0.02402 - Train loss: 0.01155 - Test loss: 0.04053\n",
      "Epoch 4647 - lr: 0.02401 - Train loss: 0.01155 - Test loss: 0.04053\n",
      "Epoch 4648 - lr: 0.02400 - Train loss: 0.01155 - Test loss: 0.04053\n",
      "Epoch 4649 - lr: 0.02400 - Train loss: 0.01155 - Test loss: 0.04053\n",
      "Epoch 4650 - lr: 0.02399 - Train loss: 0.01155 - Test loss: 0.04053\n",
      "Epoch 4651 - lr: 0.02398 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4652 - lr: 0.02397 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4653 - lr: 0.02397 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4654 - lr: 0.02396 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4655 - lr: 0.02395 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4656 - lr: 0.02394 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4657 - lr: 0.02394 - Train loss: 0.01154 - Test loss: 0.04053\n",
      "Epoch 4658 - lr: 0.02393 - Train loss: 0.01153 - Test loss: 0.04053\n",
      "Epoch 4659 - lr: 0.02392 - Train loss: 0.01153 - Test loss: 0.04053\n",
      "Epoch 4660 - lr: 0.02391 - Train loss: 0.01153 - Test loss: 0.04053\n",
      "Epoch 4661 - lr: 0.02391 - Train loss: 0.01153 - Test loss: 0.04053\n",
      "Epoch 4662 - lr: 0.02390 - Train loss: 0.01153 - Test loss: 0.04053\n",
      "Epoch 4663 - lr: 0.02389 - Train loss: 0.01153 - Test loss: 0.04053\n",
      "Epoch 4664 - lr: 0.02389 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4665 - lr: 0.02388 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4666 - lr: 0.02387 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4667 - lr: 0.02386 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4668 - lr: 0.02386 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4669 - lr: 0.02385 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4670 - lr: 0.02384 - Train loss: 0.01152 - Test loss: 0.04053\n",
      "Epoch 4671 - lr: 0.02383 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4672 - lr: 0.02383 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4673 - lr: 0.02382 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4674 - lr: 0.02381 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4675 - lr: 0.02380 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4676 - lr: 0.02380 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4677 - lr: 0.02379 - Train loss: 0.01151 - Test loss: 0.04053\n",
      "Epoch 4678 - lr: 0.02378 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4679 - lr: 0.02378 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4680 - lr: 0.02377 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4681 - lr: 0.02376 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4682 - lr: 0.02375 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4683 - lr: 0.02375 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4684 - lr: 0.02374 - Train loss: 0.01150 - Test loss: 0.04052\n",
      "Epoch 4685 - lr: 0.02373 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4686 - lr: 0.02372 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4687 - lr: 0.02372 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4688 - lr: 0.02371 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4689 - lr: 0.02370 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4690 - lr: 0.02370 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4691 - lr: 0.02369 - Train loss: 0.01149 - Test loss: 0.04052\n",
      "Epoch 4692 - lr: 0.02368 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4693 - lr: 0.02367 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4694 - lr: 0.02367 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4695 - lr: 0.02366 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4696 - lr: 0.02365 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4697 - lr: 0.02364 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4698 - lr: 0.02364 - Train loss: 0.01148 - Test loss: 0.04052\n",
      "Epoch 4699 - lr: 0.02363 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4700 - lr: 0.02362 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4701 - lr: 0.02362 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4702 - lr: 0.02361 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4703 - lr: 0.02360 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4704 - lr: 0.02359 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4705 - lr: 0.02359 - Train loss: 0.01147 - Test loss: 0.04052\n",
      "Epoch 4706 - lr: 0.02358 - Train loss: 0.01146 - Test loss: 0.04052\n",
      "Epoch 4707 - lr: 0.02357 - Train loss: 0.01146 - Test loss: 0.04052\n",
      "Epoch 4708 - lr: 0.02356 - Train loss: 0.01146 - Test loss: 0.04052\n",
      "Epoch 4709 - lr: 0.02356 - Train loss: 0.01146 - Test loss: 0.04052\n",
      "Epoch 4710 - lr: 0.02355 - Train loss: 0.01146 - Test loss: 0.04052\n",
      "Epoch 4711 - lr: 0.02354 - Train loss: 0.01146 - Test loss: 0.04051\n",
      "Epoch 4712 - lr: 0.02354 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4713 - lr: 0.02353 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4714 - lr: 0.02352 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4715 - lr: 0.02351 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4716 - lr: 0.02351 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4717 - lr: 0.02350 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4718 - lr: 0.02349 - Train loss: 0.01145 - Test loss: 0.04051\n",
      "Epoch 4719 - lr: 0.02349 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4720 - lr: 0.02348 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4721 - lr: 0.02347 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4722 - lr: 0.02346 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4723 - lr: 0.02346 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4724 - lr: 0.02345 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4725 - lr: 0.02344 - Train loss: 0.01144 - Test loss: 0.04051\n",
      "Epoch 4726 - lr: 0.02344 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4727 - lr: 0.02343 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4728 - lr: 0.02342 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4729 - lr: 0.02341 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4730 - lr: 0.02341 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4731 - lr: 0.02340 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4732 - lr: 0.02339 - Train loss: 0.01143 - Test loss: 0.04051\n",
      "Epoch 4733 - lr: 0.02338 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4734 - lr: 0.02338 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4735 - lr: 0.02337 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4736 - lr: 0.02336 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4737 - lr: 0.02336 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4738 - lr: 0.02335 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4739 - lr: 0.02334 - Train loss: 0.01142 - Test loss: 0.04051\n",
      "Epoch 4740 - lr: 0.02333 - Train loss: 0.01141 - Test loss: 0.04051\n",
      "Epoch 4741 - lr: 0.02333 - Train loss: 0.01141 - Test loss: 0.04051\n",
      "Epoch 4742 - lr: 0.02332 - Train loss: 0.01141 - Test loss: 0.04051\n",
      "Epoch 4743 - lr: 0.02331 - Train loss: 0.01141 - Test loss: 0.04051\n",
      "Epoch 4744 - lr: 0.02331 - Train loss: 0.01141 - Test loss: 0.04051\n",
      "Epoch 4745 - lr: 0.02330 - Train loss: 0.01141 - Test loss: 0.04050\n",
      "Epoch 4746 - lr: 0.02329 - Train loss: 0.01141 - Test loss: 0.04050\n",
      "Epoch 4747 - lr: 0.02328 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4748 - lr: 0.02328 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4749 - lr: 0.02327 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4750 - lr: 0.02326 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4751 - lr: 0.02326 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4752 - lr: 0.02325 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4753 - lr: 0.02324 - Train loss: 0.01140 - Test loss: 0.04050\n",
      "Epoch 4754 - lr: 0.02323 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4755 - lr: 0.02323 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4756 - lr: 0.02322 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4757 - lr: 0.02321 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4758 - lr: 0.02321 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4759 - lr: 0.02320 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4760 - lr: 0.02319 - Train loss: 0.01139 - Test loss: 0.04050\n",
      "Epoch 4761 - lr: 0.02318 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4762 - lr: 0.02318 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4763 - lr: 0.02317 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4764 - lr: 0.02316 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4765 - lr: 0.02316 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4766 - lr: 0.02315 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4767 - lr: 0.02314 - Train loss: 0.01138 - Test loss: 0.04050\n",
      "Epoch 4768 - lr: 0.02313 - Train loss: 0.01137 - Test loss: 0.04050\n",
      "Epoch 4769 - lr: 0.02313 - Train loss: 0.01137 - Test loss: 0.04050\n",
      "Epoch 4770 - lr: 0.02312 - Train loss: 0.01137 - Test loss: 0.04050\n",
      "Epoch 4771 - lr: 0.02311 - Train loss: 0.01137 - Test loss: 0.04050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4772 - lr: 0.02311 - Train loss: 0.01137 - Test loss: 0.04050\n",
      "Epoch 4773 - lr: 0.02310 - Train loss: 0.01137 - Test loss: 0.04050\n",
      "Epoch 4774 - lr: 0.02309 - Train loss: 0.01137 - Test loss: 0.04050\n",
      "Epoch 4775 - lr: 0.02309 - Train loss: 0.01136 - Test loss: 0.04050\n",
      "Epoch 4776 - lr: 0.02308 - Train loss: 0.01136 - Test loss: 0.04050\n",
      "Epoch 4777 - lr: 0.02307 - Train loss: 0.01136 - Test loss: 0.04050\n",
      "Epoch 4778 - lr: 0.02306 - Train loss: 0.01136 - Test loss: 0.04050\n",
      "Epoch 4779 - lr: 0.02306 - Train loss: 0.01136 - Test loss: 0.04050\n",
      "Epoch 4780 - lr: 0.02305 - Train loss: 0.01136 - Test loss: 0.04049\n",
      "Epoch 4781 - lr: 0.02304 - Train loss: 0.01136 - Test loss: 0.04049\n",
      "Epoch 4782 - lr: 0.02304 - Train loss: 0.01136 - Test loss: 0.04049\n",
      "Epoch 4783 - lr: 0.02303 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4784 - lr: 0.02302 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4785 - lr: 0.02301 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4786 - lr: 0.02301 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4787 - lr: 0.02300 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4788 - lr: 0.02299 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4789 - lr: 0.02299 - Train loss: 0.01135 - Test loss: 0.04049\n",
      "Epoch 4790 - lr: 0.02298 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4791 - lr: 0.02297 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4792 - lr: 0.02297 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4793 - lr: 0.02296 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4794 - lr: 0.02295 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4795 - lr: 0.02294 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4796 - lr: 0.02294 - Train loss: 0.01134 - Test loss: 0.04049\n",
      "Epoch 4797 - lr: 0.02293 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4798 - lr: 0.02292 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4799 - lr: 0.02292 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4800 - lr: 0.02291 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4801 - lr: 0.02290 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4802 - lr: 0.02289 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4803 - lr: 0.02289 - Train loss: 0.01133 - Test loss: 0.04049\n",
      "Epoch 4804 - lr: 0.02288 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4805 - lr: 0.02287 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4806 - lr: 0.02287 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4807 - lr: 0.02286 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4808 - lr: 0.02285 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4809 - lr: 0.02285 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4810 - lr: 0.02284 - Train loss: 0.01132 - Test loss: 0.04049\n",
      "Epoch 4811 - lr: 0.02283 - Train loss: 0.01131 - Test loss: 0.04049\n",
      "Epoch 4812 - lr: 0.02282 - Train loss: 0.01131 - Test loss: 0.04049\n",
      "Epoch 4813 - lr: 0.02282 - Train loss: 0.01131 - Test loss: 0.04049\n",
      "Epoch 4814 - lr: 0.02281 - Train loss: 0.01131 - Test loss: 0.04049\n",
      "Epoch 4815 - lr: 0.02280 - Train loss: 0.01131 - Test loss: 0.04049\n",
      "Epoch 4816 - lr: 0.02280 - Train loss: 0.01131 - Test loss: 0.04048\n",
      "Epoch 4817 - lr: 0.02279 - Train loss: 0.01131 - Test loss: 0.04048\n",
      "Epoch 4818 - lr: 0.02278 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4819 - lr: 0.02278 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4820 - lr: 0.02277 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4821 - lr: 0.02276 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4822 - lr: 0.02275 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4823 - lr: 0.02275 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4824 - lr: 0.02274 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4825 - lr: 0.02273 - Train loss: 0.01130 - Test loss: 0.04048\n",
      "Epoch 4826 - lr: 0.02273 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4827 - lr: 0.02272 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4828 - lr: 0.02271 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4829 - lr: 0.02271 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4830 - lr: 0.02270 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4831 - lr: 0.02269 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4832 - lr: 0.02268 - Train loss: 0.01129 - Test loss: 0.04048\n",
      "Epoch 4833 - lr: 0.02268 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4834 - lr: 0.02267 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4835 - lr: 0.02266 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4836 - lr: 0.02266 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4837 - lr: 0.02265 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4838 - lr: 0.02264 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4839 - lr: 0.02264 - Train loss: 0.01128 - Test loss: 0.04048\n",
      "Epoch 4840 - lr: 0.02263 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4841 - lr: 0.02262 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4842 - lr: 0.02262 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4843 - lr: 0.02261 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4844 - lr: 0.02260 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4845 - lr: 0.02259 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4846 - lr: 0.02259 - Train loss: 0.01127 - Test loss: 0.04048\n",
      "Epoch 4847 - lr: 0.02258 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4848 - lr: 0.02257 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4849 - lr: 0.02257 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4850 - lr: 0.02256 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4851 - lr: 0.02255 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4852 - lr: 0.02255 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4853 - lr: 0.02254 - Train loss: 0.01126 - Test loss: 0.04048\n",
      "Epoch 4854 - lr: 0.02253 - Train loss: 0.01126 - Test loss: 0.04047\n",
      "Epoch 4855 - lr: 0.02253 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4856 - lr: 0.02252 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4857 - lr: 0.02251 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4858 - lr: 0.02250 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4859 - lr: 0.02250 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4860 - lr: 0.02249 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4861 - lr: 0.02248 - Train loss: 0.01125 - Test loss: 0.04047\n",
      "Epoch 4862 - lr: 0.02248 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4863 - lr: 0.02247 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4864 - lr: 0.02246 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4865 - lr: 0.02246 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4866 - lr: 0.02245 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4867 - lr: 0.02244 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4868 - lr: 0.02244 - Train loss: 0.01124 - Test loss: 0.04047\n",
      "Epoch 4869 - lr: 0.02243 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4870 - lr: 0.02242 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4871 - lr: 0.02241 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4872 - lr: 0.02241 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4873 - lr: 0.02240 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4874 - lr: 0.02239 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4875 - lr: 0.02239 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4876 - lr: 0.02238 - Train loss: 0.01123 - Test loss: 0.04047\n",
      "Epoch 4877 - lr: 0.02237 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4878 - lr: 0.02237 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4879 - lr: 0.02236 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4880 - lr: 0.02235 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4881 - lr: 0.02235 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4882 - lr: 0.02234 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4883 - lr: 0.02233 - Train loss: 0.01122 - Test loss: 0.04047\n",
      "Epoch 4884 - lr: 0.02233 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4885 - lr: 0.02232 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4886 - lr: 0.02231 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4887 - lr: 0.02230 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4888 - lr: 0.02230 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4889 - lr: 0.02229 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4890 - lr: 0.02228 - Train loss: 0.01121 - Test loss: 0.04047\n",
      "Epoch 4891 - lr: 0.02228 - Train loss: 0.01120 - Test loss: 0.04047\n",
      "Epoch 4892 - lr: 0.02227 - Train loss: 0.01120 - Test loss: 0.04047\n",
      "Epoch 4893 - lr: 0.02226 - Train loss: 0.01120 - Test loss: 0.04046\n",
      "Epoch 4894 - lr: 0.02226 - Train loss: 0.01120 - Test loss: 0.04046\n",
      "Epoch 4895 - lr: 0.02225 - Train loss: 0.01120 - Test loss: 0.04046\n",
      "Epoch 4896 - lr: 0.02224 - Train loss: 0.01120 - Test loss: 0.04046\n",
      "Epoch 4897 - lr: 0.02224 - Train loss: 0.01120 - Test loss: 0.04046\n",
      "Epoch 4898 - lr: 0.02223 - Train loss: 0.01120 - Test loss: 0.04046\n",
      "Epoch 4899 - lr: 0.02222 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4900 - lr: 0.02222 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4901 - lr: 0.02221 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4902 - lr: 0.02220 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4903 - lr: 0.02220 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4904 - lr: 0.02219 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4905 - lr: 0.02218 - Train loss: 0.01119 - Test loss: 0.04046\n",
      "Epoch 4906 - lr: 0.02218 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4907 - lr: 0.02217 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4908 - lr: 0.02216 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4909 - lr: 0.02215 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4910 - lr: 0.02215 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4911 - lr: 0.02214 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4912 - lr: 0.02213 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4913 - lr: 0.02213 - Train loss: 0.01118 - Test loss: 0.04046\n",
      "Epoch 4914 - lr: 0.02212 - Train loss: 0.01117 - Test loss: 0.04046\n",
      "Epoch 4915 - lr: 0.02211 - Train loss: 0.01117 - Test loss: 0.04046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4916 - lr: 0.02211 - Train loss: 0.01117 - Test loss: 0.04046\n",
      "Epoch 4917 - lr: 0.02210 - Train loss: 0.01117 - Test loss: 0.04046\n",
      "Epoch 4918 - lr: 0.02209 - Train loss: 0.01117 - Test loss: 0.04046\n",
      "Epoch 4919 - lr: 0.02209 - Train loss: 0.01117 - Test loss: 0.04046\n",
      "Epoch 4920 - lr: 0.02208 - Train loss: 0.01117 - Test loss: 0.04046\n",
      "Epoch 4921 - lr: 0.02207 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4922 - lr: 0.02207 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4923 - lr: 0.02206 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4924 - lr: 0.02205 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4925 - lr: 0.02205 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4926 - lr: 0.02204 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4927 - lr: 0.02203 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4928 - lr: 0.02203 - Train loss: 0.01116 - Test loss: 0.04046\n",
      "Epoch 4929 - lr: 0.02202 - Train loss: 0.01115 - Test loss: 0.04046\n",
      "Epoch 4930 - lr: 0.02201 - Train loss: 0.01115 - Test loss: 0.04046\n",
      "Epoch 4931 - lr: 0.02201 - Train loss: 0.01115 - Test loss: 0.04046\n",
      "Epoch 4932 - lr: 0.02200 - Train loss: 0.01115 - Test loss: 0.04046\n",
      "Epoch 4933 - lr: 0.02199 - Train loss: 0.01115 - Test loss: 0.04045\n",
      "Epoch 4934 - lr: 0.02199 - Train loss: 0.01115 - Test loss: 0.04045\n",
      "Epoch 4935 - lr: 0.02198 - Train loss: 0.01115 - Test loss: 0.04045\n",
      "Epoch 4936 - lr: 0.02197 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4937 - lr: 0.02197 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4938 - lr: 0.02196 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4939 - lr: 0.02195 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4940 - lr: 0.02194 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4941 - lr: 0.02194 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4942 - lr: 0.02193 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4943 - lr: 0.02192 - Train loss: 0.01114 - Test loss: 0.04045\n",
      "Epoch 4944 - lr: 0.02192 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4945 - lr: 0.02191 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4946 - lr: 0.02190 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4947 - lr: 0.02190 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4948 - lr: 0.02189 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4949 - lr: 0.02188 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4950 - lr: 0.02188 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4951 - lr: 0.02187 - Train loss: 0.01113 - Test loss: 0.04045\n",
      "Epoch 4952 - lr: 0.02186 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4953 - lr: 0.02186 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4954 - lr: 0.02185 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4955 - lr: 0.02184 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4956 - lr: 0.02184 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4957 - lr: 0.02183 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4958 - lr: 0.02182 - Train loss: 0.01112 - Test loss: 0.04045\n",
      "Epoch 4959 - lr: 0.02182 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4960 - lr: 0.02181 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4961 - lr: 0.02180 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4962 - lr: 0.02180 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4963 - lr: 0.02179 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4964 - lr: 0.02178 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4965 - lr: 0.02178 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4966 - lr: 0.02177 - Train loss: 0.01111 - Test loss: 0.04045\n",
      "Epoch 4967 - lr: 0.02176 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4968 - lr: 0.02176 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4969 - lr: 0.02175 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4970 - lr: 0.02174 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4971 - lr: 0.02174 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4972 - lr: 0.02173 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4973 - lr: 0.02172 - Train loss: 0.01110 - Test loss: 0.04045\n",
      "Epoch 4974 - lr: 0.02172 - Train loss: 0.01109 - Test loss: 0.04045\n",
      "Epoch 4975 - lr: 0.02171 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4976 - lr: 0.02170 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4977 - lr: 0.02170 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4978 - lr: 0.02169 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4979 - lr: 0.02168 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4980 - lr: 0.02168 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4981 - lr: 0.02167 - Train loss: 0.01109 - Test loss: 0.04044\n",
      "Epoch 4982 - lr: 0.02166 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4983 - lr: 0.02166 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4984 - lr: 0.02165 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4985 - lr: 0.02164 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4986 - lr: 0.02164 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4987 - lr: 0.02163 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4988 - lr: 0.02162 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4989 - lr: 0.02162 - Train loss: 0.01108 - Test loss: 0.04044\n",
      "Epoch 4990 - lr: 0.02161 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4991 - lr: 0.02160 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4992 - lr: 0.02160 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4993 - lr: 0.02159 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4994 - lr: 0.02158 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4995 - lr: 0.02158 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4996 - lr: 0.02157 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4997 - lr: 0.02156 - Train loss: 0.01107 - Test loss: 0.04044\n",
      "Epoch 4998 - lr: 0.02156 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 4999 - lr: 0.02155 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 5000 - lr: 0.02154 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 5001 - lr: 0.02154 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 5002 - lr: 0.02153 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 5003 - lr: 0.02152 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 5004 - lr: 0.02152 - Train loss: 0.01106 - Test loss: 0.04044\n",
      "Epoch 5005 - lr: 0.02151 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5006 - lr: 0.02150 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5007 - lr: 0.02150 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5008 - lr: 0.02149 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5009 - lr: 0.02148 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5010 - lr: 0.02148 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5011 - lr: 0.02147 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5012 - lr: 0.02147 - Train loss: 0.01105 - Test loss: 0.04044\n",
      "Epoch 5013 - lr: 0.02146 - Train loss: 0.01104 - Test loss: 0.04044\n",
      "Epoch 5014 - lr: 0.02145 - Train loss: 0.01104 - Test loss: 0.04044\n",
      "Epoch 5015 - lr: 0.02145 - Train loss: 0.01104 - Test loss: 0.04044\n",
      "Epoch 5016 - lr: 0.02144 - Train loss: 0.01104 - Test loss: 0.04044\n",
      "Epoch 5017 - lr: 0.02143 - Train loss: 0.01104 - Test loss: 0.04044\n",
      "Epoch 5018 - lr: 0.02143 - Train loss: 0.01104 - Test loss: 0.04044\n",
      "Epoch 5019 - lr: 0.02142 - Train loss: 0.01104 - Test loss: 0.04043\n",
      "Epoch 5020 - lr: 0.02141 - Train loss: 0.01104 - Test loss: 0.04043\n",
      "Epoch 5021 - lr: 0.02141 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5022 - lr: 0.02140 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5023 - lr: 0.02139 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5024 - lr: 0.02139 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5025 - lr: 0.02138 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5026 - lr: 0.02137 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5027 - lr: 0.02137 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5028 - lr: 0.02136 - Train loss: 0.01103 - Test loss: 0.04043\n",
      "Epoch 5029 - lr: 0.02135 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5030 - lr: 0.02135 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5031 - lr: 0.02134 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5032 - lr: 0.02133 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5033 - lr: 0.02133 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5034 - lr: 0.02132 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5035 - lr: 0.02131 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5036 - lr: 0.02131 - Train loss: 0.01102 - Test loss: 0.04043\n",
      "Epoch 5037 - lr: 0.02130 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5038 - lr: 0.02129 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5039 - lr: 0.02129 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5040 - lr: 0.02128 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5041 - lr: 0.02127 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5042 - lr: 0.02127 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5043 - lr: 0.02126 - Train loss: 0.01101 - Test loss: 0.04043\n",
      "Epoch 5044 - lr: 0.02126 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5045 - lr: 0.02125 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5046 - lr: 0.02124 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5047 - lr: 0.02124 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5048 - lr: 0.02123 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5049 - lr: 0.02122 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5050 - lr: 0.02122 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5051 - lr: 0.02121 - Train loss: 0.01100 - Test loss: 0.04043\n",
      "Epoch 5052 - lr: 0.02120 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5053 - lr: 0.02120 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5054 - lr: 0.02119 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5055 - lr: 0.02118 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5056 - lr: 0.02118 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5057 - lr: 0.02117 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5058 - lr: 0.02116 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5059 - lr: 0.02116 - Train loss: 0.01099 - Test loss: 0.04043\n",
      "Epoch 5060 - lr: 0.02115 - Train loss: 0.01098 - Test loss: 0.04043\n",
      "Epoch 5061 - lr: 0.02114 - Train loss: 0.01098 - Test loss: 0.04043\n",
      "Epoch 5062 - lr: 0.02114 - Train loss: 0.01098 - Test loss: 0.04043\n",
      "Epoch 5063 - lr: 0.02113 - Train loss: 0.01098 - Test loss: 0.04043\n",
      "Epoch 5064 - lr: 0.02113 - Train loss: 0.01098 - Test loss: 0.04042\n",
      "Epoch 5065 - lr: 0.02112 - Train loss: 0.01098 - Test loss: 0.04042\n",
      "Epoch 5066 - lr: 0.02111 - Train loss: 0.01098 - Test loss: 0.04042\n",
      "Epoch 5067 - lr: 0.02111 - Train loss: 0.01098 - Test loss: 0.04042\n",
      "Epoch 5068 - lr: 0.02110 - Train loss: 0.01097 - Test loss: 0.04042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5069 - lr: 0.02109 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5070 - lr: 0.02109 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5071 - lr: 0.02108 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5072 - lr: 0.02107 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5073 - lr: 0.02107 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5074 - lr: 0.02106 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5075 - lr: 0.02105 - Train loss: 0.01097 - Test loss: 0.04042\n",
      "Epoch 5076 - lr: 0.02105 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5077 - lr: 0.02104 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5078 - lr: 0.02103 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5079 - lr: 0.02103 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5080 - lr: 0.02102 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5081 - lr: 0.02102 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5082 - lr: 0.02101 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5083 - lr: 0.02100 - Train loss: 0.01096 - Test loss: 0.04042\n",
      "Epoch 5084 - lr: 0.02100 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5085 - lr: 0.02099 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5086 - lr: 0.02098 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5087 - lr: 0.02098 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5088 - lr: 0.02097 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5089 - lr: 0.02096 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5090 - lr: 0.02096 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5091 - lr: 0.02095 - Train loss: 0.01095 - Test loss: 0.04042\n",
      "Epoch 5092 - lr: 0.02094 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5093 - lr: 0.02094 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5094 - lr: 0.02093 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5095 - lr: 0.02093 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5096 - lr: 0.02092 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5097 - lr: 0.02091 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5098 - lr: 0.02091 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5099 - lr: 0.02090 - Train loss: 0.01094 - Test loss: 0.04042\n",
      "Epoch 5100 - lr: 0.02089 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5101 - lr: 0.02089 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5102 - lr: 0.02088 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5103 - lr: 0.02087 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5104 - lr: 0.02087 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5105 - lr: 0.02086 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5106 - lr: 0.02085 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5107 - lr: 0.02085 - Train loss: 0.01093 - Test loss: 0.04042\n",
      "Epoch 5108 - lr: 0.02084 - Train loss: 0.01092 - Test loss: 0.04042\n",
      "Epoch 5109 - lr: 0.02084 - Train loss: 0.01092 - Test loss: 0.04042\n",
      "Epoch 5110 - lr: 0.02083 - Train loss: 0.01092 - Test loss: 0.04042\n",
      "Epoch 5111 - lr: 0.02082 - Train loss: 0.01092 - Test loss: 0.04042\n",
      "Epoch 5112 - lr: 0.02082 - Train loss: 0.01092 - Test loss: 0.04041\n",
      "Epoch 5113 - lr: 0.02081 - Train loss: 0.01092 - Test loss: 0.04041\n",
      "Epoch 5114 - lr: 0.02080 - Train loss: 0.01092 - Test loss: 0.04041\n",
      "Epoch 5115 - lr: 0.02080 - Train loss: 0.01092 - Test loss: 0.04041\n",
      "Epoch 5116 - lr: 0.02079 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5117 - lr: 0.02078 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5118 - lr: 0.02078 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5119 - lr: 0.02077 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5120 - lr: 0.02077 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5121 - lr: 0.02076 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5122 - lr: 0.02075 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5123 - lr: 0.02075 - Train loss: 0.01091 - Test loss: 0.04041\n",
      "Epoch 5124 - lr: 0.02074 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5125 - lr: 0.02073 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5126 - lr: 0.02073 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5127 - lr: 0.02072 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5128 - lr: 0.02071 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5129 - lr: 0.02071 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5130 - lr: 0.02070 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5131 - lr: 0.02070 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5132 - lr: 0.02069 - Train loss: 0.01090 - Test loss: 0.04041\n",
      "Epoch 5133 - lr: 0.02068 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5134 - lr: 0.02068 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5135 - lr: 0.02067 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5136 - lr: 0.02066 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5137 - lr: 0.02066 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5138 - lr: 0.02065 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5139 - lr: 0.02064 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5140 - lr: 0.02064 - Train loss: 0.01089 - Test loss: 0.04041\n",
      "Epoch 5141 - lr: 0.02063 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5142 - lr: 0.02063 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5143 - lr: 0.02062 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5144 - lr: 0.02061 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5145 - lr: 0.02061 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5146 - lr: 0.02060 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5147 - lr: 0.02059 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5148 - lr: 0.02059 - Train loss: 0.01088 - Test loss: 0.04041\n",
      "Epoch 5149 - lr: 0.02058 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5150 - lr: 0.02057 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5151 - lr: 0.02057 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5152 - lr: 0.02056 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5153 - lr: 0.02056 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5154 - lr: 0.02055 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5155 - lr: 0.02054 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5156 - lr: 0.02054 - Train loss: 0.01087 - Test loss: 0.04041\n",
      "Epoch 5157 - lr: 0.02053 - Train loss: 0.01086 - Test loss: 0.04041\n",
      "Epoch 5158 - lr: 0.02052 - Train loss: 0.01086 - Test loss: 0.04041\n",
      "Epoch 5159 - lr: 0.02052 - Train loss: 0.01086 - Test loss: 0.04041\n",
      "Epoch 5160 - lr: 0.02051 - Train loss: 0.01086 - Test loss: 0.04041\n",
      "Epoch 5161 - lr: 0.02051 - Train loss: 0.01086 - Test loss: 0.04040\n",
      "Epoch 5162 - lr: 0.02050 - Train loss: 0.01086 - Test loss: 0.04040\n",
      "Epoch 5163 - lr: 0.02049 - Train loss: 0.01086 - Test loss: 0.04040\n",
      "Epoch 5164 - lr: 0.02049 - Train loss: 0.01086 - Test loss: 0.04040\n",
      "Epoch 5165 - lr: 0.02048 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5166 - lr: 0.02047 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5167 - lr: 0.02047 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5168 - lr: 0.02046 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5169 - lr: 0.02046 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5170 - lr: 0.02045 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5171 - lr: 0.02044 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5172 - lr: 0.02044 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5173 - lr: 0.02043 - Train loss: 0.01085 - Test loss: 0.04040\n",
      "Epoch 5174 - lr: 0.02042 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5175 - lr: 0.02042 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5176 - lr: 0.02041 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5177 - lr: 0.02040 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5178 - lr: 0.02040 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5179 - lr: 0.02039 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5180 - lr: 0.02039 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5181 - lr: 0.02038 - Train loss: 0.01084 - Test loss: 0.04040\n",
      "Epoch 5182 - lr: 0.02037 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5183 - lr: 0.02037 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5184 - lr: 0.02036 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5185 - lr: 0.02035 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5186 - lr: 0.02035 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5187 - lr: 0.02034 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5188 - lr: 0.02034 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5189 - lr: 0.02033 - Train loss: 0.01083 - Test loss: 0.04040\n",
      "Epoch 5190 - lr: 0.02032 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5191 - lr: 0.02032 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5192 - lr: 0.02031 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5193 - lr: 0.02030 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5194 - lr: 0.02030 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5195 - lr: 0.02029 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5196 - lr: 0.02029 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5197 - lr: 0.02028 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5198 - lr: 0.02027 - Train loss: 0.01082 - Test loss: 0.04040\n",
      "Epoch 5199 - lr: 0.02027 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5200 - lr: 0.02026 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5201 - lr: 0.02026 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5202 - lr: 0.02025 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5203 - lr: 0.02024 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5204 - lr: 0.02024 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5205 - lr: 0.02023 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5206 - lr: 0.02022 - Train loss: 0.01081 - Test loss: 0.04040\n",
      "Epoch 5207 - lr: 0.02022 - Train loss: 0.01080 - Test loss: 0.04040\n",
      "Epoch 5208 - lr: 0.02021 - Train loss: 0.01080 - Test loss: 0.04040\n",
      "Epoch 5209 - lr: 0.02021 - Train loss: 0.01080 - Test loss: 0.04040\n",
      "Epoch 5210 - lr: 0.02020 - Train loss: 0.01080 - Test loss: 0.04040\n",
      "Epoch 5211 - lr: 0.02019 - Train loss: 0.01080 - Test loss: 0.04040\n",
      "Epoch 5212 - lr: 0.02019 - Train loss: 0.01080 - Test loss: 0.04039\n",
      "Epoch 5213 - lr: 0.02018 - Train loss: 0.01080 - Test loss: 0.04039\n",
      "Epoch 5214 - lr: 0.02017 - Train loss: 0.01080 - Test loss: 0.04039\n",
      "Epoch 5215 - lr: 0.02017 - Train loss: 0.01080 - Test loss: 0.04039\n",
      "Epoch 5216 - lr: 0.02016 - Train loss: 0.01079 - Test loss: 0.04039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5217 - lr: 0.02016 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5218 - lr: 0.02015 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5219 - lr: 0.02014 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5220 - lr: 0.02014 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5221 - lr: 0.02013 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5222 - lr: 0.02012 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5223 - lr: 0.02012 - Train loss: 0.01079 - Test loss: 0.04039\n",
      "Epoch 5224 - lr: 0.02011 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5225 - lr: 0.02011 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5226 - lr: 0.02010 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5227 - lr: 0.02009 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5228 - lr: 0.02009 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5229 - lr: 0.02008 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5230 - lr: 0.02008 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5231 - lr: 0.02007 - Train loss: 0.01078 - Test loss: 0.04039\n",
      "Epoch 5232 - lr: 0.02006 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5233 - lr: 0.02006 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5234 - lr: 0.02005 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5235 - lr: 0.02004 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5236 - lr: 0.02004 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5237 - lr: 0.02003 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5238 - lr: 0.02003 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5239 - lr: 0.02002 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5240 - lr: 0.02001 - Train loss: 0.01077 - Test loss: 0.04039\n",
      "Epoch 5241 - lr: 0.02001 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5242 - lr: 0.02000 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5243 - lr: 0.02000 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5244 - lr: 0.01999 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5245 - lr: 0.01998 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5246 - lr: 0.01998 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5247 - lr: 0.01997 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5248 - lr: 0.01996 - Train loss: 0.01076 - Test loss: 0.04039\n",
      "Epoch 5249 - lr: 0.01996 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5250 - lr: 0.01995 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5251 - lr: 0.01995 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5252 - lr: 0.01994 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5253 - lr: 0.01993 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5254 - lr: 0.01993 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5255 - lr: 0.01992 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5256 - lr: 0.01992 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5257 - lr: 0.01991 - Train loss: 0.01075 - Test loss: 0.04039\n",
      "Epoch 5258 - lr: 0.01990 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5259 - lr: 0.01990 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5260 - lr: 0.01989 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5261 - lr: 0.01989 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5262 - lr: 0.01988 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5263 - lr: 0.01987 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5264 - lr: 0.01987 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5265 - lr: 0.01986 - Train loss: 0.01074 - Test loss: 0.04039\n",
      "Epoch 5266 - lr: 0.01985 - Train loss: 0.01074 - Test loss: 0.04038\n",
      "Epoch 5267 - lr: 0.01985 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5268 - lr: 0.01984 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5269 - lr: 0.01984 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5270 - lr: 0.01983 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5271 - lr: 0.01982 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5272 - lr: 0.01982 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5273 - lr: 0.01981 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5274 - lr: 0.01981 - Train loss: 0.01073 - Test loss: 0.04038\n",
      "Epoch 5275 - lr: 0.01980 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5276 - lr: 0.01979 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5277 - lr: 0.01979 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5278 - lr: 0.01978 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5279 - lr: 0.01978 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5280 - lr: 0.01977 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5281 - lr: 0.01976 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5282 - lr: 0.01976 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5283 - lr: 0.01975 - Train loss: 0.01072 - Test loss: 0.04038\n",
      "Epoch 5284 - lr: 0.01975 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5285 - lr: 0.01974 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5286 - lr: 0.01973 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5287 - lr: 0.01973 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5288 - lr: 0.01972 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5289 - lr: 0.01972 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5290 - lr: 0.01971 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5291 - lr: 0.01970 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5292 - lr: 0.01970 - Train loss: 0.01071 - Test loss: 0.04038\n",
      "Epoch 5293 - lr: 0.01969 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5294 - lr: 0.01968 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5295 - lr: 0.01968 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5296 - lr: 0.01967 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5297 - lr: 0.01967 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5298 - lr: 0.01966 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5299 - lr: 0.01965 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5300 - lr: 0.01965 - Train loss: 0.01070 - Test loss: 0.04038\n",
      "Epoch 5301 - lr: 0.01964 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5302 - lr: 0.01964 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5303 - lr: 0.01963 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5304 - lr: 0.01962 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5305 - lr: 0.01962 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5306 - lr: 0.01961 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5307 - lr: 0.01961 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5308 - lr: 0.01960 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5309 - lr: 0.01959 - Train loss: 0.01069 - Test loss: 0.04038\n",
      "Epoch 5310 - lr: 0.01959 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5311 - lr: 0.01958 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5312 - lr: 0.01958 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5313 - lr: 0.01957 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5314 - lr: 0.01956 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5315 - lr: 0.01956 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5316 - lr: 0.01955 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5317 - lr: 0.01955 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5318 - lr: 0.01954 - Train loss: 0.01068 - Test loss: 0.04038\n",
      "Epoch 5319 - lr: 0.01953 - Train loss: 0.01067 - Test loss: 0.04038\n",
      "Epoch 5320 - lr: 0.01953 - Train loss: 0.01067 - Test loss: 0.04038\n",
      "Epoch 5321 - lr: 0.01952 - Train loss: 0.01067 - Test loss: 0.04038\n",
      "Epoch 5322 - lr: 0.01952 - Train loss: 0.01067 - Test loss: 0.04037\n",
      "Epoch 5323 - lr: 0.01951 - Train loss: 0.01067 - Test loss: 0.04037\n",
      "Epoch 5324 - lr: 0.01950 - Train loss: 0.01067 - Test loss: 0.04037\n",
      "Epoch 5325 - lr: 0.01950 - Train loss: 0.01067 - Test loss: 0.04037\n",
      "Epoch 5326 - lr: 0.01949 - Train loss: 0.01067 - Test loss: 0.04037\n",
      "Epoch 5327 - lr: 0.01949 - Train loss: 0.01067 - Test loss: 0.04037\n",
      "Epoch 5328 - lr: 0.01948 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5329 - lr: 0.01947 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5330 - lr: 0.01947 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5331 - lr: 0.01946 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5332 - lr: 0.01946 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5333 - lr: 0.01945 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5334 - lr: 0.01944 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5335 - lr: 0.01944 - Train loss: 0.01066 - Test loss: 0.04037\n",
      "Epoch 5336 - lr: 0.01943 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5337 - lr: 0.01943 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5338 - lr: 0.01942 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5339 - lr: 0.01941 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5340 - lr: 0.01941 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5341 - lr: 0.01940 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5342 - lr: 0.01940 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5343 - lr: 0.01939 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5344 - lr: 0.01939 - Train loss: 0.01065 - Test loss: 0.04037\n",
      "Epoch 5345 - lr: 0.01938 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5346 - lr: 0.01937 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5347 - lr: 0.01937 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5348 - lr: 0.01936 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5349 - lr: 0.01936 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5350 - lr: 0.01935 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5351 - lr: 0.01934 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5352 - lr: 0.01934 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5353 - lr: 0.01933 - Train loss: 0.01064 - Test loss: 0.04037\n",
      "Epoch 5354 - lr: 0.01933 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5355 - lr: 0.01932 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5356 - lr: 0.01931 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5357 - lr: 0.01931 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5358 - lr: 0.01930 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5359 - lr: 0.01930 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5360 - lr: 0.01929 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5361 - lr: 0.01928 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5362 - lr: 0.01928 - Train loss: 0.01063 - Test loss: 0.04037\n",
      "Epoch 5363 - lr: 0.01927 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5364 - lr: 0.01927 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5365 - lr: 0.01926 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5366 - lr: 0.01925 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5367 - lr: 0.01925 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5368 - lr: 0.01924 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5369 - lr: 0.01924 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5370 - lr: 0.01923 - Train loss: 0.01062 - Test loss: 0.04037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5371 - lr: 0.01923 - Train loss: 0.01062 - Test loss: 0.04037\n",
      "Epoch 5372 - lr: 0.01922 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5373 - lr: 0.01921 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5374 - lr: 0.01921 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5375 - lr: 0.01920 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5376 - lr: 0.01920 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5377 - lr: 0.01919 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5378 - lr: 0.01918 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5379 - lr: 0.01918 - Train loss: 0.01061 - Test loss: 0.04037\n",
      "Epoch 5380 - lr: 0.01917 - Train loss: 0.01061 - Test loss: 0.04036\n",
      "Epoch 5381 - lr: 0.01917 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5382 - lr: 0.01916 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5383 - lr: 0.01915 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5384 - lr: 0.01915 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5385 - lr: 0.01914 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5386 - lr: 0.01914 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5387 - lr: 0.01913 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5388 - lr: 0.01912 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5389 - lr: 0.01912 - Train loss: 0.01060 - Test loss: 0.04036\n",
      "Epoch 5390 - lr: 0.01911 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5391 - lr: 0.01911 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5392 - lr: 0.01910 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5393 - lr: 0.01910 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5394 - lr: 0.01909 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5395 - lr: 0.01908 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5396 - lr: 0.01908 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5397 - lr: 0.01907 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5398 - lr: 0.01907 - Train loss: 0.01059 - Test loss: 0.04036\n",
      "Epoch 5399 - lr: 0.01906 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5400 - lr: 0.01905 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5401 - lr: 0.01905 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5402 - lr: 0.01904 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5403 - lr: 0.01904 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5404 - lr: 0.01903 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5405 - lr: 0.01903 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5406 - lr: 0.01902 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5407 - lr: 0.01901 - Train loss: 0.01058 - Test loss: 0.04036\n",
      "Epoch 5408 - lr: 0.01901 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5409 - lr: 0.01900 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5410 - lr: 0.01900 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5411 - lr: 0.01899 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5412 - lr: 0.01898 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5413 - lr: 0.01898 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5414 - lr: 0.01897 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5415 - lr: 0.01897 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5416 - lr: 0.01896 - Train loss: 0.01057 - Test loss: 0.04036\n",
      "Epoch 5417 - lr: 0.01896 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5418 - lr: 0.01895 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5419 - lr: 0.01894 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5420 - lr: 0.01894 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5421 - lr: 0.01893 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5422 - lr: 0.01893 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5423 - lr: 0.01892 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5424 - lr: 0.01891 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5425 - lr: 0.01891 - Train loss: 0.01056 - Test loss: 0.04036\n",
      "Epoch 5426 - lr: 0.01890 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5427 - lr: 0.01890 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5428 - lr: 0.01889 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5429 - lr: 0.01889 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5430 - lr: 0.01888 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5431 - lr: 0.01887 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5432 - lr: 0.01887 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5433 - lr: 0.01886 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5434 - lr: 0.01886 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5435 - lr: 0.01885 - Train loss: 0.01055 - Test loss: 0.04036\n",
      "Epoch 5436 - lr: 0.01885 - Train loss: 0.01054 - Test loss: 0.04036\n",
      "Epoch 5437 - lr: 0.01884 - Train loss: 0.01054 - Test loss: 0.04036\n",
      "Epoch 5438 - lr: 0.01883 - Train loss: 0.01054 - Test loss: 0.04036\n",
      "Epoch 5439 - lr: 0.01883 - Train loss: 0.01054 - Test loss: 0.04036\n",
      "Epoch 5440 - lr: 0.01882 - Train loss: 0.01054 - Test loss: 0.04036\n",
      "Epoch 5441 - lr: 0.01882 - Train loss: 0.01054 - Test loss: 0.04035\n",
      "Epoch 5442 - lr: 0.01881 - Train loss: 0.01054 - Test loss: 0.04035\n",
      "Epoch 5443 - lr: 0.01880 - Train loss: 0.01054 - Test loss: 0.04035\n",
      "Epoch 5444 - lr: 0.01880 - Train loss: 0.01054 - Test loss: 0.04035\n",
      "Epoch 5445 - lr: 0.01879 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5446 - lr: 0.01879 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5447 - lr: 0.01878 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5448 - lr: 0.01878 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5449 - lr: 0.01877 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5450 - lr: 0.01876 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5451 - lr: 0.01876 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5452 - lr: 0.01875 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5453 - lr: 0.01875 - Train loss: 0.01053 - Test loss: 0.04035\n",
      "Epoch 5454 - lr: 0.01874 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5455 - lr: 0.01874 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5456 - lr: 0.01873 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5457 - lr: 0.01872 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5458 - lr: 0.01872 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5459 - lr: 0.01871 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5460 - lr: 0.01871 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5461 - lr: 0.01870 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5462 - lr: 0.01870 - Train loss: 0.01052 - Test loss: 0.04035\n",
      "Epoch 5463 - lr: 0.01869 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5464 - lr: 0.01868 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5465 - lr: 0.01868 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5466 - lr: 0.01867 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5467 - lr: 0.01867 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5468 - lr: 0.01866 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5469 - lr: 0.01866 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5470 - lr: 0.01865 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5471 - lr: 0.01864 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5472 - lr: 0.01864 - Train loss: 0.01051 - Test loss: 0.04035\n",
      "Epoch 5473 - lr: 0.01863 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5474 - lr: 0.01863 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5475 - lr: 0.01862 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5476 - lr: 0.01862 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5477 - lr: 0.01861 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5478 - lr: 0.01860 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5479 - lr: 0.01860 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5480 - lr: 0.01859 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5481 - lr: 0.01859 - Train loss: 0.01050 - Test loss: 0.04035\n",
      "Epoch 5482 - lr: 0.01858 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5483 - lr: 0.01858 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5484 - lr: 0.01857 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5485 - lr: 0.01856 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5486 - lr: 0.01856 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5487 - lr: 0.01855 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5488 - lr: 0.01855 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5489 - lr: 0.01854 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5490 - lr: 0.01854 - Train loss: 0.01049 - Test loss: 0.04035\n",
      "Epoch 5491 - lr: 0.01853 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5492 - lr: 0.01852 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5493 - lr: 0.01852 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5494 - lr: 0.01851 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5495 - lr: 0.01851 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5496 - lr: 0.01850 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5497 - lr: 0.01850 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5498 - lr: 0.01849 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5499 - lr: 0.01848 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5500 - lr: 0.01848 - Train loss: 0.01048 - Test loss: 0.04035\n",
      "Epoch 5501 - lr: 0.01847 - Train loss: 0.01047 - Test loss: 0.04035\n",
      "Epoch 5502 - lr: 0.01847 - Train loss: 0.01047 - Test loss: 0.04035\n",
      "Epoch 5503 - lr: 0.01846 - Train loss: 0.01047 - Test loss: 0.04035\n",
      "Epoch 5504 - lr: 0.01846 - Train loss: 0.01047 - Test loss: 0.04034\n",
      "Epoch 5505 - lr: 0.01845 - Train loss: 0.01047 - Test loss: 0.04034\n",
      "Epoch 5506 - lr: 0.01844 - Train loss: 0.01047 - Test loss: 0.04034\n",
      "Epoch 5507 - lr: 0.01844 - Train loss: 0.01047 - Test loss: 0.04034\n",
      "Epoch 5508 - lr: 0.01843 - Train loss: 0.01047 - Test loss: 0.04034\n",
      "Epoch 5509 - lr: 0.01843 - Train loss: 0.01047 - Test loss: 0.04034\n",
      "Epoch 5510 - lr: 0.01842 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5511 - lr: 0.01842 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5512 - lr: 0.01841 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5513 - lr: 0.01840 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5514 - lr: 0.01840 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5515 - lr: 0.01839 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5516 - lr: 0.01839 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5517 - lr: 0.01838 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5518 - lr: 0.01838 - Train loss: 0.01046 - Test loss: 0.04034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5519 - lr: 0.01837 - Train loss: 0.01046 - Test loss: 0.04034\n",
      "Epoch 5520 - lr: 0.01837 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5521 - lr: 0.01836 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5522 - lr: 0.01835 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5523 - lr: 0.01835 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5524 - lr: 0.01834 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5525 - lr: 0.01834 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5526 - lr: 0.01833 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5527 - lr: 0.01833 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5528 - lr: 0.01832 - Train loss: 0.01045 - Test loss: 0.04034\n",
      "Epoch 5529 - lr: 0.01831 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5530 - lr: 0.01831 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5531 - lr: 0.01830 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5532 - lr: 0.01830 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5533 - lr: 0.01829 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5534 - lr: 0.01829 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5535 - lr: 0.01828 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5536 - lr: 0.01828 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5537 - lr: 0.01827 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5538 - lr: 0.01826 - Train loss: 0.01044 - Test loss: 0.04034\n",
      "Epoch 5539 - lr: 0.01826 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5540 - lr: 0.01825 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5541 - lr: 0.01825 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5542 - lr: 0.01824 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5543 - lr: 0.01824 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5544 - lr: 0.01823 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5545 - lr: 0.01822 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5546 - lr: 0.01822 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5547 - lr: 0.01821 - Train loss: 0.01043 - Test loss: 0.04034\n",
      "Epoch 5548 - lr: 0.01821 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5549 - lr: 0.01820 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5550 - lr: 0.01820 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5551 - lr: 0.01819 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5552 - lr: 0.01819 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5553 - lr: 0.01818 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5554 - lr: 0.01817 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5555 - lr: 0.01817 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5556 - lr: 0.01816 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5557 - lr: 0.01816 - Train loss: 0.01042 - Test loss: 0.04034\n",
      "Epoch 5558 - lr: 0.01815 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5559 - lr: 0.01815 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5560 - lr: 0.01814 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5561 - lr: 0.01814 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5562 - lr: 0.01813 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5563 - lr: 0.01812 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5564 - lr: 0.01812 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5565 - lr: 0.01811 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5566 - lr: 0.01811 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5567 - lr: 0.01810 - Train loss: 0.01041 - Test loss: 0.04034\n",
      "Epoch 5568 - lr: 0.01810 - Train loss: 0.01040 - Test loss: 0.04034\n",
      "Epoch 5569 - lr: 0.01809 - Train loss: 0.01040 - Test loss: 0.04034\n",
      "Epoch 5570 - lr: 0.01809 - Train loss: 0.01040 - Test loss: 0.04034\n",
      "Epoch 5571 - lr: 0.01808 - Train loss: 0.01040 - Test loss: 0.04033\n",
      "Epoch 5572 - lr: 0.01807 - Train loss: 0.01040 - Test loss: 0.04033\n",
      "Epoch 5573 - lr: 0.01807 - Train loss: 0.01040 - Test loss: 0.04033\n",
      "Epoch 5574 - lr: 0.01806 - Train loss: 0.01040 - Test loss: 0.04033\n",
      "Epoch 5575 - lr: 0.01806 - Train loss: 0.01040 - Test loss: 0.04033\n",
      "Epoch 5576 - lr: 0.01805 - Train loss: 0.01040 - Test loss: 0.04033\n",
      "Epoch 5577 - lr: 0.01805 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5578 - lr: 0.01804 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5579 - lr: 0.01804 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5580 - lr: 0.01803 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5581 - lr: 0.01802 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5582 - lr: 0.01802 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5583 - lr: 0.01801 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5584 - lr: 0.01801 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5585 - lr: 0.01800 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5586 - lr: 0.01800 - Train loss: 0.01039 - Test loss: 0.04033\n",
      "Epoch 5587 - lr: 0.01799 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5588 - lr: 0.01799 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5589 - lr: 0.01798 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5590 - lr: 0.01797 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5591 - lr: 0.01797 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5592 - lr: 0.01796 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5593 - lr: 0.01796 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5594 - lr: 0.01795 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5595 - lr: 0.01795 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5596 - lr: 0.01794 - Train loss: 0.01038 - Test loss: 0.04033\n",
      "Epoch 5597 - lr: 0.01794 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5598 - lr: 0.01793 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5599 - lr: 0.01793 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5600 - lr: 0.01792 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5601 - lr: 0.01791 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5602 - lr: 0.01791 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5603 - lr: 0.01790 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5604 - lr: 0.01790 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5605 - lr: 0.01789 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5606 - lr: 0.01789 - Train loss: 0.01037 - Test loss: 0.04033\n",
      "Epoch 5607 - lr: 0.01788 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5608 - lr: 0.01788 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5609 - lr: 0.01787 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5610 - lr: 0.01786 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5611 - lr: 0.01786 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5612 - lr: 0.01785 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5613 - lr: 0.01785 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5614 - lr: 0.01784 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5615 - lr: 0.01784 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5616 - lr: 0.01783 - Train loss: 0.01036 - Test loss: 0.04033\n",
      "Epoch 5617 - lr: 0.01783 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5618 - lr: 0.01782 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5619 - lr: 0.01782 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5620 - lr: 0.01781 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5621 - lr: 0.01780 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5622 - lr: 0.01780 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5623 - lr: 0.01779 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5624 - lr: 0.01779 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5625 - lr: 0.01778 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5626 - lr: 0.01778 - Train loss: 0.01035 - Test loss: 0.04033\n",
      "Epoch 5627 - lr: 0.01777 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5628 - lr: 0.01777 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5629 - lr: 0.01776 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5630 - lr: 0.01776 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5631 - lr: 0.01775 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5632 - lr: 0.01774 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5633 - lr: 0.01774 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5634 - lr: 0.01773 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5635 - lr: 0.01773 - Train loss: 0.01034 - Test loss: 0.04033\n",
      "Epoch 5636 - lr: 0.01772 - Train loss: 0.01033 - Test loss: 0.04033\n",
      "Epoch 5637 - lr: 0.01772 - Train loss: 0.01033 - Test loss: 0.04033\n",
      "Epoch 5638 - lr: 0.01771 - Train loss: 0.01033 - Test loss: 0.04033\n",
      "Epoch 5639 - lr: 0.01771 - Train loss: 0.01033 - Test loss: 0.04033\n",
      "Epoch 5640 - lr: 0.01770 - Train loss: 0.01033 - Test loss: 0.04032\n",
      "Epoch 5641 - lr: 0.01770 - Train loss: 0.01033 - Test loss: 0.04032\n",
      "Epoch 5642 - lr: 0.01769 - Train loss: 0.01033 - Test loss: 0.04032\n",
      "Epoch 5643 - lr: 0.01768 - Train loss: 0.01033 - Test loss: 0.04032\n",
      "Epoch 5644 - lr: 0.01768 - Train loss: 0.01033 - Test loss: 0.04032\n",
      "Epoch 5645 - lr: 0.01767 - Train loss: 0.01033 - Test loss: 0.04032\n",
      "Epoch 5646 - lr: 0.01767 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5647 - lr: 0.01766 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5648 - lr: 0.01766 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5649 - lr: 0.01765 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5650 - lr: 0.01765 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5651 - lr: 0.01764 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5652 - lr: 0.01764 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5653 - lr: 0.01763 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5654 - lr: 0.01763 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5655 - lr: 0.01762 - Train loss: 0.01032 - Test loss: 0.04032\n",
      "Epoch 5656 - lr: 0.01761 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5657 - lr: 0.01761 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5658 - lr: 0.01760 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5659 - lr: 0.01760 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5660 - lr: 0.01759 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5661 - lr: 0.01759 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5662 - lr: 0.01758 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5663 - lr: 0.01758 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5664 - lr: 0.01757 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5665 - lr: 0.01757 - Train loss: 0.01031 - Test loss: 0.04032\n",
      "Epoch 5666 - lr: 0.01756 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5667 - lr: 0.01755 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5668 - lr: 0.01755 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5669 - lr: 0.01754 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5670 - lr: 0.01754 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5671 - lr: 0.01753 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5672 - lr: 0.01753 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5673 - lr: 0.01752 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5674 - lr: 0.01752 - Train loss: 0.01030 - Test loss: 0.04032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5675 - lr: 0.01751 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5676 - lr: 0.01751 - Train loss: 0.01030 - Test loss: 0.04032\n",
      "Epoch 5677 - lr: 0.01750 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5678 - lr: 0.01750 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5679 - lr: 0.01749 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5680 - lr: 0.01749 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5681 - lr: 0.01748 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5682 - lr: 0.01747 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5683 - lr: 0.01747 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5684 - lr: 0.01746 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5685 - lr: 0.01746 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5686 - lr: 0.01745 - Train loss: 0.01029 - Test loss: 0.04032\n",
      "Epoch 5687 - lr: 0.01745 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5688 - lr: 0.01744 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5689 - lr: 0.01744 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5690 - lr: 0.01743 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5691 - lr: 0.01743 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5692 - lr: 0.01742 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5693 - lr: 0.01742 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5694 - lr: 0.01741 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5695 - lr: 0.01740 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5696 - lr: 0.01740 - Train loss: 0.01028 - Test loss: 0.04032\n",
      "Epoch 5697 - lr: 0.01739 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5698 - lr: 0.01739 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5699 - lr: 0.01738 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5700 - lr: 0.01738 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5701 - lr: 0.01737 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5702 - lr: 0.01737 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5703 - lr: 0.01736 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5704 - lr: 0.01736 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5705 - lr: 0.01735 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5706 - lr: 0.01735 - Train loss: 0.01027 - Test loss: 0.04032\n",
      "Epoch 5707 - lr: 0.01734 - Train loss: 0.01026 - Test loss: 0.04032\n",
      "Epoch 5708 - lr: 0.01734 - Train loss: 0.01026 - Test loss: 0.04032\n",
      "Epoch 5709 - lr: 0.01733 - Train loss: 0.01026 - Test loss: 0.04032\n",
      "Epoch 5710 - lr: 0.01732 - Train loss: 0.01026 - Test loss: 0.04032\n",
      "Epoch 5711 - lr: 0.01732 - Train loss: 0.01026 - Test loss: 0.04032\n",
      "Epoch 5712 - lr: 0.01731 - Train loss: 0.01026 - Test loss: 0.04031\n",
      "Epoch 5713 - lr: 0.01731 - Train loss: 0.01026 - Test loss: 0.04031\n",
      "Epoch 5714 - lr: 0.01730 - Train loss: 0.01026 - Test loss: 0.04031\n",
      "Epoch 5715 - lr: 0.01730 - Train loss: 0.01026 - Test loss: 0.04031\n",
      "Epoch 5716 - lr: 0.01729 - Train loss: 0.01026 - Test loss: 0.04031\n",
      "Epoch 5717 - lr: 0.01729 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5718 - lr: 0.01728 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5719 - lr: 0.01728 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5720 - lr: 0.01727 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5721 - lr: 0.01727 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5722 - lr: 0.01726 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5723 - lr: 0.01726 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5724 - lr: 0.01725 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5725 - lr: 0.01725 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5726 - lr: 0.01724 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5727 - lr: 0.01723 - Train loss: 0.01025 - Test loss: 0.04031\n",
      "Epoch 5728 - lr: 0.01723 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5729 - lr: 0.01722 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5730 - lr: 0.01722 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5731 - lr: 0.01721 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5732 - lr: 0.01721 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5733 - lr: 0.01720 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5734 - lr: 0.01720 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5735 - lr: 0.01719 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5736 - lr: 0.01719 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5737 - lr: 0.01718 - Train loss: 0.01024 - Test loss: 0.04031\n",
      "Epoch 5738 - lr: 0.01718 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5739 - lr: 0.01717 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5740 - lr: 0.01717 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5741 - lr: 0.01716 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5742 - lr: 0.01716 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5743 - lr: 0.01715 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5744 - lr: 0.01714 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5745 - lr: 0.01714 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5746 - lr: 0.01713 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5747 - lr: 0.01713 - Train loss: 0.01023 - Test loss: 0.04031\n",
      "Epoch 5748 - lr: 0.01712 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5749 - lr: 0.01712 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5750 - lr: 0.01711 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5751 - lr: 0.01711 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5752 - lr: 0.01710 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5753 - lr: 0.01710 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5754 - lr: 0.01709 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5755 - lr: 0.01709 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5756 - lr: 0.01708 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5757 - lr: 0.01708 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5758 - lr: 0.01707 - Train loss: 0.01022 - Test loss: 0.04031\n",
      "Epoch 5759 - lr: 0.01707 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5760 - lr: 0.01706 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5761 - lr: 0.01706 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5762 - lr: 0.01705 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5763 - lr: 0.01705 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5764 - lr: 0.01704 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5765 - lr: 0.01703 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5766 - lr: 0.01703 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5767 - lr: 0.01702 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5768 - lr: 0.01702 - Train loss: 0.01021 - Test loss: 0.04031\n",
      "Epoch 5769 - lr: 0.01701 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5770 - lr: 0.01701 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5771 - lr: 0.01700 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5772 - lr: 0.01700 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5773 - lr: 0.01699 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5774 - lr: 0.01699 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5775 - lr: 0.01698 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5776 - lr: 0.01698 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5777 - lr: 0.01697 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5778 - lr: 0.01697 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5779 - lr: 0.01696 - Train loss: 0.01020 - Test loss: 0.04031\n",
      "Epoch 5780 - lr: 0.01696 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5781 - lr: 0.01695 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5782 - lr: 0.01695 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5783 - lr: 0.01694 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5784 - lr: 0.01694 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5785 - lr: 0.01693 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5786 - lr: 0.01693 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5787 - lr: 0.01692 - Train loss: 0.01019 - Test loss: 0.04031\n",
      "Epoch 5788 - lr: 0.01691 - Train loss: 0.01019 - Test loss: 0.04030\n",
      "Epoch 5789 - lr: 0.01691 - Train loss: 0.01019 - Test loss: 0.04030\n",
      "Epoch 5790 - lr: 0.01690 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5791 - lr: 0.01690 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5792 - lr: 0.01689 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5793 - lr: 0.01689 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5794 - lr: 0.01688 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5795 - lr: 0.01688 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5796 - lr: 0.01687 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5797 - lr: 0.01687 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5798 - lr: 0.01686 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5799 - lr: 0.01686 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5800 - lr: 0.01685 - Train loss: 0.01018 - Test loss: 0.04030\n",
      "Epoch 5801 - lr: 0.01685 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5802 - lr: 0.01684 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5803 - lr: 0.01684 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5804 - lr: 0.01683 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5805 - lr: 0.01683 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5806 - lr: 0.01682 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5807 - lr: 0.01682 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5808 - lr: 0.01681 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5809 - lr: 0.01681 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5810 - lr: 0.01680 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5811 - lr: 0.01680 - Train loss: 0.01017 - Test loss: 0.04030\n",
      "Epoch 5812 - lr: 0.01679 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5813 - lr: 0.01679 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5814 - lr: 0.01678 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5815 - lr: 0.01678 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5816 - lr: 0.01677 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5817 - lr: 0.01676 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5818 - lr: 0.01676 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5819 - lr: 0.01675 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5820 - lr: 0.01675 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5821 - lr: 0.01674 - Train loss: 0.01016 - Test loss: 0.04030\n",
      "Epoch 5822 - lr: 0.01674 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5823 - lr: 0.01673 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5824 - lr: 0.01673 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5825 - lr: 0.01672 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5826 - lr: 0.01672 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5827 - lr: 0.01671 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5828 - lr: 0.01671 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5829 - lr: 0.01670 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5830 - lr: 0.01670 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5831 - lr: 0.01669 - Train loss: 0.01015 - Test loss: 0.04030\n",
      "Epoch 5832 - lr: 0.01669 - Train loss: 0.01015 - Test loss: 0.04030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5833 - lr: 0.01668 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5834 - lr: 0.01668 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5835 - lr: 0.01667 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5836 - lr: 0.01667 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5837 - lr: 0.01666 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5838 - lr: 0.01666 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5839 - lr: 0.01665 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5840 - lr: 0.01665 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5841 - lr: 0.01664 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5842 - lr: 0.01664 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5843 - lr: 0.01663 - Train loss: 0.01014 - Test loss: 0.04030\n",
      "Epoch 5844 - lr: 0.01663 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5845 - lr: 0.01662 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5846 - lr: 0.01662 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5847 - lr: 0.01661 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5848 - lr: 0.01661 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5849 - lr: 0.01660 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5850 - lr: 0.01660 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5851 - lr: 0.01659 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5852 - lr: 0.01659 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5853 - lr: 0.01658 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5854 - lr: 0.01658 - Train loss: 0.01013 - Test loss: 0.04030\n",
      "Epoch 5855 - lr: 0.01657 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5856 - lr: 0.01657 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5857 - lr: 0.01656 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5858 - lr: 0.01656 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5859 - lr: 0.01655 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5860 - lr: 0.01654 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5861 - lr: 0.01654 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5862 - lr: 0.01653 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5863 - lr: 0.01653 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5864 - lr: 0.01652 - Train loss: 0.01012 - Test loss: 0.04030\n",
      "Epoch 5865 - lr: 0.01652 - Train loss: 0.01011 - Test loss: 0.04030\n",
      "Epoch 5866 - lr: 0.01651 - Train loss: 0.01011 - Test loss: 0.04030\n",
      "Epoch 5867 - lr: 0.01651 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5868 - lr: 0.01650 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5869 - lr: 0.01650 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5870 - lr: 0.01649 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5871 - lr: 0.01649 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5872 - lr: 0.01648 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5873 - lr: 0.01648 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5874 - lr: 0.01647 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5875 - lr: 0.01647 - Train loss: 0.01011 - Test loss: 0.04029\n",
      "Epoch 5876 - lr: 0.01646 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5877 - lr: 0.01646 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5878 - lr: 0.01645 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5879 - lr: 0.01645 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5880 - lr: 0.01644 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5881 - lr: 0.01644 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5882 - lr: 0.01643 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5883 - lr: 0.01643 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5884 - lr: 0.01642 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5885 - lr: 0.01642 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5886 - lr: 0.01641 - Train loss: 0.01010 - Test loss: 0.04029\n",
      "Epoch 5887 - lr: 0.01641 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5888 - lr: 0.01640 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5889 - lr: 0.01640 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5890 - lr: 0.01639 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5891 - lr: 0.01639 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5892 - lr: 0.01638 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5893 - lr: 0.01638 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5894 - lr: 0.01637 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5895 - lr: 0.01637 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5896 - lr: 0.01636 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5897 - lr: 0.01636 - Train loss: 0.01009 - Test loss: 0.04029\n",
      "Epoch 5898 - lr: 0.01635 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5899 - lr: 0.01635 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5900 - lr: 0.01634 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5901 - lr: 0.01634 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5902 - lr: 0.01633 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5903 - lr: 0.01633 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5904 - lr: 0.01632 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5905 - lr: 0.01632 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5906 - lr: 0.01631 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5907 - lr: 0.01631 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5908 - lr: 0.01630 - Train loss: 0.01008 - Test loss: 0.04029\n",
      "Epoch 5909 - lr: 0.01630 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5910 - lr: 0.01629 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5911 - lr: 0.01629 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5912 - lr: 0.01628 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5913 - lr: 0.01628 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5914 - lr: 0.01627 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5915 - lr: 0.01627 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5916 - lr: 0.01626 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5917 - lr: 0.01626 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5918 - lr: 0.01625 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5919 - lr: 0.01625 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5920 - lr: 0.01624 - Train loss: 0.01007 - Test loss: 0.04029\n",
      "Epoch 5921 - lr: 0.01624 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5922 - lr: 0.01623 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5923 - lr: 0.01623 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5924 - lr: 0.01622 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5925 - lr: 0.01622 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5926 - lr: 0.01621 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5927 - lr: 0.01621 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5928 - lr: 0.01620 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5929 - lr: 0.01620 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5930 - lr: 0.01619 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5931 - lr: 0.01619 - Train loss: 0.01006 - Test loss: 0.04029\n",
      "Epoch 5932 - lr: 0.01618 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5933 - lr: 0.01618 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5934 - lr: 0.01617 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5935 - lr: 0.01617 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5936 - lr: 0.01616 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5937 - lr: 0.01616 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5938 - lr: 0.01615 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5939 - lr: 0.01615 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5940 - lr: 0.01614 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5941 - lr: 0.01614 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5942 - lr: 0.01613 - Train loss: 0.01005 - Test loss: 0.04029\n",
      "Epoch 5943 - lr: 0.01613 - Train loss: 0.01004 - Test loss: 0.04029\n",
      "Epoch 5944 - lr: 0.01612 - Train loss: 0.01004 - Test loss: 0.04029\n",
      "Epoch 5945 - lr: 0.01612 - Train loss: 0.01004 - Test loss: 0.04029\n",
      "Epoch 5946 - lr: 0.01611 - Train loss: 0.01004 - Test loss: 0.04029\n",
      "Epoch 5947 - lr: 0.01611 - Train loss: 0.01004 - Test loss: 0.04029\n",
      "Epoch 5948 - lr: 0.01610 - Train loss: 0.01004 - Test loss: 0.04029\n",
      "Epoch 5949 - lr: 0.01610 - Train loss: 0.01004 - Test loss: 0.04028\n",
      "Epoch 5950 - lr: 0.01609 - Train loss: 0.01004 - Test loss: 0.04028\n",
      "Epoch 5951 - lr: 0.01609 - Train loss: 0.01004 - Test loss: 0.04028\n",
      "Epoch 5952 - lr: 0.01608 - Train loss: 0.01004 - Test loss: 0.04028\n",
      "Epoch 5953 - lr: 0.01608 - Train loss: 0.01004 - Test loss: 0.04028\n",
      "Epoch 5954 - lr: 0.01607 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5955 - lr: 0.01607 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5956 - lr: 0.01606 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5957 - lr: 0.01606 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5958 - lr: 0.01605 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5959 - lr: 0.01605 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5960 - lr: 0.01604 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5961 - lr: 0.01604 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5962 - lr: 0.01603 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5963 - lr: 0.01603 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5964 - lr: 0.01603 - Train loss: 0.01003 - Test loss: 0.04028\n",
      "Epoch 5965 - lr: 0.01602 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5966 - lr: 0.01602 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5967 - lr: 0.01601 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5968 - lr: 0.01601 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5969 - lr: 0.01600 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5970 - lr: 0.01600 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5971 - lr: 0.01599 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5972 - lr: 0.01599 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5973 - lr: 0.01598 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5974 - lr: 0.01598 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5975 - lr: 0.01597 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5976 - lr: 0.01597 - Train loss: 0.01002 - Test loss: 0.04028\n",
      "Epoch 5977 - lr: 0.01596 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5978 - lr: 0.01596 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5979 - lr: 0.01595 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5980 - lr: 0.01595 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5981 - lr: 0.01594 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5982 - lr: 0.01594 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5983 - lr: 0.01593 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5984 - lr: 0.01593 - Train loss: 0.01001 - Test loss: 0.04028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5985 - lr: 0.01592 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5986 - lr: 0.01592 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5987 - lr: 0.01591 - Train loss: 0.01001 - Test loss: 0.04028\n",
      "Epoch 5988 - lr: 0.01591 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5989 - lr: 0.01590 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5990 - lr: 0.01590 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5991 - lr: 0.01589 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5992 - lr: 0.01589 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5993 - lr: 0.01588 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5994 - lr: 0.01588 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5995 - lr: 0.01587 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5996 - lr: 0.01587 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5997 - lr: 0.01586 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5998 - lr: 0.01586 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 5999 - lr: 0.01585 - Train loss: 0.01000 - Test loss: 0.04028\n",
      "Epoch 6000 - lr: 0.01585 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6001 - lr: 0.01584 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6002 - lr: 0.01584 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6003 - lr: 0.01583 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6004 - lr: 0.01583 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6005 - lr: 0.01582 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6006 - lr: 0.01582 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6007 - lr: 0.01581 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6008 - lr: 0.01581 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6009 - lr: 0.01581 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6010 - lr: 0.01580 - Train loss: 0.00999 - Test loss: 0.04028\n",
      "Epoch 6011 - lr: 0.01580 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6012 - lr: 0.01579 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6013 - lr: 0.01579 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6014 - lr: 0.01578 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6015 - lr: 0.01578 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6016 - lr: 0.01577 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6017 - lr: 0.01577 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6018 - lr: 0.01576 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6019 - lr: 0.01576 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6020 - lr: 0.01575 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6021 - lr: 0.01575 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6022 - lr: 0.01574 - Train loss: 0.00998 - Test loss: 0.04028\n",
      "Epoch 6023 - lr: 0.01574 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6024 - lr: 0.01573 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6025 - lr: 0.01573 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6026 - lr: 0.01572 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6027 - lr: 0.01572 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6028 - lr: 0.01571 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6029 - lr: 0.01571 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6030 - lr: 0.01570 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6031 - lr: 0.01570 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6032 - lr: 0.01569 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6033 - lr: 0.01569 - Train loss: 0.00997 - Test loss: 0.04028\n",
      "Epoch 6034 - lr: 0.01568 - Train loss: 0.00996 - Test loss: 0.04028\n",
      "Epoch 6035 - lr: 0.01568 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6036 - lr: 0.01567 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6037 - lr: 0.01567 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6038 - lr: 0.01567 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6039 - lr: 0.01566 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6040 - lr: 0.01566 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6041 - lr: 0.01565 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6042 - lr: 0.01565 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6043 - lr: 0.01564 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6044 - lr: 0.01564 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6045 - lr: 0.01563 - Train loss: 0.00996 - Test loss: 0.04027\n",
      "Epoch 6046 - lr: 0.01563 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6047 - lr: 0.01562 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6048 - lr: 0.01562 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6049 - lr: 0.01561 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6050 - lr: 0.01561 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6051 - lr: 0.01560 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6052 - lr: 0.01560 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6053 - lr: 0.01559 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6054 - lr: 0.01559 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6055 - lr: 0.01558 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6056 - lr: 0.01558 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6057 - lr: 0.01557 - Train loss: 0.00995 - Test loss: 0.04027\n",
      "Epoch 6058 - lr: 0.01557 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6059 - lr: 0.01556 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6060 - lr: 0.01556 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6061 - lr: 0.01555 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6062 - lr: 0.01555 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6063 - lr: 0.01555 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6064 - lr: 0.01554 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6065 - lr: 0.01554 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6066 - lr: 0.01553 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6067 - lr: 0.01553 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6068 - lr: 0.01552 - Train loss: 0.00994 - Test loss: 0.04027\n",
      "Epoch 6069 - lr: 0.01552 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6070 - lr: 0.01551 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6071 - lr: 0.01551 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6072 - lr: 0.01550 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6073 - lr: 0.01550 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6074 - lr: 0.01549 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6075 - lr: 0.01549 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6076 - lr: 0.01548 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6077 - lr: 0.01548 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6078 - lr: 0.01547 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6079 - lr: 0.01547 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6080 - lr: 0.01546 - Train loss: 0.00993 - Test loss: 0.04027\n",
      "Epoch 6081 - lr: 0.01546 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6082 - lr: 0.01545 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6083 - lr: 0.01545 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6084 - lr: 0.01545 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6085 - lr: 0.01544 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6086 - lr: 0.01544 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6087 - lr: 0.01543 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6088 - lr: 0.01543 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6089 - lr: 0.01542 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6090 - lr: 0.01542 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6091 - lr: 0.01541 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6092 - lr: 0.01541 - Train loss: 0.00992 - Test loss: 0.04027\n",
      "Epoch 6093 - lr: 0.01540 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6094 - lr: 0.01540 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6095 - lr: 0.01539 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6096 - lr: 0.01539 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6097 - lr: 0.01538 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6098 - lr: 0.01538 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6099 - lr: 0.01537 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6100 - lr: 0.01537 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6101 - lr: 0.01537 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6102 - lr: 0.01536 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6103 - lr: 0.01536 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6104 - lr: 0.01535 - Train loss: 0.00991 - Test loss: 0.04027\n",
      "Epoch 6105 - lr: 0.01535 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6106 - lr: 0.01534 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6107 - lr: 0.01534 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6108 - lr: 0.01533 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6109 - lr: 0.01533 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6110 - lr: 0.01532 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6111 - lr: 0.01532 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6112 - lr: 0.01531 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6113 - lr: 0.01531 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6114 - lr: 0.01530 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6115 - lr: 0.01530 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6116 - lr: 0.01529 - Train loss: 0.00990 - Test loss: 0.04027\n",
      "Epoch 6117 - lr: 0.01529 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6118 - lr: 0.01529 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6119 - lr: 0.01528 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6120 - lr: 0.01528 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6121 - lr: 0.01527 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6122 - lr: 0.01527 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6123 - lr: 0.01526 - Train loss: 0.00989 - Test loss: 0.04027\n",
      "Epoch 6124 - lr: 0.01526 - Train loss: 0.00989 - Test loss: 0.04026\n",
      "Epoch 6125 - lr: 0.01525 - Train loss: 0.00989 - Test loss: 0.04026\n",
      "Epoch 6126 - lr: 0.01525 - Train loss: 0.00989 - Test loss: 0.04026\n",
      "Epoch 6127 - lr: 0.01524 - Train loss: 0.00989 - Test loss: 0.04026\n",
      "Epoch 6128 - lr: 0.01524 - Train loss: 0.00989 - Test loss: 0.04026\n",
      "Epoch 6129 - lr: 0.01523 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6130 - lr: 0.01523 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6131 - lr: 0.01522 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6132 - lr: 0.01522 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6133 - lr: 0.01521 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6134 - lr: 0.01521 - Train loss: 0.00988 - Test loss: 0.04026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6135 - lr: 0.01521 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6136 - lr: 0.01520 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6137 - lr: 0.01520 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6138 - lr: 0.01519 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6139 - lr: 0.01519 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6140 - lr: 0.01518 - Train loss: 0.00988 - Test loss: 0.04026\n",
      "Epoch 6141 - lr: 0.01518 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6142 - lr: 0.01517 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6143 - lr: 0.01517 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6144 - lr: 0.01516 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6145 - lr: 0.01516 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6146 - lr: 0.01515 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6147 - lr: 0.01515 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6148 - lr: 0.01514 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6149 - lr: 0.01514 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6150 - lr: 0.01514 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6151 - lr: 0.01513 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6152 - lr: 0.01513 - Train loss: 0.00987 - Test loss: 0.04026\n",
      "Epoch 6153 - lr: 0.01512 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6154 - lr: 0.01512 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6155 - lr: 0.01511 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6156 - lr: 0.01511 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6157 - lr: 0.01510 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6158 - lr: 0.01510 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6159 - lr: 0.01509 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6160 - lr: 0.01509 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6161 - lr: 0.01508 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6162 - lr: 0.01508 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6163 - lr: 0.01508 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6164 - lr: 0.01507 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6165 - lr: 0.01507 - Train loss: 0.00986 - Test loss: 0.04026\n",
      "Epoch 6166 - lr: 0.01506 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6167 - lr: 0.01506 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6168 - lr: 0.01505 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6169 - lr: 0.01505 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6170 - lr: 0.01504 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6171 - lr: 0.01504 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6172 - lr: 0.01503 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6173 - lr: 0.01503 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6174 - lr: 0.01502 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6175 - lr: 0.01502 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6176 - lr: 0.01502 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6177 - lr: 0.01501 - Train loss: 0.00985 - Test loss: 0.04026\n",
      "Epoch 6178 - lr: 0.01501 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6179 - lr: 0.01500 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6180 - lr: 0.01500 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6181 - lr: 0.01499 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6182 - lr: 0.01499 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6183 - lr: 0.01498 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6184 - lr: 0.01498 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6185 - lr: 0.01497 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6186 - lr: 0.01497 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6187 - lr: 0.01496 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6188 - lr: 0.01496 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6189 - lr: 0.01496 - Train loss: 0.00984 - Test loss: 0.04026\n",
      "Epoch 6190 - lr: 0.01495 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6191 - lr: 0.01495 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6192 - lr: 0.01494 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6193 - lr: 0.01494 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6194 - lr: 0.01493 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6195 - lr: 0.01493 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6196 - lr: 0.01492 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6197 - lr: 0.01492 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6198 - lr: 0.01491 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6199 - lr: 0.01491 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6200 - lr: 0.01491 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6201 - lr: 0.01490 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6202 - lr: 0.01490 - Train loss: 0.00983 - Test loss: 0.04026\n",
      "Epoch 6203 - lr: 0.01489 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6204 - lr: 0.01489 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6205 - lr: 0.01488 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6206 - lr: 0.01488 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6207 - lr: 0.01487 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6208 - lr: 0.01487 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6209 - lr: 0.01486 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6210 - lr: 0.01486 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6211 - lr: 0.01485 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6212 - lr: 0.01485 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6213 - lr: 0.01485 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6214 - lr: 0.01484 - Train loss: 0.00982 - Test loss: 0.04026\n",
      "Epoch 6215 - lr: 0.01484 - Train loss: 0.00981 - Test loss: 0.04026\n",
      "Epoch 6216 - lr: 0.01483 - Train loss: 0.00981 - Test loss: 0.04026\n",
      "Epoch 6217 - lr: 0.01483 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6218 - lr: 0.01482 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6219 - lr: 0.01482 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6220 - lr: 0.01481 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6221 - lr: 0.01481 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6222 - lr: 0.01480 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6223 - lr: 0.01480 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6224 - lr: 0.01480 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6225 - lr: 0.01479 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6226 - lr: 0.01479 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6227 - lr: 0.01478 - Train loss: 0.00981 - Test loss: 0.04025\n",
      "Epoch 6228 - lr: 0.01478 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6229 - lr: 0.01477 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6230 - lr: 0.01477 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6231 - lr: 0.01476 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6232 - lr: 0.01476 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6233 - lr: 0.01475 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6234 - lr: 0.01475 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6235 - lr: 0.01475 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6236 - lr: 0.01474 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6237 - lr: 0.01474 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6238 - lr: 0.01473 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6239 - lr: 0.01473 - Train loss: 0.00980 - Test loss: 0.04025\n",
      "Epoch 6240 - lr: 0.01472 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6241 - lr: 0.01472 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6242 - lr: 0.01471 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6243 - lr: 0.01471 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6244 - lr: 0.01471 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6245 - lr: 0.01470 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6246 - lr: 0.01470 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6247 - lr: 0.01469 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6248 - lr: 0.01469 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6249 - lr: 0.01468 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6250 - lr: 0.01468 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6251 - lr: 0.01467 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6252 - lr: 0.01467 - Train loss: 0.00979 - Test loss: 0.04025\n",
      "Epoch 6253 - lr: 0.01466 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6254 - lr: 0.01466 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6255 - lr: 0.01466 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6256 - lr: 0.01465 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6257 - lr: 0.01465 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6258 - lr: 0.01464 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6259 - lr: 0.01464 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6260 - lr: 0.01463 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6261 - lr: 0.01463 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6262 - lr: 0.01462 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6263 - lr: 0.01462 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6264 - lr: 0.01462 - Train loss: 0.00978 - Test loss: 0.04025\n",
      "Epoch 6265 - lr: 0.01461 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6266 - lr: 0.01461 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6267 - lr: 0.01460 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6268 - lr: 0.01460 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6269 - lr: 0.01459 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6270 - lr: 0.01459 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6271 - lr: 0.01458 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6272 - lr: 0.01458 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6273 - lr: 0.01457 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6274 - lr: 0.01457 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6275 - lr: 0.01457 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6276 - lr: 0.01456 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6277 - lr: 0.01456 - Train loss: 0.00977 - Test loss: 0.04025\n",
      "Epoch 6278 - lr: 0.01455 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6279 - lr: 0.01455 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6280 - lr: 0.01454 - Train loss: 0.00976 - Test loss: 0.04025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6281 - lr: 0.01454 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6282 - lr: 0.01453 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6283 - lr: 0.01453 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6284 - lr: 0.01453 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6285 - lr: 0.01452 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6286 - lr: 0.01452 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6287 - lr: 0.01451 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6288 - lr: 0.01451 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6289 - lr: 0.01450 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6290 - lr: 0.01450 - Train loss: 0.00976 - Test loss: 0.04025\n",
      "Epoch 6291 - lr: 0.01449 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6292 - lr: 0.01449 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6293 - lr: 0.01449 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6294 - lr: 0.01448 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6295 - lr: 0.01448 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6296 - lr: 0.01447 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6297 - lr: 0.01447 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6298 - lr: 0.01446 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6299 - lr: 0.01446 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6300 - lr: 0.01445 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6301 - lr: 0.01445 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6302 - lr: 0.01445 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6303 - lr: 0.01444 - Train loss: 0.00975 - Test loss: 0.04025\n",
      "Epoch 6304 - lr: 0.01444 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6305 - lr: 0.01443 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6306 - lr: 0.01443 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6307 - lr: 0.01442 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6308 - lr: 0.01442 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6309 - lr: 0.01441 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6310 - lr: 0.01441 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6311 - lr: 0.01441 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6312 - lr: 0.01440 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6313 - lr: 0.01440 - Train loss: 0.00974 - Test loss: 0.04025\n",
      "Epoch 6314 - lr: 0.01439 - Train loss: 0.00974 - Test loss: 0.04024\n",
      "Epoch 6315 - lr: 0.01439 - Train loss: 0.00974 - Test loss: 0.04024\n",
      "Epoch 6316 - lr: 0.01438 - Train loss: 0.00974 - Test loss: 0.04024\n",
      "Epoch 6317 - lr: 0.01438 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6318 - lr: 0.01437 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6319 - lr: 0.01437 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6320 - lr: 0.01437 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6321 - lr: 0.01436 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6322 - lr: 0.01436 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6323 - lr: 0.01435 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6324 - lr: 0.01435 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6325 - lr: 0.01434 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6326 - lr: 0.01434 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6327 - lr: 0.01434 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6328 - lr: 0.01433 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6329 - lr: 0.01433 - Train loss: 0.00973 - Test loss: 0.04024\n",
      "Epoch 6330 - lr: 0.01432 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6331 - lr: 0.01432 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6332 - lr: 0.01431 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6333 - lr: 0.01431 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6334 - lr: 0.01430 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6335 - lr: 0.01430 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6336 - lr: 0.01430 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6337 - lr: 0.01429 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6338 - lr: 0.01429 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6339 - lr: 0.01428 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6340 - lr: 0.01428 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6341 - lr: 0.01427 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6342 - lr: 0.01427 - Train loss: 0.00972 - Test loss: 0.04024\n",
      "Epoch 6343 - lr: 0.01426 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6344 - lr: 0.01426 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6345 - lr: 0.01426 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6346 - lr: 0.01425 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6347 - lr: 0.01425 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6348 - lr: 0.01424 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6349 - lr: 0.01424 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6350 - lr: 0.01423 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6351 - lr: 0.01423 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6352 - lr: 0.01423 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6353 - lr: 0.01422 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6354 - lr: 0.01422 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6355 - lr: 0.01421 - Train loss: 0.00971 - Test loss: 0.04024\n",
      "Epoch 6356 - lr: 0.01421 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6357 - lr: 0.01420 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6358 - lr: 0.01420 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6359 - lr: 0.01419 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6360 - lr: 0.01419 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6361 - lr: 0.01419 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6362 - lr: 0.01418 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6363 - lr: 0.01418 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6364 - lr: 0.01417 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6365 - lr: 0.01417 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6366 - lr: 0.01416 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6367 - lr: 0.01416 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6368 - lr: 0.01416 - Train loss: 0.00970 - Test loss: 0.04024\n",
      "Epoch 6369 - lr: 0.01415 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6370 - lr: 0.01415 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6371 - lr: 0.01414 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6372 - lr: 0.01414 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6373 - lr: 0.01413 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6374 - lr: 0.01413 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6375 - lr: 0.01413 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6376 - lr: 0.01412 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6377 - lr: 0.01412 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6378 - lr: 0.01411 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6379 - lr: 0.01411 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6380 - lr: 0.01410 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6381 - lr: 0.01410 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6382 - lr: 0.01410 - Train loss: 0.00969 - Test loss: 0.04024\n",
      "Epoch 6383 - lr: 0.01409 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6384 - lr: 0.01409 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6385 - lr: 0.01408 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6386 - lr: 0.01408 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6387 - lr: 0.01407 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6388 - lr: 0.01407 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6389 - lr: 0.01406 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6390 - lr: 0.01406 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6391 - lr: 0.01406 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6392 - lr: 0.01405 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6393 - lr: 0.01405 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6394 - lr: 0.01404 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6395 - lr: 0.01404 - Train loss: 0.00968 - Test loss: 0.04024\n",
      "Epoch 6396 - lr: 0.01403 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6397 - lr: 0.01403 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6398 - lr: 0.01403 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6399 - lr: 0.01402 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6400 - lr: 0.01402 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6401 - lr: 0.01401 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6402 - lr: 0.01401 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6403 - lr: 0.01400 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6404 - lr: 0.01400 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6405 - lr: 0.01400 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6406 - lr: 0.01399 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6407 - lr: 0.01399 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6408 - lr: 0.01398 - Train loss: 0.00967 - Test loss: 0.04024\n",
      "Epoch 6409 - lr: 0.01398 - Train loss: 0.00966 - Test loss: 0.04024\n",
      "Epoch 6410 - lr: 0.01397 - Train loss: 0.00966 - Test loss: 0.04024\n",
      "Epoch 6411 - lr: 0.01397 - Train loss: 0.00966 - Test loss: 0.04024\n",
      "Epoch 6412 - lr: 0.01397 - Train loss: 0.00966 - Test loss: 0.04024\n",
      "Epoch 6413 - lr: 0.01396 - Train loss: 0.00966 - Test loss: 0.04024\n",
      "Epoch 6414 - lr: 0.01396 - Train loss: 0.00966 - Test loss: 0.04024\n",
      "Epoch 6415 - lr: 0.01395 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6416 - lr: 0.01395 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6417 - lr: 0.01394 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6418 - lr: 0.01394 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6419 - lr: 0.01394 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6420 - lr: 0.01393 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6421 - lr: 0.01393 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6422 - lr: 0.01392 - Train loss: 0.00966 - Test loss: 0.04023\n",
      "Epoch 6423 - lr: 0.01392 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6424 - lr: 0.01391 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6425 - lr: 0.01391 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6426 - lr: 0.01391 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6427 - lr: 0.01390 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6428 - lr: 0.01390 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6429 - lr: 0.01389 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6430 - lr: 0.01389 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6431 - lr: 0.01388 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6432 - lr: 0.01388 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6433 - lr: 0.01388 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6434 - lr: 0.01387 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6435 - lr: 0.01387 - Train loss: 0.00965 - Test loss: 0.04023\n",
      "Epoch 6436 - lr: 0.01386 - Train loss: 0.00964 - Test loss: 0.04023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6437 - lr: 0.01386 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6438 - lr: 0.01385 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6439 - lr: 0.01385 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6440 - lr: 0.01385 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6441 - lr: 0.01384 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6442 - lr: 0.01384 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6443 - lr: 0.01383 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6444 - lr: 0.01383 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6445 - lr: 0.01383 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6446 - lr: 0.01382 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6447 - lr: 0.01382 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6448 - lr: 0.01381 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6449 - lr: 0.01381 - Train loss: 0.00964 - Test loss: 0.04023\n",
      "Epoch 6450 - lr: 0.01380 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6451 - lr: 0.01380 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6452 - lr: 0.01380 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6453 - lr: 0.01379 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6454 - lr: 0.01379 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6455 - lr: 0.01378 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6456 - lr: 0.01378 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6457 - lr: 0.01377 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6458 - lr: 0.01377 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6459 - lr: 0.01377 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6460 - lr: 0.01376 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6461 - lr: 0.01376 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6462 - lr: 0.01375 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6463 - lr: 0.01375 - Train loss: 0.00963 - Test loss: 0.04023\n",
      "Epoch 6464 - lr: 0.01374 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6465 - lr: 0.01374 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6466 - lr: 0.01374 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6467 - lr: 0.01373 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6468 - lr: 0.01373 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6469 - lr: 0.01372 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6470 - lr: 0.01372 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6471 - lr: 0.01372 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6472 - lr: 0.01371 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6473 - lr: 0.01371 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6474 - lr: 0.01370 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6475 - lr: 0.01370 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6476 - lr: 0.01369 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6477 - lr: 0.01369 - Train loss: 0.00962 - Test loss: 0.04023\n",
      "Epoch 6478 - lr: 0.01369 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6479 - lr: 0.01368 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6480 - lr: 0.01368 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6481 - lr: 0.01367 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6482 - lr: 0.01367 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6483 - lr: 0.01366 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6484 - lr: 0.01366 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6485 - lr: 0.01366 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6486 - lr: 0.01365 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6487 - lr: 0.01365 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6488 - lr: 0.01364 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6489 - lr: 0.01364 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6490 - lr: 0.01364 - Train loss: 0.00961 - Test loss: 0.04023\n",
      "Epoch 6491 - lr: 0.01363 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6492 - lr: 0.01363 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6493 - lr: 0.01362 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6494 - lr: 0.01362 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6495 - lr: 0.01361 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6496 - lr: 0.01361 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6497 - lr: 0.01361 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6498 - lr: 0.01360 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6499 - lr: 0.01360 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6500 - lr: 0.01359 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6501 - lr: 0.01359 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6502 - lr: 0.01359 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6503 - lr: 0.01358 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6504 - lr: 0.01358 - Train loss: 0.00960 - Test loss: 0.04023\n",
      "Epoch 6505 - lr: 0.01357 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6506 - lr: 0.01357 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6507 - lr: 0.01356 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6508 - lr: 0.01356 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6509 - lr: 0.01356 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6510 - lr: 0.01355 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6511 - lr: 0.01355 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6512 - lr: 0.01354 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6513 - lr: 0.01354 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6514 - lr: 0.01354 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6515 - lr: 0.01353 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6516 - lr: 0.01353 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6517 - lr: 0.01352 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6518 - lr: 0.01352 - Train loss: 0.00959 - Test loss: 0.04023\n",
      "Epoch 6519 - lr: 0.01351 - Train loss: 0.00958 - Test loss: 0.04023\n",
      "Epoch 6520 - lr: 0.01351 - Train loss: 0.00958 - Test loss: 0.04023\n",
      "Epoch 6521 - lr: 0.01351 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6522 - lr: 0.01350 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6523 - lr: 0.01350 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6524 - lr: 0.01349 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6525 - lr: 0.01349 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6526 - lr: 0.01349 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6527 - lr: 0.01348 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6528 - lr: 0.01348 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6529 - lr: 0.01347 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6530 - lr: 0.01347 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6531 - lr: 0.01346 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6532 - lr: 0.01346 - Train loss: 0.00958 - Test loss: 0.04022\n",
      "Epoch 6533 - lr: 0.01346 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6534 - lr: 0.01345 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6535 - lr: 0.01345 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6536 - lr: 0.01344 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6537 - lr: 0.01344 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6538 - lr: 0.01344 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6539 - lr: 0.01343 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6540 - lr: 0.01343 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6541 - lr: 0.01342 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6542 - lr: 0.01342 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6543 - lr: 0.01342 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6544 - lr: 0.01341 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6545 - lr: 0.01341 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6546 - lr: 0.01340 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6547 - lr: 0.01340 - Train loss: 0.00957 - Test loss: 0.04022\n",
      "Epoch 6548 - lr: 0.01339 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6549 - lr: 0.01339 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6550 - lr: 0.01339 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6551 - lr: 0.01338 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6552 - lr: 0.01338 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6553 - lr: 0.01337 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6554 - lr: 0.01337 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6555 - lr: 0.01337 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6556 - lr: 0.01336 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6557 - lr: 0.01336 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6558 - lr: 0.01335 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6559 - lr: 0.01335 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6560 - lr: 0.01335 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6561 - lr: 0.01334 - Train loss: 0.00956 - Test loss: 0.04022\n",
      "Epoch 6562 - lr: 0.01334 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6563 - lr: 0.01333 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6564 - lr: 0.01333 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6565 - lr: 0.01332 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6566 - lr: 0.01332 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6567 - lr: 0.01332 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6568 - lr: 0.01331 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6569 - lr: 0.01331 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6570 - lr: 0.01330 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6571 - lr: 0.01330 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6572 - lr: 0.01330 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6573 - lr: 0.01329 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6574 - lr: 0.01329 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6575 - lr: 0.01328 - Train loss: 0.00955 - Test loss: 0.04022\n",
      "Epoch 6576 - lr: 0.01328 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6577 - lr: 0.01328 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6578 - lr: 0.01327 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6579 - lr: 0.01327 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6580 - lr: 0.01326 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6581 - lr: 0.01326 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6582 - lr: 0.01326 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6583 - lr: 0.01325 - Train loss: 0.00954 - Test loss: 0.04022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6584 - lr: 0.01325 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6585 - lr: 0.01324 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6586 - lr: 0.01324 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6587 - lr: 0.01324 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6588 - lr: 0.01323 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6589 - lr: 0.01323 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6590 - lr: 0.01322 - Train loss: 0.00954 - Test loss: 0.04022\n",
      "Epoch 6591 - lr: 0.01322 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6592 - lr: 0.01321 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6593 - lr: 0.01321 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6594 - lr: 0.01321 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6595 - lr: 0.01320 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6596 - lr: 0.01320 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6597 - lr: 0.01319 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6598 - lr: 0.01319 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6599 - lr: 0.01319 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6600 - lr: 0.01318 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6601 - lr: 0.01318 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6602 - lr: 0.01317 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6603 - lr: 0.01317 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6604 - lr: 0.01317 - Train loss: 0.00953 - Test loss: 0.04022\n",
      "Epoch 6605 - lr: 0.01316 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6606 - lr: 0.01316 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6607 - lr: 0.01315 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6608 - lr: 0.01315 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6609 - lr: 0.01315 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6610 - lr: 0.01314 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6611 - lr: 0.01314 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6612 - lr: 0.01313 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6613 - lr: 0.01313 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6614 - lr: 0.01313 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6615 - lr: 0.01312 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6616 - lr: 0.01312 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6617 - lr: 0.01311 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6618 - lr: 0.01311 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6619 - lr: 0.01311 - Train loss: 0.00952 - Test loss: 0.04022\n",
      "Epoch 6620 - lr: 0.01310 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6621 - lr: 0.01310 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6622 - lr: 0.01309 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6623 - lr: 0.01309 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6624 - lr: 0.01309 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6625 - lr: 0.01308 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6626 - lr: 0.01308 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6627 - lr: 0.01307 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6628 - lr: 0.01307 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6629 - lr: 0.01307 - Train loss: 0.00951 - Test loss: 0.04022\n",
      "Epoch 6630 - lr: 0.01306 - Train loss: 0.00951 - Test loss: 0.04021\n",
      "Epoch 6631 - lr: 0.01306 - Train loss: 0.00951 - Test loss: 0.04021\n",
      "Epoch 6632 - lr: 0.01305 - Train loss: 0.00951 - Test loss: 0.04021\n",
      "Epoch 6633 - lr: 0.01305 - Train loss: 0.00951 - Test loss: 0.04021\n",
      "Epoch 6634 - lr: 0.01305 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6635 - lr: 0.01304 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6636 - lr: 0.01304 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6637 - lr: 0.01303 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6638 - lr: 0.01303 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6639 - lr: 0.01303 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6640 - lr: 0.01302 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6641 - lr: 0.01302 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6642 - lr: 0.01301 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6643 - lr: 0.01301 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6644 - lr: 0.01301 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6645 - lr: 0.01300 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6646 - lr: 0.01300 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6647 - lr: 0.01299 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6648 - lr: 0.01299 - Train loss: 0.00950 - Test loss: 0.04021\n",
      "Epoch 6649 - lr: 0.01299 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6650 - lr: 0.01298 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6651 - lr: 0.01298 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6652 - lr: 0.01297 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6653 - lr: 0.01297 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6654 - lr: 0.01297 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6655 - lr: 0.01296 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6656 - lr: 0.01296 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6657 - lr: 0.01295 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6658 - lr: 0.01295 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6659 - lr: 0.01295 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6660 - lr: 0.01294 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6661 - lr: 0.01294 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6662 - lr: 0.01293 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6663 - lr: 0.01293 - Train loss: 0.00949 - Test loss: 0.04021\n",
      "Epoch 6664 - lr: 0.01293 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6665 - lr: 0.01292 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6666 - lr: 0.01292 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6667 - lr: 0.01291 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6668 - lr: 0.01291 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6669 - lr: 0.01291 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6670 - lr: 0.01290 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6671 - lr: 0.01290 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6672 - lr: 0.01289 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6673 - lr: 0.01289 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6674 - lr: 0.01289 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6675 - lr: 0.01288 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6676 - lr: 0.01288 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6677 - lr: 0.01287 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6678 - lr: 0.01287 - Train loss: 0.00948 - Test loss: 0.04021\n",
      "Epoch 6679 - lr: 0.01287 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6680 - lr: 0.01286 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6681 - lr: 0.01286 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6682 - lr: 0.01285 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6683 - lr: 0.01285 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6684 - lr: 0.01285 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6685 - lr: 0.01284 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6686 - lr: 0.01284 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6687 - lr: 0.01284 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6688 - lr: 0.01283 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6689 - lr: 0.01283 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6690 - lr: 0.01282 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6691 - lr: 0.01282 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6692 - lr: 0.01282 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6693 - lr: 0.01281 - Train loss: 0.00947 - Test loss: 0.04021\n",
      "Epoch 6694 - lr: 0.01281 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6695 - lr: 0.01280 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6696 - lr: 0.01280 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6697 - lr: 0.01280 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6698 - lr: 0.01279 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6699 - lr: 0.01279 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6700 - lr: 0.01278 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6701 - lr: 0.01278 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6702 - lr: 0.01278 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6703 - lr: 0.01277 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6704 - lr: 0.01277 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6705 - lr: 0.01276 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6706 - lr: 0.01276 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6707 - lr: 0.01276 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6708 - lr: 0.01275 - Train loss: 0.00946 - Test loss: 0.04021\n",
      "Epoch 6709 - lr: 0.01275 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6710 - lr: 0.01274 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6711 - lr: 0.01274 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6712 - lr: 0.01274 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6713 - lr: 0.01273 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6714 - lr: 0.01273 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6715 - lr: 0.01273 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6716 - lr: 0.01272 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6717 - lr: 0.01272 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6718 - lr: 0.01271 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6719 - lr: 0.01271 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6720 - lr: 0.01271 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6721 - lr: 0.01270 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6722 - lr: 0.01270 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6723 - lr: 0.01269 - Train loss: 0.00945 - Test loss: 0.04021\n",
      "Epoch 6724 - lr: 0.01269 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6725 - lr: 0.01269 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6726 - lr: 0.01268 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6727 - lr: 0.01268 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6728 - lr: 0.01267 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6729 - lr: 0.01267 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6730 - lr: 0.01267 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6731 - lr: 0.01266 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6732 - lr: 0.01266 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6733 - lr: 0.01266 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6734 - lr: 0.01265 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6735 - lr: 0.01265 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6736 - lr: 0.01264 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6737 - lr: 0.01264 - Train loss: 0.00944 - Test loss: 0.04021\n",
      "Epoch 6738 - lr: 0.01264 - Train loss: 0.00944 - Test loss: 0.04021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6739 - lr: 0.01263 - Train loss: 0.00943 - Test loss: 0.04021\n",
      "Epoch 6740 - lr: 0.01263 - Train loss: 0.00943 - Test loss: 0.04021\n",
      "Epoch 6741 - lr: 0.01262 - Train loss: 0.00943 - Test loss: 0.04021\n",
      "Epoch 6742 - lr: 0.01262 - Train loss: 0.00943 - Test loss: 0.04021\n",
      "Epoch 6743 - lr: 0.01262 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6744 - lr: 0.01261 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6745 - lr: 0.01261 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6746 - lr: 0.01260 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6747 - lr: 0.01260 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6748 - lr: 0.01260 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6749 - lr: 0.01259 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6750 - lr: 0.01259 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6751 - lr: 0.01259 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6752 - lr: 0.01258 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6753 - lr: 0.01258 - Train loss: 0.00943 - Test loss: 0.04020\n",
      "Epoch 6754 - lr: 0.01257 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6755 - lr: 0.01257 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6756 - lr: 0.01257 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6757 - lr: 0.01256 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6758 - lr: 0.01256 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6759 - lr: 0.01255 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6760 - lr: 0.01255 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6761 - lr: 0.01255 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6762 - lr: 0.01254 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6763 - lr: 0.01254 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6764 - lr: 0.01254 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6765 - lr: 0.01253 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6766 - lr: 0.01253 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6767 - lr: 0.01252 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6768 - lr: 0.01252 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6769 - lr: 0.01252 - Train loss: 0.00942 - Test loss: 0.04020\n",
      "Epoch 6770 - lr: 0.01251 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6771 - lr: 0.01251 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6772 - lr: 0.01250 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6773 - lr: 0.01250 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6774 - lr: 0.01250 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6775 - lr: 0.01249 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6776 - lr: 0.01249 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6777 - lr: 0.01249 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6778 - lr: 0.01248 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6779 - lr: 0.01248 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6780 - lr: 0.01247 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6781 - lr: 0.01247 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6782 - lr: 0.01247 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6783 - lr: 0.01246 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6784 - lr: 0.01246 - Train loss: 0.00941 - Test loss: 0.04020\n",
      "Epoch 6785 - lr: 0.01245 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6786 - lr: 0.01245 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6787 - lr: 0.01245 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6788 - lr: 0.01244 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6789 - lr: 0.01244 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6790 - lr: 0.01244 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6791 - lr: 0.01243 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6792 - lr: 0.01243 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6793 - lr: 0.01242 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6794 - lr: 0.01242 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6795 - lr: 0.01242 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6796 - lr: 0.01241 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6797 - lr: 0.01241 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6798 - lr: 0.01241 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6799 - lr: 0.01240 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6800 - lr: 0.01240 - Train loss: 0.00940 - Test loss: 0.04020\n",
      "Epoch 6801 - lr: 0.01239 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6802 - lr: 0.01239 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6803 - lr: 0.01239 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6804 - lr: 0.01238 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6805 - lr: 0.01238 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6806 - lr: 0.01237 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6807 - lr: 0.01237 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6808 - lr: 0.01237 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6809 - lr: 0.01236 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6810 - lr: 0.01236 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6811 - lr: 0.01236 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6812 - lr: 0.01235 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6813 - lr: 0.01235 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6814 - lr: 0.01234 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6815 - lr: 0.01234 - Train loss: 0.00939 - Test loss: 0.04020\n",
      "Epoch 6816 - lr: 0.01234 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6817 - lr: 0.01233 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6818 - lr: 0.01233 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6819 - lr: 0.01233 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6820 - lr: 0.01232 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6821 - lr: 0.01232 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6822 - lr: 0.01231 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6823 - lr: 0.01231 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6824 - lr: 0.01231 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6825 - lr: 0.01230 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6826 - lr: 0.01230 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6827 - lr: 0.01230 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6828 - lr: 0.01229 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6829 - lr: 0.01229 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6830 - lr: 0.01228 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6831 - lr: 0.01228 - Train loss: 0.00938 - Test loss: 0.04020\n",
      "Epoch 6832 - lr: 0.01228 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6833 - lr: 0.01227 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6834 - lr: 0.01227 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6835 - lr: 0.01226 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6836 - lr: 0.01226 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6837 - lr: 0.01226 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6838 - lr: 0.01225 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6839 - lr: 0.01225 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6840 - lr: 0.01225 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6841 - lr: 0.01224 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6842 - lr: 0.01224 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6843 - lr: 0.01223 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6844 - lr: 0.01223 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6845 - lr: 0.01223 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6846 - lr: 0.01222 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6847 - lr: 0.01222 - Train loss: 0.00937 - Test loss: 0.04020\n",
      "Epoch 6848 - lr: 0.01222 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6849 - lr: 0.01221 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6850 - lr: 0.01221 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6851 - lr: 0.01220 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6852 - lr: 0.01220 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6853 - lr: 0.01220 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6854 - lr: 0.01219 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6855 - lr: 0.01219 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6856 - lr: 0.01219 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6857 - lr: 0.01218 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6858 - lr: 0.01218 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6859 - lr: 0.01217 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6860 - lr: 0.01217 - Train loss: 0.00936 - Test loss: 0.04020\n",
      "Epoch 6861 - lr: 0.01217 - Train loss: 0.00936 - Test loss: 0.04019\n",
      "Epoch 6862 - lr: 0.01216 - Train loss: 0.00936 - Test loss: 0.04019\n",
      "Epoch 6863 - lr: 0.01216 - Train loss: 0.00936 - Test loss: 0.04019\n",
      "Epoch 6864 - lr: 0.01216 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6865 - lr: 0.01215 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6866 - lr: 0.01215 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6867 - lr: 0.01215 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6868 - lr: 0.01214 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6869 - lr: 0.01214 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6870 - lr: 0.01213 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6871 - lr: 0.01213 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6872 - lr: 0.01213 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6873 - lr: 0.01212 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6874 - lr: 0.01212 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6875 - lr: 0.01212 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6876 - lr: 0.01211 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6877 - lr: 0.01211 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6878 - lr: 0.01210 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6879 - lr: 0.01210 - Train loss: 0.00935 - Test loss: 0.04019\n",
      "Epoch 6880 - lr: 0.01210 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6881 - lr: 0.01209 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6882 - lr: 0.01209 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6883 - lr: 0.01209 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6884 - lr: 0.01208 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6885 - lr: 0.01208 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6886 - lr: 0.01207 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6887 - lr: 0.01207 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6888 - lr: 0.01207 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6889 - lr: 0.01206 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6890 - lr: 0.01206 - Train loss: 0.00934 - Test loss: 0.04019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6891 - lr: 0.01206 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6892 - lr: 0.01205 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6893 - lr: 0.01205 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6894 - lr: 0.01204 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6895 - lr: 0.01204 - Train loss: 0.00934 - Test loss: 0.04019\n",
      "Epoch 6896 - lr: 0.01204 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6897 - lr: 0.01203 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6898 - lr: 0.01203 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6899 - lr: 0.01203 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6900 - lr: 0.01202 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6901 - lr: 0.01202 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6902 - lr: 0.01202 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6903 - lr: 0.01201 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6904 - lr: 0.01201 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6905 - lr: 0.01200 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6906 - lr: 0.01200 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6907 - lr: 0.01200 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6908 - lr: 0.01199 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6909 - lr: 0.01199 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6910 - lr: 0.01199 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6911 - lr: 0.01198 - Train loss: 0.00933 - Test loss: 0.04019\n",
      "Epoch 6912 - lr: 0.01198 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6913 - lr: 0.01197 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6914 - lr: 0.01197 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6915 - lr: 0.01197 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6916 - lr: 0.01196 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6917 - lr: 0.01196 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6918 - lr: 0.01196 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6919 - lr: 0.01195 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6920 - lr: 0.01195 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6921 - lr: 0.01195 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6922 - lr: 0.01194 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6923 - lr: 0.01194 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6924 - lr: 0.01193 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6925 - lr: 0.01193 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6926 - lr: 0.01193 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6927 - lr: 0.01192 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6928 - lr: 0.01192 - Train loss: 0.00932 - Test loss: 0.04019\n",
      "Epoch 6929 - lr: 0.01192 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6930 - lr: 0.01191 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6931 - lr: 0.01191 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6932 - lr: 0.01191 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6933 - lr: 0.01190 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6934 - lr: 0.01190 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6935 - lr: 0.01189 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6936 - lr: 0.01189 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6937 - lr: 0.01189 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6938 - lr: 0.01188 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6939 - lr: 0.01188 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6940 - lr: 0.01188 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6941 - lr: 0.01187 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6942 - lr: 0.01187 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6943 - lr: 0.01186 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6944 - lr: 0.01186 - Train loss: 0.00931 - Test loss: 0.04019\n",
      "Epoch 6945 - lr: 0.01186 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6946 - lr: 0.01185 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6947 - lr: 0.01185 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6948 - lr: 0.01185 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6949 - lr: 0.01184 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6950 - lr: 0.01184 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6951 - lr: 0.01184 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6952 - lr: 0.01183 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6953 - lr: 0.01183 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6954 - lr: 0.01182 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6955 - lr: 0.01182 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6956 - lr: 0.01182 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6957 - lr: 0.01181 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6958 - lr: 0.01181 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6959 - lr: 0.01181 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6960 - lr: 0.01180 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6961 - lr: 0.01180 - Train loss: 0.00930 - Test loss: 0.04019\n",
      "Epoch 6962 - lr: 0.01180 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6963 - lr: 0.01179 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6964 - lr: 0.01179 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6965 - lr: 0.01179 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6966 - lr: 0.01178 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6967 - lr: 0.01178 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6968 - lr: 0.01177 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6969 - lr: 0.01177 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6970 - lr: 0.01177 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6971 - lr: 0.01176 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6972 - lr: 0.01176 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6973 - lr: 0.01176 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6974 - lr: 0.01175 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6975 - lr: 0.01175 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6976 - lr: 0.01175 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6977 - lr: 0.01174 - Train loss: 0.00929 - Test loss: 0.04019\n",
      "Epoch 6978 - lr: 0.01174 - Train loss: 0.00928 - Test loss: 0.04019\n",
      "Epoch 6979 - lr: 0.01173 - Train loss: 0.00928 - Test loss: 0.04019\n",
      "Epoch 6980 - lr: 0.01173 - Train loss: 0.00928 - Test loss: 0.04019\n",
      "Epoch 6981 - lr: 0.01173 - Train loss: 0.00928 - Test loss: 0.04019\n",
      "Epoch 6982 - lr: 0.01172 - Train loss: 0.00928 - Test loss: 0.04019\n",
      "Epoch 6983 - lr: 0.01172 - Train loss: 0.00928 - Test loss: 0.04019\n",
      "Epoch 6984 - lr: 0.01172 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6985 - lr: 0.01171 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6986 - lr: 0.01171 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6987 - lr: 0.01171 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6988 - lr: 0.01170 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6989 - lr: 0.01170 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6990 - lr: 0.01169 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6991 - lr: 0.01169 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6992 - lr: 0.01169 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6993 - lr: 0.01168 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6994 - lr: 0.01168 - Train loss: 0.00928 - Test loss: 0.04018\n",
      "Epoch 6995 - lr: 0.01168 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 6996 - lr: 0.01167 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 6997 - lr: 0.01167 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 6998 - lr: 0.01167 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 6999 - lr: 0.01166 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7000 - lr: 0.01166 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7001 - lr: 0.01166 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7002 - lr: 0.01165 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7003 - lr: 0.01165 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7004 - lr: 0.01164 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7005 - lr: 0.01164 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7006 - lr: 0.01164 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7007 - lr: 0.01163 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7008 - lr: 0.01163 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7009 - lr: 0.01163 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7010 - lr: 0.01162 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7011 - lr: 0.01162 - Train loss: 0.00927 - Test loss: 0.04018\n",
      "Epoch 7012 - lr: 0.01162 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7013 - lr: 0.01161 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7014 - lr: 0.01161 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7015 - lr: 0.01161 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7016 - lr: 0.01160 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7017 - lr: 0.01160 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7018 - lr: 0.01159 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7019 - lr: 0.01159 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7020 - lr: 0.01159 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7021 - lr: 0.01158 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7022 - lr: 0.01158 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7023 - lr: 0.01158 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7024 - lr: 0.01157 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7025 - lr: 0.01157 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7026 - lr: 0.01157 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7027 - lr: 0.01156 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7028 - lr: 0.01156 - Train loss: 0.00926 - Test loss: 0.04018\n",
      "Epoch 7029 - lr: 0.01156 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7030 - lr: 0.01155 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7031 - lr: 0.01155 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7032 - lr: 0.01155 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7033 - lr: 0.01154 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7034 - lr: 0.01154 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7035 - lr: 0.01153 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7036 - lr: 0.01153 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7037 - lr: 0.01153 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7038 - lr: 0.01152 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7039 - lr: 0.01152 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7040 - lr: 0.01152 - Train loss: 0.00925 - Test loss: 0.04018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7041 - lr: 0.01151 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7042 - lr: 0.01151 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7043 - lr: 0.01151 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7044 - lr: 0.01150 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7045 - lr: 0.01150 - Train loss: 0.00925 - Test loss: 0.04018\n",
      "Epoch 7046 - lr: 0.01150 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7047 - lr: 0.01149 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7048 - lr: 0.01149 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7049 - lr: 0.01149 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7050 - lr: 0.01148 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7051 - lr: 0.01148 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7052 - lr: 0.01147 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7053 - lr: 0.01147 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7054 - lr: 0.01147 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7055 - lr: 0.01146 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7056 - lr: 0.01146 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7057 - lr: 0.01146 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7058 - lr: 0.01145 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7059 - lr: 0.01145 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7060 - lr: 0.01145 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7061 - lr: 0.01144 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7062 - lr: 0.01144 - Train loss: 0.00924 - Test loss: 0.04018\n",
      "Epoch 7063 - lr: 0.01144 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7064 - lr: 0.01143 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7065 - lr: 0.01143 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7066 - lr: 0.01143 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7067 - lr: 0.01142 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7068 - lr: 0.01142 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7069 - lr: 0.01141 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7070 - lr: 0.01141 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7071 - lr: 0.01141 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7072 - lr: 0.01140 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7073 - lr: 0.01140 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7074 - lr: 0.01140 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7075 - lr: 0.01139 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7076 - lr: 0.01139 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7077 - lr: 0.01139 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7078 - lr: 0.01138 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7079 - lr: 0.01138 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7080 - lr: 0.01138 - Train loss: 0.00923 - Test loss: 0.04018\n",
      "Epoch 7081 - lr: 0.01137 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7082 - lr: 0.01137 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7083 - lr: 0.01137 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7084 - lr: 0.01136 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7085 - lr: 0.01136 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7086 - lr: 0.01136 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7087 - lr: 0.01135 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7088 - lr: 0.01135 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7089 - lr: 0.01134 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7090 - lr: 0.01134 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7091 - lr: 0.01134 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7092 - lr: 0.01133 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7093 - lr: 0.01133 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7094 - lr: 0.01133 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7095 - lr: 0.01132 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7096 - lr: 0.01132 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7097 - lr: 0.01132 - Train loss: 0.00922 - Test loss: 0.04018\n",
      "Epoch 7098 - lr: 0.01131 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7099 - lr: 0.01131 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7100 - lr: 0.01131 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7101 - lr: 0.01130 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7102 - lr: 0.01130 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7103 - lr: 0.01130 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7104 - lr: 0.01129 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7105 - lr: 0.01129 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7106 - lr: 0.01129 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7107 - lr: 0.01128 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7108 - lr: 0.01128 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7109 - lr: 0.01128 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7110 - lr: 0.01127 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7111 - lr: 0.01127 - Train loss: 0.00921 - Test loss: 0.04018\n",
      "Epoch 7112 - lr: 0.01127 - Train loss: 0.00921 - Test loss: 0.04017\n",
      "Epoch 7113 - lr: 0.01126 - Train loss: 0.00921 - Test loss: 0.04017\n",
      "Epoch 7114 - lr: 0.01126 - Train loss: 0.00921 - Test loss: 0.04017\n",
      "Epoch 7115 - lr: 0.01125 - Train loss: 0.00921 - Test loss: 0.04017\n",
      "Epoch 7116 - lr: 0.01125 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7117 - lr: 0.01125 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7118 - lr: 0.01124 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7119 - lr: 0.01124 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7120 - lr: 0.01124 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7121 - lr: 0.01123 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7122 - lr: 0.01123 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7123 - lr: 0.01123 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7124 - lr: 0.01122 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7125 - lr: 0.01122 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7126 - lr: 0.01122 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7127 - lr: 0.01121 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7128 - lr: 0.01121 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7129 - lr: 0.01121 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7130 - lr: 0.01120 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7131 - lr: 0.01120 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7132 - lr: 0.01120 - Train loss: 0.00920 - Test loss: 0.04017\n",
      "Epoch 7133 - lr: 0.01119 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7134 - lr: 0.01119 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7135 - lr: 0.01119 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7136 - lr: 0.01118 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7137 - lr: 0.01118 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7138 - lr: 0.01118 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7139 - lr: 0.01117 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7140 - lr: 0.01117 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7141 - lr: 0.01117 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7142 - lr: 0.01116 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7143 - lr: 0.01116 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7144 - lr: 0.01115 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7145 - lr: 0.01115 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7146 - lr: 0.01115 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7147 - lr: 0.01114 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7148 - lr: 0.01114 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7149 - lr: 0.01114 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7150 - lr: 0.01113 - Train loss: 0.00919 - Test loss: 0.04017\n",
      "Epoch 7151 - lr: 0.01113 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7152 - lr: 0.01113 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7153 - lr: 0.01112 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7154 - lr: 0.01112 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7155 - lr: 0.01112 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7156 - lr: 0.01111 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7157 - lr: 0.01111 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7158 - lr: 0.01111 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7159 - lr: 0.01110 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7160 - lr: 0.01110 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7161 - lr: 0.01110 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7162 - lr: 0.01109 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7163 - lr: 0.01109 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7164 - lr: 0.01109 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7165 - lr: 0.01108 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7166 - lr: 0.01108 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7167 - lr: 0.01108 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7168 - lr: 0.01107 - Train loss: 0.00918 - Test loss: 0.04017\n",
      "Epoch 7169 - lr: 0.01107 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7170 - lr: 0.01107 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7171 - lr: 0.01106 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7172 - lr: 0.01106 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7173 - lr: 0.01106 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7174 - lr: 0.01105 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7175 - lr: 0.01105 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7176 - lr: 0.01105 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7177 - lr: 0.01104 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7178 - lr: 0.01104 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7179 - lr: 0.01104 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7180 - lr: 0.01103 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7181 - lr: 0.01103 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7182 - lr: 0.01103 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7183 - lr: 0.01102 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7184 - lr: 0.01102 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7185 - lr: 0.01102 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7186 - lr: 0.01101 - Train loss: 0.00917 - Test loss: 0.04017\n",
      "Epoch 7187 - lr: 0.01101 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7188 - lr: 0.01101 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7189 - lr: 0.01100 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7190 - lr: 0.01100 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7191 - lr: 0.01100 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7192 - lr: 0.01099 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7193 - lr: 0.01099 - Train loss: 0.00916 - Test loss: 0.04017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7194 - lr: 0.01098 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7195 - lr: 0.01098 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7196 - lr: 0.01098 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7197 - lr: 0.01097 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7198 - lr: 0.01097 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7199 - lr: 0.01097 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7200 - lr: 0.01096 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7201 - lr: 0.01096 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7202 - lr: 0.01096 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7203 - lr: 0.01095 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7204 - lr: 0.01095 - Train loss: 0.00916 - Test loss: 0.04017\n",
      "Epoch 7205 - lr: 0.01095 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7206 - lr: 0.01094 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7207 - lr: 0.01094 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7208 - lr: 0.01094 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7209 - lr: 0.01093 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7210 - lr: 0.01093 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7211 - lr: 0.01093 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7212 - lr: 0.01092 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7213 - lr: 0.01092 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7214 - lr: 0.01092 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7215 - lr: 0.01091 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7216 - lr: 0.01091 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7217 - lr: 0.01091 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7218 - lr: 0.01090 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7219 - lr: 0.01090 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7220 - lr: 0.01090 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7221 - lr: 0.01089 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7222 - lr: 0.01089 - Train loss: 0.00915 - Test loss: 0.04017\n",
      "Epoch 7223 - lr: 0.01089 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7224 - lr: 0.01088 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7225 - lr: 0.01088 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7226 - lr: 0.01088 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7227 - lr: 0.01087 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7228 - lr: 0.01087 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7229 - lr: 0.01087 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7230 - lr: 0.01086 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7231 - lr: 0.01086 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7232 - lr: 0.01086 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7233 - lr: 0.01085 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7234 - lr: 0.01085 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7235 - lr: 0.01085 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7236 - lr: 0.01084 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7237 - lr: 0.01084 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7238 - lr: 0.01084 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7239 - lr: 0.01083 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7240 - lr: 0.01083 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7241 - lr: 0.01083 - Train loss: 0.00914 - Test loss: 0.04017\n",
      "Epoch 7242 - lr: 0.01082 - Train loss: 0.00913 - Test loss: 0.04017\n",
      "Epoch 7243 - lr: 0.01082 - Train loss: 0.00913 - Test loss: 0.04017\n",
      "Epoch 7244 - lr: 0.01082 - Train loss: 0.00913 - Test loss: 0.04017\n",
      "Epoch 7245 - lr: 0.01081 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7246 - lr: 0.01081 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7247 - lr: 0.01081 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7248 - lr: 0.01080 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7249 - lr: 0.01080 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7250 - lr: 0.01080 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7251 - lr: 0.01079 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7252 - lr: 0.01079 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7253 - lr: 0.01079 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7254 - lr: 0.01078 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7255 - lr: 0.01078 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7256 - lr: 0.01078 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7257 - lr: 0.01077 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7258 - lr: 0.01077 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7259 - lr: 0.01077 - Train loss: 0.00913 - Test loss: 0.04016\n",
      "Epoch 7260 - lr: 0.01076 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7261 - lr: 0.01076 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7262 - lr: 0.01076 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7263 - lr: 0.01075 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7264 - lr: 0.01075 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7265 - lr: 0.01075 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7266 - lr: 0.01074 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7267 - lr: 0.01074 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7268 - lr: 0.01074 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7269 - lr: 0.01073 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7270 - lr: 0.01073 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7271 - lr: 0.01073 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7272 - lr: 0.01073 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7273 - lr: 0.01072 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7274 - lr: 0.01072 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7275 - lr: 0.01072 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7276 - lr: 0.01071 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7277 - lr: 0.01071 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7278 - lr: 0.01071 - Train loss: 0.00912 - Test loss: 0.04016\n",
      "Epoch 7279 - lr: 0.01070 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7280 - lr: 0.01070 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7281 - lr: 0.01070 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7282 - lr: 0.01069 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7283 - lr: 0.01069 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7284 - lr: 0.01069 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7285 - lr: 0.01068 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7286 - lr: 0.01068 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7287 - lr: 0.01068 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7288 - lr: 0.01067 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7289 - lr: 0.01067 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7290 - lr: 0.01067 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7291 - lr: 0.01066 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7292 - lr: 0.01066 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7293 - lr: 0.01066 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7294 - lr: 0.01065 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7295 - lr: 0.01065 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7296 - lr: 0.01065 - Train loss: 0.00911 - Test loss: 0.04016\n",
      "Epoch 7297 - lr: 0.01064 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7298 - lr: 0.01064 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7299 - lr: 0.01064 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7300 - lr: 0.01063 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7301 - lr: 0.01063 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7302 - lr: 0.01063 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7303 - lr: 0.01062 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7304 - lr: 0.01062 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7305 - lr: 0.01062 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7306 - lr: 0.01061 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7307 - lr: 0.01061 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7308 - lr: 0.01061 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7309 - lr: 0.01060 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7310 - lr: 0.01060 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7311 - lr: 0.01060 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7312 - lr: 0.01059 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7313 - lr: 0.01059 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7314 - lr: 0.01059 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7315 - lr: 0.01058 - Train loss: 0.00910 - Test loss: 0.04016\n",
      "Epoch 7316 - lr: 0.01058 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7317 - lr: 0.01058 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7318 - lr: 0.01057 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7319 - lr: 0.01057 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7320 - lr: 0.01057 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7321 - lr: 0.01056 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7322 - lr: 0.01056 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7323 - lr: 0.01056 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7324 - lr: 0.01056 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7325 - lr: 0.01055 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7326 - lr: 0.01055 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7327 - lr: 0.01055 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7328 - lr: 0.01054 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7329 - lr: 0.01054 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7330 - lr: 0.01054 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7331 - lr: 0.01053 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7332 - lr: 0.01053 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7333 - lr: 0.01053 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7334 - lr: 0.01052 - Train loss: 0.00909 - Test loss: 0.04016\n",
      "Epoch 7335 - lr: 0.01052 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7336 - lr: 0.01052 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7337 - lr: 0.01051 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7338 - lr: 0.01051 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7339 - lr: 0.01051 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7340 - lr: 0.01050 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7341 - lr: 0.01050 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7342 - lr: 0.01050 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7343 - lr: 0.01049 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7344 - lr: 0.01049 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7345 - lr: 0.01049 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7346 - lr: 0.01048 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7347 - lr: 0.01048 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7348 - lr: 0.01048 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7349 - lr: 0.01047 - Train loss: 0.00908 - Test loss: 0.04016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7350 - lr: 0.01047 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7351 - lr: 0.01047 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7352 - lr: 0.01046 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7353 - lr: 0.01046 - Train loss: 0.00908 - Test loss: 0.04016\n",
      "Epoch 7354 - lr: 0.01046 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7355 - lr: 0.01046 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7356 - lr: 0.01045 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7357 - lr: 0.01045 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7358 - lr: 0.01045 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7359 - lr: 0.01044 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7360 - lr: 0.01044 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7361 - lr: 0.01044 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7362 - lr: 0.01043 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7363 - lr: 0.01043 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7364 - lr: 0.01043 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7365 - lr: 0.01042 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7366 - lr: 0.01042 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7367 - lr: 0.01042 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7368 - lr: 0.01041 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7369 - lr: 0.01041 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7370 - lr: 0.01041 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7371 - lr: 0.01040 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7372 - lr: 0.01040 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7373 - lr: 0.01040 - Train loss: 0.00907 - Test loss: 0.04016\n",
      "Epoch 7374 - lr: 0.01039 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7375 - lr: 0.01039 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7376 - lr: 0.01039 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7377 - lr: 0.01038 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7378 - lr: 0.01038 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7379 - lr: 0.01038 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7380 - lr: 0.01038 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7381 - lr: 0.01037 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7382 - lr: 0.01037 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7383 - lr: 0.01037 - Train loss: 0.00906 - Test loss: 0.04016\n",
      "Epoch 7384 - lr: 0.01036 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7385 - lr: 0.01036 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7386 - lr: 0.01036 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7387 - lr: 0.01035 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7388 - lr: 0.01035 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7389 - lr: 0.01035 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7390 - lr: 0.01034 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7391 - lr: 0.01034 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7392 - lr: 0.01034 - Train loss: 0.00906 - Test loss: 0.04015\n",
      "Epoch 7393 - lr: 0.01033 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7394 - lr: 0.01033 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7395 - lr: 0.01033 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7396 - lr: 0.01032 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7397 - lr: 0.01032 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7398 - lr: 0.01032 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7399 - lr: 0.01031 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7400 - lr: 0.01031 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7401 - lr: 0.01031 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7402 - lr: 0.01031 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7403 - lr: 0.01030 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7404 - lr: 0.01030 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7405 - lr: 0.01030 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7406 - lr: 0.01029 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7407 - lr: 0.01029 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7408 - lr: 0.01029 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7409 - lr: 0.01028 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7410 - lr: 0.01028 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7411 - lr: 0.01028 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7412 - lr: 0.01027 - Train loss: 0.00905 - Test loss: 0.04015\n",
      "Epoch 7413 - lr: 0.01027 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7414 - lr: 0.01027 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7415 - lr: 0.01026 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7416 - lr: 0.01026 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7417 - lr: 0.01026 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7418 - lr: 0.01025 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7419 - lr: 0.01025 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7420 - lr: 0.01025 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7421 - lr: 0.01025 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7422 - lr: 0.01024 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7423 - lr: 0.01024 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7424 - lr: 0.01024 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7425 - lr: 0.01023 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7426 - lr: 0.01023 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7427 - lr: 0.01023 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7428 - lr: 0.01022 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7429 - lr: 0.01022 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7430 - lr: 0.01022 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7431 - lr: 0.01021 - Train loss: 0.00904 - Test loss: 0.04015\n",
      "Epoch 7432 - lr: 0.01021 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7433 - lr: 0.01021 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7434 - lr: 0.01020 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7435 - lr: 0.01020 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7436 - lr: 0.01020 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7437 - lr: 0.01020 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7438 - lr: 0.01019 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7439 - lr: 0.01019 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7440 - lr: 0.01019 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7441 - lr: 0.01018 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7442 - lr: 0.01018 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7443 - lr: 0.01018 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7444 - lr: 0.01017 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7445 - lr: 0.01017 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7446 - lr: 0.01017 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7447 - lr: 0.01016 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7448 - lr: 0.01016 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7449 - lr: 0.01016 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7450 - lr: 0.01015 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7451 - lr: 0.01015 - Train loss: 0.00903 - Test loss: 0.04015\n",
      "Epoch 7452 - lr: 0.01015 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7453 - lr: 0.01015 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7454 - lr: 0.01014 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7455 - lr: 0.01014 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7456 - lr: 0.01014 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7457 - lr: 0.01013 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7458 - lr: 0.01013 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7459 - lr: 0.01013 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7460 - lr: 0.01012 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7461 - lr: 0.01012 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7462 - lr: 0.01012 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7463 - lr: 0.01011 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7464 - lr: 0.01011 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7465 - lr: 0.01011 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7466 - lr: 0.01010 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7467 - lr: 0.01010 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7468 - lr: 0.01010 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7469 - lr: 0.01010 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7470 - lr: 0.01009 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7471 - lr: 0.01009 - Train loss: 0.00902 - Test loss: 0.04015\n",
      "Epoch 7472 - lr: 0.01009 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7473 - lr: 0.01008 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7474 - lr: 0.01008 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7475 - lr: 0.01008 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7476 - lr: 0.01007 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7477 - lr: 0.01007 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7478 - lr: 0.01007 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7479 - lr: 0.01006 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7480 - lr: 0.01006 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7481 - lr: 0.01006 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7482 - lr: 0.01006 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7483 - lr: 0.01005 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7484 - lr: 0.01005 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7485 - lr: 0.01005 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7486 - lr: 0.01004 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7487 - lr: 0.01004 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7488 - lr: 0.01004 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7489 - lr: 0.01003 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7490 - lr: 0.01003 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7491 - lr: 0.01003 - Train loss: 0.00901 - Test loss: 0.04015\n",
      "Epoch 7492 - lr: 0.01002 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7493 - lr: 0.01002 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7494 - lr: 0.01002 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7495 - lr: 0.01002 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7496 - lr: 0.01001 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7497 - lr: 0.01001 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7498 - lr: 0.01001 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7499 - lr: 0.01000 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7500 - lr: 0.01000 - Train loss: 0.00900 - Test loss: 0.04015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7501 - lr: 0.01000 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7502 - lr: 0.00999 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7503 - lr: 0.00999 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7504 - lr: 0.00999 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7505 - lr: 0.00998 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7506 - lr: 0.00998 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7507 - lr: 0.00998 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7508 - lr: 0.00998 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7509 - lr: 0.00997 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7510 - lr: 0.00997 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7511 - lr: 0.00997 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7512 - lr: 0.00996 - Train loss: 0.00900 - Test loss: 0.04015\n",
      "Epoch 7513 - lr: 0.00996 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7514 - lr: 0.00996 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7515 - lr: 0.00995 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7516 - lr: 0.00995 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7517 - lr: 0.00995 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7518 - lr: 0.00994 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7519 - lr: 0.00994 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7520 - lr: 0.00994 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7521 - lr: 0.00994 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7522 - lr: 0.00993 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7523 - lr: 0.00993 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7524 - lr: 0.00993 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7525 - lr: 0.00992 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7526 - lr: 0.00992 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7527 - lr: 0.00992 - Train loss: 0.00899 - Test loss: 0.04015\n",
      "Epoch 7528 - lr: 0.00991 - Train loss: 0.00899 - Test loss: 0.04014\n",
      "Epoch 7529 - lr: 0.00991 - Train loss: 0.00899 - Test loss: 0.04014\n",
      "Epoch 7530 - lr: 0.00991 - Train loss: 0.00899 - Test loss: 0.04014\n",
      "Epoch 7531 - lr: 0.00991 - Train loss: 0.00899 - Test loss: 0.04014\n",
      "Epoch 7532 - lr: 0.00990 - Train loss: 0.00899 - Test loss: 0.04014\n",
      "Epoch 7533 - lr: 0.00990 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7534 - lr: 0.00990 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7535 - lr: 0.00989 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7536 - lr: 0.00989 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7537 - lr: 0.00989 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7538 - lr: 0.00988 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7539 - lr: 0.00988 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7540 - lr: 0.00988 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7541 - lr: 0.00987 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7542 - lr: 0.00987 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7543 - lr: 0.00987 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7544 - lr: 0.00987 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7545 - lr: 0.00986 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7546 - lr: 0.00986 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7547 - lr: 0.00986 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7548 - lr: 0.00985 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7549 - lr: 0.00985 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7550 - lr: 0.00985 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7551 - lr: 0.00984 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7552 - lr: 0.00984 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7553 - lr: 0.00984 - Train loss: 0.00898 - Test loss: 0.04014\n",
      "Epoch 7554 - lr: 0.00984 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7555 - lr: 0.00983 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7556 - lr: 0.00983 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7557 - lr: 0.00983 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7558 - lr: 0.00982 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7559 - lr: 0.00982 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7560 - lr: 0.00982 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7561 - lr: 0.00981 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7562 - lr: 0.00981 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7563 - lr: 0.00981 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7564 - lr: 0.00981 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7565 - lr: 0.00980 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7566 - lr: 0.00980 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7567 - lr: 0.00980 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7568 - lr: 0.00979 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7569 - lr: 0.00979 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7570 - lr: 0.00979 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7571 - lr: 0.00978 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7572 - lr: 0.00978 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7573 - lr: 0.00978 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7574 - lr: 0.00978 - Train loss: 0.00897 - Test loss: 0.04014\n",
      "Epoch 7575 - lr: 0.00977 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7576 - lr: 0.00977 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7577 - lr: 0.00977 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7578 - lr: 0.00976 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7579 - lr: 0.00976 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7580 - lr: 0.00976 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7581 - lr: 0.00975 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7582 - lr: 0.00975 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7583 - lr: 0.00975 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7584 - lr: 0.00975 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7585 - lr: 0.00974 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7586 - lr: 0.00974 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7587 - lr: 0.00974 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7588 - lr: 0.00973 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7589 - lr: 0.00973 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7590 - lr: 0.00973 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7591 - lr: 0.00972 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7592 - lr: 0.00972 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7593 - lr: 0.00972 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7594 - lr: 0.00972 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7595 - lr: 0.00971 - Train loss: 0.00896 - Test loss: 0.04014\n",
      "Epoch 7596 - lr: 0.00971 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7597 - lr: 0.00971 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7598 - lr: 0.00970 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7599 - lr: 0.00970 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7600 - lr: 0.00970 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7601 - lr: 0.00969 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7602 - lr: 0.00969 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7603 - lr: 0.00969 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7604 - lr: 0.00969 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7605 - lr: 0.00968 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7606 - lr: 0.00968 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7607 - lr: 0.00968 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7608 - lr: 0.00967 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7609 - lr: 0.00967 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7610 - lr: 0.00967 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7611 - lr: 0.00966 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7612 - lr: 0.00966 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7613 - lr: 0.00966 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7614 - lr: 0.00966 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7615 - lr: 0.00965 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7616 - lr: 0.00965 - Train loss: 0.00895 - Test loss: 0.04014\n",
      "Epoch 7617 - lr: 0.00965 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7618 - lr: 0.00964 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7619 - lr: 0.00964 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7620 - lr: 0.00964 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7621 - lr: 0.00964 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7622 - lr: 0.00963 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7623 - lr: 0.00963 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7624 - lr: 0.00963 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7625 - lr: 0.00962 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7626 - lr: 0.00962 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7627 - lr: 0.00962 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7628 - lr: 0.00961 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7629 - lr: 0.00961 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7630 - lr: 0.00961 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7631 - lr: 0.00961 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7632 - lr: 0.00960 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7633 - lr: 0.00960 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7634 - lr: 0.00960 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7635 - lr: 0.00959 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7636 - lr: 0.00959 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7637 - lr: 0.00959 - Train loss: 0.00894 - Test loss: 0.04014\n",
      "Epoch 7638 - lr: 0.00959 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7639 - lr: 0.00958 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7640 - lr: 0.00958 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7641 - lr: 0.00958 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7642 - lr: 0.00957 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7643 - lr: 0.00957 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7644 - lr: 0.00957 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7645 - lr: 0.00956 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7646 - lr: 0.00956 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7647 - lr: 0.00956 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7648 - lr: 0.00956 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7649 - lr: 0.00955 - Train loss: 0.00893 - Test loss: 0.04014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7650 - lr: 0.00955 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7651 - lr: 0.00955 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7652 - lr: 0.00954 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7653 - lr: 0.00954 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7654 - lr: 0.00954 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7655 - lr: 0.00954 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7656 - lr: 0.00953 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7657 - lr: 0.00953 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7658 - lr: 0.00953 - Train loss: 0.00893 - Test loss: 0.04014\n",
      "Epoch 7659 - lr: 0.00952 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7660 - lr: 0.00952 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7661 - lr: 0.00952 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7662 - lr: 0.00951 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7663 - lr: 0.00951 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7664 - lr: 0.00951 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7665 - lr: 0.00951 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7666 - lr: 0.00950 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7667 - lr: 0.00950 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7668 - lr: 0.00950 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7669 - lr: 0.00949 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7670 - lr: 0.00949 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7671 - lr: 0.00949 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7672 - lr: 0.00949 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7673 - lr: 0.00948 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7674 - lr: 0.00948 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7675 - lr: 0.00948 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7676 - lr: 0.00947 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7677 - lr: 0.00947 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7678 - lr: 0.00947 - Train loss: 0.00892 - Test loss: 0.04014\n",
      "Epoch 7679 - lr: 0.00947 - Train loss: 0.00892 - Test loss: 0.04013\n",
      "Epoch 7680 - lr: 0.00946 - Train loss: 0.00892 - Test loss: 0.04013\n",
      "Epoch 7681 - lr: 0.00946 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7682 - lr: 0.00946 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7683 - lr: 0.00945 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7684 - lr: 0.00945 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7685 - lr: 0.00945 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7686 - lr: 0.00944 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7687 - lr: 0.00944 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7688 - lr: 0.00944 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7689 - lr: 0.00944 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7690 - lr: 0.00943 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7691 - lr: 0.00943 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7692 - lr: 0.00943 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7693 - lr: 0.00942 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7694 - lr: 0.00942 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7695 - lr: 0.00942 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7696 - lr: 0.00942 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7697 - lr: 0.00941 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7698 - lr: 0.00941 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7699 - lr: 0.00941 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7700 - lr: 0.00940 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7701 - lr: 0.00940 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7702 - lr: 0.00940 - Train loss: 0.00891 - Test loss: 0.04013\n",
      "Epoch 7703 - lr: 0.00940 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7704 - lr: 0.00939 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7705 - lr: 0.00939 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7706 - lr: 0.00939 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7707 - lr: 0.00938 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7708 - lr: 0.00938 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7709 - lr: 0.00938 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7710 - lr: 0.00938 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7711 - lr: 0.00937 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7712 - lr: 0.00937 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7713 - lr: 0.00937 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7714 - lr: 0.00936 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7715 - lr: 0.00936 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7716 - lr: 0.00936 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7717 - lr: 0.00936 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7718 - lr: 0.00935 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7719 - lr: 0.00935 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7720 - lr: 0.00935 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7721 - lr: 0.00934 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7722 - lr: 0.00934 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7723 - lr: 0.00934 - Train loss: 0.00890 - Test loss: 0.04013\n",
      "Epoch 7724 - lr: 0.00934 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7725 - lr: 0.00933 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7726 - lr: 0.00933 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7727 - lr: 0.00933 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7728 - lr: 0.00932 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7729 - lr: 0.00932 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7730 - lr: 0.00932 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7731 - lr: 0.00932 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7732 - lr: 0.00931 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7733 - lr: 0.00931 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7734 - lr: 0.00931 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7735 - lr: 0.00930 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7736 - lr: 0.00930 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7737 - lr: 0.00930 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7738 - lr: 0.00930 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7739 - lr: 0.00929 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7740 - lr: 0.00929 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7741 - lr: 0.00929 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7742 - lr: 0.00928 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7743 - lr: 0.00928 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7744 - lr: 0.00928 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7745 - lr: 0.00928 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7746 - lr: 0.00927 - Train loss: 0.00889 - Test loss: 0.04013\n",
      "Epoch 7747 - lr: 0.00927 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7748 - lr: 0.00927 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7749 - lr: 0.00926 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7750 - lr: 0.00926 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7751 - lr: 0.00926 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7752 - lr: 0.00926 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7753 - lr: 0.00925 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7754 - lr: 0.00925 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7755 - lr: 0.00925 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7756 - lr: 0.00924 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7757 - lr: 0.00924 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7758 - lr: 0.00924 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7759 - lr: 0.00924 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7760 - lr: 0.00923 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7761 - lr: 0.00923 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7762 - lr: 0.00923 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7763 - lr: 0.00922 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7764 - lr: 0.00922 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7765 - lr: 0.00922 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7766 - lr: 0.00922 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7767 - lr: 0.00921 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7768 - lr: 0.00921 - Train loss: 0.00888 - Test loss: 0.04013\n",
      "Epoch 7769 - lr: 0.00921 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7770 - lr: 0.00920 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7771 - lr: 0.00920 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7772 - lr: 0.00920 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7773 - lr: 0.00920 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7774 - lr: 0.00919 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7775 - lr: 0.00919 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7776 - lr: 0.00919 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7777 - lr: 0.00918 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7778 - lr: 0.00918 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7779 - lr: 0.00918 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7780 - lr: 0.00918 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7781 - lr: 0.00917 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7782 - lr: 0.00917 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7783 - lr: 0.00917 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7784 - lr: 0.00917 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7785 - lr: 0.00916 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7786 - lr: 0.00916 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7787 - lr: 0.00916 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7788 - lr: 0.00915 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7789 - lr: 0.00915 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7790 - lr: 0.00915 - Train loss: 0.00887 - Test loss: 0.04013\n",
      "Epoch 7791 - lr: 0.00915 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7792 - lr: 0.00914 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7793 - lr: 0.00914 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7794 - lr: 0.00914 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7795 - lr: 0.00913 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7796 - lr: 0.00913 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7797 - lr: 0.00913 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7798 - lr: 0.00913 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7799 - lr: 0.00912 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7800 - lr: 0.00912 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7801 - lr: 0.00912 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7802 - lr: 0.00911 - Train loss: 0.00886 - Test loss: 0.04013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7803 - lr: 0.00911 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7804 - lr: 0.00911 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7805 - lr: 0.00911 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7806 - lr: 0.00910 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7807 - lr: 0.00910 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7808 - lr: 0.00910 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7809 - lr: 0.00909 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7810 - lr: 0.00909 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7811 - lr: 0.00909 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7812 - lr: 0.00909 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7813 - lr: 0.00908 - Train loss: 0.00886 - Test loss: 0.04013\n",
      "Epoch 7814 - lr: 0.00908 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7815 - lr: 0.00908 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7816 - lr: 0.00908 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7817 - lr: 0.00907 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7818 - lr: 0.00907 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7819 - lr: 0.00907 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7820 - lr: 0.00906 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7821 - lr: 0.00906 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7822 - lr: 0.00906 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7823 - lr: 0.00906 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7824 - lr: 0.00905 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7825 - lr: 0.00905 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7826 - lr: 0.00905 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7827 - lr: 0.00904 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7828 - lr: 0.00904 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7829 - lr: 0.00904 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7830 - lr: 0.00904 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7831 - lr: 0.00903 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7832 - lr: 0.00903 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7833 - lr: 0.00903 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7834 - lr: 0.00903 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7835 - lr: 0.00902 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7836 - lr: 0.00902 - Train loss: 0.00885 - Test loss: 0.04013\n",
      "Epoch 7837 - lr: 0.00902 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7838 - lr: 0.00901 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7839 - lr: 0.00901 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7840 - lr: 0.00901 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7841 - lr: 0.00901 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7842 - lr: 0.00900 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7843 - lr: 0.00900 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7844 - lr: 0.00900 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7845 - lr: 0.00899 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7846 - lr: 0.00899 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7847 - lr: 0.00899 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7848 - lr: 0.00899 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7849 - lr: 0.00898 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7850 - lr: 0.00898 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7851 - lr: 0.00898 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7852 - lr: 0.00898 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7853 - lr: 0.00897 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7854 - lr: 0.00897 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7855 - lr: 0.00897 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7856 - lr: 0.00896 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7857 - lr: 0.00896 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7858 - lr: 0.00896 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7859 - lr: 0.00896 - Train loss: 0.00884 - Test loss: 0.04012\n",
      "Epoch 7860 - lr: 0.00895 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7861 - lr: 0.00895 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7862 - lr: 0.00895 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7863 - lr: 0.00895 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7864 - lr: 0.00894 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7865 - lr: 0.00894 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7866 - lr: 0.00894 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7867 - lr: 0.00893 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7868 - lr: 0.00893 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7869 - lr: 0.00893 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7870 - lr: 0.00893 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7871 - lr: 0.00892 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7872 - lr: 0.00892 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7873 - lr: 0.00892 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7874 - lr: 0.00892 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7875 - lr: 0.00891 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7876 - lr: 0.00891 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7877 - lr: 0.00891 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7878 - lr: 0.00890 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7879 - lr: 0.00890 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7880 - lr: 0.00890 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7881 - lr: 0.00890 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7882 - lr: 0.00889 - Train loss: 0.00883 - Test loss: 0.04012\n",
      "Epoch 7883 - lr: 0.00889 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7884 - lr: 0.00889 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7885 - lr: 0.00889 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7886 - lr: 0.00888 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7887 - lr: 0.00888 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7888 - lr: 0.00888 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7889 - lr: 0.00887 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7890 - lr: 0.00887 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7891 - lr: 0.00887 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7892 - lr: 0.00887 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7893 - lr: 0.00886 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7894 - lr: 0.00886 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7895 - lr: 0.00886 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7896 - lr: 0.00886 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7897 - lr: 0.00885 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7898 - lr: 0.00885 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7899 - lr: 0.00885 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7900 - lr: 0.00884 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7901 - lr: 0.00884 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7902 - lr: 0.00884 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7903 - lr: 0.00884 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7904 - lr: 0.00883 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7905 - lr: 0.00883 - Train loss: 0.00882 - Test loss: 0.04012\n",
      "Epoch 7906 - lr: 0.00883 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7907 - lr: 0.00883 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7908 - lr: 0.00882 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7909 - lr: 0.00882 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7910 - lr: 0.00882 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7911 - lr: 0.00881 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7912 - lr: 0.00881 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7913 - lr: 0.00881 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7914 - lr: 0.00881 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7915 - lr: 0.00880 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7916 - lr: 0.00880 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7917 - lr: 0.00880 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7918 - lr: 0.00880 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7919 - lr: 0.00879 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7920 - lr: 0.00879 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7921 - lr: 0.00879 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7922 - lr: 0.00878 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7923 - lr: 0.00878 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7924 - lr: 0.00878 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7925 - lr: 0.00878 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7926 - lr: 0.00877 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7927 - lr: 0.00877 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7928 - lr: 0.00877 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7929 - lr: 0.00877 - Train loss: 0.00881 - Test loss: 0.04012\n",
      "Epoch 7930 - lr: 0.00876 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7931 - lr: 0.00876 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7932 - lr: 0.00876 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7933 - lr: 0.00876 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7934 - lr: 0.00875 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7935 - lr: 0.00875 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7936 - lr: 0.00875 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7937 - lr: 0.00874 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7938 - lr: 0.00874 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7939 - lr: 0.00874 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7940 - lr: 0.00874 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7941 - lr: 0.00873 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7942 - lr: 0.00873 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7943 - lr: 0.00873 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7944 - lr: 0.00873 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7945 - lr: 0.00872 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7946 - lr: 0.00872 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7947 - lr: 0.00872 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7948 - lr: 0.00871 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7949 - lr: 0.00871 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7950 - lr: 0.00871 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7951 - lr: 0.00871 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7952 - lr: 0.00870 - Train loss: 0.00880 - Test loss: 0.04012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7953 - lr: 0.00870 - Train loss: 0.00880 - Test loss: 0.04012\n",
      "Epoch 7954 - lr: 0.00870 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7955 - lr: 0.00870 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7956 - lr: 0.00869 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7957 - lr: 0.00869 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7958 - lr: 0.00869 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7959 - lr: 0.00869 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7960 - lr: 0.00868 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7961 - lr: 0.00868 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7962 - lr: 0.00868 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7963 - lr: 0.00867 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7964 - lr: 0.00867 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7965 - lr: 0.00867 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7966 - lr: 0.00867 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7967 - lr: 0.00866 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7968 - lr: 0.00866 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7969 - lr: 0.00866 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7970 - lr: 0.00866 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7971 - lr: 0.00865 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7972 - lr: 0.00865 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7973 - lr: 0.00865 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7974 - lr: 0.00865 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7975 - lr: 0.00864 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7976 - lr: 0.00864 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7977 - lr: 0.00864 - Train loss: 0.00879 - Test loss: 0.04012\n",
      "Epoch 7978 - lr: 0.00864 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7979 - lr: 0.00863 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7980 - lr: 0.00863 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7981 - lr: 0.00863 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7982 - lr: 0.00862 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7983 - lr: 0.00862 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7984 - lr: 0.00862 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7985 - lr: 0.00862 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7986 - lr: 0.00861 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7987 - lr: 0.00861 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7988 - lr: 0.00861 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7989 - lr: 0.00861 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7990 - lr: 0.00860 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7991 - lr: 0.00860 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7992 - lr: 0.00860 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7993 - lr: 0.00860 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7994 - lr: 0.00859 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7995 - lr: 0.00859 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7996 - lr: 0.00859 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7997 - lr: 0.00858 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7998 - lr: 0.00858 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 7999 - lr: 0.00858 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 8000 - lr: 0.00858 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 8001 - lr: 0.00857 - Train loss: 0.00878 - Test loss: 0.04012\n",
      "Epoch 8002 - lr: 0.00857 - Train loss: 0.00877 - Test loss: 0.04012\n",
      "Epoch 8003 - lr: 0.00857 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8004 - lr: 0.00857 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8005 - lr: 0.00856 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8006 - lr: 0.00856 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8007 - lr: 0.00856 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8008 - lr: 0.00856 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8009 - lr: 0.00855 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8010 - lr: 0.00855 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8011 - lr: 0.00855 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8012 - lr: 0.00855 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8013 - lr: 0.00854 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8014 - lr: 0.00854 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8015 - lr: 0.00854 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8016 - lr: 0.00853 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8017 - lr: 0.00853 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8018 - lr: 0.00853 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8019 - lr: 0.00853 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8020 - lr: 0.00852 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8021 - lr: 0.00852 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8022 - lr: 0.00852 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8023 - lr: 0.00852 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8024 - lr: 0.00851 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8025 - lr: 0.00851 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8026 - lr: 0.00851 - Train loss: 0.00877 - Test loss: 0.04011\n",
      "Epoch 8027 - lr: 0.00851 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8028 - lr: 0.00850 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8029 - lr: 0.00850 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8030 - lr: 0.00850 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8031 - lr: 0.00850 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8032 - lr: 0.00849 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8033 - lr: 0.00849 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8034 - lr: 0.00849 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8035 - lr: 0.00849 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8036 - lr: 0.00848 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8037 - lr: 0.00848 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8038 - lr: 0.00848 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8039 - lr: 0.00847 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8040 - lr: 0.00847 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8041 - lr: 0.00847 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8042 - lr: 0.00847 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8043 - lr: 0.00846 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8044 - lr: 0.00846 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8045 - lr: 0.00846 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8046 - lr: 0.00846 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8047 - lr: 0.00845 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8048 - lr: 0.00845 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8049 - lr: 0.00845 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8050 - lr: 0.00845 - Train loss: 0.00876 - Test loss: 0.04011\n",
      "Epoch 8051 - lr: 0.00844 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8052 - lr: 0.00844 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8053 - lr: 0.00844 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8054 - lr: 0.00844 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8055 - lr: 0.00843 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8056 - lr: 0.00843 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8057 - lr: 0.00843 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8058 - lr: 0.00843 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8059 - lr: 0.00842 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8060 - lr: 0.00842 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8061 - lr: 0.00842 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8062 - lr: 0.00842 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8063 - lr: 0.00841 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8064 - lr: 0.00841 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8065 - lr: 0.00841 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8066 - lr: 0.00840 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8067 - lr: 0.00840 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8068 - lr: 0.00840 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8069 - lr: 0.00840 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8070 - lr: 0.00839 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8071 - lr: 0.00839 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8072 - lr: 0.00839 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8073 - lr: 0.00839 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8074 - lr: 0.00838 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8075 - lr: 0.00838 - Train loss: 0.00875 - Test loss: 0.04011\n",
      "Epoch 8076 - lr: 0.00838 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8077 - lr: 0.00838 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8078 - lr: 0.00837 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8079 - lr: 0.00837 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8080 - lr: 0.00837 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8081 - lr: 0.00837 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8082 - lr: 0.00836 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8083 - lr: 0.00836 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8084 - lr: 0.00836 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8085 - lr: 0.00836 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8086 - lr: 0.00835 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8087 - lr: 0.00835 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8088 - lr: 0.00835 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8089 - lr: 0.00835 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8090 - lr: 0.00834 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8091 - lr: 0.00834 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8092 - lr: 0.00834 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8093 - lr: 0.00834 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8094 - lr: 0.00833 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8095 - lr: 0.00833 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8096 - lr: 0.00833 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8097 - lr: 0.00833 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8098 - lr: 0.00832 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8099 - lr: 0.00832 - Train loss: 0.00874 - Test loss: 0.04011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8100 - lr: 0.00832 - Train loss: 0.00874 - Test loss: 0.04011\n",
      "Epoch 8101 - lr: 0.00832 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8102 - lr: 0.00831 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8103 - lr: 0.00831 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8104 - lr: 0.00831 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8105 - lr: 0.00830 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8106 - lr: 0.00830 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8107 - lr: 0.00830 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8108 - lr: 0.00830 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8109 - lr: 0.00829 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8110 - lr: 0.00829 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8111 - lr: 0.00829 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8112 - lr: 0.00829 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8113 - lr: 0.00828 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8114 - lr: 0.00828 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8115 - lr: 0.00828 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8116 - lr: 0.00828 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8117 - lr: 0.00827 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8118 - lr: 0.00827 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8119 - lr: 0.00827 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8120 - lr: 0.00827 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8121 - lr: 0.00826 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8122 - lr: 0.00826 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8123 - lr: 0.00826 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8124 - lr: 0.00826 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8125 - lr: 0.00825 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8126 - lr: 0.00825 - Train loss: 0.00873 - Test loss: 0.04011\n",
      "Epoch 8127 - lr: 0.00825 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8128 - lr: 0.00825 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8129 - lr: 0.00824 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8130 - lr: 0.00824 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8131 - lr: 0.00824 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8132 - lr: 0.00824 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8133 - lr: 0.00823 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8134 - lr: 0.00823 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8135 - lr: 0.00823 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8136 - lr: 0.00823 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8137 - lr: 0.00822 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8138 - lr: 0.00822 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8139 - lr: 0.00822 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8140 - lr: 0.00822 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8141 - lr: 0.00821 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8142 - lr: 0.00821 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8143 - lr: 0.00821 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8144 - lr: 0.00821 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8145 - lr: 0.00820 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8146 - lr: 0.00820 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8147 - lr: 0.00820 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8148 - lr: 0.00820 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8149 - lr: 0.00819 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8150 - lr: 0.00819 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8151 - lr: 0.00819 - Train loss: 0.00872 - Test loss: 0.04011\n",
      "Epoch 8152 - lr: 0.00819 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8153 - lr: 0.00818 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8154 - lr: 0.00818 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8155 - lr: 0.00818 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8156 - lr: 0.00818 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8157 - lr: 0.00817 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8158 - lr: 0.00817 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8159 - lr: 0.00817 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8160 - lr: 0.00817 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8161 - lr: 0.00816 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8162 - lr: 0.00816 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8163 - lr: 0.00816 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8164 - lr: 0.00816 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8165 - lr: 0.00815 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8166 - lr: 0.00815 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8167 - lr: 0.00815 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8168 - lr: 0.00815 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8169 - lr: 0.00814 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8170 - lr: 0.00814 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8171 - lr: 0.00814 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8172 - lr: 0.00814 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8173 - lr: 0.00813 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8174 - lr: 0.00813 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8175 - lr: 0.00813 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8176 - lr: 0.00813 - Train loss: 0.00871 - Test loss: 0.04011\n",
      "Epoch 8177 - lr: 0.00812 - Train loss: 0.00871 - Test loss: 0.04010\n",
      "Epoch 8178 - lr: 0.00812 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8179 - lr: 0.00812 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8180 - lr: 0.00812 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8181 - lr: 0.00811 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8182 - lr: 0.00811 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8183 - lr: 0.00811 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8184 - lr: 0.00811 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8185 - lr: 0.00810 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8186 - lr: 0.00810 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8187 - lr: 0.00810 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8188 - lr: 0.00810 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8189 - lr: 0.00809 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8190 - lr: 0.00809 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8191 - lr: 0.00809 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8192 - lr: 0.00809 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8193 - lr: 0.00808 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8194 - lr: 0.00808 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8195 - lr: 0.00808 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8196 - lr: 0.00808 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8197 - lr: 0.00807 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8198 - lr: 0.00807 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8199 - lr: 0.00807 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8200 - lr: 0.00807 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8201 - lr: 0.00806 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8202 - lr: 0.00806 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8203 - lr: 0.00806 - Train loss: 0.00870 - Test loss: 0.04010\n",
      "Epoch 8204 - lr: 0.00806 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8205 - lr: 0.00805 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8206 - lr: 0.00805 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8207 - lr: 0.00805 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8208 - lr: 0.00805 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8209 - lr: 0.00804 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8210 - lr: 0.00804 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8211 - lr: 0.00804 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8212 - lr: 0.00804 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8213 - lr: 0.00803 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8214 - lr: 0.00803 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8215 - lr: 0.00803 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8216 - lr: 0.00803 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8217 - lr: 0.00802 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8218 - lr: 0.00802 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8219 - lr: 0.00802 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8220 - lr: 0.00802 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8221 - lr: 0.00801 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8222 - lr: 0.00801 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8223 - lr: 0.00801 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8224 - lr: 0.00801 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8225 - lr: 0.00800 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8226 - lr: 0.00800 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8227 - lr: 0.00800 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8228 - lr: 0.00800 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8229 - lr: 0.00799 - Train loss: 0.00869 - Test loss: 0.04010\n",
      "Epoch 8230 - lr: 0.00799 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8231 - lr: 0.00799 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8232 - lr: 0.00799 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8233 - lr: 0.00798 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8234 - lr: 0.00798 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8235 - lr: 0.00798 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8236 - lr: 0.00798 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8237 - lr: 0.00798 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8238 - lr: 0.00797 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8239 - lr: 0.00797 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8240 - lr: 0.00797 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8241 - lr: 0.00797 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8242 - lr: 0.00796 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8243 - lr: 0.00796 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8244 - lr: 0.00796 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8245 - lr: 0.00796 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8246 - lr: 0.00795 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8247 - lr: 0.00795 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8248 - lr: 0.00795 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8249 - lr: 0.00795 - Train loss: 0.00868 - Test loss: 0.04010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8250 - lr: 0.00794 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8251 - lr: 0.00794 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8252 - lr: 0.00794 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8253 - lr: 0.00794 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8254 - lr: 0.00793 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8255 - lr: 0.00793 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8256 - lr: 0.00793 - Train loss: 0.00868 - Test loss: 0.04010\n",
      "Epoch 8257 - lr: 0.00793 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8258 - lr: 0.00792 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8259 - lr: 0.00792 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8260 - lr: 0.00792 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8261 - lr: 0.00792 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8262 - lr: 0.00791 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8263 - lr: 0.00791 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8264 - lr: 0.00791 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8265 - lr: 0.00791 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8266 - lr: 0.00790 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8267 - lr: 0.00790 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8268 - lr: 0.00790 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8269 - lr: 0.00790 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8270 - lr: 0.00789 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8271 - lr: 0.00789 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8272 - lr: 0.00789 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8273 - lr: 0.00789 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8274 - lr: 0.00788 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8275 - lr: 0.00788 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8276 - lr: 0.00788 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8277 - lr: 0.00788 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8278 - lr: 0.00788 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8279 - lr: 0.00787 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8280 - lr: 0.00787 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8281 - lr: 0.00787 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8282 - lr: 0.00787 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8283 - lr: 0.00786 - Train loss: 0.00867 - Test loss: 0.04010\n",
      "Epoch 8284 - lr: 0.00786 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8285 - lr: 0.00786 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8286 - lr: 0.00786 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8287 - lr: 0.00785 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8288 - lr: 0.00785 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8289 - lr: 0.00785 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8290 - lr: 0.00785 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8291 - lr: 0.00784 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8292 - lr: 0.00784 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8293 - lr: 0.00784 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8294 - lr: 0.00784 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8295 - lr: 0.00783 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8296 - lr: 0.00783 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8297 - lr: 0.00783 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8298 - lr: 0.00783 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8299 - lr: 0.00782 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8300 - lr: 0.00782 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8301 - lr: 0.00782 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8302 - lr: 0.00782 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8303 - lr: 0.00782 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8304 - lr: 0.00781 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8305 - lr: 0.00781 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8306 - lr: 0.00781 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8307 - lr: 0.00781 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8308 - lr: 0.00780 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8309 - lr: 0.00780 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8310 - lr: 0.00780 - Train loss: 0.00866 - Test loss: 0.04010\n",
      "Epoch 8311 - lr: 0.00780 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8312 - lr: 0.00779 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8313 - lr: 0.00779 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8314 - lr: 0.00779 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8315 - lr: 0.00779 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8316 - lr: 0.00778 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8317 - lr: 0.00778 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8318 - lr: 0.00778 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8319 - lr: 0.00778 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8320 - lr: 0.00777 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8321 - lr: 0.00777 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8322 - lr: 0.00777 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8323 - lr: 0.00777 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8324 - lr: 0.00776 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8325 - lr: 0.00776 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8326 - lr: 0.00776 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8327 - lr: 0.00776 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8328 - lr: 0.00776 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8329 - lr: 0.00775 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8330 - lr: 0.00775 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8331 - lr: 0.00775 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8332 - lr: 0.00775 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8333 - lr: 0.00774 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8334 - lr: 0.00774 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8335 - lr: 0.00774 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8336 - lr: 0.00774 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8337 - lr: 0.00773 - Train loss: 0.00865 - Test loss: 0.04010\n",
      "Epoch 8338 - lr: 0.00773 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8339 - lr: 0.00773 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8340 - lr: 0.00773 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8341 - lr: 0.00772 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8342 - lr: 0.00772 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8343 - lr: 0.00772 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8344 - lr: 0.00772 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8345 - lr: 0.00771 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8346 - lr: 0.00771 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8347 - lr: 0.00771 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8348 - lr: 0.00771 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8349 - lr: 0.00771 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8350 - lr: 0.00770 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8351 - lr: 0.00770 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8352 - lr: 0.00770 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8353 - lr: 0.00770 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8354 - lr: 0.00769 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8355 - lr: 0.00769 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8356 - lr: 0.00769 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8357 - lr: 0.00769 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8358 - lr: 0.00768 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8359 - lr: 0.00768 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8360 - lr: 0.00768 - Train loss: 0.00864 - Test loss: 0.04010\n",
      "Epoch 8361 - lr: 0.00768 - Train loss: 0.00864 - Test loss: 0.04009\n",
      "Epoch 8362 - lr: 0.00767 - Train loss: 0.00864 - Test loss: 0.04009\n",
      "Epoch 8363 - lr: 0.00767 - Train loss: 0.00864 - Test loss: 0.04009\n",
      "Epoch 8364 - lr: 0.00767 - Train loss: 0.00864 - Test loss: 0.04009\n",
      "Epoch 8365 - lr: 0.00767 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8366 - lr: 0.00767 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8367 - lr: 0.00766 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8368 - lr: 0.00766 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8369 - lr: 0.00766 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8370 - lr: 0.00766 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8371 - lr: 0.00765 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8372 - lr: 0.00765 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8373 - lr: 0.00765 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8374 - lr: 0.00765 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8375 - lr: 0.00764 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8376 - lr: 0.00764 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8377 - lr: 0.00764 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8378 - lr: 0.00764 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8379 - lr: 0.00763 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8380 - lr: 0.00763 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8381 - lr: 0.00763 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8382 - lr: 0.00763 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8383 - lr: 0.00763 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8384 - lr: 0.00762 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8385 - lr: 0.00762 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8386 - lr: 0.00762 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8387 - lr: 0.00762 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8388 - lr: 0.00761 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8389 - lr: 0.00761 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8390 - lr: 0.00761 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8391 - lr: 0.00761 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8392 - lr: 0.00760 - Train loss: 0.00863 - Test loss: 0.04009\n",
      "Epoch 8393 - lr: 0.00760 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8394 - lr: 0.00760 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8395 - lr: 0.00760 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8396 - lr: 0.00760 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8397 - lr: 0.00759 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8398 - lr: 0.00759 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8399 - lr: 0.00759 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8400 - lr: 0.00759 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8401 - lr: 0.00758 - Train loss: 0.00862 - Test loss: 0.04009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8402 - lr: 0.00758 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8403 - lr: 0.00758 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8404 - lr: 0.00758 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8405 - lr: 0.00757 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8406 - lr: 0.00757 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8407 - lr: 0.00757 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8408 - lr: 0.00757 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8409 - lr: 0.00756 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8410 - lr: 0.00756 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8411 - lr: 0.00756 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8412 - lr: 0.00756 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8413 - lr: 0.00756 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8414 - lr: 0.00755 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8415 - lr: 0.00755 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8416 - lr: 0.00755 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8417 - lr: 0.00755 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8418 - lr: 0.00754 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8419 - lr: 0.00754 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8420 - lr: 0.00754 - Train loss: 0.00862 - Test loss: 0.04009\n",
      "Epoch 8421 - lr: 0.00754 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8422 - lr: 0.00753 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8423 - lr: 0.00753 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8424 - lr: 0.00753 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8425 - lr: 0.00753 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8426 - lr: 0.00753 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8427 - lr: 0.00752 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8428 - lr: 0.00752 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8429 - lr: 0.00752 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8430 - lr: 0.00752 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8431 - lr: 0.00751 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8432 - lr: 0.00751 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8433 - lr: 0.00751 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8434 - lr: 0.00751 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8435 - lr: 0.00750 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8436 - lr: 0.00750 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8437 - lr: 0.00750 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8438 - lr: 0.00750 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8439 - lr: 0.00750 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8440 - lr: 0.00749 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8441 - lr: 0.00749 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8442 - lr: 0.00749 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8443 - lr: 0.00749 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8444 - lr: 0.00748 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8445 - lr: 0.00748 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8446 - lr: 0.00748 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8447 - lr: 0.00748 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8448 - lr: 0.00747 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8449 - lr: 0.00747 - Train loss: 0.00861 - Test loss: 0.04009\n",
      "Epoch 8450 - lr: 0.00747 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8451 - lr: 0.00747 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8452 - lr: 0.00747 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8453 - lr: 0.00746 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8454 - lr: 0.00746 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8455 - lr: 0.00746 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8456 - lr: 0.00746 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8457 - lr: 0.00745 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8458 - lr: 0.00745 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8459 - lr: 0.00745 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8460 - lr: 0.00745 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8461 - lr: 0.00745 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8462 - lr: 0.00744 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8463 - lr: 0.00744 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8464 - lr: 0.00744 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8465 - lr: 0.00744 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8466 - lr: 0.00743 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8467 - lr: 0.00743 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8468 - lr: 0.00743 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8469 - lr: 0.00743 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8470 - lr: 0.00742 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8471 - lr: 0.00742 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8472 - lr: 0.00742 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8473 - lr: 0.00742 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8474 - lr: 0.00742 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8475 - lr: 0.00741 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8476 - lr: 0.00741 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8477 - lr: 0.00741 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8478 - lr: 0.00741 - Train loss: 0.00860 - Test loss: 0.04009\n",
      "Epoch 8479 - lr: 0.00740 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8480 - lr: 0.00740 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8481 - lr: 0.00740 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8482 - lr: 0.00740 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8483 - lr: 0.00739 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8484 - lr: 0.00739 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8485 - lr: 0.00739 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8486 - lr: 0.00739 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8487 - lr: 0.00739 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8488 - lr: 0.00738 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8489 - lr: 0.00738 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8490 - lr: 0.00738 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8491 - lr: 0.00738 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8492 - lr: 0.00737 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8493 - lr: 0.00737 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8494 - lr: 0.00737 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8495 - lr: 0.00737 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8496 - lr: 0.00737 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8497 - lr: 0.00736 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8498 - lr: 0.00736 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8499 - lr: 0.00736 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8500 - lr: 0.00736 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8501 - lr: 0.00735 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8502 - lr: 0.00735 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8503 - lr: 0.00735 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8504 - lr: 0.00735 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8505 - lr: 0.00735 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8506 - lr: 0.00734 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8507 - lr: 0.00734 - Train loss: 0.00859 - Test loss: 0.04009\n",
      "Epoch 8508 - lr: 0.00734 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8509 - lr: 0.00734 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8510 - lr: 0.00733 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8511 - lr: 0.00733 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8512 - lr: 0.00733 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8513 - lr: 0.00733 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8514 - lr: 0.00732 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8515 - lr: 0.00732 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8516 - lr: 0.00732 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8517 - lr: 0.00732 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8518 - lr: 0.00732 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8519 - lr: 0.00731 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8520 - lr: 0.00731 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8521 - lr: 0.00731 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8522 - lr: 0.00731 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8523 - lr: 0.00730 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8524 - lr: 0.00730 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8525 - lr: 0.00730 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8526 - lr: 0.00730 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8527 - lr: 0.00730 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8528 - lr: 0.00729 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8529 - lr: 0.00729 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8530 - lr: 0.00729 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8531 - lr: 0.00729 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8532 - lr: 0.00728 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8533 - lr: 0.00728 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8534 - lr: 0.00728 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8535 - lr: 0.00728 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8536 - lr: 0.00728 - Train loss: 0.00858 - Test loss: 0.04009\n",
      "Epoch 8537 - lr: 0.00727 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8538 - lr: 0.00727 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8539 - lr: 0.00727 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8540 - lr: 0.00727 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8541 - lr: 0.00726 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8542 - lr: 0.00726 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8543 - lr: 0.00726 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8544 - lr: 0.00726 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8545 - lr: 0.00726 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8546 - lr: 0.00725 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8547 - lr: 0.00725 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8548 - lr: 0.00725 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8549 - lr: 0.00725 - Train loss: 0.00857 - Test loss: 0.04009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8550 - lr: 0.00724 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8551 - lr: 0.00724 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8552 - lr: 0.00724 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8553 - lr: 0.00724 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8554 - lr: 0.00724 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8555 - lr: 0.00723 - Train loss: 0.00857 - Test loss: 0.04009\n",
      "Epoch 8556 - lr: 0.00723 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8557 - lr: 0.00723 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8558 - lr: 0.00723 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8559 - lr: 0.00722 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8560 - lr: 0.00722 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8561 - lr: 0.00722 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8562 - lr: 0.00722 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8563 - lr: 0.00722 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8564 - lr: 0.00721 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8565 - lr: 0.00721 - Train loss: 0.00857 - Test loss: 0.04008\n",
      "Epoch 8566 - lr: 0.00721 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8567 - lr: 0.00721 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8568 - lr: 0.00720 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8569 - lr: 0.00720 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8570 - lr: 0.00720 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8571 - lr: 0.00720 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8572 - lr: 0.00720 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8573 - lr: 0.00719 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8574 - lr: 0.00719 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8575 - lr: 0.00719 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8576 - lr: 0.00719 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8577 - lr: 0.00718 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8578 - lr: 0.00718 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8579 - lr: 0.00718 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8580 - lr: 0.00718 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8581 - lr: 0.00718 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8582 - lr: 0.00717 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8583 - lr: 0.00717 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8584 - lr: 0.00717 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8585 - lr: 0.00717 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8586 - lr: 0.00716 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8587 - lr: 0.00716 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8588 - lr: 0.00716 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8589 - lr: 0.00716 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8590 - lr: 0.00716 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8591 - lr: 0.00715 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8592 - lr: 0.00715 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8593 - lr: 0.00715 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8594 - lr: 0.00715 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8595 - lr: 0.00714 - Train loss: 0.00856 - Test loss: 0.04008\n",
      "Epoch 8596 - lr: 0.00714 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8597 - lr: 0.00714 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8598 - lr: 0.00714 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8599 - lr: 0.00714 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8600 - lr: 0.00713 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8601 - lr: 0.00713 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8602 - lr: 0.00713 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8603 - lr: 0.00713 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8604 - lr: 0.00713 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8605 - lr: 0.00712 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8606 - lr: 0.00712 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8607 - lr: 0.00712 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8608 - lr: 0.00712 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8609 - lr: 0.00711 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8610 - lr: 0.00711 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8611 - lr: 0.00711 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8612 - lr: 0.00711 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8613 - lr: 0.00711 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8614 - lr: 0.00710 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8615 - lr: 0.00710 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8616 - lr: 0.00710 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8617 - lr: 0.00710 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8618 - lr: 0.00709 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8619 - lr: 0.00709 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8620 - lr: 0.00709 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8621 - lr: 0.00709 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8622 - lr: 0.00709 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8623 - lr: 0.00708 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8624 - lr: 0.00708 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8625 - lr: 0.00708 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8626 - lr: 0.00708 - Train loss: 0.00855 - Test loss: 0.04008\n",
      "Epoch 8627 - lr: 0.00708 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8628 - lr: 0.00707 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8629 - lr: 0.00707 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8630 - lr: 0.00707 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8631 - lr: 0.00707 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8632 - lr: 0.00706 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8633 - lr: 0.00706 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8634 - lr: 0.00706 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8635 - lr: 0.00706 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8636 - lr: 0.00706 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8637 - lr: 0.00705 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8638 - lr: 0.00705 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8639 - lr: 0.00705 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8640 - lr: 0.00705 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8641 - lr: 0.00704 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8642 - lr: 0.00704 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8643 - lr: 0.00704 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8644 - lr: 0.00704 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8645 - lr: 0.00704 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8646 - lr: 0.00703 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8647 - lr: 0.00703 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8648 - lr: 0.00703 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8649 - lr: 0.00703 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8650 - lr: 0.00703 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8651 - lr: 0.00702 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8652 - lr: 0.00702 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8653 - lr: 0.00702 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8654 - lr: 0.00702 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8655 - lr: 0.00701 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8656 - lr: 0.00701 - Train loss: 0.00854 - Test loss: 0.04008\n",
      "Epoch 8657 - lr: 0.00701 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8658 - lr: 0.00701 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8659 - lr: 0.00701 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8660 - lr: 0.00700 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8661 - lr: 0.00700 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8662 - lr: 0.00700 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8663 - lr: 0.00700 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8664 - lr: 0.00700 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8665 - lr: 0.00699 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8666 - lr: 0.00699 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8667 - lr: 0.00699 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8668 - lr: 0.00699 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8669 - lr: 0.00698 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8670 - lr: 0.00698 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8671 - lr: 0.00698 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8672 - lr: 0.00698 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8673 - lr: 0.00698 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8674 - lr: 0.00697 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8675 - lr: 0.00697 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8676 - lr: 0.00697 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8677 - lr: 0.00697 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8678 - lr: 0.00697 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8679 - lr: 0.00696 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8680 - lr: 0.00696 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8681 - lr: 0.00696 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8682 - lr: 0.00696 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8683 - lr: 0.00695 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8684 - lr: 0.00695 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8685 - lr: 0.00695 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8686 - lr: 0.00695 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8687 - lr: 0.00695 - Train loss: 0.00853 - Test loss: 0.04008\n",
      "Epoch 8688 - lr: 0.00694 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8689 - lr: 0.00694 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8690 - lr: 0.00694 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8691 - lr: 0.00694 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8692 - lr: 0.00694 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8693 - lr: 0.00693 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8694 - lr: 0.00693 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8695 - lr: 0.00693 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8696 - lr: 0.00693 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8697 - lr: 0.00692 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8698 - lr: 0.00692 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8699 - lr: 0.00692 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8700 - lr: 0.00692 - Train loss: 0.00852 - Test loss: 0.04008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8701 - lr: 0.00692 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8702 - lr: 0.00691 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8703 - lr: 0.00691 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8704 - lr: 0.00691 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8705 - lr: 0.00691 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8706 - lr: 0.00691 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8707 - lr: 0.00690 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8708 - lr: 0.00690 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8709 - lr: 0.00690 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8710 - lr: 0.00690 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8711 - lr: 0.00689 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8712 - lr: 0.00689 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8713 - lr: 0.00689 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8714 - lr: 0.00689 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8715 - lr: 0.00689 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8716 - lr: 0.00688 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8717 - lr: 0.00688 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8718 - lr: 0.00688 - Train loss: 0.00852 - Test loss: 0.04008\n",
      "Epoch 8719 - lr: 0.00688 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8720 - lr: 0.00688 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8721 - lr: 0.00687 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8722 - lr: 0.00687 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8723 - lr: 0.00687 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8724 - lr: 0.00687 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8725 - lr: 0.00687 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8726 - lr: 0.00686 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8727 - lr: 0.00686 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8728 - lr: 0.00686 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8729 - lr: 0.00686 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8730 - lr: 0.00685 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8731 - lr: 0.00685 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8732 - lr: 0.00685 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8733 - lr: 0.00685 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8734 - lr: 0.00685 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8735 - lr: 0.00684 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8736 - lr: 0.00684 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8737 - lr: 0.00684 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8738 - lr: 0.00684 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8739 - lr: 0.00684 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8740 - lr: 0.00683 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8741 - lr: 0.00683 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8742 - lr: 0.00683 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8743 - lr: 0.00683 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8744 - lr: 0.00683 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8745 - lr: 0.00682 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8746 - lr: 0.00682 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8747 - lr: 0.00682 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8748 - lr: 0.00682 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8749 - lr: 0.00682 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8750 - lr: 0.00681 - Train loss: 0.00851 - Test loss: 0.04008\n",
      "Epoch 8751 - lr: 0.00681 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8752 - lr: 0.00681 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8753 - lr: 0.00681 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8754 - lr: 0.00680 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8755 - lr: 0.00680 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8756 - lr: 0.00680 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8757 - lr: 0.00680 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8758 - lr: 0.00680 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8759 - lr: 0.00679 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8760 - lr: 0.00679 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8761 - lr: 0.00679 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8762 - lr: 0.00679 - Train loss: 0.00850 - Test loss: 0.04008\n",
      "Epoch 8763 - lr: 0.00679 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8764 - lr: 0.00678 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8765 - lr: 0.00678 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8766 - lr: 0.00678 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8767 - lr: 0.00678 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8768 - lr: 0.00678 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8769 - lr: 0.00677 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8770 - lr: 0.00677 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8771 - lr: 0.00677 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8772 - lr: 0.00677 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8773 - lr: 0.00676 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8774 - lr: 0.00676 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8775 - lr: 0.00676 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8776 - lr: 0.00676 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8777 - lr: 0.00676 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8778 - lr: 0.00675 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8779 - lr: 0.00675 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8780 - lr: 0.00675 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8781 - lr: 0.00675 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8782 - lr: 0.00675 - Train loss: 0.00850 - Test loss: 0.04007\n",
      "Epoch 8783 - lr: 0.00674 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8784 - lr: 0.00674 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8785 - lr: 0.00674 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8786 - lr: 0.00674 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8787 - lr: 0.00674 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8788 - lr: 0.00673 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8789 - lr: 0.00673 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8790 - lr: 0.00673 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8791 - lr: 0.00673 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8792 - lr: 0.00673 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8793 - lr: 0.00672 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8794 - lr: 0.00672 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8795 - lr: 0.00672 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8796 - lr: 0.00672 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8797 - lr: 0.00672 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8798 - lr: 0.00671 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8799 - lr: 0.00671 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8800 - lr: 0.00671 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8801 - lr: 0.00671 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8802 - lr: 0.00671 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8803 - lr: 0.00670 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8804 - lr: 0.00670 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8805 - lr: 0.00670 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8806 - lr: 0.00670 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8807 - lr: 0.00669 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8808 - lr: 0.00669 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8809 - lr: 0.00669 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8810 - lr: 0.00669 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8811 - lr: 0.00669 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8812 - lr: 0.00668 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8813 - lr: 0.00668 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8814 - lr: 0.00668 - Train loss: 0.00849 - Test loss: 0.04007\n",
      "Epoch 8815 - lr: 0.00668 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8816 - lr: 0.00668 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8817 - lr: 0.00667 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8818 - lr: 0.00667 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8819 - lr: 0.00667 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8820 - lr: 0.00667 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8821 - lr: 0.00667 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8822 - lr: 0.00666 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8823 - lr: 0.00666 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8824 - lr: 0.00666 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8825 - lr: 0.00666 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8826 - lr: 0.00666 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8827 - lr: 0.00665 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8828 - lr: 0.00665 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8829 - lr: 0.00665 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8830 - lr: 0.00665 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8831 - lr: 0.00665 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8832 - lr: 0.00664 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8833 - lr: 0.00664 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8834 - lr: 0.00664 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8835 - lr: 0.00664 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8836 - lr: 0.00664 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8837 - lr: 0.00663 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8838 - lr: 0.00663 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8839 - lr: 0.00663 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8840 - lr: 0.00663 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8841 - lr: 0.00663 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8842 - lr: 0.00662 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8843 - lr: 0.00662 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8844 - lr: 0.00662 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8845 - lr: 0.00662 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8846 - lr: 0.00662 - Train loss: 0.00848 - Test loss: 0.04007\n",
      "Epoch 8847 - lr: 0.00661 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8848 - lr: 0.00661 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8849 - lr: 0.00661 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8850 - lr: 0.00661 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8851 - lr: 0.00660 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8852 - lr: 0.00660 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8853 - lr: 0.00660 - Train loss: 0.00847 - Test loss: 0.04007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8854 - lr: 0.00660 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8855 - lr: 0.00660 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8856 - lr: 0.00659 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8857 - lr: 0.00659 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8858 - lr: 0.00659 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8859 - lr: 0.00659 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8860 - lr: 0.00659 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8861 - lr: 0.00658 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8862 - lr: 0.00658 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8863 - lr: 0.00658 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8864 - lr: 0.00658 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8865 - lr: 0.00658 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8866 - lr: 0.00657 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8867 - lr: 0.00657 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8868 - lr: 0.00657 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8869 - lr: 0.00657 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8870 - lr: 0.00657 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8871 - lr: 0.00656 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8872 - lr: 0.00656 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8873 - lr: 0.00656 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8874 - lr: 0.00656 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8875 - lr: 0.00656 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8876 - lr: 0.00655 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8877 - lr: 0.00655 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8878 - lr: 0.00655 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8879 - lr: 0.00655 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8880 - lr: 0.00655 - Train loss: 0.00847 - Test loss: 0.04007\n",
      "Epoch 8881 - lr: 0.00654 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8882 - lr: 0.00654 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8883 - lr: 0.00654 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8884 - lr: 0.00654 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8885 - lr: 0.00654 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8886 - lr: 0.00653 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8887 - lr: 0.00653 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8888 - lr: 0.00653 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8889 - lr: 0.00653 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8890 - lr: 0.00653 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8891 - lr: 0.00652 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8892 - lr: 0.00652 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8893 - lr: 0.00652 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8894 - lr: 0.00652 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8895 - lr: 0.00652 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8896 - lr: 0.00651 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8897 - lr: 0.00651 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8898 - lr: 0.00651 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8899 - lr: 0.00651 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8900 - lr: 0.00651 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8901 - lr: 0.00650 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8902 - lr: 0.00650 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8903 - lr: 0.00650 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8904 - lr: 0.00650 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8905 - lr: 0.00650 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8906 - lr: 0.00649 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8907 - lr: 0.00649 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8908 - lr: 0.00649 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8909 - lr: 0.00649 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8910 - lr: 0.00649 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8911 - lr: 0.00648 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8912 - lr: 0.00648 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8913 - lr: 0.00648 - Train loss: 0.00846 - Test loss: 0.04007\n",
      "Epoch 8914 - lr: 0.00648 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8915 - lr: 0.00648 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8916 - lr: 0.00647 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8917 - lr: 0.00647 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8918 - lr: 0.00647 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8919 - lr: 0.00647 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8920 - lr: 0.00647 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8921 - lr: 0.00646 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8922 - lr: 0.00646 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8923 - lr: 0.00646 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8924 - lr: 0.00646 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8925 - lr: 0.00646 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8926 - lr: 0.00645 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8927 - lr: 0.00645 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8928 - lr: 0.00645 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8929 - lr: 0.00645 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8930 - lr: 0.00645 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8931 - lr: 0.00644 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8932 - lr: 0.00644 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8933 - lr: 0.00644 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8934 - lr: 0.00644 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8935 - lr: 0.00644 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8936 - lr: 0.00643 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8937 - lr: 0.00643 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8938 - lr: 0.00643 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8939 - lr: 0.00643 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8940 - lr: 0.00643 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8941 - lr: 0.00642 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8942 - lr: 0.00642 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8943 - lr: 0.00642 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8944 - lr: 0.00642 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8945 - lr: 0.00642 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8946 - lr: 0.00642 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8947 - lr: 0.00641 - Train loss: 0.00845 - Test loss: 0.04007\n",
      "Epoch 8948 - lr: 0.00641 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8949 - lr: 0.00641 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8950 - lr: 0.00641 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8951 - lr: 0.00641 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8952 - lr: 0.00640 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8953 - lr: 0.00640 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8954 - lr: 0.00640 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8955 - lr: 0.00640 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8956 - lr: 0.00640 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8957 - lr: 0.00639 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8958 - lr: 0.00639 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8959 - lr: 0.00639 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8960 - lr: 0.00639 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8961 - lr: 0.00639 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8962 - lr: 0.00638 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8963 - lr: 0.00638 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8964 - lr: 0.00638 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8965 - lr: 0.00638 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8966 - lr: 0.00638 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8967 - lr: 0.00637 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8968 - lr: 0.00637 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8969 - lr: 0.00637 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8970 - lr: 0.00637 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8971 - lr: 0.00637 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8972 - lr: 0.00636 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8973 - lr: 0.00636 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8974 - lr: 0.00636 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8975 - lr: 0.00636 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8976 - lr: 0.00636 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8977 - lr: 0.00635 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8978 - lr: 0.00635 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8979 - lr: 0.00635 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8980 - lr: 0.00635 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8981 - lr: 0.00635 - Train loss: 0.00844 - Test loss: 0.04007\n",
      "Epoch 8982 - lr: 0.00634 - Train loss: 0.00843 - Test loss: 0.04007\n",
      "Epoch 8983 - lr: 0.00634 - Train loss: 0.00843 - Test loss: 0.04007\n",
      "Epoch 8984 - lr: 0.00634 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8985 - lr: 0.00634 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8986 - lr: 0.00634 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8987 - lr: 0.00633 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8988 - lr: 0.00633 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8989 - lr: 0.00633 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8990 - lr: 0.00633 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8991 - lr: 0.00633 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8992 - lr: 0.00633 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8993 - lr: 0.00632 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8994 - lr: 0.00632 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8995 - lr: 0.00632 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8996 - lr: 0.00632 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8997 - lr: 0.00632 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8998 - lr: 0.00631 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 8999 - lr: 0.00631 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9000 - lr: 0.00631 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9001 - lr: 0.00631 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9002 - lr: 0.00631 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9003 - lr: 0.00630 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9004 - lr: 0.00630 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9005 - lr: 0.00630 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9006 - lr: 0.00630 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9007 - lr: 0.00630 - Train loss: 0.00843 - Test loss: 0.04006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9008 - lr: 0.00629 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9009 - lr: 0.00629 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9010 - lr: 0.00629 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9011 - lr: 0.00629 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9012 - lr: 0.00629 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9013 - lr: 0.00628 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9014 - lr: 0.00628 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9015 - lr: 0.00628 - Train loss: 0.00843 - Test loss: 0.04006\n",
      "Epoch 9016 - lr: 0.00628 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9017 - lr: 0.00628 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9018 - lr: 0.00627 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9019 - lr: 0.00627 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9020 - lr: 0.00627 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9021 - lr: 0.00627 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9022 - lr: 0.00627 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9023 - lr: 0.00627 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9024 - lr: 0.00626 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9025 - lr: 0.00626 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9026 - lr: 0.00626 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9027 - lr: 0.00626 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9028 - lr: 0.00626 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9029 - lr: 0.00625 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9030 - lr: 0.00625 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9031 - lr: 0.00625 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9032 - lr: 0.00625 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9033 - lr: 0.00625 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9034 - lr: 0.00624 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9035 - lr: 0.00624 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9036 - lr: 0.00624 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9037 - lr: 0.00624 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9038 - lr: 0.00624 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9039 - lr: 0.00623 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9040 - lr: 0.00623 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9041 - lr: 0.00623 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9042 - lr: 0.00623 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9043 - lr: 0.00623 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9044 - lr: 0.00622 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9045 - lr: 0.00622 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9046 - lr: 0.00622 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9047 - lr: 0.00622 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9048 - lr: 0.00622 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9049 - lr: 0.00622 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9050 - lr: 0.00621 - Train loss: 0.00842 - Test loss: 0.04006\n",
      "Epoch 9051 - lr: 0.00621 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9052 - lr: 0.00621 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9053 - lr: 0.00621 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9054 - lr: 0.00621 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9055 - lr: 0.00620 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9056 - lr: 0.00620 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9057 - lr: 0.00620 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9058 - lr: 0.00620 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9059 - lr: 0.00620 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9060 - lr: 0.00619 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9061 - lr: 0.00619 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9062 - lr: 0.00619 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9063 - lr: 0.00619 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9064 - lr: 0.00619 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9065 - lr: 0.00618 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9066 - lr: 0.00618 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9067 - lr: 0.00618 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9068 - lr: 0.00618 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9069 - lr: 0.00618 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9070 - lr: 0.00618 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9071 - lr: 0.00617 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9072 - lr: 0.00617 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9073 - lr: 0.00617 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9074 - lr: 0.00617 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9075 - lr: 0.00617 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9076 - lr: 0.00616 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9077 - lr: 0.00616 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9078 - lr: 0.00616 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9079 - lr: 0.00616 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9080 - lr: 0.00616 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9081 - lr: 0.00615 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9082 - lr: 0.00615 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9083 - lr: 0.00615 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9084 - lr: 0.00615 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9085 - lr: 0.00615 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9086 - lr: 0.00615 - Train loss: 0.00841 - Test loss: 0.04006\n",
      "Epoch 9087 - lr: 0.00614 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9088 - lr: 0.00614 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9089 - lr: 0.00614 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9090 - lr: 0.00614 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9091 - lr: 0.00614 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9092 - lr: 0.00613 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9093 - lr: 0.00613 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9094 - lr: 0.00613 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9095 - lr: 0.00613 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9096 - lr: 0.00613 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9097 - lr: 0.00612 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9098 - lr: 0.00612 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9099 - lr: 0.00612 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9100 - lr: 0.00612 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9101 - lr: 0.00612 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9102 - lr: 0.00612 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9103 - lr: 0.00611 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9104 - lr: 0.00611 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9105 - lr: 0.00611 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9106 - lr: 0.00611 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9107 - lr: 0.00611 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9108 - lr: 0.00610 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9109 - lr: 0.00610 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9110 - lr: 0.00610 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9111 - lr: 0.00610 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9112 - lr: 0.00610 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9113 - lr: 0.00609 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9114 - lr: 0.00609 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9115 - lr: 0.00609 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9116 - lr: 0.00609 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9117 - lr: 0.00609 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9118 - lr: 0.00609 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9119 - lr: 0.00608 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9120 - lr: 0.00608 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9121 - lr: 0.00608 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9122 - lr: 0.00608 - Train loss: 0.00840 - Test loss: 0.04006\n",
      "Epoch 9123 - lr: 0.00608 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9124 - lr: 0.00607 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9125 - lr: 0.00607 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9126 - lr: 0.00607 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9127 - lr: 0.00607 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9128 - lr: 0.00607 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9129 - lr: 0.00606 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9130 - lr: 0.00606 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9131 - lr: 0.00606 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9132 - lr: 0.00606 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9133 - lr: 0.00606 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9134 - lr: 0.00606 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9135 - lr: 0.00605 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9136 - lr: 0.00605 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9137 - lr: 0.00605 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9138 - lr: 0.00605 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9139 - lr: 0.00605 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9140 - lr: 0.00604 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9141 - lr: 0.00604 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9142 - lr: 0.00604 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9143 - lr: 0.00604 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9144 - lr: 0.00604 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9145 - lr: 0.00603 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9146 - lr: 0.00603 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9147 - lr: 0.00603 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9148 - lr: 0.00603 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9149 - lr: 0.00603 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9150 - lr: 0.00603 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9151 - lr: 0.00602 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9152 - lr: 0.00602 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9153 - lr: 0.00602 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9154 - lr: 0.00602 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9155 - lr: 0.00602 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9156 - lr: 0.00601 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9157 - lr: 0.00601 - Train loss: 0.00839 - Test loss: 0.04006\n",
      "Epoch 9158 - lr: 0.00601 - Train loss: 0.00839 - Test loss: 0.04006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9159 - lr: 0.00601 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9160 - lr: 0.00601 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9161 - lr: 0.00601 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9162 - lr: 0.00600 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9163 - lr: 0.00600 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9164 - lr: 0.00600 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9165 - lr: 0.00600 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9166 - lr: 0.00600 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9167 - lr: 0.00599 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9168 - lr: 0.00599 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9169 - lr: 0.00599 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9170 - lr: 0.00599 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9171 - lr: 0.00599 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9172 - lr: 0.00599 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9173 - lr: 0.00598 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9174 - lr: 0.00598 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9175 - lr: 0.00598 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9176 - lr: 0.00598 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9177 - lr: 0.00598 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9178 - lr: 0.00597 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9179 - lr: 0.00597 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9180 - lr: 0.00597 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9181 - lr: 0.00597 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9182 - lr: 0.00597 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9183 - lr: 0.00596 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9184 - lr: 0.00596 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9185 - lr: 0.00596 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9186 - lr: 0.00596 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9187 - lr: 0.00596 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9188 - lr: 0.00596 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9189 - lr: 0.00595 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9190 - lr: 0.00595 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9191 - lr: 0.00595 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9192 - lr: 0.00595 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9193 - lr: 0.00595 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9194 - lr: 0.00594 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9195 - lr: 0.00594 - Train loss: 0.00838 - Test loss: 0.04006\n",
      "Epoch 9196 - lr: 0.00594 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9197 - lr: 0.00594 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9198 - lr: 0.00594 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9199 - lr: 0.00594 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9200 - lr: 0.00593 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9201 - lr: 0.00593 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9202 - lr: 0.00593 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9203 - lr: 0.00593 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9204 - lr: 0.00593 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9205 - lr: 0.00592 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9206 - lr: 0.00592 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9207 - lr: 0.00592 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9208 - lr: 0.00592 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9209 - lr: 0.00592 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9210 - lr: 0.00592 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9211 - lr: 0.00591 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9212 - lr: 0.00591 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9213 - lr: 0.00591 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9214 - lr: 0.00591 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9215 - lr: 0.00591 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9216 - lr: 0.00590 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9217 - lr: 0.00590 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9218 - lr: 0.00590 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9219 - lr: 0.00590 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9220 - lr: 0.00590 - Train loss: 0.00837 - Test loss: 0.04006\n",
      "Epoch 9221 - lr: 0.00590 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9222 - lr: 0.00589 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9223 - lr: 0.00589 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9224 - lr: 0.00589 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9225 - lr: 0.00589 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9226 - lr: 0.00589 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9227 - lr: 0.00588 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9228 - lr: 0.00588 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9229 - lr: 0.00588 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9230 - lr: 0.00588 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9231 - lr: 0.00588 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9232 - lr: 0.00588 - Train loss: 0.00837 - Test loss: 0.04005\n",
      "Epoch 9233 - lr: 0.00587 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9234 - lr: 0.00587 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9235 - lr: 0.00587 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9236 - lr: 0.00587 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9237 - lr: 0.00587 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9238 - lr: 0.00586 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9239 - lr: 0.00586 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9240 - lr: 0.00586 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9241 - lr: 0.00586 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9242 - lr: 0.00586 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9243 - lr: 0.00586 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9244 - lr: 0.00585 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9245 - lr: 0.00585 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9246 - lr: 0.00585 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9247 - lr: 0.00585 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9248 - lr: 0.00585 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9249 - lr: 0.00585 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9250 - lr: 0.00584 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9251 - lr: 0.00584 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9252 - lr: 0.00584 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9253 - lr: 0.00584 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9254 - lr: 0.00584 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9255 - lr: 0.00583 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9256 - lr: 0.00583 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9257 - lr: 0.00583 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9258 - lr: 0.00583 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9259 - lr: 0.00583 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9260 - lr: 0.00583 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9261 - lr: 0.00582 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9262 - lr: 0.00582 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9263 - lr: 0.00582 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9264 - lr: 0.00582 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9265 - lr: 0.00582 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9266 - lr: 0.00581 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9267 - lr: 0.00581 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9268 - lr: 0.00581 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9269 - lr: 0.00581 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9270 - lr: 0.00581 - Train loss: 0.00836 - Test loss: 0.04005\n",
      "Epoch 9271 - lr: 0.00581 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9272 - lr: 0.00580 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9273 - lr: 0.00580 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9274 - lr: 0.00580 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9275 - lr: 0.00580 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9276 - lr: 0.00580 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9277 - lr: 0.00580 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9278 - lr: 0.00579 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9279 - lr: 0.00579 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9280 - lr: 0.00579 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9281 - lr: 0.00579 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9282 - lr: 0.00579 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9283 - lr: 0.00578 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9284 - lr: 0.00578 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9285 - lr: 0.00578 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9286 - lr: 0.00578 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9287 - lr: 0.00578 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9288 - lr: 0.00578 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9289 - lr: 0.00577 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9290 - lr: 0.00577 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9291 - lr: 0.00577 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9292 - lr: 0.00577 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9293 - lr: 0.00577 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9294 - lr: 0.00577 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9295 - lr: 0.00576 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9296 - lr: 0.00576 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9297 - lr: 0.00576 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9298 - lr: 0.00576 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9299 - lr: 0.00576 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9300 - lr: 0.00575 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9301 - lr: 0.00575 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9302 - lr: 0.00575 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9303 - lr: 0.00575 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9304 - lr: 0.00575 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9305 - lr: 0.00575 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9306 - lr: 0.00574 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9307 - lr: 0.00574 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9308 - lr: 0.00574 - Train loss: 0.00835 - Test loss: 0.04005\n",
      "Epoch 9309 - lr: 0.00574 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9310 - lr: 0.00574 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9311 - lr: 0.00573 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9312 - lr: 0.00573 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9313 - lr: 0.00573 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9314 - lr: 0.00573 - Train loss: 0.00834 - Test loss: 0.04005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9315 - lr: 0.00573 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9316 - lr: 0.00573 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9317 - lr: 0.00572 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9318 - lr: 0.00572 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9319 - lr: 0.00572 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9320 - lr: 0.00572 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9321 - lr: 0.00572 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9322 - lr: 0.00572 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9323 - lr: 0.00571 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9324 - lr: 0.00571 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9325 - lr: 0.00571 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9326 - lr: 0.00571 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9327 - lr: 0.00571 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9328 - lr: 0.00571 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9329 - lr: 0.00570 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9330 - lr: 0.00570 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9331 - lr: 0.00570 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9332 - lr: 0.00570 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9333 - lr: 0.00570 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9334 - lr: 0.00569 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9335 - lr: 0.00569 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9336 - lr: 0.00569 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9337 - lr: 0.00569 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9338 - lr: 0.00569 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9339 - lr: 0.00569 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9340 - lr: 0.00568 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9341 - lr: 0.00568 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9342 - lr: 0.00568 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9343 - lr: 0.00568 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9344 - lr: 0.00568 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9345 - lr: 0.00568 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9346 - lr: 0.00567 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9347 - lr: 0.00567 - Train loss: 0.00834 - Test loss: 0.04005\n",
      "Epoch 9348 - lr: 0.00567 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9349 - lr: 0.00567 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9350 - lr: 0.00567 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9351 - lr: 0.00567 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9352 - lr: 0.00566 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9353 - lr: 0.00566 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9354 - lr: 0.00566 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9355 - lr: 0.00566 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9356 - lr: 0.00566 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9357 - lr: 0.00565 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9358 - lr: 0.00565 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9359 - lr: 0.00565 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9360 - lr: 0.00565 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9361 - lr: 0.00565 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9362 - lr: 0.00565 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9363 - lr: 0.00564 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9364 - lr: 0.00564 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9365 - lr: 0.00564 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9366 - lr: 0.00564 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9367 - lr: 0.00564 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9368 - lr: 0.00564 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9369 - lr: 0.00563 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9370 - lr: 0.00563 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9371 - lr: 0.00563 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9372 - lr: 0.00563 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9373 - lr: 0.00563 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9374 - lr: 0.00563 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9375 - lr: 0.00562 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9376 - lr: 0.00562 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9377 - lr: 0.00562 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9378 - lr: 0.00562 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9379 - lr: 0.00562 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9380 - lr: 0.00561 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9381 - lr: 0.00561 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9382 - lr: 0.00561 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9383 - lr: 0.00561 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9384 - lr: 0.00561 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9385 - lr: 0.00561 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9386 - lr: 0.00560 - Train loss: 0.00833 - Test loss: 0.04005\n",
      "Epoch 9387 - lr: 0.00560 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9388 - lr: 0.00560 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9389 - lr: 0.00560 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9390 - lr: 0.00560 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9391 - lr: 0.00560 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9392 - lr: 0.00559 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9393 - lr: 0.00559 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9394 - lr: 0.00559 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9395 - lr: 0.00559 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9396 - lr: 0.00559 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9397 - lr: 0.00559 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9398 - lr: 0.00558 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9399 - lr: 0.00558 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9400 - lr: 0.00558 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9401 - lr: 0.00558 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9402 - lr: 0.00558 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9403 - lr: 0.00558 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9404 - lr: 0.00557 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9405 - lr: 0.00557 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9406 - lr: 0.00557 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9407 - lr: 0.00557 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9408 - lr: 0.00557 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9409 - lr: 0.00557 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9410 - lr: 0.00556 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9411 - lr: 0.00556 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9412 - lr: 0.00556 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9413 - lr: 0.00556 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9414 - lr: 0.00556 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9415 - lr: 0.00555 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9416 - lr: 0.00555 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9417 - lr: 0.00555 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9418 - lr: 0.00555 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9419 - lr: 0.00555 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9420 - lr: 0.00555 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9421 - lr: 0.00554 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9422 - lr: 0.00554 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9423 - lr: 0.00554 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9424 - lr: 0.00554 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9425 - lr: 0.00554 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9426 - lr: 0.00554 - Train loss: 0.00832 - Test loss: 0.04005\n",
      "Epoch 9427 - lr: 0.00553 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9428 - lr: 0.00553 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9429 - lr: 0.00553 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9430 - lr: 0.00553 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9431 - lr: 0.00553 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9432 - lr: 0.00553 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9433 - lr: 0.00552 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9434 - lr: 0.00552 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9435 - lr: 0.00552 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9436 - lr: 0.00552 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9437 - lr: 0.00552 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9438 - lr: 0.00552 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9439 - lr: 0.00551 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9440 - lr: 0.00551 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9441 - lr: 0.00551 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9442 - lr: 0.00551 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9443 - lr: 0.00551 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9444 - lr: 0.00551 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9445 - lr: 0.00550 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9446 - lr: 0.00550 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9447 - lr: 0.00550 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9448 - lr: 0.00550 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9449 - lr: 0.00550 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9450 - lr: 0.00550 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9451 - lr: 0.00549 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9452 - lr: 0.00549 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9453 - lr: 0.00549 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9454 - lr: 0.00549 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9455 - lr: 0.00549 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9456 - lr: 0.00549 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9457 - lr: 0.00548 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9458 - lr: 0.00548 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9459 - lr: 0.00548 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9460 - lr: 0.00548 - Train loss: 0.00831 - Test loss: 0.04005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9461 - lr: 0.00548 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9462 - lr: 0.00548 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9463 - lr: 0.00547 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9464 - lr: 0.00547 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9465 - lr: 0.00547 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9466 - lr: 0.00547 - Train loss: 0.00831 - Test loss: 0.04005\n",
      "Epoch 9467 - lr: 0.00547 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9468 - lr: 0.00547 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9469 - lr: 0.00546 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9470 - lr: 0.00546 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9471 - lr: 0.00546 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9472 - lr: 0.00546 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9473 - lr: 0.00546 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9474 - lr: 0.00546 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9475 - lr: 0.00545 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9476 - lr: 0.00545 - Train loss: 0.00830 - Test loss: 0.04005\n",
      "Epoch 9477 - lr: 0.00545 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9478 - lr: 0.00545 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9479 - lr: 0.00545 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9480 - lr: 0.00545 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9481 - lr: 0.00544 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9482 - lr: 0.00544 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9483 - lr: 0.00544 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9484 - lr: 0.00544 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9485 - lr: 0.00544 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9486 - lr: 0.00544 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9487 - lr: 0.00543 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9488 - lr: 0.00543 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9489 - lr: 0.00543 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9490 - lr: 0.00543 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9491 - lr: 0.00543 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9492 - lr: 0.00543 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9493 - lr: 0.00542 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9494 - lr: 0.00542 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9495 - lr: 0.00542 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9496 - lr: 0.00542 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9497 - lr: 0.00542 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9498 - lr: 0.00542 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9499 - lr: 0.00541 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9500 - lr: 0.00541 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9501 - lr: 0.00541 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9502 - lr: 0.00541 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9503 - lr: 0.00541 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9504 - lr: 0.00541 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9505 - lr: 0.00540 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9506 - lr: 0.00540 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9507 - lr: 0.00540 - Train loss: 0.00830 - Test loss: 0.04004\n",
      "Epoch 9508 - lr: 0.00540 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9509 - lr: 0.00540 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9510 - lr: 0.00540 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9511 - lr: 0.00539 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9512 - lr: 0.00539 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9513 - lr: 0.00539 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9514 - lr: 0.00539 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9515 - lr: 0.00539 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9516 - lr: 0.00539 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9517 - lr: 0.00538 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9518 - lr: 0.00538 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9519 - lr: 0.00538 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9520 - lr: 0.00538 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9521 - lr: 0.00538 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9522 - lr: 0.00538 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9523 - lr: 0.00537 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9524 - lr: 0.00537 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9525 - lr: 0.00537 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9526 - lr: 0.00537 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9527 - lr: 0.00537 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9528 - lr: 0.00537 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9529 - lr: 0.00536 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9530 - lr: 0.00536 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9531 - lr: 0.00536 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9532 - lr: 0.00536 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9533 - lr: 0.00536 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9534 - lr: 0.00536 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9535 - lr: 0.00535 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9536 - lr: 0.00535 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9537 - lr: 0.00535 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9538 - lr: 0.00535 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9539 - lr: 0.00535 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9540 - lr: 0.00535 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9541 - lr: 0.00534 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9542 - lr: 0.00534 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9543 - lr: 0.00534 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9544 - lr: 0.00534 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9545 - lr: 0.00534 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9546 - lr: 0.00534 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9547 - lr: 0.00533 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9548 - lr: 0.00533 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9549 - lr: 0.00533 - Train loss: 0.00829 - Test loss: 0.04004\n",
      "Epoch 9550 - lr: 0.00533 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9551 - lr: 0.00533 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9552 - lr: 0.00533 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9553 - lr: 0.00532 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9554 - lr: 0.00532 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9555 - lr: 0.00532 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9556 - lr: 0.00532 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9557 - lr: 0.00532 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9558 - lr: 0.00532 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9559 - lr: 0.00531 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9560 - lr: 0.00531 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9561 - lr: 0.00531 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9562 - lr: 0.00531 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9563 - lr: 0.00531 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9564 - lr: 0.00531 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9565 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9566 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9567 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9568 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9569 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9570 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9571 - lr: 0.00530 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9572 - lr: 0.00529 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9573 - lr: 0.00529 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9574 - lr: 0.00529 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9575 - lr: 0.00529 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9576 - lr: 0.00529 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9577 - lr: 0.00529 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9578 - lr: 0.00528 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9579 - lr: 0.00528 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9580 - lr: 0.00528 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9581 - lr: 0.00528 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9582 - lr: 0.00528 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9583 - lr: 0.00528 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9584 - lr: 0.00527 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9585 - lr: 0.00527 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9586 - lr: 0.00527 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9587 - lr: 0.00527 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9588 - lr: 0.00527 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9589 - lr: 0.00527 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9590 - lr: 0.00526 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9591 - lr: 0.00526 - Train loss: 0.00828 - Test loss: 0.04004\n",
      "Epoch 9592 - lr: 0.00526 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9593 - lr: 0.00526 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9594 - lr: 0.00526 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9595 - lr: 0.00526 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9596 - lr: 0.00525 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9597 - lr: 0.00525 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9598 - lr: 0.00525 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9599 - lr: 0.00525 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9600 - lr: 0.00525 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9601 - lr: 0.00525 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9602 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9603 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9604 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9605 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9606 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9607 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9608 - lr: 0.00524 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9609 - lr: 0.00523 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9610 - lr: 0.00523 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9611 - lr: 0.00523 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9612 - lr: 0.00523 - Train loss: 0.00827 - Test loss: 0.04004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9613 - lr: 0.00523 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9614 - lr: 0.00523 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9615 - lr: 0.00522 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9616 - lr: 0.00522 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9617 - lr: 0.00522 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9618 - lr: 0.00522 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9619 - lr: 0.00522 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9620 - lr: 0.00522 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9621 - lr: 0.00521 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9622 - lr: 0.00521 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9623 - lr: 0.00521 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9624 - lr: 0.00521 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9625 - lr: 0.00521 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9626 - lr: 0.00521 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9627 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9628 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9629 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9630 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9631 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9632 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9633 - lr: 0.00520 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9634 - lr: 0.00519 - Train loss: 0.00827 - Test loss: 0.04004\n",
      "Epoch 9635 - lr: 0.00519 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9636 - lr: 0.00519 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9637 - lr: 0.00519 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9638 - lr: 0.00519 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9639 - lr: 0.00519 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9640 - lr: 0.00518 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9641 - lr: 0.00518 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9642 - lr: 0.00518 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9643 - lr: 0.00518 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9644 - lr: 0.00518 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9645 - lr: 0.00518 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9646 - lr: 0.00517 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9647 - lr: 0.00517 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9648 - lr: 0.00517 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9649 - lr: 0.00517 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9650 - lr: 0.00517 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9651 - lr: 0.00517 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9652 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9653 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9654 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9655 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9656 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9657 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9658 - lr: 0.00516 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9659 - lr: 0.00515 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9660 - lr: 0.00515 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9661 - lr: 0.00515 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9662 - lr: 0.00515 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9663 - lr: 0.00515 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9664 - lr: 0.00515 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9665 - lr: 0.00514 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9666 - lr: 0.00514 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9667 - lr: 0.00514 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9668 - lr: 0.00514 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9669 - lr: 0.00514 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9670 - lr: 0.00514 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9671 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9672 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9673 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9674 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9675 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9676 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9677 - lr: 0.00513 - Train loss: 0.00826 - Test loss: 0.04004\n",
      "Epoch 9678 - lr: 0.00512 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9679 - lr: 0.00512 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9680 - lr: 0.00512 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9681 - lr: 0.00512 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9682 - lr: 0.00512 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9683 - lr: 0.00512 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9684 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9685 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9686 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9687 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9688 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9689 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9690 - lr: 0.00511 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9691 - lr: 0.00510 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9692 - lr: 0.00510 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9693 - lr: 0.00510 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9694 - lr: 0.00510 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9695 - lr: 0.00510 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9696 - lr: 0.00510 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9697 - lr: 0.00509 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9698 - lr: 0.00509 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9699 - lr: 0.00509 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9700 - lr: 0.00509 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9701 - lr: 0.00509 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9702 - lr: 0.00509 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9703 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9704 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9705 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9706 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9707 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9708 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9709 - lr: 0.00508 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9710 - lr: 0.00507 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9711 - lr: 0.00507 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9712 - lr: 0.00507 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9713 - lr: 0.00507 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9714 - lr: 0.00507 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9715 - lr: 0.00507 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9716 - lr: 0.00506 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9717 - lr: 0.00506 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9718 - lr: 0.00506 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9719 - lr: 0.00506 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9720 - lr: 0.00506 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9721 - lr: 0.00506 - Train loss: 0.00825 - Test loss: 0.04004\n",
      "Epoch 9722 - lr: 0.00506 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9723 - lr: 0.00505 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9724 - lr: 0.00505 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9725 - lr: 0.00505 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9726 - lr: 0.00505 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9727 - lr: 0.00505 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9728 - lr: 0.00505 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9729 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9730 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9731 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9732 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9733 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9734 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9735 - lr: 0.00504 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9736 - lr: 0.00503 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9737 - lr: 0.00503 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9738 - lr: 0.00503 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9739 - lr: 0.00503 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9740 - lr: 0.00503 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9741 - lr: 0.00503 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9742 - lr: 0.00502 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9743 - lr: 0.00502 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9744 - lr: 0.00502 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9745 - lr: 0.00502 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9746 - lr: 0.00502 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9747 - lr: 0.00502 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9748 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9749 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9750 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9751 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9752 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9753 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9754 - lr: 0.00501 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9755 - lr: 0.00500 - Train loss: 0.00824 - Test loss: 0.04004\n",
      "Epoch 9756 - lr: 0.00500 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9757 - lr: 0.00500 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9758 - lr: 0.00500 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9759 - lr: 0.00500 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9760 - lr: 0.00500 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9761 - lr: 0.00499 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9762 - lr: 0.00499 - Train loss: 0.00824 - Test loss: 0.04003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9763 - lr: 0.00499 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9764 - lr: 0.00499 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9765 - lr: 0.00499 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9766 - lr: 0.00499 - Train loss: 0.00824 - Test loss: 0.04003\n",
      "Epoch 9767 - lr: 0.00499 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9768 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9769 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9770 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9771 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9772 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9773 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9774 - lr: 0.00498 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9775 - lr: 0.00497 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9776 - lr: 0.00497 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9777 - lr: 0.00497 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9778 - lr: 0.00497 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9779 - lr: 0.00497 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9780 - lr: 0.00497 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9781 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9782 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9783 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9784 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9785 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9786 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9787 - lr: 0.00496 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9788 - lr: 0.00495 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9789 - lr: 0.00495 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9790 - lr: 0.00495 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9791 - lr: 0.00495 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9792 - lr: 0.00495 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9793 - lr: 0.00495 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9794 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9795 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9796 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9797 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9798 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9799 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9800 - lr: 0.00494 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9801 - lr: 0.00493 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9802 - lr: 0.00493 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9803 - lr: 0.00493 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9804 - lr: 0.00493 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9805 - lr: 0.00493 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9806 - lr: 0.00493 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9807 - lr: 0.00492 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9808 - lr: 0.00492 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9809 - lr: 0.00492 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9810 - lr: 0.00492 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9811 - lr: 0.00492 - Train loss: 0.00823 - Test loss: 0.04003\n",
      "Epoch 9812 - lr: 0.00492 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9813 - lr: 0.00492 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9814 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9815 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9816 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9817 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9818 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9819 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9820 - lr: 0.00491 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9821 - lr: 0.00490 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9822 - lr: 0.00490 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9823 - lr: 0.00490 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9824 - lr: 0.00490 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9825 - lr: 0.00490 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9826 - lr: 0.00490 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9827 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9828 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9829 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9830 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9831 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9832 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9833 - lr: 0.00489 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9834 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9835 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9836 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9837 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9838 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9839 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9840 - lr: 0.00488 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9841 - lr: 0.00487 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9842 - lr: 0.00487 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9843 - lr: 0.00487 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9844 - lr: 0.00487 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9845 - lr: 0.00487 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9846 - lr: 0.00487 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9847 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9848 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9849 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9850 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9851 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9852 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9853 - lr: 0.00486 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9854 - lr: 0.00485 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9855 - lr: 0.00485 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9856 - lr: 0.00485 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9857 - lr: 0.00485 - Train loss: 0.00822 - Test loss: 0.04003\n",
      "Epoch 9858 - lr: 0.00485 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9859 - lr: 0.00485 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9860 - lr: 0.00485 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9861 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9862 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9863 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9864 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9865 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9866 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9867 - lr: 0.00484 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9868 - lr: 0.00483 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9869 - lr: 0.00483 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9870 - lr: 0.00483 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9871 - lr: 0.00483 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9872 - lr: 0.00483 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9873 - lr: 0.00483 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9874 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9875 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9876 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9877 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9878 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9879 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9880 - lr: 0.00482 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9881 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9882 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9883 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9884 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9885 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9886 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9887 - lr: 0.00481 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9888 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9889 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9890 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9891 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9892 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9893 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9894 - lr: 0.00480 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9895 - lr: 0.00479 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9896 - lr: 0.00479 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9897 - lr: 0.00479 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9898 - lr: 0.00479 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9899 - lr: 0.00479 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9900 - lr: 0.00479 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9901 - lr: 0.00478 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9902 - lr: 0.00478 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9903 - lr: 0.00478 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9904 - lr: 0.00478 - Train loss: 0.00821 - Test loss: 0.04003\n",
      "Epoch 9905 - lr: 0.00478 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9906 - lr: 0.00478 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9907 - lr: 0.00478 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9908 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9909 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9910 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9911 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9912 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9913 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9914 - lr: 0.00477 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9915 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9916 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9917 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9918 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9919 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9920 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9921 - lr: 0.00476 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9922 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9923 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9924 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9925 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9926 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9927 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9928 - lr: 0.00475 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9929 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9930 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9931 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9932 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9933 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9934 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9935 - lr: 0.00474 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9936 - lr: 0.00473 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9937 - lr: 0.00473 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9938 - lr: 0.00473 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9939 - lr: 0.00473 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9940 - lr: 0.00473 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9941 - lr: 0.00473 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9942 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9943 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9944 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9945 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9946 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9947 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9948 - lr: 0.00472 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9949 - lr: 0.00471 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9950 - lr: 0.00471 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9951 - lr: 0.00471 - Train loss: 0.00820 - Test loss: 0.04003\n",
      "Epoch 9952 - lr: 0.00471 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9953 - lr: 0.00471 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9954 - lr: 0.00471 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9955 - lr: 0.00471 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9956 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9957 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9958 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9959 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9960 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9961 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9962 - lr: 0.00470 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9963 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9964 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9965 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9966 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9967 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9968 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9969 - lr: 0.00469 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9970 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9971 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9972 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9973 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9974 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9975 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9976 - lr: 0.00468 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9977 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9978 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9979 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9980 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9981 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9982 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9983 - lr: 0.00467 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9984 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9985 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9986 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9987 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9988 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9989 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9990 - lr: 0.00466 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9991 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9992 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9993 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9994 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9995 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9996 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9997 - lr: 0.00465 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9998 - lr: 0.00464 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 9999 - lr: 0.00464 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 10000 - lr: 0.00464 - Train loss: 0.00819 - Test loss: 0.04003\n",
      "Epoch 10001 - lr: 0.00464 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10002 - lr: 0.00464 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10003 - lr: 0.00464 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10004 - lr: 0.00464 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10005 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10006 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10007 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10008 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10009 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10010 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10011 - lr: 0.00463 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10012 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10013 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10014 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10015 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10016 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10017 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10018 - lr: 0.00462 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10019 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10020 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10021 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10022 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10023 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10024 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10025 - lr: 0.00461 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10026 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10027 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10028 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10029 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10030 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10031 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10032 - lr: 0.00460 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10033 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10034 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10035 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10036 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10037 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10038 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10039 - lr: 0.00459 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10040 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10041 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10042 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10043 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10044 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10045 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10046 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10047 - lr: 0.00458 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10048 - lr: 0.00457 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10049 - lr: 0.00457 - Train loss: 0.00818 - Test loss: 0.04003\n",
      "Epoch 10050 - lr: 0.00457 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10051 - lr: 0.00457 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10052 - lr: 0.00457 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10053 - lr: 0.00457 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10054 - lr: 0.00457 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10055 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10056 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10057 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10058 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10059 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10060 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10061 - lr: 0.00456 - Train loss: 0.00817 - Test loss: 0.04003\n",
      "Epoch 10062 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10063 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10064 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10065 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10066 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10067 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10068 - lr: 0.00455 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10069 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10070 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10071 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10072 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10073 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10074 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10075 - lr: 0.00454 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10076 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10077 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10078 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10079 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10080 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10081 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10082 - lr: 0.00453 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10083 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10084 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10085 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10086 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10087 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10088 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10089 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10090 - lr: 0.00452 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10091 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10092 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10093 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10094 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10095 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10096 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10097 - lr: 0.00451 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10098 - lr: 0.00450 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10099 - lr: 0.00450 - Train loss: 0.00817 - Test loss: 0.04002\n",
      "Epoch 10100 - lr: 0.00450 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10101 - lr: 0.00450 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10102 - lr: 0.00450 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10103 - lr: 0.00450 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10104 - lr: 0.00450 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10105 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10106 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10107 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10108 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10109 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10110 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10111 - lr: 0.00449 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10112 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10113 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10114 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10115 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10116 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10117 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10118 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10119 - lr: 0.00448 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10120 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10121 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10122 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10123 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10124 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10125 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10126 - lr: 0.00447 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10127 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10128 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10129 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10130 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10131 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10132 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10133 - lr: 0.00446 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10134 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10135 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10136 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10137 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10138 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10139 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10140 - lr: 0.00445 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10141 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10142 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10143 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10144 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10145 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10146 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10147 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10148 - lr: 0.00444 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10149 - lr: 0.00443 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10150 - lr: 0.00443 - Train loss: 0.00816 - Test loss: 0.04002\n",
      "Epoch 10151 - lr: 0.00443 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10152 - lr: 0.00443 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10153 - lr: 0.00443 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10154 - lr: 0.00443 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10155 - lr: 0.00443 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10156 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10157 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10158 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10159 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10160 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10161 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10162 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10163 - lr: 0.00442 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10164 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10165 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10166 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10167 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10168 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10169 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10170 - lr: 0.00441 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10171 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10172 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10173 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10174 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10175 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10176 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10177 - lr: 0.00440 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10178 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10179 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10180 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10181 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10182 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10183 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10184 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10185 - lr: 0.00439 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10186 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10187 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10188 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10189 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10190 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10191 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10192 - lr: 0.00438 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10193 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10194 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10195 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10196 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10197 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10198 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10199 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10200 - lr: 0.00437 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10201 - lr: 0.00436 - Train loss: 0.00815 - Test loss: 0.04002\n",
      "Epoch 10202 - lr: 0.00436 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10203 - lr: 0.00436 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10204 - lr: 0.00436 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10205 - lr: 0.00436 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10206 - lr: 0.00436 - Train loss: 0.00814 - Test loss: 0.04002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10207 - lr: 0.00436 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10208 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10209 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10210 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10211 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10212 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10213 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10214 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10215 - lr: 0.00435 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10216 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10217 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10218 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10219 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10220 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10221 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10222 - lr: 0.00434 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10223 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10224 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10225 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10226 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10227 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10228 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10229 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10230 - lr: 0.00433 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10231 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10232 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10233 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10234 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10235 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10236 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10237 - lr: 0.00432 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10238 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10239 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10240 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10241 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10242 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10243 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10244 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10245 - lr: 0.00431 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10246 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10247 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10248 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10249 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10250 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10251 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10252 - lr: 0.00430 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10253 - lr: 0.00429 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10254 - lr: 0.00429 - Train loss: 0.00814 - Test loss: 0.04002\n",
      "Epoch 10255 - lr: 0.00429 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10256 - lr: 0.00429 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10257 - lr: 0.00429 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10258 - lr: 0.00429 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10259 - lr: 0.00429 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10260 - lr: 0.00429 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10261 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10262 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10263 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10264 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10265 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10266 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10267 - lr: 0.00428 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10268 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10269 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10270 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10271 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10272 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10273 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10274 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10275 - lr: 0.00427 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10276 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10277 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10278 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10279 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10280 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10281 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10282 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10283 - lr: 0.00426 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10284 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10285 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10286 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10287 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10288 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10289 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10290 - lr: 0.00425 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10291 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10292 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10293 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10294 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10295 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10296 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10297 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10298 - lr: 0.00424 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10299 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10300 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10301 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10302 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10303 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10304 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10305 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10306 - lr: 0.00423 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10307 - lr: 0.00422 - Train loss: 0.00813 - Test loss: 0.04002\n",
      "Epoch 10308 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10309 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10310 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10311 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10312 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10313 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10314 - lr: 0.00422 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10315 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10316 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10317 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10318 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10319 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10320 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10321 - lr: 0.00421 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10322 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10323 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10324 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10325 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10326 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10327 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10328 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10329 - lr: 0.00420 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10330 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10331 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10332 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10333 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10334 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10335 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10336 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10337 - lr: 0.00419 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10338 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10339 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10340 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10341 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10342 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10343 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10344 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10345 - lr: 0.00418 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10346 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10347 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10348 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10349 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10350 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10351 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10352 - lr: 0.00417 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10353 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10354 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10355 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10356 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10357 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10358 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10359 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10360 - lr: 0.00416 - Train loss: 0.00812 - Test loss: 0.04002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10361 - lr: 0.00415 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10362 - lr: 0.00415 - Train loss: 0.00812 - Test loss: 0.04002\n",
      "Epoch 10363 - lr: 0.00415 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10364 - lr: 0.00415 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10365 - lr: 0.00415 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10366 - lr: 0.00415 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10367 - lr: 0.00415 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10368 - lr: 0.00415 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10369 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10370 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10371 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10372 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10373 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10374 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10375 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10376 - lr: 0.00414 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10377 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10378 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10379 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10380 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10381 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10382 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10383 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10384 - lr: 0.00413 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10385 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10386 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10387 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10388 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10389 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10390 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10391 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10392 - lr: 0.00412 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10393 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10394 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10395 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10396 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10397 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10398 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10399 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10400 - lr: 0.00411 - Train loss: 0.00811 - Test loss: 0.04002\n",
      "Epoch 10401 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10402 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10403 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10404 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10405 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10406 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10407 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10408 - lr: 0.00410 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10409 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10410 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10411 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10412 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10413 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10414 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10415 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10416 - lr: 0.00409 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10417 - lr: 0.00408 - Train loss: 0.00811 - Test loss: 0.04001\n",
      "Epoch 10418 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10419 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10420 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10421 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10422 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10423 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10424 - lr: 0.00408 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10425 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10426 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10427 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10428 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10429 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10430 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10431 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10432 - lr: 0.00407 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10433 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10434 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10435 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10436 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10437 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10438 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10439 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10440 - lr: 0.00406 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10441 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10442 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10443 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10444 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10445 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10446 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10447 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10448 - lr: 0.00405 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10449 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10450 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10451 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10452 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10453 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10454 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10455 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10456 - lr: 0.00404 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10457 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10458 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10459 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10460 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10461 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10462 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10463 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10464 - lr: 0.00403 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10465 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10466 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10467 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10468 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10469 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10470 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10471 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10472 - lr: 0.00402 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10473 - lr: 0.00401 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10474 - lr: 0.00401 - Train loss: 0.00810 - Test loss: 0.04001\n",
      "Epoch 10475 - lr: 0.00401 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10476 - lr: 0.00401 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10477 - lr: 0.00401 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10478 - lr: 0.00401 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10479 - lr: 0.00401 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10480 - lr: 0.00401 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10481 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10482 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10483 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10484 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10485 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10486 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10487 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10488 - lr: 0.00400 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10489 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10490 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10491 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10492 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10493 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10494 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10495 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10496 - lr: 0.00399 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10497 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10498 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10499 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10500 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10501 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10502 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10503 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10504 - lr: 0.00398 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10505 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10506 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10507 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10508 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10509 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10510 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10511 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10512 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10513 - lr: 0.00397 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10514 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10515 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10516 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10517 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10518 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10519 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10520 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10521 - lr: 0.00396 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10522 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10523 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10524 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10525 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10526 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10527 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10528 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10529 - lr: 0.00395 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10530 - lr: 0.00394 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10531 - lr: 0.00394 - Train loss: 0.00809 - Test loss: 0.04001\n",
      "Epoch 10532 - lr: 0.00394 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10533 - lr: 0.00394 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10534 - lr: 0.00394 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10535 - lr: 0.00394 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10536 - lr: 0.00394 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10537 - lr: 0.00394 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10538 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10539 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10540 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10541 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10542 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10543 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10544 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10545 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10546 - lr: 0.00393 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10547 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10548 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10549 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10550 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10551 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10552 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10553 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10554 - lr: 0.00392 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10555 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10556 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10557 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10558 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10559 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10560 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10561 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10562 - lr: 0.00391 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10563 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10564 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10565 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10566 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10567 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10568 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10569 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10570 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10571 - lr: 0.00390 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10572 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10573 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10574 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10575 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10576 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10577 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10578 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10579 - lr: 0.00389 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10580 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10581 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10582 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10583 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10584 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10585 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10586 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10587 - lr: 0.00388 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10588 - lr: 0.00387 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10589 - lr: 0.00387 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10590 - lr: 0.00387 - Train loss: 0.00808 - Test loss: 0.04001\n",
      "Epoch 10591 - lr: 0.00387 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10592 - lr: 0.00387 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10593 - lr: 0.00387 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10594 - lr: 0.00387 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10595 - lr: 0.00387 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10596 - lr: 0.00387 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10597 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10598 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10599 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10600 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10601 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10602 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10603 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10604 - lr: 0.00386 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10605 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10606 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10607 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10608 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10609 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10610 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10611 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10612 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10613 - lr: 0.00385 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10614 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10615 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10616 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10617 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10618 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10619 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10620 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10621 - lr: 0.00384 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10622 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10623 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10624 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10625 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10626 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10627 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10628 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10629 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10630 - lr: 0.00383 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10631 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10632 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10633 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10634 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10635 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10636 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10637 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10638 - lr: 0.00382 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10639 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10640 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10641 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10642 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10643 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10644 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10645 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10646 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10647 - lr: 0.00381 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10648 - lr: 0.00380 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10649 - lr: 0.00380 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10650 - lr: 0.00380 - Train loss: 0.00807 - Test loss: 0.04001\n",
      "Epoch 10651 - lr: 0.00380 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10652 - lr: 0.00380 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10653 - lr: 0.00380 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10654 - lr: 0.00380 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10655 - lr: 0.00380 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10656 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10657 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10658 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10659 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10660 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10661 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10662 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10663 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10664 - lr: 0.00379 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10665 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10666 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10667 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10668 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10669 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10670 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10671 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10672 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10673 - lr: 0.00378 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10674 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10675 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10676 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10677 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10678 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10679 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10680 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10681 - lr: 0.00377 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10682 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10683 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10684 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10685 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10686 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10687 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10688 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10689 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10690 - lr: 0.00376 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10691 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10692 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10693 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10694 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10695 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10696 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10697 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10698 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10699 - lr: 0.00375 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10700 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10701 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10702 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10703 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10704 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10705 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10706 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10707 - lr: 0.00374 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10708 - lr: 0.00373 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10709 - lr: 0.00373 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10710 - lr: 0.00373 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10711 - lr: 0.00373 - Train loss: 0.00806 - Test loss: 0.04001\n",
      "Epoch 10712 - lr: 0.00373 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10713 - lr: 0.00373 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10714 - lr: 0.00373 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10715 - lr: 0.00373 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10716 - lr: 0.00373 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10717 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10718 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10719 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10720 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10721 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10722 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10723 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10724 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10725 - lr: 0.00372 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10726 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10727 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10728 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10729 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10730 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10731 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10732 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10733 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10734 - lr: 0.00371 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10735 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10736 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10737 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10738 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10739 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10740 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10741 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10742 - lr: 0.00370 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10743 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10744 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10745 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10746 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10747 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10748 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10749 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10750 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10751 - lr: 0.00369 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10752 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10753 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10754 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10755 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10756 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10757 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10758 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10759 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10760 - lr: 0.00368 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10761 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10762 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10763 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10764 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10765 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10766 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10767 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10768 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10769 - lr: 0.00367 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10770 - lr: 0.00366 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10771 - lr: 0.00366 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10772 - lr: 0.00366 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10773 - lr: 0.00366 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10774 - lr: 0.00366 - Train loss: 0.00805 - Test loss: 0.04001\n",
      "Epoch 10775 - lr: 0.00366 - Train loss: 0.00804 - Test loss: 0.04001\n",
      "Epoch 10776 - lr: 0.00366 - Train loss: 0.00804 - Test loss: 0.04001\n",
      "Epoch 10777 - lr: 0.00366 - Train loss: 0.00804 - Test loss: 0.04001\n",
      "Epoch 10778 - lr: 0.00366 - Train loss: 0.00804 - Test loss: 0.04001\n",
      "Epoch 10779 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04001\n",
      "Epoch 10780 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04001\n",
      "Epoch 10781 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10782 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10783 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10784 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10785 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10786 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10787 - lr: 0.00365 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10788 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10789 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10790 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10791 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10792 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10793 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10794 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10795 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10796 - lr: 0.00364 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10797 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10798 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10799 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10800 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10801 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10802 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10803 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10804 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10805 - lr: 0.00363 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10806 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10807 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10808 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10809 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10810 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10811 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10812 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10813 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10814 - lr: 0.00362 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10815 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10816 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10817 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10818 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10819 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10820 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10821 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10822 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10823 - lr: 0.00361 - Train loss: 0.00804 - Test loss: 0.04000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10824 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10825 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10826 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10827 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10828 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10829 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10830 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10831 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10832 - lr: 0.00360 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10833 - lr: 0.00359 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10834 - lr: 0.00359 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10835 - lr: 0.00359 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10836 - lr: 0.00359 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10837 - lr: 0.00359 - Train loss: 0.00804 - Test loss: 0.04000\n",
      "Epoch 10838 - lr: 0.00359 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10839 - lr: 0.00359 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10840 - lr: 0.00359 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10841 - lr: 0.00359 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10842 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10843 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10844 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10845 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10846 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10847 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10848 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10849 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10850 - lr: 0.00358 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10851 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10852 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10853 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10854 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10855 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10856 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10857 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10858 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10859 - lr: 0.00357 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10860 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10861 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10862 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10863 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10864 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10865 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10866 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10867 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10868 - lr: 0.00356 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10869 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10870 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10871 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10872 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10873 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10874 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10875 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10876 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10877 - lr: 0.00355 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10878 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10879 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10880 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10881 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10882 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10883 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10884 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10885 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10886 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10887 - lr: 0.00354 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10888 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10889 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10890 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10891 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10892 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10893 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10894 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10895 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10896 - lr: 0.00353 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10897 - lr: 0.00352 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10898 - lr: 0.00352 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10899 - lr: 0.00352 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10900 - lr: 0.00352 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10901 - lr: 0.00352 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10902 - lr: 0.00352 - Train loss: 0.00803 - Test loss: 0.04000\n",
      "Epoch 10903 - lr: 0.00352 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10904 - lr: 0.00352 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10905 - lr: 0.00352 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10906 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10907 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10908 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10909 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10910 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10911 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10912 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10913 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10914 - lr: 0.00351 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10915 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10916 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10917 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10918 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10919 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10920 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10921 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10922 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10923 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10924 - lr: 0.00350 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10925 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10926 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10927 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10928 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10929 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10930 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10931 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10932 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10933 - lr: 0.00349 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10934 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10935 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10936 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10937 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10938 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10939 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10940 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10941 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10942 - lr: 0.00348 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10943 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10944 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10945 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10946 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10947 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10948 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10949 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10950 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10951 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10952 - lr: 0.00347 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10953 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10954 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10955 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10956 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10957 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10958 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10959 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10960 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10961 - lr: 0.00346 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10962 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10963 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10964 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10965 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10966 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10967 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10968 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10969 - lr: 0.00345 - Train loss: 0.00802 - Test loss: 0.04000\n",
      "Epoch 10970 - lr: 0.00345 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10971 - lr: 0.00345 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10972 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10973 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10974 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10975 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10976 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10977 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10978 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10979 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10980 - lr: 0.00344 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10981 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10982 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10983 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10984 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10985 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10986 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10987 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10988 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10989 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10990 - lr: 0.00343 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10991 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10992 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10993 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10994 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10995 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10996 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10997 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10998 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 10999 - lr: 0.00342 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11000 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11001 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11002 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11003 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11004 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11005 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11006 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11007 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11008 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11009 - lr: 0.00341 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11010 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11011 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11012 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11013 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11014 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11015 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11016 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11017 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11018 - lr: 0.00340 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11019 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11020 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11021 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11022 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11023 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11024 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11025 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11026 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11027 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11028 - lr: 0.00339 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11029 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11030 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11031 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11032 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11033 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11034 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11035 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11036 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11037 - lr: 0.00338 - Train loss: 0.00801 - Test loss: 0.04000\n",
      "Epoch 11038 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11039 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11040 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11041 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11042 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11043 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11044 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11045 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11046 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11047 - lr: 0.00337 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11048 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11049 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11050 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11051 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11052 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11053 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11054 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11055 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11056 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11057 - lr: 0.00336 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11058 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11059 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11060 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11061 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11062 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11063 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11064 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11065 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11066 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11067 - lr: 0.00335 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11068 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11069 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11070 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11071 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11072 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11073 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11074 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11075 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11076 - lr: 0.00334 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11077 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11078 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11079 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11080 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11081 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11082 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11083 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11084 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11085 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11086 - lr: 0.00333 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11087 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11088 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11089 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11090 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11091 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11092 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11093 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11094 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11095 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11096 - lr: 0.00332 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11097 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11098 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11099 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11100 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11101 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11102 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11103 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11104 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11105 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11106 - lr: 0.00331 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11107 - lr: 0.00330 - Train loss: 0.00800 - Test loss: 0.04000\n",
      "Epoch 11108 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11109 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11110 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11111 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11112 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11113 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11114 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11115 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11116 - lr: 0.00330 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11117 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11118 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11119 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11120 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11121 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11122 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11123 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11124 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11125 - lr: 0.00329 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11126 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11127 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11128 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11129 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11130 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11131 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11132 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11133 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11134 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11135 - lr: 0.00328 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11136 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11137 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11138 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11139 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11140 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11141 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11142 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11143 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11144 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11145 - lr: 0.00327 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11146 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11147 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11148 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11149 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11150 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11151 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11152 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11153 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11154 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11155 - lr: 0.00326 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11156 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11157 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11158 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11159 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11160 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11161 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11162 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11163 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11164 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11165 - lr: 0.00325 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11166 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11167 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11168 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11169 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11170 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11171 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11172 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11173 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11174 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11175 - lr: 0.00324 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11176 - lr: 0.00323 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11177 - lr: 0.00323 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11178 - lr: 0.00323 - Train loss: 0.00799 - Test loss: 0.04000\n",
      "Epoch 11179 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11180 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11181 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11182 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11183 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11184 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11185 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11186 - lr: 0.00323 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11187 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11188 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11189 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11190 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11191 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11192 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11193 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11194 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11195 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11196 - lr: 0.00322 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11197 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11198 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11199 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11200 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11201 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11202 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11203 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11204 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11205 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11206 - lr: 0.00321 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11207 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11208 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11209 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11210 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11211 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11212 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11213 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.04000\n",
      "Epoch 11214 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11215 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11216 - lr: 0.00320 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11217 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11218 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11219 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11220 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11221 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11222 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11223 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11224 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11225 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11226 - lr: 0.00319 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11227 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11228 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11229 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11230 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11231 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11232 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11233 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11234 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11235 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11236 - lr: 0.00318 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11237 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11238 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11239 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11240 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11241 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11242 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11243 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11244 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11245 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11246 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11247 - lr: 0.00317 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11248 - lr: 0.00316 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11249 - lr: 0.00316 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11250 - lr: 0.00316 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11251 - lr: 0.00316 - Train loss: 0.00798 - Test loss: 0.03999\n",
      "Epoch 11252 - lr: 0.00316 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11253 - lr: 0.00316 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11254 - lr: 0.00316 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11255 - lr: 0.00316 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11256 - lr: 0.00316 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11257 - lr: 0.00316 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11258 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11259 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11260 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11261 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11262 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11263 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11264 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11265 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11266 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11267 - lr: 0.00315 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11268 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11269 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11270 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11271 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11272 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11273 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11274 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11275 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11276 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11277 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11278 - lr: 0.00314 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11279 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11280 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11281 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11282 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11283 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11284 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11285 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11286 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11287 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11288 - lr: 0.00313 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11289 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11290 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11291 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11292 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11293 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11294 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11295 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11296 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11297 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11298 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11299 - lr: 0.00312 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11300 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11301 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11302 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11303 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11304 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11305 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11306 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11307 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11308 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11309 - lr: 0.00311 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11310 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11311 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11312 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11313 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11314 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11315 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11316 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11317 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11318 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11319 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11320 - lr: 0.00310 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11321 - lr: 0.00309 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11322 - lr: 0.00309 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11323 - lr: 0.00309 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11324 - lr: 0.00309 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11325 - lr: 0.00309 - Train loss: 0.00797 - Test loss: 0.03999\n",
      "Epoch 11326 - lr: 0.00309 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11327 - lr: 0.00309 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11328 - lr: 0.00309 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11329 - lr: 0.00309 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11330 - lr: 0.00309 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11331 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11332 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11333 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11334 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11335 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11336 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11337 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11338 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11339 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11340 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11341 - lr: 0.00308 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11342 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11343 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11344 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11345 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11346 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11347 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11348 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11349 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11350 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11351 - lr: 0.00307 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11352 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11353 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11354 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11355 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11356 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11357 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11358 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11359 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11360 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11361 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11362 - lr: 0.00306 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11363 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11364 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11365 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11366 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11367 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11368 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11369 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11370 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11371 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11372 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11373 - lr: 0.00305 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11374 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11375 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11376 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11377 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11378 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11379 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11380 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11381 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11382 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11383 - lr: 0.00304 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11384 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11385 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11386 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11387 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11388 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11389 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11390 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11391 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11392 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11393 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11394 - lr: 0.00303 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11395 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11396 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11397 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11398 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11399 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11400 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11401 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11402 - lr: 0.00302 - Train loss: 0.00796 - Test loss: 0.03999\n",
      "Epoch 11403 - lr: 0.00302 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11404 - lr: 0.00302 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11405 - lr: 0.00302 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11406 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11407 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11408 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11409 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11410 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11411 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11412 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11413 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11414 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11415 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11416 - lr: 0.00301 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11417 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11418 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11419 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11420 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11421 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11422 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11423 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11424 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11425 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11426 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11427 - lr: 0.00300 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11428 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11429 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11430 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11431 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11432 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11433 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11434 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11435 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11436 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11437 - lr: 0.00299 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11438 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11439 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11440 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11441 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11442 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11443 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11444 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11445 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11446 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11447 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11448 - lr: 0.00298 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11449 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11450 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11451 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11452 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11453 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11454 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11455 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11456 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11457 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11458 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11459 - lr: 0.00297 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11460 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11461 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11462 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11463 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11464 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11465 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11466 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11467 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11468 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11469 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11470 - lr: 0.00296 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11471 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11472 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11473 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11474 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11475 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11476 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11477 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11478 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11479 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11480 - lr: 0.00295 - Train loss: 0.00795 - Test loss: 0.03999\n",
      "Epoch 11481 - lr: 0.00295 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11482 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11483 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11484 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11485 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11486 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11487 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11488 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11489 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11490 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11491 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11492 - lr: 0.00294 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11493 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11494 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11495 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11496 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11497 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11498 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11499 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11500 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11501 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11502 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11503 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11504 - lr: 0.00293 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11505 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11506 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11507 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11508 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11509 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11510 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11511 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11512 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11513 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11514 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11515 - lr: 0.00292 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11516 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11517 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11518 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11519 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11520 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11521 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11522 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11523 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11524 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11525 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11526 - lr: 0.00291 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11527 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11528 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11529 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11530 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11531 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11532 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11533 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11534 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11535 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11536 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11537 - lr: 0.00290 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11538 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11539 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11540 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11541 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11542 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11543 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11544 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11545 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11546 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11547 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11548 - lr: 0.00289 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11549 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11550 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11551 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11552 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11553 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11554 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11555 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11556 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11557 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11558 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11559 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11560 - lr: 0.00288 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11561 - lr: 0.00287 - Train loss: 0.00794 - Test loss: 0.03999\n",
      "Epoch 11562 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11563 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11564 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11565 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11566 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11567 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11568 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11569 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11570 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11571 - lr: 0.00287 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11572 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11573 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11574 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11575 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11576 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11577 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11578 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11579 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11580 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11581 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11582 - lr: 0.00286 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11583 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11584 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11585 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11586 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11587 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11588 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11589 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11590 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11591 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11592 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11593 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11594 - lr: 0.00285 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11595 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11596 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11597 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11598 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11599 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11600 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11601 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11602 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11603 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11604 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11605 - lr: 0.00284 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11606 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11607 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11608 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11609 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11610 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11611 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11612 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11613 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11614 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11615 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11616 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11617 - lr: 0.00283 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11618 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11619 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11620 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11621 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11622 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11623 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11624 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11625 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11626 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11627 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11628 - lr: 0.00282 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11629 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11630 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11631 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11632 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11633 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11634 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11635 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11636 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11637 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11638 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11639 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11640 - lr: 0.00281 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11641 - lr: 0.00280 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11642 - lr: 0.00280 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11643 - lr: 0.00280 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11644 - lr: 0.00280 - Train loss: 0.00793 - Test loss: 0.03999\n",
      "Epoch 11645 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11646 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11647 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11648 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11649 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11650 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11651 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11652 - lr: 0.00280 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11653 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11654 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11655 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11656 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11657 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11658 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11659 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11660 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11661 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11662 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11663 - lr: 0.00279 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11664 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11665 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11666 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11667 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11668 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11669 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11670 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11671 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11672 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11673 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11674 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11675 - lr: 0.00278 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11676 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11677 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11678 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11679 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11680 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11681 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11682 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11683 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11684 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11685 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11686 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11687 - lr: 0.00277 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11688 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11689 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11690 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11691 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11692 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11693 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11694 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11695 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11696 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11697 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11698 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11699 - lr: 0.00276 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11700 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11701 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11702 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11703 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11704 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11705 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11706 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11707 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11708 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11709 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11710 - lr: 0.00275 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11711 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11712 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11713 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11714 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11715 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11716 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11717 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11718 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03999\n",
      "Epoch 11719 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11720 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11721 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11722 - lr: 0.00274 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11723 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11724 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11725 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11726 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11727 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11728 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11729 - lr: 0.00273 - Train loss: 0.00792 - Test loss: 0.03998\n",
      "Epoch 11730 - lr: 0.00273 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11731 - lr: 0.00273 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11732 - lr: 0.00273 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11733 - lr: 0.00273 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11734 - lr: 0.00273 - Train loss: 0.00791 - Test loss: 0.03998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11735 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11736 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11737 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11738 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11739 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11740 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11741 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11742 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11743 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11744 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11745 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11746 - lr: 0.00272 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11747 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11748 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11749 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11750 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11751 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11752 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11753 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11754 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11755 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11756 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11757 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11758 - lr: 0.00271 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11759 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11760 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11761 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11762 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11763 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11764 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11765 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11766 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11767 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11768 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11769 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11770 - lr: 0.00270 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11771 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11772 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11773 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11774 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11775 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11776 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11777 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11778 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11779 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11780 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11781 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11782 - lr: 0.00269 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11783 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11784 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11785 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11786 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11787 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11788 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11789 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11790 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11791 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11792 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11793 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11794 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11795 - lr: 0.00268 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11796 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11797 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11798 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11799 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11800 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11801 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11802 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11803 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11804 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11805 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11806 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11807 - lr: 0.00267 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11808 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11809 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11810 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11811 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11812 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11813 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11814 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11815 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11816 - lr: 0.00266 - Train loss: 0.00791 - Test loss: 0.03998\n",
      "Epoch 11817 - lr: 0.00266 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11818 - lr: 0.00266 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11819 - lr: 0.00266 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11820 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11821 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11822 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11823 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11824 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11825 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11826 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11827 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11828 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11829 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11830 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11831 - lr: 0.00265 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11832 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11833 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11834 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11835 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11836 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11837 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11838 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11839 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11840 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11841 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11842 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11843 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11844 - lr: 0.00264 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11845 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11846 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11847 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11848 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11849 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11850 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11851 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11852 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11853 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11854 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11855 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11856 - lr: 0.00263 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11857 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11858 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11859 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11860 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11861 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11862 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11863 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11864 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11865 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11866 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11867 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11868 - lr: 0.00262 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11869 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11870 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11871 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11872 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11873 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11874 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11875 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11876 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11877 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11878 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11879 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11880 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11881 - lr: 0.00261 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11882 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11883 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11884 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11885 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11886 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11887 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11888 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11889 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11890 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11891 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11892 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11893 - lr: 0.00260 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11894 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11895 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11896 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11897 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11898 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11899 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11900 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11901 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11902 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11903 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11904 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11905 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11906 - lr: 0.00259 - Train loss: 0.00790 - Test loss: 0.03998\n",
      "Epoch 11907 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11908 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11909 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11910 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11911 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11912 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11913 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11914 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11915 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11916 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11917 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11918 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11919 - lr: 0.00258 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11920 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11921 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11922 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11923 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11924 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11925 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11926 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11927 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11928 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11929 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11930 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11931 - lr: 0.00257 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11932 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11933 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11934 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11935 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11936 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11937 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11938 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11939 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11940 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11941 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11942 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11943 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11944 - lr: 0.00256 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11945 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11946 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11947 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11948 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11949 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11950 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11951 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11952 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11953 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11954 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11955 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11956 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11957 - lr: 0.00255 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11958 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11959 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11960 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11961 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11962 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11963 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11964 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11965 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11966 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11967 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11968 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11969 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11970 - lr: 0.00254 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11971 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11972 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11973 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11974 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11975 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11976 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11977 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11978 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11979 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11980 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11981 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11982 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11983 - lr: 0.00253 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11984 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11985 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11986 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11987 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11988 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11989 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11990 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11991 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11992 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11993 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11994 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11995 - lr: 0.00252 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11996 - lr: 0.00251 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11997 - lr: 0.00251 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11998 - lr: 0.00251 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 11999 - lr: 0.00251 - Train loss: 0.00789 - Test loss: 0.03998\n",
      "Epoch 12000 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12001 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12002 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12003 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12004 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12005 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12006 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12007 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12008 - lr: 0.00251 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12009 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12010 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12011 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12012 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12013 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12014 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12015 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12016 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12017 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12018 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12019 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12020 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12021 - lr: 0.00250 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12022 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12023 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12024 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12025 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12026 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12027 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12028 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12029 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12030 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12031 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12032 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12033 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12034 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12035 - lr: 0.00249 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12036 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12037 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12038 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12039 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12040 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12041 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12042 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12043 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12044 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12045 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12046 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12047 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12048 - lr: 0.00248 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12049 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12050 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12051 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12052 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12053 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12054 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12055 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12056 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12057 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12058 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12059 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12060 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12061 - lr: 0.00247 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12062 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12063 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12064 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12065 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12066 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12067 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12068 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12069 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12070 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12071 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12072 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12073 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12074 - lr: 0.00246 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12075 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12076 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12077 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12078 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12079 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12080 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12081 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12082 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12083 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12084 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12085 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12086 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12087 - lr: 0.00245 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12088 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12089 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12090 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12091 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12092 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12093 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12094 - lr: 0.00244 - Train loss: 0.00788 - Test loss: 0.03998\n",
      "Epoch 12095 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12096 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12097 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12098 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12099 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12100 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12101 - lr: 0.00244 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12102 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12103 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12104 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12105 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12106 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12107 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12108 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12109 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12110 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12111 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12112 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12113 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12114 - lr: 0.00243 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12115 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12116 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12117 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12118 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12119 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12120 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12121 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12122 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12123 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12124 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12125 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12126 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12127 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12128 - lr: 0.00242 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12129 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12130 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12131 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12132 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12133 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12134 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12135 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12136 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12137 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12138 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12139 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12140 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12141 - lr: 0.00241 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12142 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12143 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12144 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12145 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12146 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12147 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12148 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12149 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12150 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12151 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12152 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12153 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12154 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12155 - lr: 0.00240 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12156 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12157 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12158 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12159 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12160 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12161 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12162 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12163 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12164 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12165 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12166 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12167 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12168 - lr: 0.00239 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12169 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12170 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12171 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12172 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12173 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12174 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12175 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12176 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12177 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12178 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12179 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12180 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12181 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12182 - lr: 0.00238 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12183 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12184 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12185 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12186 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12187 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12188 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12189 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12190 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12191 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12192 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n",
      "Epoch 12193 - lr: 0.00237 - Train loss: 0.00787 - Test loss: 0.03998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12194 - lr: 0.00237 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12195 - lr: 0.00237 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12196 - lr: 0.00237 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12197 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12198 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12199 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12200 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12201 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12202 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12203 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12204 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12205 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12206 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12207 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12208 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12209 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12210 - lr: 0.00236 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12211 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12212 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12213 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12214 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12215 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12216 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12217 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12218 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12219 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12220 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12221 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12222 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12223 - lr: 0.00235 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12224 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12225 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12226 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12227 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12228 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12229 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12230 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12231 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12232 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12233 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12234 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12235 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12236 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12237 - lr: 0.00234 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12238 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12239 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12240 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12241 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12242 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12243 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12244 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12245 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12246 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12247 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12248 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12249 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12250 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12251 - lr: 0.00233 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12252 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12253 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12254 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12255 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12256 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12257 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12258 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12259 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12260 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12261 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12262 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12263 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12264 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12265 - lr: 0.00232 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12266 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12267 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12268 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12269 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12270 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12271 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12272 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12273 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12274 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12275 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12276 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12277 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12278 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12279 - lr: 0.00231 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12280 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12281 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12282 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12283 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12284 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12285 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12286 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12287 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12288 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12289 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12290 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12291 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12292 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12293 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12294 - lr: 0.00230 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12295 - lr: 0.00229 - Train loss: 0.00786 - Test loss: 0.03998\n",
      "Epoch 12296 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12297 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12298 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12299 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12300 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12301 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12302 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12303 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12304 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12305 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12306 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12307 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12308 - lr: 0.00229 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12309 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12310 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12311 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12312 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12313 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12314 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12315 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12316 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12317 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12318 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12319 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12320 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12321 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03998\n",
      "Epoch 12322 - lr: 0.00228 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12323 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12324 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12325 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12326 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12327 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12328 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12329 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12330 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12331 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12332 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12333 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12334 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12335 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12336 - lr: 0.00227 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12337 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12338 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12339 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12340 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12341 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12342 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12343 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12344 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12345 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12346 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12347 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12348 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12349 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12350 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12351 - lr: 0.00226 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12352 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12353 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12354 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12355 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12356 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12357 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12358 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12359 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12360 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12361 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12362 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12363 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12364 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12365 - lr: 0.00225 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12366 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12367 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12368 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12369 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12370 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12371 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12372 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12373 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12374 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12375 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12376 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12377 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12378 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12379 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12380 - lr: 0.00224 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12381 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12382 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12383 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12384 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12385 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12386 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12387 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12388 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12389 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12390 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12391 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12392 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12393 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12394 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12395 - lr: 0.00223 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12396 - lr: 0.00222 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12397 - lr: 0.00222 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12398 - lr: 0.00222 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12399 - lr: 0.00222 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12400 - lr: 0.00222 - Train loss: 0.00785 - Test loss: 0.03997\n",
      "Epoch 12401 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12402 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12403 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12404 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12405 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12406 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12407 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12408 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12409 - lr: 0.00222 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12410 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12411 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12412 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12413 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12414 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12415 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12416 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12417 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12418 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12419 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12420 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12421 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12422 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12423 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12424 - lr: 0.00221 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12425 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12426 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12427 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12428 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12429 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12430 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12431 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12432 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12433 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12434 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12435 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12436 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12437 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12438 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12439 - lr: 0.00220 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12440 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12441 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12442 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12443 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12444 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12445 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12446 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12447 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12448 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12449 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12450 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12451 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12452 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12453 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12454 - lr: 0.00219 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12455 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12456 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12457 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12458 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12459 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12460 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12461 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12462 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12463 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12464 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12465 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12466 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12467 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12468 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12469 - lr: 0.00218 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12470 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12471 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12472 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12473 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12474 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12475 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12476 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12477 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12478 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12479 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12480 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12481 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12482 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12483 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12484 - lr: 0.00217 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12485 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12486 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12487 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12488 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12489 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12490 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12491 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12492 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12493 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12494 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12495 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12496 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12497 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12498 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12499 - lr: 0.00216 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12500 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12501 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12502 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12503 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12504 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12505 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12506 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12507 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12508 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12509 - lr: 0.00215 - Train loss: 0.00784 - Test loss: 0.03997\n",
      "Epoch 12510 - lr: 0.00215 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12511 - lr: 0.00215 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12512 - lr: 0.00215 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12513 - lr: 0.00215 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12514 - lr: 0.00215 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12515 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12516 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12517 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12518 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12519 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12520 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12521 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12522 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12523 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12524 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12525 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12526 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12527 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12528 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12529 - lr: 0.00214 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12530 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12531 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12532 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12533 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12534 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12535 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12536 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12537 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12538 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12539 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12540 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12541 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12542 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12543 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12544 - lr: 0.00213 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12545 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12546 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12547 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12548 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12549 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12550 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12551 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12552 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12553 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12554 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12555 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12556 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12557 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12558 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12559 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12560 - lr: 0.00212 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12561 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12562 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12563 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12564 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12565 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12566 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12567 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12568 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12569 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12570 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12571 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12572 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12573 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12574 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12575 - lr: 0.00211 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12576 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12577 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12578 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12579 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12580 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12581 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12582 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12583 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12584 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12585 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12586 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12587 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12588 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12589 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12590 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12591 - lr: 0.00210 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12592 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12593 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12594 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12595 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12596 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12597 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12598 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12599 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12600 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12601 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12602 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12603 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12604 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12605 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12606 - lr: 0.00209 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12607 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12608 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12609 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12610 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12611 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12612 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12613 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12614 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12615 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12616 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12617 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12618 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12619 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12620 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12621 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12622 - lr: 0.00208 - Train loss: 0.00783 - Test loss: 0.03997\n",
      "Epoch 12623 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12624 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12625 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12626 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12627 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12628 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12629 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12630 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12631 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12632 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12633 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12634 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12635 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12636 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12637 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12638 - lr: 0.00207 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12639 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12640 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12641 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12642 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12643 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12644 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12645 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12646 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12647 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12648 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12649 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12650 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12651 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12652 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12653 - lr: 0.00206 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12654 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12655 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12656 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12657 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12658 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12659 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12660 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12661 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12662 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12663 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12664 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12665 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12666 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12667 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12668 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12669 - lr: 0.00205 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12670 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12671 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12672 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12673 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12674 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12675 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12676 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12677 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12678 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12679 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12680 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12681 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12682 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12683 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12684 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12685 - lr: 0.00204 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12686 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12687 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12688 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12689 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12690 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12691 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12692 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12693 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12694 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12695 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12696 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12697 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12698 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12699 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12700 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12701 - lr: 0.00203 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12702 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12703 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12704 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12705 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12706 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12707 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12708 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12709 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12710 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12711 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12712 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12713 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12714 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12715 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12716 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12717 - lr: 0.00202 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12718 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12719 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12720 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12721 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12722 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12723 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12724 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12725 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12726 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12727 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12728 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12729 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12730 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12731 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12732 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12733 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12734 - lr: 0.00201 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12735 - lr: 0.00200 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12736 - lr: 0.00200 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12737 - lr: 0.00200 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12738 - lr: 0.00200 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12739 - lr: 0.00200 - Train loss: 0.00782 - Test loss: 0.03997\n",
      "Epoch 12740 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12741 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12742 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12743 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12744 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12745 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12746 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12747 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12748 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12749 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12750 - lr: 0.00200 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12751 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12752 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12753 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12754 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12755 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12756 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12757 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12758 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12759 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12760 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12761 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12762 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12763 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12764 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12765 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12766 - lr: 0.00199 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12767 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12768 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12769 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12770 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12771 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12772 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12773 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12774 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12775 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12776 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12777 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12778 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12779 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12780 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12781 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12782 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12783 - lr: 0.00198 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12784 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12785 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12786 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12787 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12788 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12789 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12790 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12791 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12792 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12793 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12794 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12795 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12796 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12797 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12798 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12799 - lr: 0.00197 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12800 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12801 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12802 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12803 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12804 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12805 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12806 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12807 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12808 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12809 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12810 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12811 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12812 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12813 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12814 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12815 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12816 - lr: 0.00196 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12817 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12818 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12819 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12820 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12821 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12822 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12823 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12824 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12825 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12826 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12827 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12828 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12829 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12830 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12831 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12832 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12833 - lr: 0.00195 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12834 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12835 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12836 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12837 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12838 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12839 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12840 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12841 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12842 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12843 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12844 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12845 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12846 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12847 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12848 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12849 - lr: 0.00194 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12850 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12851 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12852 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12853 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12854 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12855 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12856 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12857 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12858 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12859 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12860 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12861 - lr: 0.00193 - Train loss: 0.00781 - Test loss: 0.03997\n",
      "Epoch 12862 - lr: 0.00193 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12863 - lr: 0.00193 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12864 - lr: 0.00193 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12865 - lr: 0.00193 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12866 - lr: 0.00193 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12867 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12868 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12869 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12870 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12871 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12872 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12873 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12874 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12875 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12876 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12877 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12878 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12879 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12880 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12881 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12882 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12883 - lr: 0.00192 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12884 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12885 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12886 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12887 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12888 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12889 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12890 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12891 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12892 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12893 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12894 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12895 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12896 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12897 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12898 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12899 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12900 - lr: 0.00191 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12901 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12902 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12903 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12904 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12905 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12906 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12907 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12908 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12909 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12910 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12911 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12912 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12913 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12914 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12915 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12916 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12917 - lr: 0.00190 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12918 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12919 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12920 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12921 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12922 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12923 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12924 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12925 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12926 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12927 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12928 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12929 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12930 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12931 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12932 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12933 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12934 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12935 - lr: 0.00189 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12936 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12937 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12938 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12939 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12940 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12941 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12942 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12943 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12944 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12945 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12946 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12947 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12948 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12949 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12950 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12951 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12952 - lr: 0.00188 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12953 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12954 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12955 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12956 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12957 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12958 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12959 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12960 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12961 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12962 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12963 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12964 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12965 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12966 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12967 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12968 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12969 - lr: 0.00187 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12970 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12971 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12972 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12973 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12974 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12975 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12976 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12977 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12978 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12979 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12980 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12981 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12982 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12983 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12984 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12985 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12986 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12987 - lr: 0.00186 - Train loss: 0.00780 - Test loss: 0.03997\n",
      "Epoch 12988 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12989 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12990 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12991 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12992 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12993 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12994 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12995 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12996 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12997 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12998 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 12999 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13000 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13001 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13002 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13003 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13004 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13005 - lr: 0.00185 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13006 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13007 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13008 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13009 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13010 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13011 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13012 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13013 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13014 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13015 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13016 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13017 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13018 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13019 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13020 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13021 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13022 - lr: 0.00184 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13023 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13024 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13025 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13026 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13027 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13028 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13029 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13030 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13031 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13032 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13033 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13034 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13035 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13036 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13037 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13038 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13039 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13040 - lr: 0.00183 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13041 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13042 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13043 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13044 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13045 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13046 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13047 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13048 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13049 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13050 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13051 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13052 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13053 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13054 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13055 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13056 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13057 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13058 - lr: 0.00182 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13059 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13060 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13061 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13062 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13063 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13064 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13065 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13066 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13067 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13068 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13069 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03997\n",
      "Epoch 13070 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13071 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13072 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13073 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13074 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13075 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13076 - lr: 0.00181 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13077 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13078 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13079 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13080 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13081 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13082 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13083 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13084 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13085 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13086 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13087 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13088 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13089 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13090 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13091 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13092 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13093 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13094 - lr: 0.00180 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13095 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13096 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13097 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13098 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13099 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13100 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13101 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13102 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13103 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13104 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13105 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13106 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13107 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13108 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13109 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13110 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13111 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13112 - lr: 0.00179 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13113 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13114 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13115 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13116 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13117 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13118 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13119 - lr: 0.00178 - Train loss: 0.00779 - Test loss: 0.03996\n",
      "Epoch 13120 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13121 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13122 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13123 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13124 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13125 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13126 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13127 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13128 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13129 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13130 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13131 - lr: 0.00178 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13132 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13133 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13134 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13135 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13136 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13137 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13138 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13139 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13140 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13141 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13142 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13143 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13144 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13145 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13146 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13147 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13148 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13149 - lr: 0.00177 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13150 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13151 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13152 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13153 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13154 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13155 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13156 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13157 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13158 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13159 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13160 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13161 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13162 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13163 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13164 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13165 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13166 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13167 - lr: 0.00176 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13168 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13169 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13170 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13171 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13172 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13173 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13174 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13175 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13176 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13177 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13178 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13179 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13180 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13181 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13182 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13183 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13184 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13185 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13186 - lr: 0.00175 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13187 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13188 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13189 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13190 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13191 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13192 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13193 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13194 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13195 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13196 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13197 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13198 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13199 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13200 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13201 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13202 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13203 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13204 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13205 - lr: 0.00174 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13206 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13207 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13208 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13209 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13210 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13211 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13212 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13213 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13214 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13215 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13216 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13217 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13218 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13219 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13220 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13221 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13222 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13223 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13224 - lr: 0.00173 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13225 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13226 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13227 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13228 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13229 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13230 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13231 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13232 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13233 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13234 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13235 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13236 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13237 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13238 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13239 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13240 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13241 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13242 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13243 - lr: 0.00172 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13244 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13245 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13246 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13247 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13248 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13249 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13250 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13251 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13252 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13253 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13254 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13255 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13256 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13257 - lr: 0.00171 - Train loss: 0.00778 - Test loss: 0.03996\n",
      "Epoch 13258 - lr: 0.00171 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13259 - lr: 0.00171 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13260 - lr: 0.00171 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13261 - lr: 0.00171 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13262 - lr: 0.00171 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13263 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13264 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13265 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13266 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13267 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13268 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13269 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13270 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13271 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13272 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13273 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13274 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13275 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13276 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13277 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13278 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13279 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13280 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13281 - lr: 0.00170 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13282 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13283 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13284 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13285 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13286 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13287 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13288 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13289 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13290 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13291 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13292 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13293 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13294 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13295 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13296 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13297 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13298 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13299 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13300 - lr: 0.00169 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13301 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13302 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13303 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13304 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13305 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13306 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13307 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13308 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13309 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13310 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13311 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13312 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13313 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13314 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13315 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13316 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13317 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13318 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13319 - lr: 0.00168 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13320 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13321 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13322 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13323 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13324 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13325 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13326 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13327 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13328 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13329 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13330 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13331 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13332 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13333 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13334 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13335 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13336 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13337 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13338 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13339 - lr: 0.00167 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13340 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13341 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13342 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13343 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13344 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13345 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13346 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13347 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13348 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13349 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13350 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13351 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13352 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13353 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13354 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13355 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13356 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13357 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13358 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13359 - lr: 0.00166 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13360 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13361 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13362 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13363 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13364 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13365 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13366 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13367 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13368 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13369 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13370 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13371 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13372 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13373 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13374 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13375 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13376 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13377 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13378 - lr: 0.00165 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13379 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13380 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13381 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13382 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13383 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13384 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13385 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13386 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13387 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13388 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13389 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13390 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13391 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13392 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13393 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13394 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13395 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13396 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13397 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13398 - lr: 0.00164 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13399 - lr: 0.00163 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13400 - lr: 0.00163 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13401 - lr: 0.00163 - Train loss: 0.00777 - Test loss: 0.03996\n",
      "Epoch 13402 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13403 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13404 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13405 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13406 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13407 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13408 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13409 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13410 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13411 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13412 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13413 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13414 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13415 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13416 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13417 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13418 - lr: 0.00163 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13419 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13420 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13421 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13422 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13423 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13424 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13425 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13426 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13427 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13428 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13429 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13430 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13431 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13432 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13433 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13434 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13435 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13436 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13437 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13438 - lr: 0.00162 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13439 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13440 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13441 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13442 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13443 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13444 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13445 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13446 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13447 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13448 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13449 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13450 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13451 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13452 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13453 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13454 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13455 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13456 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13457 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13458 - lr: 0.00161 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13459 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13460 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13461 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13462 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13463 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13464 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13465 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13466 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13467 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13468 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13469 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13470 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13471 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13472 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13473 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13474 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13475 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13476 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13477 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13478 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13479 - lr: 0.00160 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13480 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13481 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13482 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13483 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13484 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13485 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13486 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13487 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13488 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13489 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13490 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13491 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13492 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13493 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13494 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13495 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13496 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13497 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13498 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13499 - lr: 0.00159 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13500 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13501 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13502 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13503 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13504 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13505 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13506 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13507 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13508 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13509 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13510 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13511 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13512 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13513 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13514 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13515 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13516 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13517 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13518 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13519 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13520 - lr: 0.00158 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13521 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13522 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13523 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13524 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13525 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13526 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13527 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13528 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13529 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13530 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13531 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13532 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13533 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13534 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13535 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13536 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13537 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13538 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13539 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13540 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13541 - lr: 0.00157 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13542 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13543 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13544 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13545 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13546 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13547 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13548 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13549 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13550 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13551 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13552 - lr: 0.00156 - Train loss: 0.00776 - Test loss: 0.03996\n",
      "Epoch 13553 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13554 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13555 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13556 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13557 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13558 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13559 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13560 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13561 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13562 - lr: 0.00156 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13563 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13564 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13565 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13566 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13567 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13568 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13569 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13570 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13571 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13572 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13573 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13574 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13575 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13576 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13577 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13578 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13579 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13580 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13581 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13582 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13583 - lr: 0.00155 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13584 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13585 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13586 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13587 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13588 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13589 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13590 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13591 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13592 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13593 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13594 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13595 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13596 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13597 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13598 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13599 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13600 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13601 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13602 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13603 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13604 - lr: 0.00154 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13605 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13606 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13607 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13608 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13609 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13610 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13611 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13612 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13613 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13614 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13615 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13616 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13617 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13618 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13619 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13620 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13621 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13622 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13623 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13624 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13625 - lr: 0.00153 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13626 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13627 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13628 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13629 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13630 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13631 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13632 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13633 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13634 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13635 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13636 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13637 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13638 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13639 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13640 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13641 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13642 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13643 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13644 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13645 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13646 - lr: 0.00152 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13647 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13648 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13649 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13650 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13651 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13652 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13653 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13654 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13655 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13656 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13657 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13658 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13659 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13660 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13661 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13662 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13663 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13664 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13665 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13666 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13667 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13668 - lr: 0.00151 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13669 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13670 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13671 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13672 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13673 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13674 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13675 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13676 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13677 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13678 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13679 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13680 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13681 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13682 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13683 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13684 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13685 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13686 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13687 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13688 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13689 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13690 - lr: 0.00150 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13691 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13692 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13693 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13694 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13695 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13696 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13697 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13698 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13699 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13700 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13701 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13702 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13703 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13704 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13705 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13706 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13707 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13708 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13709 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13710 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13711 - lr: 0.00149 - Train loss: 0.00775 - Test loss: 0.03996\n",
      "Epoch 13712 - lr: 0.00149 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13713 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13714 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13715 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13716 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13717 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13718 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13719 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13720 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13721 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13722 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13723 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13724 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13725 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13726 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13727 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13728 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13729 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13730 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13731 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13732 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13733 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13734 - lr: 0.00148 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13735 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13736 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13737 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13738 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13739 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13740 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13741 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13742 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13743 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13744 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13745 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13746 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13747 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13748 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13749 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13750 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13751 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13752 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13753 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13754 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13755 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13756 - lr: 0.00147 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13757 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13758 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13759 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13760 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13761 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13762 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13763 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13764 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13765 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13766 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13767 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13768 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13769 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13770 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13771 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13772 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13773 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13774 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13775 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13776 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13777 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13778 - lr: 0.00146 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13779 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13780 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13781 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13782 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13783 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13784 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13785 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13786 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13787 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13788 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13789 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13790 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13791 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13792 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13793 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13794 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13795 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13796 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13797 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13798 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13799 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13800 - lr: 0.00145 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13801 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13802 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13803 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13804 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13805 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13806 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13807 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13808 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13809 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13810 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13811 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13812 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13813 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13814 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13815 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13816 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13817 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13818 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13819 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13820 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13821 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13822 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13823 - lr: 0.00144 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13824 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13825 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13826 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13827 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13828 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13829 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13830 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13831 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13832 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13833 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13834 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13835 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13836 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13837 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13838 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13839 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13840 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13841 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13842 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13843 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13844 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13845 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13846 - lr: 0.00143 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13847 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13848 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13849 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13850 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13851 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13852 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13853 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13854 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13855 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13856 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13857 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13858 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13859 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13860 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13861 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13862 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13863 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13864 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13865 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13866 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13867 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13868 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13869 - lr: 0.00142 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13870 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13871 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13872 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13873 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13874 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13875 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13876 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13877 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13878 - lr: 0.00141 - Train loss: 0.00774 - Test loss: 0.03996\n",
      "Epoch 13879 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13880 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13881 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13882 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13883 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13884 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13885 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13886 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13887 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13888 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13889 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13890 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13891 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13892 - lr: 0.00141 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13893 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13894 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13895 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13896 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13897 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13898 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13899 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13900 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13901 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13902 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13903 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13904 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13905 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13906 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13907 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13908 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13909 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13910 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13911 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13912 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13913 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13914 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13915 - lr: 0.00140 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13916 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13917 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13918 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13919 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13920 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13921 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13922 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13923 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13924 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13925 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13926 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13927 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13928 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13929 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13930 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13931 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13932 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13933 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13934 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13935 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13936 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13937 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13938 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13939 - lr: 0.00139 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13940 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13941 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13942 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13943 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13944 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13945 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13946 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13947 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13948 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13949 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13950 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13951 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13952 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13953 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13954 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13955 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13956 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13957 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13958 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13959 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13960 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13961 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13962 - lr: 0.00138 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13963 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13964 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13965 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13966 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13967 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13968 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13969 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13970 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13971 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13972 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13973 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13974 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13975 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13976 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13977 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13978 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13979 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13980 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13981 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13982 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13983 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13984 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13985 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13986 - lr: 0.00137 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13987 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13988 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13989 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13990 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13991 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13992 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13993 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13994 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13995 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13996 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13997 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13998 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 13999 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14000 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14001 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14002 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14003 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14004 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14005 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14006 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14007 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14008 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14009 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14010 - lr: 0.00136 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14011 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14012 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14013 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14014 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14015 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14016 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14017 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14018 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14019 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14020 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14021 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14022 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14023 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14024 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14025 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14026 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14027 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14028 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14029 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14030 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14031 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14032 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14033 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14034 - lr: 0.00135 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14035 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14036 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14037 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14038 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14039 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14040 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14041 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14042 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14043 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14044 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14045 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14046 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14047 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14048 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14049 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14050 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14051 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14052 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14053 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14054 - lr: 0.00134 - Train loss: 0.00773 - Test loss: 0.03996\n",
      "Epoch 14055 - lr: 0.00134 - Train loss: 0.00772 - Test loss: 0.03996\n",
      "Epoch 14056 - lr: 0.00134 - Train loss: 0.00772 - Test loss: 0.03996\n",
      "Epoch 14057 - lr: 0.00134 - Train loss: 0.00772 - Test loss: 0.03996\n",
      "Epoch 14058 - lr: 0.00134 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14059 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14060 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14061 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14062 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14063 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14064 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14065 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14066 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14067 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14068 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14069 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14070 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14071 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14072 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14073 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14074 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14075 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14076 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14077 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14078 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14079 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14080 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14081 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14082 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14083 - lr: 0.00133 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14084 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14085 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14086 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14087 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14088 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14089 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14090 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14091 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14092 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14093 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14094 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14095 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14096 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14097 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14098 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14099 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14100 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14101 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14102 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14103 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14104 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14105 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14106 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14107 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14108 - lr: 0.00132 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14109 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14110 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14111 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14112 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14113 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14114 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14115 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14116 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14117 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14118 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14119 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14120 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14121 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14122 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14123 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14124 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14125 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14126 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14127 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14128 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14129 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14130 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14131 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14132 - lr: 0.00131 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14133 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14134 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14135 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14136 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14137 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14138 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14139 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14140 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14141 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14142 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14143 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14144 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14145 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14146 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14147 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14148 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14149 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14150 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14151 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14152 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14153 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14154 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14155 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14156 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14157 - lr: 0.00130 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14158 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14159 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14160 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14161 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14162 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14163 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14164 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14165 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14166 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14167 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14168 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14169 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14170 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14171 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14172 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14173 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14174 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14175 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14176 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14177 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14178 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14179 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14180 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14181 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14182 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14183 - lr: 0.00129 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14184 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14185 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14186 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14187 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14188 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14189 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14190 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14191 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14192 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14193 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14194 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14195 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14196 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14197 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14198 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14199 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14200 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14201 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14202 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14203 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14204 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14205 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14206 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14207 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14208 - lr: 0.00128 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14209 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14210 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14211 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14212 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14213 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14214 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14215 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14216 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14217 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14218 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14219 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14220 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14221 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14222 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14223 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14224 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14225 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14226 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14227 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14228 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14229 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14230 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14231 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14232 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14233 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14234 - lr: 0.00127 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14235 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14236 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14237 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14238 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14239 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14240 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14241 - lr: 0.00126 - Train loss: 0.00772 - Test loss: 0.03995\n",
      "Epoch 14242 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14243 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14244 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14245 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14246 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14247 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14248 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14249 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14250 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14251 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14252 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14253 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14254 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14255 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14256 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14257 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14258 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14259 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14260 - lr: 0.00126 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14261 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14262 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14263 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14264 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14265 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14266 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14267 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14268 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14269 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14270 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14271 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14272 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14273 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14274 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14275 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14276 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14277 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14278 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14279 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14280 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14281 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14282 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14283 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14284 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14285 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14286 - lr: 0.00125 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14287 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14288 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14289 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14290 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14291 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14292 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14293 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14294 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14295 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14296 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14297 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14298 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14299 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14300 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14301 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14302 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14303 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14304 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14305 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14306 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14307 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14308 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14309 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14310 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14311 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14312 - lr: 0.00124 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14313 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14314 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14315 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14316 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14317 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14318 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14319 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14320 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14321 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14322 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14323 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14324 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14325 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14326 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14327 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14328 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14329 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14330 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14331 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14332 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14333 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14334 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14335 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14336 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14337 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14338 - lr: 0.00123 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14339 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14340 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14341 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14342 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14343 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14344 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14345 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14346 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14347 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14348 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14349 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14350 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14351 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14352 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14353 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14354 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14355 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14356 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14357 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14358 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14359 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14360 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14361 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14362 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14363 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14364 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14365 - lr: 0.00122 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14366 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14367 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14368 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14369 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14370 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14371 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14372 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14373 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14374 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14375 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14376 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14377 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14378 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14379 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14380 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14381 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14382 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14383 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14384 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14385 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14386 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14387 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14388 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14389 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14390 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14391 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14392 - lr: 0.00121 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14393 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14394 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14395 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14396 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14397 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14398 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14399 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14400 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14401 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14402 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14403 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14404 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14405 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14406 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14407 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14408 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14409 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14410 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14411 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14412 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14413 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14414 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14415 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14416 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14417 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14418 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14419 - lr: 0.00120 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14420 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14421 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14422 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14423 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14424 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14425 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14426 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14427 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14428 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14429 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14430 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14431 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14432 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14433 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14434 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14435 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14436 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14437 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14438 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14439 - lr: 0.00119 - Train loss: 0.00771 - Test loss: 0.03995\n",
      "Epoch 14440 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14441 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14442 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14443 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14444 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14445 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14446 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14447 - lr: 0.00119 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14448 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14449 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14450 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14451 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14452 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14453 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14454 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14455 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14456 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14457 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14458 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14459 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14460 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14461 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14462 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14463 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14464 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14465 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14466 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14467 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14468 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14469 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14470 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14471 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14472 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14473 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14474 - lr: 0.00118 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14475 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14476 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14477 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14478 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14479 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14480 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14481 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14482 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14483 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14484 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14485 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14486 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14487 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14488 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14489 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14490 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14491 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14492 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14493 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14494 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14495 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14496 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14497 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14498 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14499 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14500 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14501 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14502 - lr: 0.00117 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14503 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14504 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14505 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14506 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14507 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14508 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14509 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14510 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14511 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14512 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14513 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14514 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14515 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14516 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14517 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14518 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14519 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14520 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14521 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14522 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14523 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14524 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14525 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14526 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14527 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14528 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14529 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14530 - lr: 0.00116 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14531 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14532 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14533 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14534 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14535 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14536 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14537 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14538 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14539 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14540 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14541 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14542 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14543 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14544 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14545 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14546 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14547 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14548 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14549 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14550 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14551 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14552 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14553 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14554 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14555 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14556 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14557 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14558 - lr: 0.00115 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14559 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14560 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14561 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14562 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14563 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14564 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14565 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14566 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14567 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14568 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14569 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14570 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14571 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14572 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14573 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14574 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14575 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14576 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14577 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14578 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14579 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14580 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14581 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14582 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14583 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14584 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14585 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14586 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14587 - lr: 0.00114 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14588 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14589 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14590 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14591 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14592 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14593 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14594 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14595 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14596 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14597 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14598 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14599 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14600 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14601 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14602 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14603 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14604 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14605 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14606 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14607 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14608 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14609 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14610 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14611 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14612 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14613 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14614 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14615 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14616 - lr: 0.00113 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14617 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14618 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14619 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14620 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14621 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14622 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14623 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14624 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14625 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14626 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14627 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14628 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14629 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14630 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14631 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14632 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14633 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14634 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14635 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14636 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14637 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14638 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14639 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14640 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14641 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14642 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14643 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14644 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14645 - lr: 0.00112 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14646 - lr: 0.00111 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14647 - lr: 0.00111 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14648 - lr: 0.00111 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14649 - lr: 0.00111 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14650 - lr: 0.00111 - Train loss: 0.00770 - Test loss: 0.03995\n",
      "Epoch 14651 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14652 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14653 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14654 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14655 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14656 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14657 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14658 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14659 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14660 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14661 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14662 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14663 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14664 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14665 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14666 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14667 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14668 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14669 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14670 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14671 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14672 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14673 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14674 - lr: 0.00111 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14675 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14676 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14677 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14678 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14679 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14680 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14681 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14682 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14683 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14684 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14685 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14686 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14687 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14688 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14689 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14690 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14691 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14692 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14693 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14694 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14695 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14696 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14697 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14698 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14699 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14700 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14701 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14702 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14703 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14704 - lr: 0.00110 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14705 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14706 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14707 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14708 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14709 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14710 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14711 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14712 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14713 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14714 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14715 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14716 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14717 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14718 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14719 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14720 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14721 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14722 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14723 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14724 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14725 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14726 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14727 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14728 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14729 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14730 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14731 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14732 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14733 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14734 - lr: 0.00109 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14735 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14736 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14737 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14738 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14739 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14740 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14741 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14742 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14743 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14744 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14745 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14746 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14747 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14748 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14749 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14750 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14751 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14752 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14753 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14754 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14755 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14756 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14757 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14758 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14759 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14760 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14761 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14762 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14763 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14764 - lr: 0.00108 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14765 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14766 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14767 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14768 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14769 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14770 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14771 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14772 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14773 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14774 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14775 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14776 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14777 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14778 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14779 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14780 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14781 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14782 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14783 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14784 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14785 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14786 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14787 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14788 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14789 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14790 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14791 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14792 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14793 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14794 - lr: 0.00107 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14795 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14796 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14797 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14798 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14799 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14800 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14801 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14802 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14803 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14804 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14805 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14806 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14807 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14808 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14809 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14810 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14811 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14812 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14813 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14814 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14815 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14816 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14817 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14818 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14819 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14820 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14821 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14822 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14823 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14824 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14825 - lr: 0.00106 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14826 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14827 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14828 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14829 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14830 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14831 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14832 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14833 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14834 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14835 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14836 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14837 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14838 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14839 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14840 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14841 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14842 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14843 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14844 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14845 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14846 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14847 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14848 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14849 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14850 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14851 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14852 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14853 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14854 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14855 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14856 - lr: 0.00105 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14857 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14858 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14859 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14860 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14861 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14862 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14863 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14864 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14865 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14866 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14867 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14868 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14869 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14870 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14871 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14872 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14873 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14874 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14875 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14876 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14877 - lr: 0.00104 - Train loss: 0.00769 - Test loss: 0.03995\n",
      "Epoch 14878 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14879 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14880 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14881 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14882 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14883 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14884 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14885 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14886 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14887 - lr: 0.00104 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14888 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14889 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14890 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14891 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14892 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14893 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14894 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14895 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14896 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14897 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14898 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14899 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14900 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14901 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14902 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14903 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14904 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14905 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14906 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14907 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14908 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14909 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14910 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14911 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14912 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14913 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14914 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14915 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14916 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14917 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14918 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14919 - lr: 0.00103 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14920 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14921 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14922 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14923 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14924 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14925 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14926 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14927 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14928 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14929 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14930 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14931 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14932 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14933 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14934 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14935 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14936 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14937 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14938 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14939 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14940 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14941 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14942 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14943 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14944 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14945 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14946 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14947 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14948 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14949 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14950 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14951 - lr: 0.00102 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14952 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14953 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14954 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14955 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14956 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14957 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14958 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14959 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14960 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14961 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14962 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14963 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14964 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14965 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14966 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14967 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14968 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14969 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14970 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14971 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14972 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14973 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14974 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14975 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14976 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14977 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14978 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14979 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14980 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14981 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14982 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14983 - lr: 0.00101 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14984 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14985 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14986 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14987 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14988 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14989 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14990 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14991 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14992 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14993 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14994 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14995 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14996 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14997 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14998 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 14999 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n",
      "Epoch 15000 - lr: 0.00100 - Train loss: 0.00768 - Test loss: 0.03995\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% TRAINING\n",
    "\n",
    "num_epochs = 15000\n",
    "lr = 0.1\n",
    "en_decay = True\n",
    "lr_final = 0.001\n",
    "lr_decay = (lr_final / lr)**(1 / num_epochs)\n",
    "\n",
    "train_loss_log = []\n",
    "test_loss_log = []\n",
    "for num_ep in range(num_epochs):\n",
    "    # Learning rate decay\n",
    "    if en_decay:\n",
    "        lr *= lr_decay\n",
    "    # Train single epoch (sample by sample, no batch for now)\n",
    "    train_loss_vec = [net.update(x, y, lr) for x, y in zip(x_train, y_train)]\n",
    "    avg_train_loss = np.mean(train_loss_vec)\n",
    "    # Test network\n",
    "    y_test_est = np.array([net.forward(x) for x in x_test])\n",
    "    avg_test_loss = np.mean((y_test_est - y_test)**2/2)\n",
    "    # Log\n",
    "    train_loss_log.append(avg_train_loss)\n",
    "    test_loss_log.append(avg_test_loss)\n",
    "    print('Epoch %d - lr: %.5f - Train loss: %.5f - Test loss: %.5f' % (num_ep + 1, lr, avg_train_loss, avg_test_loss))\n",
    "\n",
    "# Plot losses\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(test_loss_log, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot weights after training\n",
    "net.plot_weights()\n",
    "\n",
    "\n",
    "#%% FORWARD PASS (after training)\n",
    "\n",
    "net_output = np.array([net.forward(x) for x in x_highres])\n",
    "\n",
    "### Plot\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,8))\n",
    "x_highres = np.linspace(0,1,1000)\n",
    "plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_highres, net_output, color='g', ls='--', label='Network output (trained weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommaso/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
      "  if sys.path[0] == '':\n",
      "/home/tommaso/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
      "  \n",
      "/home/tommaso/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Analyze actiovations\n",
    "\n",
    "x1 = 0.1\n",
    "y1, z1 = net.forward(x1, additional_out=True)\n",
    "x2 = 0.9\n",
    "y2, z2 = net.forward(x2, additional_out=True)\n",
    "x3 = 2.5\n",
    "y3, z3 = net.forward(x3, additional_out=True)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "axs[0].stem(z1)\n",
    "axs[0].set_title('Last layer activations for input x=%.2f' % x1)\n",
    "axs[1].stem(z2)\n",
    "axs[1].set_title('Last layer activations for input x=%.2f' % x2)\n",
    "axs[2].stem(z3)\n",
    "axs[2].set_title('Last layer activations for input x=%.2f' % x3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
